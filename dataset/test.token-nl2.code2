set index read only ( ro , true ) ;
log . warn ( error getting users for netgroup + netgroup , e ) ;
dfstest util . create file ( fs , file path , 1 l , ( short ) 1 , 1 l ) ; dfstest util . wait replication ( fs , file path , ( short ) 1 ) ;
assert ( m _ is inline = = false ) ;
array attribute value < annotation attribute value < ? > > new authorizations = new array attribute value < annotation attribute value < ? > > ( new java symbol name ( authorizations ) , roo security authorizations to add ) ; annotation authorizations metadata builder . add attribute ( new authorizations ) ;
if ( logger . is debug enabled ( ) ) { logger . debug ( no properties file found for [ + filename + ] - neither plain properties nor xml ) ; }
video object defining metadata . set snippet ( snippet ) ; input stream content media content = new input stream content ( video _ file _ format , upload video . class . get resource as stream ( sample - video . mp4 ) ) ;
if ( time axis is date ) { artificial time start + + ; }
org . junit . assume . assume true ( mode = test execution mode . collection ) ; final execution environment env = execution environment . get execution environment ( ) ;
string [ ] s1 = { a , c , d , e } ; 0 2 3 4 string [ ] s2 = { b , c , e } ; 1 2 4 string [ ] s3 = { a , d , f } ; 0 3 5 string [ ] s4 = { a , b , c } ; string [ ] s5 = { a , b , c , d , e , f } ; indexed < string > i1 = new array indexed < string > ( s1 , string . class ) ; indexed < string > i2 = new array indexed < string > ( s2 , string . class ) ; indexed < string > i3 = new array indexed < string > ( s3 , string . class ) ; indexed < string > i4 = new array indexed < string > ( s4 , string . class ) ; indexed < string > i5 = new array indexed < string > ( s5 , string . class ) ; index merger . dictionary merge iterator iterator = new index merger . dictionary merge iterator ( new indexed [ ] { i1 , i2 , i3 , i4 , i5 } , false ) ;
break ; case remove updates :
int hex strings length = 8 ;
string request session = get ctx dos test?session = true http 1 . 1 \ r \ n host : localhost \ r \ n \ r \ n ; string close request = get ctx dos test?session = true http 1 . 1 \ r \ n host : localhost \ r \ n connection : close \ r \ n \ r \ n ; string response = do requests ( request session + request session , 1 , 0 , 0 , close request ) ; string [ ] sessions = response . split ( \ r \ n \ r \ n ) ;
synchronized ( this ) { policy map . put ( queue , router policy ) ; cached confs . put ( queue , conf ) ; } }
navigable map < long , resource > map available capacity = net available resources . get cumulative ( ) ;
if ( operator stack . empty ( ) ) { throw new illegal argument exception ( mismatched parenthesis ) ; }
try { detector . enable input filter ( filter ) ; if ( data . length > min _ length ) { detector . set text ( data ) ; matches = detector . detect all ( ) ; } } catch ( exception e ) { log . debug ( exception from icu4 j ( ignoring ) : , e ) ; } if ( matches = null ) { for ( charset match match : matches ) { add clue ( match . get name ( ) , detect , match . get confidence ( ) ) ; } }
template . send body ( controlbus : route?route id = foo & action = stop , null ) ; assert equals ( stopped , context . get route status ( foo ) . name ( ) ) ; }
component . create endpoint ( jt400 : user : password @ host qsys . lib library . lib program . xxx ) ;
assert . assert false ( iterator . has next ( ) ) ;
byte [ ] payload = json . to json ( payload obj ) . get bytes ( utf _ 8 ) ; resp . set status ( http _ ok ) ;
if ( _ single value ) { for ( int doc id = 0 ; doc id < _ num docs ; doc id + + ) { int dict id = _ value int buffer . get ( doc id ) ; int offset = posting list current offset int buffer . get ( dict id ) ; posting list value int buffer . put ( offset + + , doc id ) ; posting list current offset int buffer . put ( dict id , offset ) ; } } else { int value offset = 0 ; for ( int doc id = 0 ; doc id < _ num docs ; doc id + + ) { int num values = _ num values int buffer . get ( doc id ) ; for ( int i = 0 ; i < num values ; i + + ) { int dict id = _ value int buffer . get ( value offset + + ) ; int offset = posting list current offset int buffer . get ( dict id ) ; posting list value int buffer . put ( offset + + , doc id ) ; posting list current offset int buffer . put ( dict id , offset ) ; } } release num values buffer mmap utils . unload byte buffer ( _ num values byte buffer ) ; _ num values byte buffer = null ; }
byte [ ] encap buffer = new byte [ encap length ] ;
if ( type = null ) { if ( body = = null & & ( endpoint = null & & endpoint . get configuration ( ) . is allow null body ( ) ) ) { throw new jmsexception ( cannot send message as message body is null , and option allow null body is false . ) ; } log . trace ( using jms message type : { } , type ) ; message answer = create jms message for type ( exchange , body , headers , session , context , type ) ; ensure default delivery mode is used by default answer . set jmsdelivery mode ( message . default _ delivery _ mode ) ; return answer ; }
settings . set frequency ( def dev . get display mode ( ) . get refresh rate ( ) ) ;
map < status type , callback > killed map = new hash map < > ( ) ;
term = new and term ( term , new not term ( new term ) ) ;
slice _ from ( q ) ;
file out file = new file ( target context . get files dir ( ) , file name ) ; return new response ( http _ ok , application octet - stream , out file . get absolute path ( ) ) ; } catch ( ioexception e ) { throw new runtime exception ( e ) ; } }
assert equals ( object , iterables . single or null ( arrays . as list ( object ) ) ) ;
while ( iter . has next ( ) ) { versioned < byte [ ] > curr = iter . next ( ) ; occurred occurred = value . get version ( ) . compare ( curr . get version ( ) ) ; if ( occurred = = occurred . before ) { obsolete = true ; break ; } else if ( occurred = = occurred . after ) { iter . remove ( ) ; } }
stream . write int ( y pels per meter ) ;
report = create registration ( ns , nn , routers [ 0 ] , federation namenode service state . standby ) ;
try { log . info ( dump of metrics as json on abort : + dump region server metrics . dump metrics ( ) ) ; } catch ( malformed object name exception | ioexception e ) { log . warn ( failed dumping metrics , e ) ; }
final meta . signature signature = statement . get signature ( ) ;
assert equals ( { \ value \ : 123 } , mapper . write value as string ( new non zero wrapper ( 123 ) ) ) ;
finish schema ( ) ; clopen ( ) ; text = mgmt . get property key ( text ) ; time = mgmt . get property key ( time ) ; weight = mgmt . get property key ( weight ) ;
dfs cluster . restart name node ( 0 ) ;
m manifest . write ( print ) ;
task service . complete ( tasks . get ( 0 ) . get id ( ) ) ; tasks = query . list ( ) ; assert equals ( 1 , tasks . size ( ) ) ; assert equals ( task b , tasks . get ( 0 ) . get name ( ) ) ;
store . swap files ( dir ) ;
good mc = true ; }
111 | 222 | 333 | 444 | 555 | 666 | 777 | 888 | 999 | 000 | \ n +
app . get context ( ) . get event handler ( ) . handle ( new task attempt event ( task1 attempt1 . get id ( ) , task attempt event type . ta _ done ) ) ; app . wait for state ( task1 attempt1 , task attempt state . succeeded ) ;
long thread safe no finally easy start = system . nano time ( ) ; for ( int i = 0 ; i < iter _ num ; + + i ) { test object . inc counter1 threaded no finally ( ) ; test object . inc counter2 threaded no finally ( ) ; } long thread safe no finally easy finish = system . nano time ( ) ;
location = e . get approval uri ( ) ;
data stream . set source buffers ( srcs , offset , len ) ;
try { perform http call ( domain test support . slave address , 8080 ) ; fail ( webapp still accessible following undeploy ) ; } catch ( ioexception good ) { desired result } }
response = check and put value xml ( table , row _ 1 , column _ 1 , value _ 1 , value _ 3 , other cells ) ;
url = new url ( test jetty helper . get jetty url ( ) , message format . format ( webhdfs v1 tmp tmp - snap - test?user . name = { 0 } & op = { 1 } & { 2 } , user , snap operation , additional params ) ) ; conn = ( http urlconnection ) url . open connection ( ) ; conn . set request method ( http method ) ; conn . connect ( ) ; return conn ;
for ( int i = 0 ; i < i labels . length ; + + i ) i labels [ i ] = encode class name ( i labels [ i ] ) ;
if ( is shutting down ) { throw omg wrapper . no object adaptor ( ) ; } try { object obj = orb . resolve _ initial _ references ( orbconstants . root _ poa _ name ) ; root poa = ( poaimpl ) obj ; } catch ( invalid name inv ) { throw wrapper . cant resolve root poa ( inv ) ; }
fill in deep substitutor ( type , type substitutor , substitution , full substitution ) ;
authentication existing auth = security context holder . get context ( ) . get authentication ( ) ; if ( existing auth = = null | | existing auth . is authenticated ( ) ) { return true ; }
if ( ( other . m _ flags & body . e _ island flag ) = 0 ) { continue ; }
throw new org . apache . axis2 . databinding . adbexception ( public ip cannot be null ) ; } else {
if ( docking color = = null | | docking color instanceof uiresource ) docking color = uimanager . get color ( tool bar . docking background ) ;
assert true ( is ahit ( parser . parse ( mÃ¶ \ \ \ \ * tley ) , mo \ \ atley , a ) ) ;
now playing intent . set component ( new component name ( com . wm . remusic , com . wm . remusic . activity . playing activity ) ) ; now playing intent . add flags ( intent . flag _ activity _ new _ task ) ; pending intent click intent = pending intent . get broadcast ( this , 0 , now playing intent , pending intent . flag _ update _ current ) ; pending intent click = pending intent . get activity ( this , 0 , now playing intent , pending intent . flag _ update _ current ) ; final bitmap bitmap = image utils . get artwork quick ( this , get album id ( ) , 160 , 160 ) ;
return false ; } return ( system . nano time ( ) - last active < time unit . minutes . to nanos ( retention . get retention time in minutes ( ) ) ) ; } catch ( exception e ) {
string value = command line . get option value ( ' v ' ) . trim ( ) ; default mqadmin ext . start ( ) ; default mqadmin ext . create and update kv config ( namespace , key , value ) ; system . out . printf ( create or update kv config to namespace success . % n ) ; } catch ( exception e ) {
for ( sketch code code : get sketch ( ) . get code ( ) ) { add breakpoint comments ( code . get file name ( ) ) ; }
snap to screen ( m current screen - 1 ) ;
if ( defn . get local assignment ( ) = null ) return true ;
back off . reset ( ) ;
jdbc dbclient . cleanup ( ) ;
writer code = new string writer ( ) ; code . write ( template < class _ char t , class _ alloc > \ n ) ; code . write ( rope < _ char t , _ alloc > : : rope ( ) \ n ) ; code . write ( { } \ n ) ; parse ( code . to string ( ) ) ; }
r = ( byte ) ( ( gray * tr ) > > 8 ) ; g = ( byte ) ( ( gray * tg ) > > 8 ) ; b = ( byte ) ( ( gray * tb ) > > 8 ) ; image in . set pixel color ( x , y , r , g , b ) ;
answer = align ( answer ) ;
metaphone3 m3 = new metaphone3 ( ) ;
final ssl context ssl ctx ; if ( ssl ) { ssl ctx = ssl context builder . for client ( ) . trust manager ( insecure trust manager factory . instance ) . build ( ) ; } else { ssl ctx = null ; } event loop group group = new nio event loop group ( ) ;
if ( off < 0 ) { throw new illegal argument exception ( cannot have negative offset : + off ) ;
final module module = unit . get attachment ( attachments . module ) ;
string java _ library _ path = get privileged property ( java . library . path ) ; string tokenizer st = new string tokenizer ( java _ library _ path , file . path separator ) ;
b = get vectorized row batch1 decimal col ( ) ;
if ( m adapter . get count ( ) > 0 ) { m expand activity overflow button . set enabled ( true ) ; } else { m expand activity overflow button . set enabled ( false ) ; }
for ( int k = left ; k < right ; run [ count ] = k ) { if ( a [ k ] < a [ k + 1 ] ) { ascending while ( + + k < = right & & a [ k - 1 ] < = a [ k ] ) ; } else if ( a [ k ] > a [ k + 1 ] ) { descending while ( + + k < = right & & a [ k - 1 ] > = a [ k ] ) ; for ( int lo = run [ count ] - 1 , hi = k ; + + lo < - - hi ; ) { char t = a [ lo ] ; a [ lo ] = a [ hi ] ; a [ hi ] = t ; } } else { equal for ( int m = max _ run _ length ; + + k < = right & & a [ k - 1 ] = = a [ k ] ; ) { if ( - - m = = 0 ) { sort ( a , left , right , true ) ; return ; } } } * * the array is not highly structured , * use quicksort instead of merge sort . * if ( + + count = = max _ run _ count ) { sort ( a , left , right , true ) ; return ; } }
now = time . milliseconds ( ) ;
in . finished = ( flags & spdy connection . flag _ unidirectional ) = 0 ;
assert starts with ( engine . get enabled protocols ( ) , tls ) ; assert starts with ( socket . get enabled protocols ( ) , tls ) ; assert starts with ( server socket . get enabled protocols ( ) , tls ) ;
assert equals ( 0 , native crypto . ssl _ get _ mode ( s ) & ssl _ mode _ handshake _ cutthrough ) ;
end y = raster . get height ( ) - 2 ; }
status . term index status status ; final int max doc = reader . max doc ( ) ; try { if ( info stream = null ) { info stream . print ( test : terms , freq , prox . . . ) ; } final fields fields = reader . get postings reader ( ) . get merge instance ( ) ; final field infos field infos = reader . get field infos ( ) ; status = check fields ( fields , reader . get live docs ( ) , max doc , field infos , true , false , info stream , verbose ) ; } catch ( throwable e ) { if ( fail fast ) { throw ioutils . rethrow always ( e ) ; } msg ( info stream , error : + e ) ; status = new status . term index status ( ) ; status . error = e ; if ( info stream = null ) { e . print stack trace ( info stream ) ; } }
branch component dto = branch component dto opt . or ( ( ) - > insert into projects table ( db session , branch uuid ) ) ; }
set next of end ( ret , node ( re . op _ end , 0 ) ) ;
m popup item type = new list popup window ( get context ( ) ) ; m popup item type . set anchor view ( m bottom sheet dialog . find view by id ( r . id . select _ item _ type ) ) ; m popup item type . set modal ( true ) ; m popup item type . set input method mode ( list popup window . input _ method _ not _ needed ) ; m popup item type . set animation style ( android . r . style . animation _ dialog ) ; m popup item type . set adapter ( m adapter item type ) ; m popup item type . set vertical offset ( - 100 ) ;
context . stop route ( route ) ;
j panel2 . set layout ( new grid bag layout ( ) ) ; final jpanel j panel3 = new jpanel ( ) ;
escape end = escape start + 6 ;
assert response ( new request ( http : localhost : 8080 nodes v2 state availablefornewallocations host6 . yahoo . com , new byte [ 0 ] , request . method . put ) , { \ message \ : \ moved host6 . yahoo . com to ready \ } ) ;
capture < async method callback < integer > > callback capture1 = new capture < async method callback < integer > > ( ) ;
list < synchronization > copy = new array list < synchronization > ( synchronizations ) ;
char ncb [ ] = new char [ read ahead limit ] ; system . arraycopy ( cb , marked char , ncb , 0 , delta ) ; cb = ncb ; marked char = 0 ; dst = delta ; } next char = n chars = delta ; } }
bus handler . push expected events ( next event . phase , next event . phase , next event . null _ invoice , next event . null _ invoice , next event . invoice , next event . payment , next event . invoice _ payment ) ; clock . add days ( 28 ) ; 26 5 assert listener status ( ) ; bus handler . push expected events ( next event . phase , next event . null _ invoice , next event . invoice , next event . payment , next event . invoice _ payment ) ;
put data ( family , row , col1 , 7 ) ;
return chunk ;
bd = new big decimal ( 123456789 . 125 , mc ) ; assert equals ( incorrect value , 123456789 . 12 , bd . to string ( ) ) ; bd = new big decimal ( - 123456789 . 125 , mc ) ;
if ( state = phone . state . idle ) { r . notify registrant ( new async result ( null , null , null ) ) ; } }
assert that ( measure1 . equals ( measure1 ) ) . is true ( ) ; assert that ( measure1 . hash code ( ) ) . is equal to ( measure2 . hash code ( ) ) ;
current segment index = segment index = = page header . page segment count ? c . index _ unset : segment index ;
int old x = get viewport offset x ( ) + get child offset ( i ) ; int new x = get viewport offset x ( ) + get child offset ( i + shift delta ) ;
assert that ( are similar ( { \ foo \ : \ 2010 - 05 - 18 t15 : 50 : 45 + 0100 \ } , { \ foo \ : \ 2010 - 05 - 18 t18 : 50 : 45 + 0400 \ } ) ) . is true ( ) ;
histogram application histogram = node histogram . get application histogram ( ) ;
csetting key = new csetting ( setting . uid , setting . type , setting . name ) ; synchronized ( m setting cache ) { if ( m setting cache . contains key ( key ) ) { result . value = m setting cache . get ( key ) . get value ( ) ; if ( result . value = = null ) result . value = setting . value ; default value return result ; } }
if ( ( r = this . request status bin sensors ) . should send next request ( timeout msec , curr time ) ) { conn . queue ( this . addr , false , pck generator . request bin sensors status ( ) ) ; r . on request sent ( curr time ) ; }
key value separator position = - 1 ;
data node . log . debug ( renaming + blkfile + to + f ) ;
styled layer descriptor sld = handler . parse ( file , null , null , null ) ; point symbolizer p = sld . point symbolizer ( sld . default style ( sld ) ) ;
if ( ( max total rows = = 0 ) & & ( count < target max rows to delete ) ) { volt queue sql ( delete all , expect _ scalar _ match ( count ) ) ; volt execute sql ( true ) ; return count ; }
if ( matcher . find ( ) ) return full servlet path ; string servlet path = matcher . group ( 0 ) ;
node allocation allocation = new node allocation ( application , cluster , requested nodes , highest index , clock ) ;
final result iterator iterator = base api . get result iterator ( ) ;
batched image request request = m in flight requests . get ( cache key ) ;
net . java . sip . communicator . service . protocol . message msg2 = op set basic im . create message ( body ) ; assert false ( message uid , msg . get message uid ( ) . equals ( msg2 . get message uid ( ) ) ) ; }
dynamic size style extractor extractor = new dynamic size style extractor ( ) ;
if ( limit spec . is limited ( ) ) { return null ; } if ( limit spec . get columns ( ) . is empty ( ) ) { descending = false ; } else {
disabled color = get resources ( ) . get color ( r . color . text _ _ primary _ dark _ 16 ) ;
screen bounds . height - s . height < 2 * ( position . y - screen bounds . y ) ) { y = 0 - y offset - pm size . height ; otherwise drop ' up '
store access token ( access token , authentication ) ; }
int p1pk = i ;
cancel animation ( m strip tabs [ i ] , strip layout tab . property . width ) ; if ( animate & & m animations disabled for testing ) { start animation ( build tab resize animation ( tab , m cached tab width ) , false ) ; } else { m strip tabs [ i ] . set width ( m cached tab width ) ; } }
time . sleep ( 1100 ) ;
assert . assert true ( rs . get ( 2 ) , rs . get ( 2 ) . starts with ( { \ transactionid \ : 21 , \ bucketid \ : 536870912 , \ rowid \ : 0 } \ t0 \ t0 \ t17 \ t ) ) ; assert . assert true ( rs . get ( 2 ) , rs . get ( 2 ) . ends with ( nobuckets delta _ 0000021 _ 0000021 _ 0000 bucket _ 00000 ) ) ; assert . assert true ( rs . get ( 3 ) , rs . get ( 3 ) . starts with ( { \ transactionid \ : 21 , \ bucketid \ : 536870912 , \ rowid \ : 1 } \ t1 \ t1 \ t17 \ t ) ) ; assert . assert true ( rs . get ( 3 ) , rs . get ( 3 ) . ends with ( nobuckets delta _ 0000021 _ 0000021 _ 0000 bucket _ 00000 ) ) ; set < string > expected files = new hash set < > ( ) ;
return latch . get count ( ) = = 0 & & accepts . get ( ) = = 0 ; }
return time ; }
return add column ;
loader . post process before initialization ( gs , geo server ) ;
status = capture response ( bulk by scroll response . class , listener ) . get status ( ) ;
result = services a . execute operation ( read operation ) ;
test subscriber < string > test subscriber = new test subscriber < > ( ) ; observable < string > stream = memory store core . get stream ( ) ; memory store core . put ( 100 , test value 1 ) ; stream . subscribe ( test subscriber ) ; memory store core . put ( 200 , test value 2 ) ; memory store core . put ( 300 , test value 3 ) ; test subscriber . assert not completed ( ) ;
return first entry ( ) ;
target snapshot = result snapshot ; }
out . println ( < a href = \ . . helloworld . html \ > ) ;
slice _ from ( \ u00 f5 ) ; break ; case 3 :
channel . put long ( position marker . get byte offset ( ) - current batch start position . get byte offset ( ) - long . bytes ) ;
assert . assert true ( in order . equals ( 7 8 2 3 6 10 11 9 1 ) ) ; }
transaction manager . reset producer id ( ) ; if ( transaction manager . is transactional ( ) ) {
start soapenvelope ( output , response ) ; }
audio _ nature = new audio node ( asset manager , sound environment ocean waves . ogg , true ) ;
response . set header ( content _ type , multipart binary ) ;
rand val [ 0 ] = 0 ; rand val [ 1 ] = 0 ; rand val [ 2 ] = 0 ; rand val [ 3 ] = 0 ; rand val [ 4 ] = 0 ; assert equals ( false , bf . test ( rand val ) ) ; assert equals ( 7800 , bf . size in bytes ( ) ) ;
val . set values ( 2 , 103 , 502 , 743 ) ;
merged bin count = combine bins ( this . bin count , this . positions , this . bins , h . bin count , h . positions , h . bins , merged positions , merged bins , null ) ; } else {
for ( string correct separator : some correct separators ) { string encoded username password = username password authentication credentials . encode ( correct separator ) ; assert that ( string . format ( failed to encode with ' % s ' , correct separator ) , encoded username password , not null value ( ) ) ; assert that ( string . format ( failed to properly encode with ' % s ' , correct separator ) , encoded username password , is ( b xl vc2 vy b xl qyxnz ) ) ; } }
if ( fields = null ) { builder . field ( common fields . fields . get preferred name ( ) , fields ) ; } if ( missing = null ) { builder . field ( common fields . missing . get preferred name ( ) , missing ) ; } if ( format = null ) { builder . field ( common fields . format . get preferred name ( ) , format ) ; } if ( value type = null ) { builder . field ( common fields . value _ type . get preferred name ( ) , value type . get preferred name ( ) ) ; } do xcontent body ( builder , params ) ; builder . end object ( ) ; return builder ;
selected title gradient color = uimanager . get color ( internal frame . active title gradient ) ;
if ( user . get authenticated session ( ) = = null ) user . set authenticated session ( session management method . create empty web session ( ) ) ; msg . set requesting user ( user ) ; return msg ; }
for ( bean deployment archive impl bda : bean deployment archives ) { if ( new bda . is accessible ( bda ) ) { new bda . add bean deployment archive ( bda ) ; } }
for ( int i = 0 ; i < loop _ indx ; i + + ) { long k = i ; double elemnt _ d = vv . at ( k ) ; element at index k returns a double system . out . println ( element at index + k + as double : + elemnt _ d ) ; if ( double . is na n ( elemnt _ d ) ) { long elemnt _ l = vv . at8 ( k ) ; element at index k returns an ( rounded ) int , throws if a value is missing system . out . println ( element at index + k + as integer : + elemnt _ l ) ; } }
int mtu = platform . get ( ) . get mtu ( socket ) ;
return new transactional entity region access strategy ( this ) ; default : throw new illegal argument exception ( unrecognized access strategy type [ + access type + ] ) ; } }
for ( int i = 0 ; i < 10 ; i + + ) { block id = create block file ( ) ; }
drill down query q = new drill down query ( config ) ; q . add ( a , 1 ) ; q . add ( a , 2 ) ; q . add ( b , 1 ) ; top docs docs = searcher . search ( q , 100 ) ; assert equals ( 5 , docs . total hits ) ; }
if ( binding = = null | | covered group by columns . size ( ) = = group bys . size ( ) ) { break ; } } }
if ( is not empty ( renderer . get custom ffmpeg options ( ) ) ) { custom options . add all ( parse options ( renderer . get custom ffmpeg options ( ) ) ) ; }
final collection < kie package > pkgs = kbuilder . get knowledge packages ( ) ; assert equals ( 2 , pkgs . size ( ) ) ; internal knowledge base kbase = knowledge base factory . new knowledge base ( ) ;
bean i . remove bean ( null ) ; vector beans = new vector ( ) ; beans . add ( bean i ) ; knowledge flow app singleton = knowledge flow app . get singleton ( ) ;
string error code = parse error code ( node ) ; if ( error code = = null | | error code . equals ( s3 location not in service region exception ) ) return null ; s3 location not in service region exception e = ( s3 location not in service region exception ) super . unmarshall ( node ) ; return e ; }
if ( model . freq = = recurrence model . freq _ monthly ) { if ( er . bymonthday count = = 1 ) { if ( model . monthly repeat = = recurrence model . monthly _ by _ nth _ day _ of _ week ) { throw new illegal state exception ( can handle only by monthday or by nth day of week , not both ) ; } model . monthly by month day = er . bymonthday [ 0 ] ; model . monthly repeat = recurrence model . monthly _ by _ date ; } else if ( er . bymonth count > 1 ) { limitation : can handle only one month day throw new illegal state exception ( can handle only one bymonthday ) ; } }
aztec text . attribute predicate uploading predicate = get predicate with class ( tag ) ; return get first element attributes ( content , uploading predicate ) = null ;
http request message2 = new default http request ( http version . http _ 1 _ 1 , http method . put , http : localhost ) ; assert false ( embedder . write inbound ( message2 ) ) ; assert false ( embedder . write inbound ( chunk2 ) ) ;
gl bind buffer ( gles20 . gl _ array _ buffer , m buffer id ) ;
result = add url option ( result , j , true ) ; but keep this until
throw new runtime exception ( channel writer output view : memory segments have been taken from return queue by different actor . ) ; } list . add ( m ) ; }
query . limit ( 4 ) ;
if ( m _ split thread = null ) { thread temp = m _ split thread ; m _ split thread = null ; temp . interrupt ( ) ; temp . stop ( ) ; } }
return session id . hash code ( ) ; } else {
database version database version2 = test database util . create database version ( database version1 ) ; file version file version2 = test database util . create file version ( file2 . jpg , file version1 ) ;
type check ( string . raw`one { 1 } two` ) ;
net . compute gradient and score ( ) ; pair < gradient , double > net grad score = net . gradient and score ( ) ; graph . compute gradient and score ( ) ; pair < gradient , double > graph grad score = graph . gradient and score ( ) ;
assert . assert equals ( test . compare to ( sub ) , 0 ) ; assert . assert equals ( sub . compare to ( test ) , 0 ) ; assert . assert true ( test . equals ( sub ) ) ; assert . assert true ( sub . equals ( test ) ) ;
input stream is = anxadapter test . class . get resource as stream ( v2 trade example - openorders - data . json ) ;
card1 . add partial on click listener ( card . click _ listener _ content _ view , new card . on card click listener ( ) { @ override public void on click ( card card , view view ) { toast . make text ( get activity ( ) , click on content area , toast . length _ long ) . show ( ) ; } } ) ;
assert equals ( delete test failed . unable to restore previous key value . , status . ok , riak client . insert ( bucket , key prefix + integer . to string ( delete key number ) , read value before delete ) ) ; }
map < string , collection state watcher > watchers = new hash map < > ( ) ; list < zk node props > created replicas = new array list < > ( ) ; atomic boolean any one failed = new atomic boolean ( false ) ;
boolean got is same class package = false ; boolean is same class package = false ; if ( current class = = member class ) { always succeeds return true ; }
adapter . set dtdhandler ( null ) ;
clip bounds . set ( clip rect ) ;
if ( m _ module . is loaded ( ) ) { m _ action provider . load module ( m _ module ) ; }
set < string > field names = new linked hash set < > ( ) ;
windowed query = select a + b , mod ( a , b ) , b , rank ( ) over ( partition by b , a order by b desc ) as arank from aaa ; ;
items decay = cache decay . get most used items ( 10 ) ;
matchers . add ( new constraint matcher < t > ( user ) { @ override public task < boolean > matches async ( t object , parse sqlite database db ) { object object value ; try { object value = get value ( object , key ) ; } catch ( parse exception e ) { return task . for error ( e ) ; } return task . for result ( matches equal constraint ( query constraint value , object value ) ) ; } } ) ;
return ( long ) number ;
return method reference bridge info . no bridge ( invoked method ) ; }
workspace info resolved = resolving proxy . resolve ( get catalog ( ) , s . get workspace ( ) ) ; if ( resolved = null ) { resolved = unwrap ( resolved ) ; s . set workspace ( resolved ) ; } else { logger . log ( level . info , failed to resolve workspace for store \ + store . get name ( ) + \ . this means the workspace has not yet been added to the catalog , keep the proxy around ) ; } }
list matcher = list matcher = = null ? ( string matcher ) entry matcher : is negated ( regular expression ) ? ( string matcher ) new and matcher ( entry matcher , list matcher ) : ( string matcher ) new or matcher ( entry matcher , list matcher ) ;
a = new field type ( text field . type _ not _ stored ) ; a . set store term vectors ( true ) ; a . set store term vector positions ( true ) ; b = new field type ( text field . type _ not _ stored ) ; b . set store term vectors ( true ) ; b . set store term vector positions ( true ) ; b . set store term vector payloads ( true ) ; do test mixup ( a , b ) ; }
check action mode ( act , 1 ) ;
signature . update ( tbs certificate local , 0 , tbs certificate local . length ) ; if ( signature . verify ( certificate . get signature value ( ) ) ) { throw new signature exception ( signature was not verified ) ; } }
broadcast manager . register receiver ( receiver , filter ) ;
record factory rf = new fake record factory ( ) ;
string address = cmd . is address command ? cmd . address args [ 0 ] : null ; string instance = cmd . is zero command ? startup option . nodezero . get name ( ) : ( cmd . is one command ? startup option . nodeone . get name ( ) : null ) ; if ( process service name ( service name , false ) ) { return - 1 ; }
class < ? > cl ;
hfile scanner s = ref hsf . create reader ( ) . get scanner ( false , false ) ;
thread factory thread factory2 = builder . build ( ) ;
assert equals ( 1 , lines . stream ( ) . filter ( p - > p . contains ( test2 ) & & p . contains ( test1 ) ) . count ( ) ) ;
scale factor = math . max ( math . min ( scale factor , max _ scale ) , - max _ scale ) ;
watchman path event event = watchman path event . of ( filesystem . get root path ( ) , watchman path event . kind . delete , more paths . relativize ( temp dir . get root ( ) . to real path ( ) , default include file ) ) ; parser . on file system change ( event ) ;
string collplan = new string ( planned statement . core . collector fragment , constants . utf8 encoding ) ;
if ( p quals . is empty ( ) & & reject policy qualifiers & & policies critical ) { throw new cert path validator exception ( critical policy qualifiers present in certificate , null , null , - 1 , pkixreason . invalid _ policy ) ; }
assert translation ( translation , \ ( ljava util map < ljava lang long ; ljava util list < tt ; > ; > ; ) v \ ) ;
x = head ; while ( x = null ) { random list node copy = x . next ; if ( x . random = null ) copy . random = x . random . next ; x = copy . next ; }
try { snapshots blob container . delete blob ( temp blob name ) ; } catch ( ioexception e ) { ex . add suppressed ( e ) ; } throw ex ;
if ( eq values [ 1 ] = = 16 ) { m equalizer helper . get equalizer ( ) . set band level ( one thirty hertz band , ( short ) 0 ) ; } else if ( eq values [ 1 ] < 16 ) { if ( eq values [ 1 ] = = 0 ) { m equalizer helper . get equalizer ( ) . set band level ( one thirty hertz band , ( short ) - 1500 ) ; } else { m equalizer helper . get equalizer ( ) . set band level ( one thirty hertz band , ( short ) ( - ( 16 - eq values [ 1 ] ) * 100 ) ) ; } } else if ( eq values [ 1 ] > 16 ) { m equalizer helper . get equalizer ( ) . set band level ( one thirty hertz band , ( short ) ( ( eq values [ 1 ] - 16 ) * 100 ) ) ; }
progress panel _ . set widget ( list templates _ ) ; template options panel _ . set visible ( true ) ; rmd document template template = list templates _ . get item at idx ( 0 ) . get template ( ) ; if ( template = null ) { template options panel _ . set visible ( template . get create dir ( ) = = true ) ; } }
if ( state = = state _ collapsed | | state = = state _ expanded | | ( m hideable & & state = = state _ hidden ) ) { m state = state ; } return ; }
if ( merged = null ) { return maybe restrict two names ( blind scope , left , left type , left is refineable ? merged . type a : null , right , right type , right is refineable ? merged . type b : null ) ; } return blind scope ;
log . d ( tag , input was disabled , but received action _ up . ) ; m listener . on value selected ( enable _ picker _ index , 1 , false ) ; return true ; } m handler . remove callbacks and messages ( null ) ; m doing touch = false ;
assert pixel ( image , 68 , 72 , new color ( 246 , 246 , 255 ) ) ;
if ( bytes = = null ) { remove avatar ( ) ; return ; }
for ( int i = 0 ; i < 1000 ; i + + ) { volt queue sql ( insert , partition value + i , partition value + i , partition value + i ) ; volt execute sql ( ) ; } return 0 ;
if ( log . is debug enabled ( ) ) { log . debug ( p mem check [ current = { } + asked = { } > allowed = { } ] , this . containers allocation . get physical memory ( ) , ( p mem bytes > > 20 ) , ( get containers monitor ( ) . get pmem allocated for containers ( ) > > 20 ) ) ; } if ( this . containers allocation . get physical memory ( ) + ( int ) ( p mem bytes > > 20 ) > ( int ) ( get containers monitor ( ) . get pmem allocated for containers ( ) > > 20 ) ) { return false ; } if ( log . is debug enabled ( ) ) { log . debug ( before v mem check + [ is enabled = { } , current = { } + asked = { } > allowed = { } ] , get containers monitor ( ) . is vmem check enabled ( ) , this . containers allocation . get virtual memory ( ) , ( v mem bytes > > 20 ) , ( get containers monitor ( ) . get vmem allocated for containers ( ) > > 20 ) ) ; }
log container provider provider = get launch provider recursive ( v ) ; if ( v = = null | | ( v . get tag ( ) instanceof item info ) | | provider = = null ) { return false ; } item info item info = ( item info ) v . get tag ( ) ; provider . fill in log container data ( v , item info , event . src target [ 0 ] , event . src target [ 1 ] ) ; return true ;
check file ( p , half , conf ) ;
body builder . append formal line ( model . add attribute ( \ concurrency \ , false ) ; ) ;
vector . set max register ( 0 , 9 ) ;
configuration loader test loader = new test loader ( ) ; system . set property ( cassandra . config . loader , test loader . get class ( ) . get name ( ) ) ; config = database descriptor . load config ( ) ; assert equals ( configuration loader test , config . cluster _ name ) ; }
change desired gossip time ( 3000 , a , b ) ; a . connect ( last message dropped test ) ; b . connect ( last message dropped test ) ; util . wait until all channels have same view ( 10000 , 500 , a , b ) ; }
if ( local attach vpn gateway = = null ) { throw new org . apache . axis2 . databinding . adbexception ( property cannot be null ) ; } local attach vpn gateway . serialize ( my _ qname , factory , xml writer ) ; }
insert ( views . analysis delaying in minutes ) ; insert ( views . status ) ; insert ( sonar . issuesdensity . weight ) ;
code change handler handler = new forbidden change ( ) ;
output stream . write ( relay board command ) ; output stream . flush ( ) ; } catch ( ioexception e ) {
m service = null ;
assert equals ( 3 , entry . get objects ( ) . size ( ) ) ;
string idx sql = create index if not exists on users ( birth _ date ) ;
if ( m property = null ) { set values ( property values holder . of float ( m property , values ) ) ; } else { set values ( property values holder . of float ( m property name , values ) ) ; }
long actual = 0 ; for ( long expected : generate four byte shards expected runnables ( ) ) { actual + = expected ; } assert equals ( expected _ four _ byte _ roundtrippable _ count , actual ) ; }
query q = qf . from ( get model factory ( ) . get user impl class ( ) ) . having ( account ids ) . contains all ( 2 , 1 ) . build ( ) ; list < user > list = q . list ( ) ;
version negotiated = null ;
string drl = global java . util . list list ; \ n + rule r1 activation - group \ fatal \ when \ n + s : string ( ) \ n + not integer ( this . to string ( ) = = s ) \ n + then \ n + list . add ( \ r1 \ ) ; \ n + end \ n + rule r2 activation - group \ fatal \ when \ n + s : string ( ) \ n + not long ( this . to string ( ) = = s ) \ n + then \ n + list . add ( \ r2 \ ) ; \ n + end \ n + rule r3 activation - group \ fatal \ when \ n + long ( ) \ n + then \ n + list . add ( \ r3 \ ) ; \ n + end \ n ; kie session ksession = new kie helper ( ) . add content ( drl , resource type . drl ) . build ( ) . new kie session ( ) ;
outer child node . m _ access paths . add ( get relevant naive path ( join outer list , parent node . m _ where outer list ) ) ; assert ( outer child node . m _ access paths . size ( ) > 0 ) ; return ; }
byte [ ] b public key = base64 . decode ( s public key , base64 . no _ wrap ) ;
system . err . println ( could not find lib folder via + jar folder . get absolute path ( ) + , switching to user . dir ) ; processing root = new file ( ) ; resolves to user . dir
long session info id = command ksession . get identifier ( ) ;
this . mvcc . advance to ( max seq id ) ; } finally {
point p = util . get screen size ( service ) ;
spec . get compile options ( ) . set compiler args ( collection utils . to string list ( spec . get compile options ( ) . get compiler args ( ) ) ) ;
state manager . activate queue ( q1 ) ;
add similar variable proposals ( cu , ast root , binding , resolved field , simple name , is write access , proposals ) ; if ( binding = = null ) { add static import favorite proposals ( context , simple name , false , proposals ) ; }
result names [ current + + ] = time _ training ;
user roles . add ( security constraint . role _ all _ authenticated _ users ) ; constraint roles . add ( security constraint . role _ all _ authenticated _ users ) ; application roles . add ( security constraint . role _ all _ authenticated _ users ) ; do role test ( user roles , constraint roles , application roles , true ) ; }
int req pre comp len ;
int image size = ( row width in bytes + ( has dummy ? dummy bytes per row . length : 0 ) ) * height ;
assert invalid message ( cannot drop user type , drop type + type ) ; execute ( drop table % s ) ;
if ( dest file = = null ) { copy ( is , os , max ) ; return ; } try { copy ( is , os , max ) ; is . close ( ) ; os . close ( ) ; copy success , rename file to destination aqutility . time ( rename ) ; temp file . rename to ( dest file ) ; aqutility . time end ( rename , 0 ) ; } catch ( ioexception e ) { aqutility . debug ( copy failed , deleting files ) ; copy is a failure , delete everything temp file . delete ( ) ; dest file . delete ( ) ; aqutility . close ( is ) ; aqutility . close ( os ) ; throw e ; }
exception handler method . invoke and handle ( web request , mav container , exception , handler method ) ;
symbol aliases all source aliases = sources match . get aliases ( ) ;
{ if ( is free standing ) { track bounds . height + = 2 ; } g . set color ( this . track color ) ; g . fill rect ( 0 , 0 , track bounds . width , track bounds . height - 2 ) ;
if ( index of first missing value > - 1 ) { distribute weights for instances with missing values for ( int i = index of first missing value ; i < data . num instances ( ) ; i + + ) { instance inst = data . instance ( i ) ; if ( attribute . is nominal ( ) ) { need to check if attribute value is missing if ( inst . is missing ( att ) ) { for ( int j = 0 ; j < dist . length ; j + + ) { dist [ j ] [ ( int ) inst . class value ( ) ] + = props [ att ] [ j ] * inst . weight ( ) ; } } } else { can be sure that value is missing , so no test required for ( int j = 0 ; j < dist . length ; j + + ) { dist [ j ] [ ( int ) inst . class value ( ) ] + = props [ att ] [ j ] * inst . weight ( ) ; } } } }
int mid = append test util . file _ size 2 ;
byte string byte string = byte string . new output ( ) . to byte string ( ) ;
if ( replacement handle = - 1 ) { objects written . put ( object , replacement handle ) ; } return replacement handle ;
x509 cert impl cert = new x509 cert impl ( info ) ;
if ( m view port handler . is in bounds ( pos [ 0 ] , pos [ 1 ] ) ) continue ;
buffer = chunk supplier . get ( ) ; slice = slices . wrapped buffer ( buffer ) ; stream offset + = buffer position ; buffer position = 0 ; }
head . next = node . next ; node . next = null ; return node ; }
war . add as resource ( non transactional em test case . class . get package ( ) , persistence . xml , meta - inf persistence . xml ) ;
chain = new service login filter chain ( * * ) ; chain . get http methods ( ) . add ( httpmethod . get ) ; chain . get http methods ( ) . add ( httpmethod . post ) ; matcher = proxy . matcher for chain ( chain ) ; assert true ( matcher . matches ( create request ( httpmethod . get , wms ) ) ) ; assert true ( matcher . matches ( create request ( httpmethod . post , wms ) ) ) ; assert true ( matcher . matches ( create request ( httpmethod . put , wms ) ) ) ;
if ( ( retrieve file ( ) = = null ) | | ( out w = = null ) ) { system . out . println ( instance to matlab ( inst ) ) ; } else { out w . println ( instance to matlab ( inst ) ) ; m _ incremental counter + + ;
assert not null ( name node adapter . get file info ( nn1 , test , true ) ) ;
string not found404 = history server test . get from http ( http : localhost : + port + hello ) ;
peer = cur ;
if ( fields = null & & fields . is empty ( ) ) { query params . put ( rest constants . fields _ param , fields . to array ( new path spec [ fields . size ( ) ] ) ) ; } return query params ;
remove task entry ( id ) ;
if ( phi < 2 * database descriptor . get phi convict threshold ( ) ) return ;
if ( reach = = coverage reach . conditional & & parent = null & & parent . is script ( ) ) { return ; }
student date student = new student date ( ) ; student . set age ( ( short ) get random value ( short . class ) ) ; student . set id ( ( date ) get random value ( date . class ) ) ; student . set name ( ( string ) get random value ( string . class ) ) ; em . persist ( student ) ;
pojo bean string bean = new pojo bean ( 11 , 2 ) ; do parse ( ( string value1 > string value2 ) , string bean , 0 , string value1 , true ) ; do parse ( ( string value1 < string value2 ) , string bean , 0 , string value1 , false ) ; }
configuration provider . clear ( ) ;
while ( ( current time < = last used time ) & & ( count - - > 0 ) ) { try { m _ clock . sleep ( 1 ) ; } catch ( interrupted exception e ) { } current time = m _ clock . get ( ) ; }
builder . prefixes . append ( p builder . prefixes ) ; builder . source . append ( p builder . source ) ; builder . suffixes . append ( p builder . suffixes ) ; builder . modifiers = p builder . modifiers ; return builder ; }
toast . make text ( this , got click : + item . to string ( ) , toast . length _ short ) . show ( ) ; return true ;
stage . set x ( location x - wide ) ;
producer producer without batches = pulsar client . create producer ( topic name ) ; producer with batches . send async ( message builder . create ( ) . set key ( key - 1 ) . set content ( msg - 1 . get bytes ( ) ) . build ( ) ) ; producer with batches . send async ( message builder . create ( ) . set key ( key - 2 ) . set content ( msg - 2 . get bytes ( ) ) . build ( ) ) . get ( ) ; producer without batches . send async ( message builder . create ( ) . set key ( key - 3 ) . set content ( msg - 3 . get bytes ( ) ) . build ( ) ) ; for ( int i = 1 ; i < = 3 ; i + + ) { message msg = consumer . receive ( ) ; assert true ( msg . has key ( ) ) ; assert equals ( msg . get key ( ) , key - + i ) ; assert equals ( msg . get data ( ) , ( msg - + i ) . get bytes ( ) ) ; consumer . acknowledge ( msg ) ; }
attributes . put ( attr name , copy bytes from ( val ) ) ; return this ; }
ext . set next incoming frames ( capture ) ;
string json = as json ( slack message ) ;
b . add dependency ( a ) ;
long start = system . nano time ( ) ; long time limit = job manager . ns _ per _ ms * 20000 ; 20 seconds while ( system . nano time ( ) - start < time limit & & multi threaded job counter . get ( ) = 0 ) { thread . sleep ( 1000 ) ; } log . d ( tag , did we reach timeout? + ( system . nano time ( ) - start > = time limit ) ) ; matcher assert . assert that ( jobmanager count should be 0 , job manager . count ( ) , equal to ( 0 ) ) ;
manager context manager = null ;
final enumeration urls = stylesheets . elements ( ) ;
types . remove if ( c - > c . is annotation present ( api . class ) ) ; types . sort ( comparator . comparing ( type - > type . get name ( ) ) ) ; logger . debug ( ( ) - > { string builder builder = new string builder ( listing of all + types . size ( ) + annotated types : ) ; builder . append ( eol ) ; types . for each ( e - > builder . append ( e ) . append ( eol ) ) ; return builder . to string ( ) ; } ) ;
if ( start . is before ( end ) ) { y1 = start . get yyyy ( ) ; y2 = end . get yyyy ( ) ; for ( year = y1 ; year = = y2 ; year + + ) { if ( serial date . is leap year ( year ) ) { feb29 = serial date . create instance ( 29 , month constants . february , year ) ; if ( feb29 . is in range ( start , end , serial date . include _ second ) ) { count + + ; } } } return count ; } else { return count feb29s ( end , start ) ; }
filter post processor fpp = new filter post processor ( asset manager ) ; fpp . add filter ( new tone map filter ( vector3f . unit _ xyz . mult ( 6 . 0f ) ) ) ;
for ( map . entry < string , string > entry : key and values . entry set ( ) ) { string key = entry . get key ( ) ; string excepted value = entry . get value ( ) ; string actual value = get from near cache ( near cache test context1 , key ) ; assert equals ( excepted value , actual value ) ; }
player util . show tips ( r . string . tips _ not _ responding ) ;
run test query find all ( 0 , query list , rule list , objects ) ; assert equals ( o count , query list . size ( ) ) ;
return has ;
if ( data len > ninety _ per _ heap _ size ) { throw new illegal argument exception ( transport content length received [ + new byte size value ( data len ) + ] exceeded [ + new byte size value ( ninety _ per _ heap _ size ) + ] ) ; } if ( buffer . length ( ) < data len + size header length ) { throw new illegal state exception ( buffer must be > = to the message size but wasn ' t ) ; }
try { slurp . get ( ) ; fail ( should have noticed the slurp failure ) ; } catch ( execution exception e ) { assert same ( failure , e . get cause ( ) ) ; } }
ws activate rules . new request ( ) . set method ( post ) . set param ( param _ target _ key , java profile . get kee ( ) ) . set param ( param _ qprofile , java profile . get kee ( ) ) . set param ( activation , false ) . execute ( ) . assert json ( get class ( ) , does _ not _ return _ warnings _ when _ bulk _ activate _ on _ profile _ and _ rules _ exist _ on _ another _ language _ than _ profile . json ) ; db session . clear cache ( ) ;
if ( is inbound ) paired tunnel = mgr . select outbound tunnel ( pool . get settings ( ) . get destination ( ) , far end ) ; else paired tunnel = mgr . select inbound tunnel ( pool . get settings ( ) . get destination ( ) , far end ) ; if ( paired tunnel = = null ) { if ( is inbound ) {
string repl db name = db name + _ dupe ; tuple bootstrap dump = bootstrap load and verify ( db name , repl db name ) ; run ( insert into table + db name + . unptned values ( ' + unptn _ data [ 1 ] + ' ) , driver ) ;
if ( domutil . get local name ( sel child ) . equals ( schema symbols . elt _ annotation ) ) { ic . add annotation ( traverse annotation decl ( sel child , attr values , false , schema doc ) ) ; sel child = domutil . get next sibling element ( sel child ) ; } else { report schema error ( s4s - elt - must - match . 1 , new object [ ] { schema symbols . elt _ selector , ( annotation? ) , domutil . get local name ( sel child ) } , sel child ) ; }
build ui ( ) ; }
mini dfscluster cluster = new mini dfscluster . builder ( conf ) . build ( ) ; cluster . wait active ( ) ; array list < data node > dns = cluster . get data nodes ( ) ; data node dn = dns . get ( 0 ) ;
m post listener = post listener ;
assert xpath evaluates to ( 123 . 0 , ( om : observation ) [ 3 ] om : metadata gsml : cgi _ numeric value gsml : principal value , doc ) ;
collection util . intro sort ( snippets , ( o1 , o2 ) - > double . compare ( o2 . get score ( ) , o1 . get score ( ) ) ) ;
for ( string disabled rule id : disabled rules ) { language tool . disable rule ( disabled rule id ) ; }
if ( p . is expected start array token ( ) ) { return handle non array ( p , ctxt , result ) ; }
return repeat status . finished ;
class path entry class path entry = new class path entry ( file , output ) ;
assert that ( update by query ( ) . source ( test ) . filter ( term query ( foo , a ) ) . refresh ( true ) . set slices ( slices ) . get ( ) , matcher ( ) . updated ( 2 ) . slices ( has size ( expected slices ) ) ) ;
prev producer node . so next ( next node ) ; store store return true ; }
parse entry ( attribute value pair , ) ;
result = new instances ( input format . relation name ( ) , atts , 0 ) ; result . set class index ( classindex ) ; return result ; }
string class path = clean route ( prefixed path ( clazz , clazz . get annotation ( path . class ) . value ( ) ) ) ; method path = class path + method path ; } } else {
assert true ( cn0 _ 1 . equals ( cn0 _ 1 _ dup ) ) ; assert false ( cn1 _ 1dup . equals ( cn0 _ 1 ) ) ; assert false ( cn0 _ 2 . equals ( cn0 _ 1 ) ) ; assert false ( cn0 _ 2 . equals ( cn1 _ 1dup ) ) ;
throw new file not found exception ( load path . to uri ( ) . to string ( ) ) ;
if ( k = = - 1 ) { cnt type = string . value of ( arr , offset + 1 , end - offset - 1 ) ; store type = hfile ; } else { cnt type = string . value of ( arr , offset + 1 , k - offset - 1 ) ; store type = string . value of ( arr , k + 1 , end - k - 1 ) ; }
data segment from descriptor file data segment = object mapper . read value ( descriptor file , data segment . class ) ; assert . assert equals ( segments [ i ] . get size ( ) , pushed segment . get size ( ) ) ; assert . assert equals ( segments [ i ] , pushed segment ) ; assert . assert equals ( immutable map . of ( type , hdfs , path , index uri ) , from descriptor file data segment . get load spec ( ) ) ;
if ( address . scope _ id _ set ) { throw new unknown host exception ( scope id not found for address : + arrays . to string ( addr ) ) ; } return address ;
m _ build cache = block ; m _ build cache start index = m _ first free - offset ; + + m _ first free ; }
attributed string as = new attributed string ( text ) ; as . add attribute ( text attribute . foreground , g . get paint ( ) ) ; as . add attribute ( text attribute . font , g . get font ( ) ) ; attributed character iterator aci = as . get iterator ( ) ; font render context frc = new font render context ( null , true , false ) ; line break measurer lbm = new line break measurer ( aci , frc ) ; while ( lbm . get position ( ) < text . length ( ) ) { text layout tl = lbm . next layout ( wrap width ) ; text y + = tl . get ascent ( ) ; rectangle2 d bb = tl . get bounds ( ) ; double t x = graphic info . get x ( ) ; if ( centered ) { t x + = ( int ) ( graphic info . get width ( ) 2 - bb . get width ( ) 2 ) ; } tl . draw ( g , ( float ) t x , text y ) ; text y + = tl . get descent ( ) + tl . get leading ( ) + ( interline - 1 . 0f ) * tl . get ascent ( ) ; }
historic process instance historic process instance = history service . create historic process instance query ( ) . process instance name like ignore case ( % \ \ % % ) . single result ( ) ;
static buffer end buf = static array buffer . of ( zero extend ( end key ) ) ;
editor editor = base . get active editor ( ) ;
for ( rmcontainer running container : running containers ) { if ( capacity scheduler preemption utils . is container already selected ( running container , selected candidates ) ) { resources . subtract from ( lacking , running container . get allocated resource ( ) ) ; } }
get capabilities . get parameter ( ) . add ( ecore util . copy ( param ) ) ;
f data access . write bit field ( element , f bit field data . get mnemonics ( ) [ val . int value ( ) ] ) ;
smack interoperability layer . add extension provider ( bundle packet extension . element _ name , bundle packet extension . namespace , new default packet extension provider < bundle packet extension > ( bundle packet extension . class ) ) ;
set execution delay ( 0 ) ; set encode delay ( 0 , 0 ) ;
else if ( file uri . get authority ( ) . equals ignore case ( context . get package name ( ) + . agent web file provider ) ) { string path = file uri . get path ( ) ; int index = path . last index of ( ) ; return get agent web file path ( context ) + file . separator + path . substring ( index + 1 , path . length ( ) ) ; } else if ( content . equals ignore case ( file uri . get scheme ( ) ) ) { return the remote address if ( is google photos uri ( file uri ) ) return file uri . get last path segment ( ) ; return get data column ( context , file uri , null , null ) ; } file else if ( file . equals ignore case ( file uri . get scheme ( ) ) ) { return file uri . get path ( ) ; }
int w = 0 ; for ( string line : list ) { log . debug ( line : + brief ( line ) ) ; if ( - . equals ( line ) ) continue ; assert equals ( line length , writes [ w ] , line . length ( ) ) ; assert equals ( line contents , line . char at ( 0 ) , ' 0 ' + ( w % 10 ) ) ; w + + ; if ( w < writes . length & & writes [ w ] < = 0 ) w + + ; } if ( content = null ) assert . assert equals ( content length , content . length , _ read . get ( ) ) ;
this . position = find ; in child = false ; return true ; }
file style file = new file ( style dir , external . sld ) ; files . copy ( this . get class ( ) . get resource as stream ( external . sld ) , style file . to path ( ) ) ; file icon file = new file ( style dir , icon . png ) ;
nd4j . get random ( ) . set seed ( 12345 ) ;
new job entity . set tenant id ( job . get tenant id ( ) ) ; new job entity . set job type ( job . get job type ( ) ) ; return new job entity ; }
assert pixel ( image , 75 , 0 , color . white ) ; image = get as image ( wrap disabled option request , image png ) ;
create scheduling request ( 1 * 1024 , root . parent b . child b1 , user3 ) ;
blender input stream bis = blender context . get input stream ( ) ;
options . set strip reflection ( true ) ;
while ( pruned index = = - 1 ) { for ( int i = 0 ; i < 100 ; i + + ) { log . append ( new raft log entry ( term , value of ( 10 * term ) ) ) ; term + + ; } safe index = log . append index ( ) - 50 ; when pruned index = log . prune ( safe index ) ; }
project workspace . process result build result = workspace . run buck command ( build , com example modern : xcompile ) ; build result . assert failure ( ) ; }
return jsonobject . quote ( number as string ) ; } }
assert false ( wpurl utils . safe to add word press com auth token ( build url ( wpcom address3 ) ) ) ;
assert equals ( 5 , run on instance ( c top , i top , get i _ not reloadable ) . return value ) ; run on instance ( c top , i top , set i _ reloadable top , 4 ) ; assert equals ( 4 , run on instance ( c top , i top , get i _ not reloadable ) . return value ) ;
found = super . get child ( name . replace ( ' ' , ' _ ' ) ) ;
conf . set ( net _ topology _ table _ mapping _ file _ key , some bad value for a file ) ; list < string > result2 = mapping . resolve ( names ) ;
external view external view = new external view ( table _ realtime ) ;
if ( this . mask = obj . mask ) { throw new illegal argument exception ( cannot copy object : masks not equal ) ; } if ( this . missing value = obj . missing value ) { throw new illegal argument exception ( cannot copy object : missing values not equal ) ; }
context . get metrics ( ) . commit physical ( input bundle , metrics container . get cumulative ( ) ) ; transform evaluation state . complete ( this ) ;
x . skip past ( ? > ) ;
am . publish container end event ( client , container status . new instance ( builder utils . new container id ( 1 , 1 , 1 , 1 ) , container state . complete , , 1 ) , domain id , ugi ) ;
list < ? extends text > text components = m text . get components ( ) ;
sys out stream . reset ( ) ; result = cli . run ( new string [ ] { application , - - app states , running , - kill , application id3 . to string ( ) + + application id1 . to string ( ) } ) ; assert . assert equals ( - 1 , result ) ; assert . assert equals ( create application clihelp message ( ) , sys out stream . to string ( ) ) ; }
p ctx . set disable map join ( disable map join with hint ( get qb ( ) . get parse info ( ) . get hint list ( ) ) ) ;
note . set name ( this is a folder note name ) ; assert equals ( this is a folder , note . get folder id ( ) ) ;
print thread status ( creator ) ;
public void test gets with multi columns and explicit tracker ( ) throws ioexception , interrupted exception { table table = null ;
boolean prev state = admin . set normalizer on ( initial state ) . get ( ) ;
string [ ] dirs = type . split ( ) ; for ( int i = 0 ; i < dirs . length ; i + + ) { if ( dirs . length > 0 ) { if ( i < dirs . length - 1 ) { data dir = get data folder ( dirs [ i ] ) ; if ( data dir = = null ) { return null ; } } type = dirs [ i ] ; } } file type dir = new file ( data dir , type ) ; if ( type dir . exists ( ) ) { if ( data dir . can write ( ) ) { if ( type dir . mkdir ( ) ) { log . e ( tag , could not create data folder named + type ) ; return null ; } } } return type dir ;
log . debug ( channel already closed . ignoring this exception . ) ; return ;
update ( ( byte ) 128 ) ; while ( x buf off = 0 ) { update ( ( byte ) 0 ) ; }
verify same references ( record , record from store ) ;
content stream content = ( content stream ) obj ;
set registration state ( registration state . connection _ failed , registration state change event . reason _ internal _ error , unable to set expires header ) ; throw new operation failed exception ( unable to set expires header , operation failed exception . internal _ error , ex ) ; }
boolean is viable original number = national number rule . matcher ( number ) . matches ( ) ;
exit search ui ( ) ;
users privileges meta data new meta data = users privileges meta data . maybe copy and replace table idents ( old meta data , source table ident . fqn ( ) , target table ident . fqn ( ) ) ; if ( new meta data = null ) { md builder . put custom ( users privileges meta data . type , new meta data ) ; return true ; }
purge calls ( wrapper . connection abort ( ex ) , true , false ) ;
util . get configuration ( ) . set long ( hconstants . hregion _ max _ filesize , 64 l * 1024 l * 1024 l ) ;
set relationship type ( namespaces . presentationml _ main ) ;
list < camera > cameras = new array list < camera > ( ) ;
( ( value pairs ) value ) . add ( ( string ) key . get value ( ) , ( string ) result . get value ( ) ) ;
analysis result analysis result = analyze expression ( context , base , expression ) ;
options = new embedded servlet options ( config , context ) ;
appender = null ; in error = false ; string class name = attributes . get value ( class _ attribute ) ;
config helper . add pipeline with group ( default group , user2 _ pipeline , user2 _ stage , user2 _ job ) ;
log utils . log raid reconstruction metrics ( logresults . failure , 0 , codec , p , lost block offset , online? logtypes . online _ reconstruction _ get _ stripe : logtypes . offline _ reconstruction _ get _ stripe , fs , caught exception , context ) ;
try { decoder . decode ( get exception byte array ( ) ) ; fail ( should throw runtime exception ) ; } catch ( runtime exception e ) { }
assert true ( low . compare to ( high ) > 0 ) ;
if ( _ any setter = null & & _ any setter . has value deserializer ( ) ) { _ any setter = _ any setter . with value deserializer ( find deserializer ( config , provider , _ any setter . get type ( ) , _ any setter . get property ( ) ) ) ; }
bitmap = bitmap . create bitmap ( get width ( ) , get height ( ) , bitmap . config . argb _ 8888 ) ;
if ( log . is debug enabled ( ) ) { log . debug ( failed to create raw erasure encoder + raw coder name + , fallback to next codec if possible , e ) ; }
if ( group count = = 0 ) { no tgs found log . info ( no enabled thread groups found ) ; } else { if ( running ) { log . info ( all thread groups have been started ) ; } else { log . info ( test stopped - no more thread groups will be started ) ; } }
element content = domutil . get first child element ( elm node ) ; xsannotation impl annotation = null ; if ( content = null & & domutil . get local name ( content ) . equals ( schema symbols . elt _ annotation ) ) { annotation = traverse annotation decl ( content , attr values , false , schema doc ) ; content = domutil . get next sibling element ( content ) ; } else { string text = domutil . get synthetic annotation ( elm node ) ; if ( text = null ) { annotation = traverse synthetic annotation ( elm node , text , attr values , false , schema doc ) ; } } xsobject list annotations ;
del quota ( zk , path , true , true ) ;
description = 02 : 59 : 59 . 999 - march 9th 2014 ;
join list . add all ( expression util . uncombine any ( get join expression ( ) ) ) ;
int group = serial message . get message payload byte ( offset + 1 ) ;
result = ( t [ ] ) array . new instance ( i class , get size ( i value ) ) ;
if ( this . set suc = = true & & this . suc id = = 0 ) {
term suffixes . write bytes ( payload bytes . get bytes ( ) , payload bytes . get position ( ) ) ;
synchronized ( results ) { for ( int i = 0 ; i < params . length ; i + + ) { parallel call call = new parallel call ( params [ i ] , results , i ) ; try { connection connection = get connection ( addresses [ i ] , protocol , ticket , 0 , call ) ; connection . send param ( call ) ; send each parameter } catch ( ioexception e ) {
if ( offset commit mode = = offset commit mode . on _ checkpoints | | offset commit mode = = offset commit mode . disabled ) { properties . set property ( consumer config . enable _ auto _ commit _ config , false ) ; } return new kafka010 fetcher < > ( source context , assigned partitions with initial offsets , watermarks periodic , watermarks punctuated , runtime context . get processing time service ( ) , runtime context . get execution config ( ) . get auto watermark interval ( ) , runtime context . get user code class loader ( ) , runtime context . get task name with subtasks ( ) , runtime context . get metric group ( ) , deserializer , properties , poll timeout , use metrics ) ;
m first frame helper = new first frame animator helper ( m view property animator , m target ) ; if ( m properties to set . contains ( properties . translation _ x ) ) { m view property animator . translation x ( m translation x ) ; } if ( m properties to set . contains ( properties . translation _ y ) ) { m view property animator . translation y ( m translation y ) ; } if ( m properties to set . contains ( properties . scale _ x ) ) { m view property animator . scale x ( m scale x ) ; }
rv . append ( ' \ n ' ) ;
( ( sign in button ) find view by id ( r . id . sign _ in _ button ) ) . set size ( sign in button . size _ wide ) ;
if ( active nodes . contains ( pw . get id ( ) ) ) { pw . get model ( ) . set in action list ( false ) ; continue ; } pw . get model ( ) . set cur ooz job id ( program widget . model . latest _ oozie _ jobid ) ;
try { service reference < ? > [ ] references = context . get all service references ( menu operations . class . get name ( ) , null ) ; for ( service reference < ? > ref : references ) { menu operations = ( menu operations ) context . get service ( ref ) ; return menu operations ; } return null ; } catch ( invalid syntax exception e ) { logger . warning ( cannot load menu operations on jsp operations impl . ) ; return null ; }
if ( job id = null ) { completable future < cancellation success > cancellation = future utils . to java ( cluster . get leader gateway ( deadline . time left ( ) ) . ask ( new job manager messages . cancel job ( job id ) , deadline . time left ( ) ) . map to ( class tag . module . < cancellation success > apply ( cancellation success . class ) ) ) ; cancellation . get ( deadline . time left ( ) . to millis ( ) , time unit . milliseconds ) ; } }
itype hierarchy hierarchy = f target method . get declaring type ( ) . new type hierarchy ( null ) ;
nn0 = cluster . get name node ( 0 ) ;
target [ offset ] = 0x0 ; if ( msg . get to tunnel ( ) = null ) target [ offset ] | = mask _ tunnel ; else if ( msg . get to router ( ) = null ) target [ offset ] | = mask _ router ; if ( fragmented ) target [ offset ] | = mask _ fragmented ; if ( _ log . should log ( log . debug ) ) _ log . debug ( control : + integer . to hex string ( target [ offset ] ) ) ;
if ( m menu items = = null ) { m menu items = new hash map < menu item , support menu item > ( ) ; } support menu item compat item = m menu items . get ( framework item ) ;
final map < topic partition , long > offset response = new hash map < > ( ) ; offset response . put ( tp0 , 3 l ) ; offset response . put ( tp1 , 3 l ) ; client . prepare response ( list offsets response ( offset response , errors . none ) ) ; assert equals ( 3 l , consumer . position ( tp0 ) ) ; assert equals ( 3 l , consumer . position ( tp1 ) ) ; client . requests ( ) . clear ( ) ;
my tristate check box . set enabled ( active ) ;
this . conf = null ;
if ( m card . is checkable ( ) ) { m card view . set long clickable ( true ) ; m card view . set on click listener ( advance click listener ) ; } else { if ( m card . get on click listener ( ) = null ) { m card view . set on click listener ( advance click listener ) ; } } }
return math . max ( over loaded bytes , under loaded bytes ) ;
assert equals ( time . get time ( ) , transformed . value ( ) ) ;
get context ( ) . get shutdown strategy ( ) . set timeout ( 10 ) ; from ( file : target upload?move failed = . . error ) . log ( uploading file { file : name } ) . to ( { { ftp . client } } ) . log ( uploaded file { file : name } complete . ) ;
return choose local rack ( local machine , excluded nodes , blocksize , max nodes per rack , results ) ; }
string view name = get views folder ( module name ) . concat ( ctx . get controller path ( ) ) . concat ( ) . concat ( list delete modal ) . concat ( get views extension ( ) ) ; entity item entity item = create entity item ( entity metadata , ctx , table _ suffix ) ;
rest client . delete entity ( session token , updated professional , pk1 , professional _ class _ name ) ;
feature type info info = get catalog ( ) . get feature type by name ( layer ) ; info . set cql filter ( name like ' red % ' ) ; get catalog ( ) . save ( info ) ; doc = get as dom ( request ) ; feature members = doc . get elements by tag name ( gml : feature member ) ; assert true ( feature members . get length ( ) = = 0 ) ; }
pms conf . set renderer default ( play station 3 ) ; pms conf . set renderer force default ( true ) ;
application id app id = application id . new instance ( 0 , 1 ) ; application attempt id attempt id = application attempt id . new instance ( app id , 1 ) ; container id cid = container id . new container id ( attempt id , 1 ) ; common launch container ( app id , cid , cm ) ;
throw new illegal argument exception ( e . get message ( ) ) ; }
ctx . execute ( clazz ) ;
coverage = setup coverage ( helper , gcr , request , reader , hints , extensions , null , null , null , coverage factory ) ;
src stats = get file status ( fs , test root , src files ) ; create dest dir ( fs , test dst , src stats , src files ) ;
execute ( create table t1 ( id integer ) with ( number _ of _ replicas = 0 , \ blocks . metadata \ = true ) ) ; ensure yellow ( ) ; execute ( alter table t1 set ( \ blocks . metadata \ = true ) ) ;
phreak join node . update child left tuple ( peer , smem . get staged left tuples ( ) , smem . get staged left tuples ( ) ) ; } else { peer = ( ( left tuple sink ) smem . get root node ( ) ) . create peer ( peer ) ; smem . get staged left tuples ( ) . add insert ( peer ) ; if ( smem . has data driven path memories ( ) ) { for ( path memory data driven pmem : smem . get data driven path memories ( ) ) {
if ( related bitsets . contains key ( type ) ) { return related bitsets . get ( type ) ; } else { throw new runtime exception ( related types should have been computed for + type : + type + but have not been . ) ; } }
if ( allow server window size ) { server window size = integer . parse int ( parameter . get value ( ) ) ; if ( server window size > max _ window _ size | | server window size < min _ window _ size ) { deflate enabled = false ; } } else { deflate enabled = false ; } } else if ( client _ no _ context . equals ignore case ( parameter . get key ( ) ) ) {
d gamma view . assign ( d gamma ) ; d beta view . assign ( d beta ) ; ret gradient . set gradient for ( batch normalization param initializer . gamma , d gamma view ) ;
if ( hbase file system . make dir on file system ( fs , store archive dir ) ) { throw new ioexception ( could not make archive directory ( + store archive dir + ) for store : + bytes . to string ( family ) + , deleting compacted files instead . ) ; }
this . retainer . add ( put ) ;
uncommitted bundle < long > second output = bundle factory . create bundle ( longs ) ; when ( context . create bundle ( longs ) ) . then return ( second output ) ; transform evaluator < unbounded source shard < long , test checkpoint mark > > second evaluator = factory . for application ( source transform , input bundle ) ; windowed value < unbounded source shard < long , test checkpoint mark > > residual = ( windowed value < unbounded source shard < long , test checkpoint mark > > ) iterables . get only element ( result . get unprocessed elements ( ) ) ; second evaluator . process element ( residual ) ; transform result < unbounded source shard < long , test checkpoint mark > > second result = second evaluator . finish bundle ( ) ;
aggregation mapper mapper = new aggregation mapper ( ) ; map driver = map driver . new map driver ( mapper ) ; configuration configuration = map driver . get configuration ( ) ; configuration . set ( io . serializations , org . apache . hadoop . io . serializer . java serialization , + org . apache . hadoop . io . serializer . writable serialization ) ; configuration . set ( aggregation phase constants . agg _ phase _ thirdeye _ config . to string ( ) , object _ mapper . write value as string ( thirdeye config ) ) ; input schema = new schema . parser ( ) . parse ( class loader . get system resource as stream ( avro _ schema ) ) ;
integer five hundred = new integer ( 500 ) ;
pce . print stack trace ( ) ;
new object [ ] { wildcard . any _ one , wildcard . any _ one , wildcard . any _ zero _ or _ one , wildcard . any _ zero _ or _ one } , arrays . as list ( r i , r r , r r i , r r r , r r r i , r r r r ) } , {
emitter table . get selection model ( ) . set selection interval ( new index , new index ) ;
ctx . reset ( ) ; }
string main file name = null ;
m adapter . notify data set changed ( ) ;
int i = 0 ; for ( int j = 0 ; j < 4 ; j + + ) { j only increased if a valid char was found . int c = ia [ s arr [ s + + ] ] ; if ( c > = 0 ) i | = c < < ( 18 - j * 6 ) ; else j - - ; }
callback captor < void > captor = new callback captor < > ( ) ; storage ( ) . async span consumer ( ) . accept ( spans , captor ) ; captor . get ( ) ; block on result }
assert that ( create map ( hash map . class , 0 ) , is ( instance of ( hash map . class ) ) ) ; assert that ( create map ( hash map . class , string . class , 0 ) , is ( instance of ( hash map . class ) ) ) ;
textarea . select ( start offset , stop offset ) ;
student short primitive student max = new student short primitive ( ) ; student max . set age ( ( short ) get max value ( short . class ) ) ; student max . set id ( ( short ) get max value ( short . class ) ) ; student max . set name ( ( string ) get max value ( string . class ) ) ; em . persist ( student max ) ;
program = new shader program ( vertex , fragment , sprite batch . attributes ) ;
transfer output properties ( _ translet ) ;
if ( f update references ) { for ( iterator < search match > iter = references in this cu . iterator ( ) ; iter . has next ( ) ; ) { search match element = iter . next ( ) ; simple update ( element , cu , manager . get ( cu ) ) ; } } pm . worked ( 1 ) ;
if ( this . get return open type ( ) . equals ( other . get return open type ( ) ) ) { return false ; }
command executor = shell command executor factory . create executor ( executor context ) ;
localized resource localized resource = localrsrc . get ( lr ) ; tracker . handle ( resource failed event ) ; dispatcher . await ( ) ;
account model dao retrieved account = account dao . get account by key ( account . get external key ( ) , internal call context ) ;
values . add ( event serializer . is immutable type ( ) ? event : event serializer . copy ( event ) ) ; } }
if ( val . length ( ) = = 0 ) { return res ; }
int cur server id = test _ util . get hbase cluster ( ) . get server with ( region name ) ; int dest server id = ( cur server id = = 0 ? 1 : 0 ) ; hregion server cur server = test _ util . get hbase cluster ( ) . get region server ( cur server id ) ;
final uri uri = incompatible ? request . get request uri ( ) : request . get base uri ( ) ;
if ( new hosts . contains ( host ) | | new excludes . contains ( host ) ) { hosts to exclude . add ( tmp node ) ; }
join itr = persistent class . get subclass join closure iterator ( ) ; while ( join itr . has next ( ) ) { final join join = ( join ) join itr . next ( ) ; final table join table = join . get table ( ) ; is concretes . add ( persistent class . is class or superclass table ( join table ) ) ; is deferreds . add ( join . is sequential select ( ) ) ; is lazies . add ( join . is lazy ( ) ) ; string join table name = determine table name ( join table , jdbc environment ) ; subclass table names . add ( join table name ) ; string [ ] key = new string [ id column span ] ; iterator citer = join table . get primary key ( ) . get column iterator ( ) ; for ( int k = 0 ; k < id column span ; k + + ) { key [ k ] = ( ( column ) citer . next ( ) ) . get quoted name ( factory . get dialect ( ) ) ; } key columns . add ( key ) ; } string [ ] natural order subclass table name closure = array helper . to string array ( subclass table names ) ;
domastnode leaf [ ] children to search = children ;
sreq . params . set ( shard params . shards _ qt , get ) ; sreq . params . set ( distrib , false ) ; sreq . params . remove ( shard params . shards ) ; sreq . params . remove ( id ) ; sreq . params . remove ( ids ) ;
string lhs = test string ; string rhs = test string . to upper case ( ) ; boolean dummy = false ; for ( int i = 0 ; i < reps ; i + + ) { dummy ^ = lhs . equals ignore case ( rhs ) ; } return dummy ; }
else if ( c = = ' \ n ' ) { buf . delete char at ( i ) ; buf . insert ( i , \ \ n ) ; } else if ( c = = ' \ t ' ) { buf . delete char at ( i ) ; buf . insert ( i , \ \ t ) ; } else if ( c = = ' \ r ' ) { buf . delete char at ( i ) ; buf . insert ( i , \ \ r ) ; } else if ( c = = ' \ b ' ) { buf . delete char at ( i ) ; buf . insert ( i , \ \ b ) ; } else if ( c = = ' \ f ' ) { buf . delete char at ( i ) ; buf . insert ( i , \ \ f ) ; }
if ( plot . get auto populate section paint ( ) ) { plot . clear section paints ( false ) ; }
hoarder = em . merge ( hoarder ) ; assert equals ( 1 , hoarder . get items ( ) . size ( ) ) ; assert same ( hoarder . get favorite item ( ) , hoarder . get items ( ) . iterator ( ) . next ( ) ) ; assert equals ( item1 . get id ( ) , hoarder . get favorite item ( ) . get id ( ) ) ; assert equals ( item1 . get category ( ) , hoarder . get favorite item ( ) . get category ( ) ) ; em . get transaction ( ) . commit ( ) ; em . close ( ) ; em = get or create entity manager ( ) ;
if ( o best operation . m _ f delta score = = - 1 e100 ) { return null ; } return o best operation ;
low level . end transaction ( ) ;
system . exit ( 100 ) ;
final string agent error = string utils . is blank ( req . get agent error ( ) ) ? swivel _ err _ code _ authn _ fail : req . get agent error ( ) ;
block = 0 ; encrypt cipher . init ( cipher . encrypt _ mode , auth key ) ; temp auth = encrypt cipher . do final ( temp auth ) ; trace hex ( bad tmp2 , temp auth ) ; } }
details place holder view = create details place holder view ( ) ;
fold ( if ( foo ( ) & & false ) z ( ) , ( foo ( ) , 0 ) & & z ( ) ) ;
byte buffer chunk2 = byte buffer . allocate ( 512 ) ; content . offer ( chunk2 ) ; assert . assert false ( proxy request latch . await ( 1 , time unit . seconds ) ) ;
folder = new file ( sketch . get folder ( ) , application . + platform name + 32 ) ;
path . line to ( r . left + r . width ( ) - radius , r . top + r . height ( ) - arrow height ) ;
for ( int i = 0 ; i < int _ count ; + + i ) { int val = 0 ; if ( no restore & & i + word shifts + 1 < z . length ) { val = z [ i + word shifts + 1 ] < < shift restore ; } if ( i + word shifts < z . length ) { val | = ( z [ i + word shifts ] > > > bit shifts in word ) ; } result [ i ] = val ; } if ( round carry ) { increment array ( result ) ; }
builder . global table ( any topic ) ; props . put ( streams config . num _ stream _ threads _ config , num threads ) ; final kafka streams streams = new kafka streams ( builder . build ( ) , props ) ; final java . lang . reflect . field threads field = streams . get class ( ) . get declared field ( threads ) ;
text view tv _ tab _ title = ( text view ) v . find view by id ( r . id . tv _ tab _ title ) ;
distributed task builder < boolean > task builder = des . create distributed task builder ( new fail only once distributed callable ( ) ) ; task builder . failover policy ( new distributed task failover policy ( ) { @ override public address failover ( failover context context ) { list < address > candidates = context . execution candidates ( ) ; address return address = null ; for ( address candidate : candidates ) { if ( candidate . equals ( context . execution failure location ( ) ) ) { return address = candidate ; break ; } } return return address ; } @ override public int max failover attempts ( ) { return 1 ; } } ) ; distributed task < boolean > task = task builder . build ( ) ;
remote endpoint remote = session . get remote ( ) ;
instanceofed super classes ( program class ) . equals ( instanceofed super classes ( target class ) ) & & ( details | | print ( program class , same caught superclasses? ) ) & &
adapter . get items ( ) . remove ( result . get index ( ) ) ;
record = proxy . read from server ( ) ; assert . assert equals ( tlsrecord . type . handshake , record . get type ( ) ) ; proxy . flush to client ( record ) ; assert . assert null ( handshake . get ( 5 , time unit . seconds ) ) ; simple proxy . automatic flow automatic proxy flow = proxy . start automatic flow ( ) ;
int started = time . current time ( ) ; user session model [ ] orig sessions = create sessions ( ) ; reset session ( ) ;
try { pyspark interpreter . get livy version ( ) ; for livy version > = 0 . 3 , input some erroneous spark code , check the shown result is more than one line interpreter result result = pyspark interpreter . interpret ( sc . parallelize ( wrong syntax ( 1 , 2 ) ) . count ( ) , context ) ; assert equals ( interpreter result . code . error , result . code ( ) ) ; assert true ( result . message ( ) . get ( 0 ) . get data ( ) . split ( \ n ) . length > 1 ) ; assert true ( result . message ( ) . get ( 0 ) . get data ( ) . contains ( traceback ) ) ; } catch ( apinot found exception e ) { only livy 0 . 2 can throw this exception since it doesn ' t have version endpoint in livy 0 . 2 , most error msg is encapsulated in evalue field , only print ( a ) in pyspark would return none - empty traceback interpreter result result = pyspark interpreter . interpret ( print ( a ) , context ) ; assert equals ( interpreter result . code . error , result . code ( ) ) ; assert true ( result . message ( ) . get ( 0 ) . get data ( ) . split ( \ n ) . length > 1 ) ; assert true ( result . message ( ) . get ( 0 ) . get data ( ) . contains ( traceback ) ) ; }
final url url = new url ( ) ; url . set loc ( latkes . get serve path ( ) + tags . html ) ;
string val = ( string ) xp . evaluate ( wadl : application wadl : resources @ base , d , xpath constants . string ) ;
throw new org . apache . axis2 . databinding . adbexception ( owner id cannot be null ) ;
for ( map . entry < data , txn value wrapper > entry : tx map . entry set ( ) ) { boolean is removed = type . removed . equals ( entry . get value ( ) . type ) ; boolean is updated = type . updated . equals ( entry . get value ( ) . type ) ; object key object = serialization service . to object ( entry . get key ( ) ) ; if ( is removed ) { key wont be included . add ( key object ) ; } else { if ( is updated ) { key wont be included . add ( key object ) ; } object entry value = entry . get value ( ) . value ; queryable entry query entry = new cached query entry ( ( internal serialization service ) serialization service , entry . get key ( ) , entry value , extractors ) ; if ( predicate . apply ( query entry ) ) { value set . add ( query entry . get value ( ) ) ; } } } remove from result set ( result , value set , key wont be included ) ;
uristatus status = m file system . get status ( file path ) ; assert . assert not equals ( persistence state . persisted , status . get persistence state ( ) ) ; assert . assert true ( status . is completed ( ) ) ; integration test utils . wait for persist ( m local alluxio cluster resource , file path ) ;
names [ j ] = names [ j ] . replace all ( \ \ p { cntrl } | \ \ p { java whitespace } , ) ; if ( 0 = = names [ j ] . length ( ) ) { names [ j ] = null ; }
particle influencer influencer ;
hint color = resources . get color state list ( r . color . g _ default _ edit _ view _ hint ) ;
version base version = null ; map < version , t > matches = new linked hash map < version , t > ( ) ; for ( t candidate : details . get candidates ( ) ) { version version = version parser . instance . transform ( candidate . get version ( ) ) ; if ( base version = = null | | version comparator . compare ( version . get base version ( ) , base version ) > 0 ) { matches . clear ( ) ; base version = version . get base version ( ) ; matches . put ( version , candidate ) ; } else if ( version . get base version ( ) . equals ( base version ) ) { matches . put ( version , candidate ) ; } } if ( matches . size ( ) = = 1 ) { details . select ( matches . values ( ) . iterator ( ) . next ( ) ) ; return ; }
list < v > values = pool . close ( ) ; for ( v value : values ) destroy resource ( entry . get key ( ) , entry . get value ( ) , value ) ; resource pool map . remove ( entry . get key ( ) ) ; } }
boolean included = by name = = boolean . true | | by location = = boolean . true | | ( by name = = null & & _ patterns . has includes ( ) & & by location = = null & & _ locations . has includes ( ) ) ;
string collection name = test solr cloud collection ; string config name = solr cloud collection config ; mini cluster . upload config set ( solr test case j4 . test _ path ( ) . resolve ( collection1 conf ) , config name ) ; collection admin request . create collection ( collection name , config name , 2 , 2 ) . process ( mini cluster . get solr client ( ) ) ; query request req = new query request ( ) ;
bind ( editor provider . class ) . annotated with ( names . named ( default editor ) ) . to ( default text editor provider . class ) ;
if ( prev < value . length ( ) ) { fragments . add ( value . substring ( prev ) ) ; } }
try { arrays . sort ( reversed array , start index + 1 , start index ) ; fail ( illegal argument exception expected ) ; } catch ( illegal argument exception ignore ) { } try { arrays . sort ( reversed array , - 1 , start index ) ; fail ( array index out of bounds exception expected ( 1 ) ) ; } catch ( array index out of bounds exception ignore ) { } try { arrays . sort ( reversed array , start index , reversed array . length + 1 ) ; fail ( array index out of bounds exception expected ( 2 ) ) ; } catch ( array index out of bounds exception ignore ) { }
this . watcher = new basic poller presence watcher ( this . irc , this . connection state , this . operation set , nick watch list , this . server identity ) ;
throw new illegal state exception ( ex ) ; }
service = database . get by id ( service . get id ( ) , service info . class ) ; assert equals ( bar , service . get maintainer ( ) ) ; }
if ( get skinnable ( ) . get tree item ( ) = null & & get skinnable ( ) . get tree item ( ) . is leaf ( ) ) { node arrow = ( ( parent ) get disclosure node ( ) ) . get children unmodifiable ( ) . get ( 0 ) ;
m list hook . add ( new hook ( null , remove updates , , 3 , null , null ) . not aosp ( 19 ) ) ;
if ( done [ h ] = j ) { done [ h ] = j ; nvals + + ; }
tomcat . add servlet ( nonlogin context , tester servlet1 , new tester servlet encode url ( ) ) ; nonlogin context . add servlet mapping decoded ( uri _ protected , tester servlet1 ) ; security collection collection1 = new security collection ( ) ;
params . type class = args . remove config args ( type class ) ;
plan id = plan selector . m _ plan id ;
latch . count down ( ) ; }
test serialized size ( statics . heartbeat st , heart beat state . serializer ) ;
assert that ( device policy manager . get organization name ( test component ) ) . is null ( ) ;
path . arc to ( new rect f ( r . left + r . width ( ) - 2 * radius , r . top , r . right , r . top + 2 * radius ) , 0 , - 90 ) ;
if ( dir in root = = null ) { return null ; } file f = new file ( dest dir , dir in root ) ; if ( f . is directory ( ) ) { throw new property exception ( could not find directory : + f . get path ( ) ) ; } return dir in root ; }
temp ec policy = fs . get erasure coding policy ( ec dir path ) ; assert . assert null ( directory should not have erasure coding policy set , temp ec policy ) ; fs . delete ( ec dir path , true ) ;
final person person = realm . where ( person . class ) . find first ( ) ;
abstract xml application context app = new class path xml application context ( org apache camel routepolicy quartz2 spring quartz clustered app one . xml ) ;
final general envelope approximate requested bbox = crs . transform ( requested bboxin native geographic crs , request crs ) ;
trade service trade service = anx . get trade service ( ) ;
set < string > profiles set = new linked hash set < > ( arrays . as list ( active profiles ) ) ; return string utils . to string array ( profiles set ) ;
find super declarations ( cls . get superclass ( ) , method ) ; for ( class iface : cls . get interfaces ( ) ) find super declarations ( iface , method ) ; }
conf . put ( config . topology _ worker _ childopts , - xx : + heap dump on out of memory error ) ;
for ( int row = 0 ; row < nids . _ len ; row + + ) { if ( weight . atd ( row ) = = 0 ) continue ; if ( is decided row ( ( int ) nids . atd ( row ) ) ) nnids [ row ] = decided _ row ; }
final double [ ] legal learn rate opts = new double [ ] { 0 . 01 , 0 . 1 , 0 . 3 } ; final double [ ] illegal learn rate opts = new double [ ] { - 1 . 0 } ; hash map < string , object [ ] > hyper parms = new hash map < string , object [ ] > ( ) { { put ( _ ntrees , new integer [ ] { 1 , 2 } ) ; put ( _ distribution , new distribution family [ ] { distribution family . multinomial } ) ; put ( _ max _ depth , new integer [ ] { 1 , 2 , 5 } ) ; put ( _ learn _ rate , array utils . join ( legal learn rate opts , illegal learn rate opts ) ) ; } } ;
if ( policy info = null & & policy info . equals ( new policy info ) ) { is dirty = false ; return ; } validate ( new policy info ) ;
return region info builder . first _ meta _ regioninfo ; }
on stop loading ( ) ; data = null ; }
if ( state = state . stopped ) { _ vms . put ( vm name , state ) ; } else { _ vms . remove ( vm name ) ; } } }
mgr = new null rmnode labels manager ( ) ;
wait no events ( instance _ a , 100 ) ; list < info diff > differences = differences ( instance _ c , instance _ a ) ; assert that ( differences . size ( ) , is ( 4 ) ) ;
report = create registration ( ns , nn , routers [ 3 ] , federation namenode service state . active ) ;
order . verify ( spy sub ) . cancel ( any string ( ) , any ( exception . class ) ) ; order . verify ( spy sub ) . cleanup ( any ( exception . class ) ) ; }
if ( portrait ) { draw circle surface ( surface , width 2 , height * 3 4 , width 4 ) ; } else { draw circle surface ( surface , width * 3 4 , height 2 , height 4 ) ; } break ;
expression = parser . parse expression ( [ 2 ] ) ; assert equals ( ( byte ) 4 , expression . get value ( bs ) ) ; assert can compile ( expression ) ; assert equals ( ( byte ) 4 , expression . get value ( bs ) ) ; assert equals ( b , get ast ( ) . get exit descriptor ( ) ) ;
conn mgr . close idle connections ( 30 , time unit . seconds ) ; } } } catch ( interrupted exception ignored ) {
if ( build . version . sdk _ int > = build . version _ codes . lollipop & & source . get parent ( ) . equals ( target . get parent ( ) ) & & file util . is on ext sd card ( source , context ) ) { document file document = get document file ( source , true , context ) ; if ( document . rename to ( target . get name ( ) ) ) { return true ; } }
string new class name = referenced class . get name ( ) ;
if ( input file . is file ( ) ) { files . as byte source ( input file ) . copy to ( zos ) ; } }
mapper . put ( iterator . next ( ) , id + + , first group ) ;
if ( ( buffer [ diff ] & 128 ) = = 128 & & ( buffer [ diff + 2 ] & 32 ) = = 32 & & ( pts . get ( id ) = = null | | ( pts . get ( id ) = null & & end ) ) ) { pts . put ( id , integer . value of ( get ts ( buffer , diff + 3 ) ) ) ; } } }
int stack trace serial number = in . read int ( ) ;
dfstest util . wait nsecond ( 5 ) ; h . do checkpoint ( ) ; standby . quiesce standby ( get current tx id ( primary ) - 1 ) ; assert equals ( 47 , get current tx id ( primary ) ) ; assert equals ( get current tx id ( primary ) , get current tx id ( standby ) ) ; }
spin handler . send empty message ( 0 ) ; }
string stmgr id = null ; for ( instance metrics im : bp symptom . get component ( ) . get metrics ( ) . values ( ) ) { if ( im . has metric above limit ( metric _ back _ pressure . text ( ) , noise filter millis ) ) { string instance id = im . get name ( ) ; int from index = instance id . index of ( ' _ ' ) + 1 ; int to index = instance id . index of ( ' _ ' , from index ) ; stmgr id = instance id . substring ( from index , to index ) ; break ; } } log . info ( restarting container : + stmgr id ) ; boolean b = scheduler client . restart topology ( restart topology request . new builder ( ) . set container index ( integer . value of ( stmgr id ) ) . set topology name ( topology name ) . build ( ) ) ; log . info ( restarted container result : + b ) ; container restart action = new container restart ( ) ;
if ( i = - 1 & & n = null ) { first access = false ; if ( i < index ) { while ( i < index & & n = null ) { i + + ; n = n . next sibling ; } } else if ( i > index ) { while ( i > index & & n = null ) { i - - ; n = n . previous sibling ( ) ; } } } else { long way if ( index < 0 ) { return null ; } n = first child ; for ( i = 0 ; i < index & & n = null ; i + + ) { n = n . next sibling ; } }
route selector . selection selection = route selector . next ( ) ; dns . assert requests ( proxy ahost ) ; route route = selection . next ( ) ; assert route ( route , address , proxy a , dns . lookup ( proxy ahost , 0 ) , proxy aport ) ; route database . failed ( route ) ; route selector = new route selector ( address , route database , null , event listener . none ) ;
assert throws ( ( ) - > { run ( failsafe . with ( retry twice ) , runnable ) ; } , sync throwables ) ; verify ( service , times ( 3 ) ) . connect ( ) ;
if ( environment . is in vr ( ) = = false | | environment . get vrinput ( ) = = null | | environment . get vrinput ( ) . is input device tracking ( input index ) = = false ) { return ; } vector2f tp delta ;
if ( event dispatcher . is enabled ( ) ) { event dispatcher . dispatch event ( activiti event builder . create entity event ( activiti event type . entity _ deleted , process definition ) ) ; }
quads . add ( new switch test quad ( \ mary smith \ , caps , \ mary smith \ ) ) ; missed first cap because of the extra quote quads . add ( new switch test quad ( \ marysmith \ , caps , \ marysmith \ ) ) ; quads . add ( new switch test quad ( \ mary smith \ , first cap , \ mary smith \ ) ) ; quads . add ( new switch test quad ( \ mary smith \ , lower , \ mary smith \ ) ) ; quads . add ( new switch test quad ( \ mary smith \ , upper , \ mary smith \ ) ) ;
if ( width < = 0 | | height < = 0 ) return ;
string result = builder . evaluate ( context , < foo > < bar > abc _ def _ ghi < bar > < foo > ) ;
final list < sorted key value iterator < key , value > > iterators = new array list < > ( ) ; for ( final string filename : accumulo tablet . get files ( ) ) { final path path = new path ( filename ) ; final file system fs = path . get file system ( configuration ) ; final rfile . reader r file reader = new rfile . reader ( new cachable block file . reader ( fs , path , configuration , null , null , accumulo configuration ) ) ; iterators . add ( r file reader ) ; } merged iterator = new multi iterator ( iterators , true ) ;
ecpoint [ ] old pre comp = pre comp ;
assert true ( display list . size ( ) = = 1 ) ;
byte [ ] kbuf1 = new byte [ buf _ size ] ;
dispute result peers dispute result = peers dispute optional . get ( ) . get dispute result property ( ) . get ( ) ; dispute result . set buyer payout amount ( peers dispute result . get buyer payout amount ( ) ) ; dispute result . set seller payout amount ( peers dispute result . get seller payout amount ( ) ) ; dispute result . set winner ( peers dispute result . get winner ( ) ) ; dispute result . set loser is publisher ( peers dispute result . is loser publisher ( ) ) ; dispute result . set reason ( peers dispute result . get reason ( ) ) ; dispute result . set summary notes ( peers dispute result . summary notes property ( ) . get ( ) ) ;
if ( pulsar client = = null ) { if ( local cluster = = null ) { if not explicitly set , read clusters data from zk local cluster = retrieve cluster data ( ) ; } pulsar client = create client instance ( local cluster ) ; } return pulsar client ;
string status ;
add tag article relation ( tags , article ) ;
execute command ( schema ls + - l : + label2 . name ( ) + - p + property1 , label2 . name ( ) , property1 , + label1 . name ( ) , + property2 ) ;
protocol version ver = parse protocol version ( buffer , cursor ) ;
string writer schema text = ( string ) mode inputs [ 2 ] ;
assert false ( node health script should start , node health checker service . should run ( conf ) ) ;
original predicate = or ( and ( equal ( c _ bigint , bigint literal ( 1 l ) ) , rand predicate ( c _ bigint , bigint ) ) , and ( equal ( c _ bigint , bigint literal ( 2 l ) ) , rand predicate ( c _ bigint , bigint ) ) ) ;
con = new tcpmaster connection ( addr ) ;
assert . assert equals ( 1 , green mail . get received messages ( ) . length ) ;
long buffer duplicate = buf . duplicate ( ) ;
m bottom pagination container . set visibility ( view . gone ) ;
long time before reload = cobar adapter . get server status ( ) . get reload time ( ) ; long sleep time = 1000 ; test utils . wait for monment ( sleep time ) ;
check from string case ( [ [ : ] ] , 86 , [ : ] , 86 , false ) ;
result = artifact collector . collect ( artifacts , root artifact , managed versions , collection request , source , collection filter , listeners , null ) ;
return - 1 * m _ attribute . name ( ) . compare to ( comp . get attribute ( ) . name ( ) ) ; }
check dns ( name ) ;
client response resp = m _ client . call procedure ( @ system catalog , tables ) ;
url = new url ( non auth url . to external form ( ) + ?op = canceldelegationtoken & token = + dt ) ; conn = ( http urlconnection ) url . open connection ( ) ; conn . set request method ( put ) ; assert . assert equals ( http urlconnection . http _ ok , conn . get response code ( ) ) ;
final buffered reader br = new buffered reader ( new input stream reader ( local fs . open ( stdout path ) ) ) ;
char last ch ; while ( true ) { last ch = buf . char at ( buf . length ( ) - 1 ) ; buf . set length ( buf . length ( ) - 1 ) ; if ( last ch = ' 9 ' ) break ; if ( buf . length ( ) = = 0 ) { k + + ; last ch = ' 0 ' ; break ; } } buf . append ( ( char ) ( last ch + 1 ) ) ; return k + 1 ;
for ( e env : coprocessors ) { if ( env . get instance ( ) . get class ( ) . get name ( ) . equals ( class name ) | | env . get instance ( ) . get class ( ) . get simple name ( ) . equals ( class name ) ) { return env ; } } return null ; }
homes . put ( tokens [ 0 ] , tokens [ 5 ] ) ;
query q = qf . from ( get model factory ( ) . get user impl class ( ) ) . having ( account ids ) . contains all ( 2 , 1 ) . build ( ) ; list < user > list = q . list ( ) ;
try { txn . commit ( ) ; } finally { close the em if necessary if ( null = did we start work . get ( ) ) { did we start work . remove ( ) ; unit of work . end ( ) ; } }
display mode [ ] dm = org . lwjgl . util . display . get available display modes ( 640 , 480 , - 1 , - 1 , - 1 , - 1 , 60 , 60 ) ; org . lwjgl . util . display . set display mode ( dm , new string [ ] { width = + 640 , height = + 480 , freq = + 60 , bpp = + org . lwjgl . opengl . display . get display mode ( ) . get bits per pixel ( ) } ) ; return true ; } catch ( exception e ) {
evaluator option option = evaluator option . get ( after , after def ) ;
if ( last line lies above ) {
double reduction factor = ( consecutive no error count * analysis period ms > = rapid _ sleep _ decrease _ transition _ period _ ms ) ? rapid _ sleep _ decrease _ factor : sleep _ decrease _ factor ; new sleep duration = sleep duration * reduction factor ;
float grid scale = ( float ) spring util . map value from range to range ( value , 0 , 1 , 1 , 0 . 95 ) ;
if ( this . get return open type ( ) . equals ( other . get return open type ( ) ) ) { return false ; }
max height = math . max ( max height , get suggested minimum height ( ) ) ; max width = math . max ( max width , get suggested minimum width ( ) ) ;
m cur translation phase = calculate current translation phase ( m cur translation phase , m req translation phase ) ; }
dl = new deep learning ( parms ) . train model ( ) . get ( ) ; model metrics auto encoder mm = ( model metrics auto encoder ) dl . _ output . _ training _ metrics ;
assert equals ( activiti event type . entity _ deleted , listener . get events received ( ) . get ( 2 ) . get type ( ) ) ;
htable table = new htable ( conf , table name ) ;
if ( use secure cookie set ) { services . get property values ( ) . add property value ( use secure cookie , boolean . value of ( use secure cookie ) ) ; } if ( token validity set ) { boolean is token validity negative = token validity seconds . starts with ( - ) ; if ( is token validity negative & & is persistent ) { pc . get reader context ( ) . error ( att _ token _ validity + cannot be negative if using + a persistent remember - me token repository , source ) ; } services . get property values ( ) . add property value ( token validity seconds , token validity seconds ) ; } if ( rememberme parameter set ) { services . get property values ( ) . add property value ( parameter , rememberme parameter ) ; }
boolean shapefile created = false ;
try { channel . prepare for flush ( ) ; fail ( should have thrown exception ) ; } catch ( closed channel exception e ) { then we should get a closed channel exception }
if ( ( cp = null ) & & ( _ consumer event buffer . get start scn ( ) < 0 ) ) {
return m circle view index ;
cs . kill all apps in queue ( a1 ) ;
cr = client . call procedure ( @ ad hoc , update + tb + set a = 1 , b = null where a = 5 and b = 5 ; ) ; assert equals ( client response . success , cr . get status ( ) ) ; cr = client . call procedure ( @ ad hoc , update + tb + set a = 1 , b = null where a = 4 and b = 4 ; ) ; assert equals ( client response . success , cr . get status ( ) ) ; vt = client . call procedure ( @ ad hoc , select a , b , c from + tb + order by a , c ) . get results ( ) [ 0 ] ; validate table of longs ( vt , new long [ ] [ ] { { 1 , 1 , 1 } , { 1 , long . min _ value , 4 } , { 1 , long . min _ value , 5 } , { 2 , long . min _ value , 5 } , { 6 , long . min _ value , 2 } , { 7 , 7 , 7 } } ) ;
context . add route definitions ( routes . get routes ( ) ) ; assert not null ( loaded bar route should be there , context . get route ( bar ) ) ; assert equals ( 2 , context . get routes ( ) . size ( ) ) ;
serialized . read bytes ( netty message . header _ length ) ;
public void test create resource ( ) { class < ? > resource class ;
for ( map . entry < string , string > entry : tier config . get tier to broker map ( ) . entry set ( ) ) { if ( base rule . get tiered replicants ( ) . contains key ( entry . get key ( ) ) ) { broker service name = entry . get value ( ) ; break ; } }
} if ( ostream = null ) { if ( check sendfile ( request , response , resource , range . end - range . start + 1 , range ) ) copy ( resource , ostream , range ) ; } else {
my parser = new pipelined msg parser ( this , hispipe , this . sip stack . get max message size ( ) ) ;
if ( p sorder = p torder ) { if ( s order = = 0 ) {
procedure . set readonly ( proc has write stmts = = false ) ; procedure . set hasseqscans ( proc has seq scans ) ; }
m media player = create media player ( this , r . raw . test _ cbr ) ;
json . flush ( ) ; json . close ( ) ; string json string = writer . to string ( ) ;
cursor c = content resolver . query ( data contract . content _ uri , null , null , null , null ) ; set list adapter ( new simple cursor adapter ( this , r . layout . list _ layout , c , data contract . all _ columns , new int [ ] { r . id . id string , r . id . data } , 0 ) ) ; }
assert equals ( 0 , lang tool . check ( este exemplo estÃ¡ correto . este exemplo tambÃ©m estÃ¡ . ) . size ( ) ) ;
assert equals ( start hit , cs . get hit count ( ) ) ; assert equals ( start miss , cs . get miss count ( ) ) ; assert equals ( start evicted , cs . get evicted count ( ) ) ;
db . users ( ) . insert project permission on user ( user , user role . admin , db . components ( ) . insert private project ( ) ) ; set < string > org uuids = under test . select organization uuids of user with global permission ( db session , user . get id ( ) , scan _ execution ) ;
info . set codec v ( wmv ) ;
container id c id = test node manager shutdown . create container id ( ) ; file context local fs = file context . get local fsfile context ( ) ; test node manager shutdown . start container ( nm , c id , local fs , nm local dir , new file ( start _ file . txt ) , port ) ; try { wait until we start stopping sync barrier . await ( 10000 , time unit . milliseconds ) ; wait until we finish stopping sync barrier . await ( 10000 , time unit . milliseconds ) ; } catch ( exception e ) { }
return register spec list . empty ; }
test split ( he won ' t go . , really . ) ;
random rand = new random ( ) ; final byte [ ] data = new byte [ data size ] ; rand . next bytes ( data ) ; final synchronous queue < byte [ ] > queue = new synchronous queue < byte [ ] > ( ) ; in band bytestream manager target byte stream manager = in band bytestream manager . get byte stream manager ( target connection ) ;
file output stream out file = new file output stream ( out name ) ; copy stream ( zis , out file ) ; out file . close ( ) ; zis . close entry ( ) ; }
long id = wrapper _ class _ counter . get and increment ( ) ; class generator cc = class generator . new instance ( cl ) ; cc . set class name ( ( modifier . is public ( c . get modifiers ( ) ) ? wrapper . class . get name ( ) : c . get name ( ) + sw ) + id ) ; cc . set super class ( wrapper . class ) ; cc . add default constructor ( ) ;
connect parent to children ( x6 , x5 ) ;
container impl container1 = create mock container ( user , 1 ) ;
execute ( insert into % s json ? default null , { \ k \ : 0 } ) ;
if ( i = = length - 1 & & is white space ( node ) ) { break ; } node new node = existing document . import node ( node , true ) ;
if ( m selection controller . get selection type ( ) = selection type . long _ press ) return false ;
bois = new basic object input stream ( new byte array input stream ( buffer ) ) ; try { bois . read object override ( ) ; fail ( test 2 : ioexception expected . ) ; } catch ( ioexception e ) { } bois . close ( ) ;
string ref = u . get ref ( ) ; boolean is rel path = false ; boolean query only = false ;
process registry . register process ( new fake nu process ( 43 ) , create params ( proc43 ) , context ) ;
map < string , object > result = new hash map < string , object > ( ) ; result . put ( timestamp , get timestamp ( ) ) ; result . put ( source , get source ( ) ) ; result . put ( context , get context ( ) ) ; result . put ( metrics , metrics map ) ; result . put ( exceptions , exceptions list ) ; return result . to string ( ) ;
pos = seek pos ;
m _ rank results [ 2 ] [ ( int ) attribute ranking [ j ] [ 0 ] ] + = ( attribute ranking [ j ] [ 1 ] * attribute ranking [ j ] [ 1 ] ) ;
chunk = m _ array [ + + m _ last chunk ] ;
switch ( state ) { case background _ disabled : paint background disabled ( g ) ; break ; case background _ enabled : paint background enabled ( g ) ; break ; case background _ focused : paint background focused ( g ) ; break ; case background _ mouseover _ focused : paint background mouse over and focused ( g ) ; break ; case background _ pressed _ focused : paint background pressed and focused ( g ) ; break ; case background _ mouseover : paint background mouse over ( g ) ; break ; case background _ pressed : paint background pressed ( g ) ; break ; case foreground _ disabled : paint foreground disabled ( g ) ; break ; case foreground _ enabled : paint foreground enabled ( g ) ; break ; case foreground _ focused : paint foreground focused ( g ) ; break ; case foreground _ mouseover _ focused : paint foreground mouse over and focused ( g ) ; break ; case foreground _ pressed _ focused : paint foreground pressed and focused ( g ) ; break ; case foreground _ mouseover : paint foreground mouse over ( g ) ; break ; case foreground _ pressed : paint foreground pressed ( g ) ; break ; }
byte [ ] value = bytes . to bytes ( after _ snapshot _ value ) ;
kdtree kd tree = new kdtree ( n ) ; for ( int i = - 1 ; i < n ; i + + ) { insert a unit vector along each dimension list < double > vec = new array list < > ( n ) ; i = - 1 ensures the origin is in the tree for ( int k = 0 ; k < n ; k + + ) { vec . add ( ( k = = i ) ? 1 . 0 : 0 . 0 ) ; } indarray ind vec = nd4j . create ( nd4j . create buffer ( doubles . to array ( vec ) ) ) ; kd tree . insert ( ind vec ) ; } random rand = new random ( ) ;
assert equals ( height _ hint , image . get height ( ) ) ; assert pixel ( image , 9 , height _ hint 2 , new color ( 115 , 38 , 0 ) ) ; assert pixel ( image , 230 , height _ hint 2 , new color ( 38 , 115 , 0 ) ) ; } finally {
try { b . put ( - 1 , ( byte ) 0 ) ; fail ( expected exception not thrown ) ; } catch ( index out of bounds exception e ) { expected } try { b . put ( b . limit ( ) , ( byte ) 0 ) ; fail ( expected exception not thrown ) ; } catch ( index out of bounds exception e ) { expected }
final string key predicate = ( string ) properties . get ( key _ predicate _ property ) ;
for ( int i = 0 ; i < 1000 ; i + + ) { jedis . set ( a - test - + i , 0 ) ; }
application manifest resources manifest = application manifest . from resources rule ( rule context ) ;
http client params client params = new http client params ( ) ;
enumeration < jar entry > elements = file . entries ( ) ; while ( elements . has more elements ( ) ) { string entry = elements . next element ( ) . get name ( ) ; if ( entry . ends with ( . class ) ) {
read test data ( test _ data , 0 , 6 , 1 , 0 , 1 , false ) ;
string alias = ( string ) pctx . get top ops ( ) . key set ( ) . to array ( ) [ 0 ] ;
generate csv position map ( clazz , obj , results ) ;
if ( reserved container . get reserved scheduler key ( ) . get container to update ( ) = null ) { this means a container update request ( like increase promote ) return false ; }
local conf . set long ( raid . policy . rescan . interval , 3600 * 1000 l ) ; cnode = raid node . create raid node ( local conf ) ; final int max _ waittime = 120000 ; dist raid node dcnode = ( dist raid node ) cnode ; long start = system . current time millis ( ) ;
writer . write ( name ) ;
day of year - - ;
item = ( item ) s . load ( item . class , non existent id ) ;
task . iteration ( request . get iteration ( ) ) ; context . get event handler ( ) . handle ( new metrics event ( metrics event type . task _ iteration _ update ) ) ; return task iteration response . new builder ( ) . build ( ) ; }
sb . append ( + ( m . group ( 2 ) + m . group ( 3 ) + m . group ( 4 ) + m . group ( 5 ) + m . group ( 6 ) + m . group ( 7 ) ) . to lower case ( ) ) ;
if ( raster . get num bands ( ) = = 3 ) { target . filter ( opaque ) ; }
target = gl20 . gl _ texture _ cube _ map _ positive _ x ;
list < coverage info > coverages = catalog . get coverages by coverage store ( store info ) ;
string collection name = test solr cloud collection ; string config name = solr cloud collection config ; mini cluster . upload config set ( solr test case j4 . test _ path ( ) . resolve ( collection1 conf ) , config name ) ; collection admin request . create collection ( collection name , config name , 2 , 2 ) . process ( mini cluster . get solr client ( ) ) ; query request req = new query request ( ) ;
future = not full future ; not full future = not _ full ; }
set paused ( false ) ;
m _ char . append ( ch , start , length ) ;
int modifier = paint = null ? setup color filter ( paint ) : modifier _ none ;
if ( this . blocked = = null ) { this . blocked = new linked list < logical dependency < simple mode > > ( ) ; } this . blocked . add ( dep ) ;
log . info ( job { } was successfully submitted to the job manager { } . , ( ( job manager messages . job submit success ) message ) . job id ( ) , get sender ( ) . path ( ) ) ;
answer = await consumer ( ) ;
if ( m content stack . is empty ( ) ) { show content ( m content stack . peek ( ) ) ; } return true ;
cases . add ( new object [ ] { d : \ \ code \ \ jetty \ \ jetty - start \ \ src \ \ test \ \ resources \ \ extra - libs \ \ example . jar , true } ) ;
unmarshaller u = jc . create unmarshaller ( ) ; jaxb validation event handler event handler = new org . docx4j . jaxb . jaxb validation event handler ( ) ;
return get confirmed connections ( ) . stream ( ) . map ( e - > e . get peers node address optional ( ) . get ( ) ) . collect ( collectors . to set ( ) ) ; }
assert equals ( 3 , state . get ( ) ) ; latch2 . count down ( ) ; } } ) ; latch2 . await ( ) ; } finally {
song _ end = true ; }
string error code = parse error code ( node ) ; if ( error code = = null | | error code . equals ( health check in use ) ) return null ; health check in use exception e = ( health check in use exception ) super . unmarshall ( node ) ; return e ; }
doc . clear ( ) ; doc . add field ( id , 5 ) ; doc . add field ( title , s2 five ) ; doc . add field ( routefield _ s , africa ) ; shard2 . add ( doc ) ; shard2 . commit ( ) ; doc counts2 = 2 ; two documents in shard2
xml . start ( expiration ) ;
expected rows = ( num rows 2 ) - 1 ;
enable http2 ( 1 ) ; configure and start web application ( ) ; open client connection ( ) ; do http upgrade ( ) ; send client preface ( ) ;
final int width = ( int ) ( h * intrinsic aspect ) ;
tdigest dist2 = tdigest . merge ( 50 , subs ) ; for ( double q : new double [ ] { 0 . 001 , 0 . 01 , 0 . 1 , 0 . 2 , 0 . 3 , 0 . 5 } ) { double z = quantile ( q , data ) ; double e1 = dist . quantile ( q ) - z ; double e2 = dist2 . quantile ( q ) - z ; system . out . printf ( quantile \ t % d \ t % . 6f \ t % . 6f \ t % . 6f \ t % . 6f \ t % . 6f \ n , parts , q , z - q , e1 , e2 , math . abs ( e2 ) q ) ; assert true ( string . format ( parts = % d , q = % . 4f , e1 = % . 5f , e2 = % . 5f , rel = % . 4f , parts , q , e1 , e2 , math . abs ( e2 ) q ) , math . abs ( e2 ) q < 0 . 1 ) ; assert true ( string . format ( parts = % d , q = % . 4f , e1 = % . 5f , e2 = % . 5f , rel = % . 4f , parts , q , e1 , e2 , math . abs ( e2 ) q ) , math . abs ( e2 ) < 0 . 015 ) ; }
path meta change file2 = new path ( no change dir , meta change file2 ) ;
if ( root . left = = null & & root . right = = null ) { return 0 ; }
if ( channel . get animation name ( ) . equals ( stand up back ) | | channel . get animation name ( ) . equals ( stand up front ) ) { channel . set loop mode ( loop mode . dont loop ) ; channel . set anim ( idle top , 5 ) ; channel . set loop mode ( loop mode . loop ) ; }
kill bill client . delete payment method ( account json . get payment method id ( ) , true , true , request options ) ; final payments payments for account = kill bill client . get payments for account ( account json . get account id ( ) , request options ) ; final payment payment = payments for account . get ( payments for account . size ( ) - 1 ) ; final invoices invoices = kill bill client . get invoices for account ( account json . get account id ( ) , true , true , request options ) ; final list < invoice item > items to be adjusted = invoices . get ( 1 ) . get items ( ) ;
clean up rows ( get id table info ( persister ) . get qualified id table name ( ) , session ) ; } } ; }
if ( result = null ) { template ( ) . request body ( direct : delete sobject , result . get id ( ) ) ; }
client . expect upgrade response ( ) ;
new set = null ;
while ( data pos < data size ) { write tag ( ) ; write length and value ( ) ; } system . arraycopy ( indef data , data size , new data , data size + num of total len bytes , unused ) ; return new data ; }
directory . mkdirs ( ) ;
inputs [ i ] . position ( input positions [ i ] + data len ) ; } }
if ( should fix empty meta cells ( ) ) { fix empty meta cells ( ) ; }
list . add change listener ( new realm change listener < realm list < object > > ( ) { @ override public void on change ( realm list < object > element ) { assert equals ( 0 , listener called count . get and increment ( ) ) ; } } ) ;
string [ ] lines = out . split ( \ r ) ;
mortar scope root = this ;
filter lower = ff . greater or equal ( ff . literal ( value ) , attribute ) ;
public void test relay chaining delayed consumer ( ) { databus relay test util . relay runner r1 = null , r2 = null ;
subscriptions . add ( observable . combine latest ( view util . get observable for res id ( get context ( ) , background res id , aesthetic . get ( get context ( ) ) . color accent ( ) ) , aesthetic . get ( get context ( ) ) . is dark ( ) , color is dark state . creator ( ) ) . compose ( rx . < color is dark state > distinct to main thread ( ) ) . subscribe ( new consumer < color is dark state > ( ) { @ override public void accept ( @ non null color is dark state color is dark state ) { invalidate colors ( color is dark state ) ; } } , on error log and rethrow ( ) ) ) ;
return inactive window panels . get ( 0 ) ;
new trust anchor ( x500p , pk , get encoding psonly ( ) ) ;
provider manager . add extension provider ( media presence extension . element _ name , media presence extension . namespace , new default packet extension provider < media presence extension > ( media presence extension . class ) ) ;
assert translation ( from commento c where c . marelo . commento . mcompr is null ) ;
prime insts . delete ( 0 ) ;
status error = status . not _ found . with description ( www . google . com not found ) ;
if ( infos . size ( ) > 1 ) { logger . log ( level . info , error : multi - level details not supported . ) ; return ; } string builder sbuilder = new string builder ( ) ;
return new simple type token < t > ( resolver . resolve type ( runtime type ) ) ;
add alert to tree ( alert ) ;
if ( latency 1000 > = buckets ) { histogramoverflow + + ; } else { histogram [ latency 1000 ] + + ; } operations + + ; totallatency + = latency ; totalsquaredlatency + = ( ( double ) latency ) * ( ( double ) latency ) ; windowoperations + + ; windowtotallatency + = latency ; if ( ( min < 0 ) | | ( latency < min ) ) { min = latency ; }
init source reader ( ) ;
for ( final timer remaining timer : active timers returned from sfsb ) { assert . assert not equals ( unexpectedly found a timer from other ejb module + module _ two _ name + when queried from a stateful bean , timer bean in other module . schedule _ one _ info . equals ( remaining timer . get info ( ) ) ) ; assert . assert not equals ( unexpectedly found a timer from other ejb module + module _ two _ name + when queried from a stateful bean , info for timer bean in other module . equals ( remaining timer . get info ( ) ) ) ; }
int to skip = ( int ) ( start offset - first chunk offset ) ;
async exception = null ;
date time utils . set current millis offset ( - flow _ finished _ time . to millis ( ) ) ; flow1 . set end time ( date time utils . current time millis ( ) ) ; this . execution flow dao . update executable flow ( flow1 ) ;
if ( i = 47 & & i = 58 & & i = 92 ) { file path list . add ( file + i + + special char + _ name ) ; }
if ( pos < = stream offset & & pos > = ( stream offset - out buffer . remaining ( ) ) ) { int forward = ( int ) ( pos - ( stream offset - out buffer . remaining ( ) ) ) ; if ( forward > 0 ) { out buffer . position ( out buffer . position ( ) + forward ) ; } } else { ( ( seekable ) in ) . seek ( pos ) ; reset stream offset ( pos ) ; }
int start index = followers model . get page ( ) * stats service . max _ results _ requested _ per _ page - stats service . max _ results _ requested _ per _ page + 1 ; int end index = start index + followers model . get followers ( ) . size ( ) - 1 ; string paged label = get string ( m top pager selected button index = = 0 ? r . string . stats _ followers _ total _ wpcom _ paged : r . string . stats _ followers _ total _ email _ paged , start index , end index , format utils . format decimal ( m top pager selected button index = = 0 ? followers model . get total wpcom ( ) : followers model . get total email ( ) ) ) ; m totals label . set text ( paged label ) ; } else {
this . memstore . add ( new key value ( row , family , qf1 , + + current ts , ( byte [ ] ) null ) , null ) ; this . memstore . add ( new key value ( row , family , qf1 , + + current ts , ( byte [ ] ) null ) , null ) ; number of cell + = 2 ; assert equals ( number of cell , memstore . get segments ( ) . stream ( ) . map to int ( segment : : get cells count ) . sum ( ) ) ; assert equals ( min ts , memstore . get segments ( ) . stream ( ) . map to long ( m - > m . get time range tracker ( ) . get min ( ) ) . min ( ) . get as long ( ) ) ; assert equals ( current ts , memstore . get segments ( ) . stream ( ) . map to long ( m - > m . get time range tracker ( ) . get max ( ) ) . max ( ) . get as long ( ) ) ; ( ( compacting mem store ) memstore ) . flush in memory ( ) ; trigger the merge
my additional modules = new array list < > ( ) ; for ( my additional module data data : modules ) { module additional module = data . my module fixture builder . get fixture ( ) . get module ( ) ; my additional modules . add ( additional module ) ; android facet facet = add android facet ( additional module ) ; facet . set project type ( data . my project type ) ; string root path = get additional module path ( data . my dir name ) ; my fixture . copy directory to project ( get res dir ( ) , root path + res ) ; my fixture . copy file to project ( sdk constants . fn _ android _ manifest _ xml , root path + ' ' + sdk constants . fn _ android _ manifest _ xml ) ; if ( data . my is main module dependency ) { module root modification util . add dependency ( my module , additional module ) ; } } if ( provides custom manifest ( ) ) { delete manifest ( ) ; }
if ( globals . is empty ( ) ) { bucket collector globals collector = bucket collector . wrap ( globals ) ; query query = context . build filtered query ( queries . new match all query ( ) ) ; try { final collector collector ; if ( context . get profilers ( ) = = null ) { collector = globals collector ; } else { internal profile collector profile collector = new internal profile collector ( globals collector , collector result . reason _ aggregation _ global , todo : report on sub collectors collections . empty list ( ) ) ; collector = profile collector ; start a new profile with this collector context . get profilers ( ) . add query profiler ( ) . set collector ( profile collector ) ; } globals collector . pre collection ( ) ; context . searcher ( ) . search ( query , collector ) ; } catch ( exception e ) { throw new query phase execution exception ( context , failed to execute global aggregators , e ) ; } finally { context . clear releasables ( search context . lifetime . collection ) ; } } list < internal aggregation > aggregations = new array list < > ( aggregators . length ) ;
value end = value start = name start = pos ; pos = name end = get token end position ( bytes , pos , end , version , true ) ;
page header . populate ( input , false ) ;
query = select s from student s where s . big decimal = : big deci ;
assert false ( entry set . contains ( entry copy ) ) ;
path dst path = store . get region file system ( ) . create temp name ( ) ; fsdata output stream stream = fs . create ( dst path , null , true , 512 , ( short ) 3 , ( long ) 1024 , null ) ; stream . write chars ( corrupt file ) ; stream . close ( ) ; path orig path = store . get region file system ( ) . commit store file ( bytes . to string ( column _ family ) , dst path ) ; try { ( ( hstore ) store ) . move file into place ( orig path ) ; } catch ( exception e ) { the complete compaction should fail and the corrupt file should remain in the ' tmp ' directory ; assert true ( fs . exists ( orig path ) ) ; assert false ( fs . exists ( dst path ) ) ; system . out . println ( test compaction with corrupt result passed ) ; return ; }
set _ dps = dps . get ( 1 ) ; assert equals ( 3 , set _ dps . length ) ; assert equals ( ts , set _ dps [ 0 ] . timestamp ( ) ) ; assert equals ( ts , set _ dps [ 1 ] . timestamp ( ) ) ; assert equals ( ts , set _ dps [ 2 ] . timestamp ( ) ) ; assert true ( double . is na n ( set _ dps [ 0 ] . to double ( ) ) ) ; assert true ( double . is na n ( set _ dps [ 1 ] . to double ( ) ) ) ; assert true ( double . is na n ( set _ dps [ 2 ] . to double ( ) ) ) ; ts + = 60000 ;
int start = 0 , end = num - 1 ;
run and eval ( ) ;
sreq . params . remove ( response builder . field _ sort _ values ) ; if ( rb . rsp . get return fields ( ) . wants field ( unique field . get name ( ) ) ) { sreq . params . add ( common params . fl , unique field . get name ( ) ) ; }
datum & = 255 < < 8 - x remaining ;
return null ; } thing control request control request = thing . get control request handler ( ) ; if ( control request = = null ) {
token settings page . navigate to ( ) ;
evaluate nodes ( page nodes , s , map , ctxt . get options ( ) . get mapped file ( ) ) ; s . optimize line section ( ) ; s . set output file name ( unqualify ( ctxt . get servlet java file name ( ) ) ) ; string class file name = ctxt . get class file name ( ) ;
assert . assert true ( queue . remove ( 3 ) ) ;
thread t dispatcher = new thread ( dispatcher ) ;
pipe line . close ( ) ;
int bits = ( packed fields & 0x7 ) + 1 ;
else { term vectors response . set exists ( false ) ; }
if ( multimodule = = multimodule . standard ) { create module ( model , jar packaging provider , model ) ; create module ( repository , jar packaging provider , repository ) ; roo - 3762 : generate integration module by default create module ( integration , jar packaging provider , integration ) ; create module ( service - api , jar packaging provider , service . api ) ; create module ( service - impl , jar packaging provider , service . impl ) ; add dependencies between modules get project operations ( ) . add dependency ( repository , pom . get group id ( ) , model , { project . version } ) ; get project operations ( ) . add dependency ( integration , pom . get group id ( ) , model , { project . version } ) ; get project operations ( ) . add dependency ( service - api , pom . get group id ( ) , model , { project . version } ) ; get project operations ( ) . add dependency ( service - impl , pom . get group id ( ) , repository , { project . version } ) ; get project operations ( ) . add dependency ( service - impl , pom . get group id ( ) , service . api , { project . version } ) ; get project operations ( ) . add dependency ( service - impl , pom . get group id ( ) , model , { project . version } ) ; get project operations ( ) . add dependency ( service - impl , pom . get group id ( ) , integration , { project . version } ) ; }
cassandra daemon daemon = new cassandra daemon ( ) ; storage service . instance . register daemon ( daemon ) ; killer for tests killer for tests = new killer for tests ( ) ;
byte [ ] first component = components [ offset ] ;
if ( e . get cause ( ) instanceof runtime exception ) { throw ( runtime exception ) e . get cause ( ) ; }
end = 0 ;
force animation to finish ( ) ; ensure source tab created ( source tab id ) ;
f . get ( ) ; }
blocking page sink . complete ( ) ;
instructions . add ( reil helpers . create bsh ( base offset + + , dw , source register , dw , sh , dw , rotate var1 ) ) ; instructions . add ( reil helpers . create bsh ( base offset + + , dw , source register , dw , string . value of ( - ( 32 - integer . decode ( sh ) ) ) , dw , rotate var2 ) ) ; instructions . add ( reil helpers . create or ( base offset + + , dw , rotate var1 , dw , rotate var2 , dw , rotate var3 ) ) ; instructions . add ( reil helpers . create and ( base offset + + , dw , rotate var3 , dw , string . value of ( 0x ffffffffl ) , dw , rotate var4 ) ) ;
wli . set abstract ( layer . get _ abstract ( ) ) ;
byte [ ] key = utils . make key ( url ) ;
file input stream fis = new file input stream ( tmp file ) ;
string template file = gen vector code . join path ( this . test template directory , template . to string ( ) + . txt ) ; string template string = remove template comments ( gen vector code . read file ( template file ) ) ; for ( boolean [ ] test matrix : new boolean [ ] [ ] {
thread thread = new thread ( new buffer request task ( local buffer pool , async exception ) ) ;
return return code . next _ col ;
parse and check error ( true or , spel message . right _ operand _ problem , 5 ) ; }
scheduled executor service executor = executors . new scheduled thread pool ( 1 ) ; executor . schedule at fixed rate ( ( ) - > pulsar . get broker service ( ) . check un ack message dispatching ( ) , 10 , 10 , time unit . milliseconds ) ;
new verifications ( ) { { scanner . reset ( ) ; times = 1 ; } } ;
map < string , string > namespaces = new hash map < > ( ) ;
if ( null = = currency pair ) { throw new not yet implemented for exchange exception ( ) ; } return jubi adapters . adapt open orders ( get jubi open order ( currency pair ) , currency pair ) ;
final float dist = ( float ) math . hypot ( to . left - from . left , to . top - from . top ) ; final resources res = get resources ( ) ; final float max dist = ( float ) res . get integer ( r . integer . config _ drop anim max dist ) ;
list < field ref > field refs = new array list < field ref > ( ) ;
double [ ] sigmay = new double [ ncoly ] ; double [ ] sigmax = new double [ ncolx ] ; double [ ] [ ] denom = new double [ ncoly ] [ ncolx ] ;
unescaped . add ( current . to string ( ) ) ; return unescaped ;
p = r . exec ( ls + mpoint + test mkdirs not there ) ;
inherit from ( base config ) ;
config = parse inner binding config ( item , binding config part , config , out _ binding _ key ) ; } else { throw new binding config parse exception ( unknown command given configuration must start with ' < ' or ' > ' ) ; } }
invite _ ts . terminate ( ) ;
for ( int i = 1 ; i < = 30 ; i + + ) { assert that ( cob . get user defined writability ( i ) , is ( true ) ) ; }
graceful = true ;
while ( node = null ) { list . add ( node ) ; node = node . get parent ( ) ; }
return 1 + 4 ; case icode _ getvar1 :
rectangle . offset ( child . get left ( ) - child . get scroll x ( ) , child . get top ( ) - child . get scroll y ( ) ) ;
instructions . add ( reil helpers . create and ( offset + 1 , result size , result , result size , mask upper , result size , upper half ) ) ;
return partial result * - 1 ; return partial result ; }
quick sort2 ( recomposable elements , 0 , recomposable elements . size ( ) - 1 ) ;
band selected coverage = ( ( select sample dimension ) processor . get operation ( select sample dimension ) ) . do operation ( param , hints ) ;
slice _ from ( l \ u00 f8s ) ;
bundle extras = get intent ( ) . get extras ( ) ; if ( null = extras ) { get bundle extras ( extras ) ; } if ( is bind event bus here ( ) ) { event bus . get default ( ) . register ( this ) ; }
template model m2 = new template model ( ) ; m2 . add lhs item ( fp ) ; m2 . name = r2 ; m2 . add row ( new string [ ] { t1 , t2 } ) ; final string expected2 = rule \ r2 _ 0 \ \ n + dialect \ mvel \ \ n + when \ n + p1 : smurf ( field1 = = \ t1 \ , field1 = = \ t2 \ , field1 = = \ value \ ) \ n + then \ n + end ;
if ( is reserved ( ) ) { cancel reservation ( ) ; reply ( malmook , dos ) ; } else {
ctxt . generate java source ( http servlet response + response name + = + ( ( http servlet response ) page context . get response ( ) ) ; ) ;
t + = ( lim = buffer . limit ( ) ) - buffer . position ( ) ;
window . add window closing handler ( new closing handler ( ) { @ override public void on window closing ( closing event event ) { if ( askpass pending _ ) { askpass pending _ = false ; server . askpass completed ( null , false , new simple request callback < void > ( ) ) ; } } } ) ;
byte util . short2bytes ( magic , header , offset ) ;
input builder = input builder . set query ( jdbctest base . select _ all _ books _ split _ by _ id ) . set parameters provider ( new numeric between parameters provider ( fetch size , min , max ) ) ;
parse directory response message = codec . get gson ( ) . from json ( response body , parse directory response message . class ) ; parse directory response message . validate response ( errors ) ; errors . add errors ( parse directory response message . get plugin errors ( ) ) ; return new crparse result ( parse directory response message . get environments ( ) , parse directory response message . get pipelines ( ) , errors ) ;
dfstest util . wait replication ( fs , file1 , factor ) ;
final byte [ ] value bytes = new byte [ 8 * 1024 ] ;
if ( s compatibility mode enabled ) { p . application info . disable compatibility mode ( ) ; }
generate class ( package name , class name , logger , context ) ;
io utils . safe close ( pooled buffer , input buffer ) ; }
super ( new signature ( mangle operator name ( operator type ) , function kind . scalar , type variable constraints , long variable constraints , return type , argument types , false ) ) ; }
super . start scroll ( start x , start y , dx , dy , m duration ) ; }
if ( r = size ) { system . arraycopy ( buffer , start + r , buffer , start + w , size - r ) ; w + = size - r ; }
this . instance . init ( dbeaver core . get instance ( ) ) ;
if ( s . to char array ( ) . to string ( ) . is empty ( ) ) { system . out . println ( char array string is empty ) ; } else { system . out . println ( char array string is nonempty ) ; }
store . put ( key , versioned , null ) ;
list < object > the list = null ; if ( bm . get parent ( ) instanceof list ) { the list = ( list ) bm . get parent ( ) ; eg block range . get contents ( ) } else { the list = ( ( content accessor ) ( bm . get parent ( ) ) ) . get content ( ) ; } object delete me = null ;
error = cam . connect ( guid ) ;
secondary region . refresh store files ( ) ; assert path lists equal ( primary region . get store file list ( families ) , secondary region . get store file list ( families ) ) ; assert equals ( families . length * 4 , secondary region . get store file list ( families ) . size ( ) ) ; log . info ( - - verifying edits from secondary ) ;
slice = slice . make ( make bound ( sk , 1 , 0 , 0 ) , make bound ( ek , 1 , 1 , 1 ) ) ; assert true ( slice . intersects ( cc , column names ( 1 , 0 , 0 ) , column names ( 1 , 1 , 1 ) ) ) ;
string select all source = select * from source _ p1 where bi = + param + order by bi , ii ; string select all target = select * from target _ p order by bi , ii ; resp = client . call procedure ( @ ad hoc , select all source ) ;
view view = inflater . inflate ( r . layout . add _ category , null ) ; m category edit text = ( edit text ) view . find view by id ( r . id . category _ name ) ; m parent spinner = ( spinner ) view . find view by id ( r . id . parent _ category ) ; load categories ( ) ;
send custom notification ( profile , simple notification , flash times , flash colour , original colour , flash duration , extra action , builder ) ;
if ( result = setup result . success ) { if ( dbg ) { log . d ( log _ tag , set link properties : error clearing link properties + status = + status + result = + result ) ; } link properties . clear ( ) ; } return result ; }
verify insert ( event , null , tbl name ) ; event = rsp . get events ( ) . get ( 9 ) ; assert equals ( first event id + 10 , event . get event id ( ) ) ; assert equals ( event type . add _ partition . to string ( ) , event . get event type ( ) ) ; event = rsp . get events ( ) . get ( 10 ) ; assert equals ( first event id + 11 , event . get event id ( ) ) ; assert equals ( event type . insert . to string ( ) , event . get event type ( ) ) ;
assert not connected ( test _ sslengine _ set use client mode ( false , false ) ) ;
fs . rename ( new path ( root dir , table link name ) , new path ( archive dir , table link name ) ) ;
request = new mock http servlet request ( get , ) ; request . add parameter ( date , 2009 - 10 - 31 ) ; request . add parameter ( percent , 99 . 99 % ) ; mock http servlet response response = new mock http servlet response ( ) ; handler execution chain chain = mapping . get handler ( request ) ;
assert that ( dom , has xpath ( count ( at : feed at : entry ) , equal to ( 3 ) ) ) ; assert that ( dom , has xpath ( at : feed at : entry [ 1 ] at : title , equal to ( sentinel2 ) ) ) ; assert that ( dom , has xpath ( at : feed at : entry [ 2 ] at : title , equal to ( sentinel1 ) ) ) ; assert that ( dom , has xpath ( at : feed at : entry [ 3 ] at : title , equal to ( landsat8 ) ) ) ; }
if ( cursor < i _ p1 ) { return false ; } cursor = i _ p1 ; v _ 2 = limit _ backward ; limit _ backward = cursor ; cursor = limit - v _ 1 ;
fsdata input stream in = dfs . open ( filepaths [ 0 ] ) ;
logger . get logger ( ) . add handler ( new console handler ( ) ) ;
map < string , object > variables = new hash map < string , object > ( ) ;
inbound channel . close ( ) ; } } } ) ; }
if ( is null or empty ( options . get gcp temp location ( ) ) ) { new job . get environment ( ) . set temp storage prefix ( dataflow options . get path validator ( ) . verify path ( options . get gcp temp location ( ) ) ) ; } new job . get environment ( ) . set dataset ( options . get temp dataset id ( ) ) ; new job . get environment ( ) . set experiments ( options . get experiments ( ) ) ;
promise . set success ( collections . singleton list ( loopback address ( ) ) ) ; return ; }
try { while ( msg buffer [ i ] < 0x20 ) i + + ; } catch ( array index out of bounds exception e ) { array contains only control char , return null . return null ; }
return adapter . decode ( bytes ) ;
relationship relationship = create header part ( word mlpackage ) ;
result . put ( get local node name ( ) , get storage ( ) . get cluster names ( ) ) ;
frame . set size ( width , height ) ; build tag panel ( ) ; build content panel ( ) ;
if ( execution = = null ) { execution = execution mode . global _ ordinals ; } if ( ( execution . needs global ordinals ( ) ) & & ( ( values source instanceof values source . bytes . with ordinals ) ) ) { execution = execution mode . map ; } return execution . create ( name , factories , shard size , max docs per value , values source , context , parent , pipeline aggregators , meta data ) ; }
string string = val . trim ( ) ;
scheduler . restart topology request restart topology request = scheduler . restart topology request . new builder ( ) . set topology name ( topology _ name ) . set container index ( 1 ) . build ( ) ;
tree state = create layout cache ( ) ; configure layout cache ( ) ; update size ( ) ; }
glyph paint . set color ( context . get resources ( ) . get color ( r . color . black _ 48 ) ) ;
segment metadata impl segment metadata = extract segment metadata ( raw table name , committing segment name str ) ; old seg metadata . set crc ( long . value of ( segment metadata . get crc ( ) ) ) ; old seg metadata . set start time ( segment metadata . get time interval ( ) . get start millis ( ) ) ; old seg metadata . set end time ( segment metadata . get time interval ( ) . get end millis ( ) ) ; old seg metadata . set time unit ( time unit . milliseconds ) ; old seg metadata . set index version ( segment metadata . get version ( ) ) ; old seg metadata . set total raw docs ( segment metadata . get total raw docs ( ) ) ; old seg metadata . set partition metadata ( get partition metadata from segment metadata ( segment metadata ) ) ; final znrecord old zn record = old seg metadata . to znrecord ( ) ; final string old znode path = zkmetadata provider . construct property store path for segment ( realtime table name , committing segment name str ) ;
show progress dialog ( get string ( r . string . progress _ auth ) ) ;
test utils . set field ( init called , new rack resolver ( ) , false ) ;
class loader loader1 = new subverted class loader ( new url [ ] { get class ( ) . get resource ( ) } , this . get class ( ) . get class loader ( ) ) ; class cheese class = loader1 . load class ( org . drools . compiler . cheese ) ; knowledge builder configuration kbuilder conf = knowledge builder factory . new knowledge builder configuration ( null , loader1 ) ;
bits . set ( work set , 0 ) ;
job runnable = concurrent list jobs ( 5 , config , false , false , list job helper . get delayed resonse answer ( 4 , new array list < job item bean > ( ) ) ) ;
quota . set enabled ( true ) ; quota . set quota store ( jdbc ) ; jdbc = new jdbcconfiguration ( ) ; jdbc . set dialect ( h2 ) ; connection pool configuration pool = new connection pool configuration ( ) ; pool . set driver ( org . h2 . driver ) ; pool . set url ( jdbc : h2 : . target quota - h2 ) ; pool . set username ( sa ) ; pool . set password ( ) ; pool . set min connections ( 1 ) ; pool . set max connections ( 1 ) ; pool . set max open prepared statements ( 50 ) ; jdbc . set connection pool ( pool ) ; gwc . save disk quota config ( quota , jdbc ) ; jdbc config file = dd . find file ( gwc geowebcache - diskquota - jdbc . xml ) ; assert not null ( jdbc config should be there , jdbc config file ) ; assert null ( jdbc store should be there , dd . find data file ( gwc diskquota _ page _ store _ h2 ) ) ; file new quota store = new file ( . target quota - h2 . data . db ) ; assert true ( new quota store . exists ( ) ) ; file input stream fis = null ;
package names = filter unwanted package ( org . apache . camel . core , package names ) ;
return stage type . repl _ dump ; }
message msg = get in ( ) ;
map < long , object > run ids map = flow activity column prefix . run _ id . read results ( result , long key converter ) ; for ( map . entry < long , object > e : run ids map . entry set ( ) ) { long run id = e . get key ( ) ; string version = ( string ) e . get value ( ) ; flow run entity flow run = new flow run entity ( ) ; flow run . set user ( user ) ; flow run . set name ( flow name ) ; flow run . set run id ( run id ) ; flow run . set version ( version ) ; set the id flow run . set id ( flow run . get id ( ) ) ; flow activity . add flow run ( flow run ) ; } flow activity . get info ( ) . put ( timeline reader utils . fromid _ key , row key . get row key as string ( ) ) ; return flow activity ;
if ( file = null ) { convert status . source ( ajax status . file ) ; result = file get ( url , file , status ) ; if result is ok if ( result = null ) { status . time ( new date ( file . last modified ( ) ) ) . done ( ) ; } }
instance = null ; instance initialized = false ; if ( is jsp servlet & & jsp monitor on = null ) { registry . get registry ( null , null ) . unregister component ( jsp monitor on ) ; }
rm1 . submit app ( 200 , app1 , user1 , null , default , false ) ; wait for ( rm1 , haservice state . standby ) ;
for ( onested projection item item : include items ) { string alias = item . alias = null ? item . alias . get string value ( ) : item . expression . get default alias ( ) . get string value ( ) ; oresult internal elem = new oresult internal ( ) ; input . entry set ( ) . for each ( x - > elem . set property ( x . get key ( ) , x . get value ( ) ) ) ; object value = item . expression . execute ( elem , ctx ) ; if ( item . expansion = null ) { value = item . expand ( expression , alias , value , ctx , recursion - 1 ) ; } result . set property ( alias , convert ( value ) ) ; }
if ( pat idx end = str idx end ) { return false ; pattern and string do not have the same size } for ( int i = 0 ; i < = pat idx end ; i + + ) { ch = pat arr [ i ] ; if ( ch = ' ? ' ) { if ( different ( case sensitive , ch , str arr [ i ] ) ) { return false ;
if ( column long list realm list = null ) { return column long list realm list ; } else { os list os list = proxy state . get row realm ( ) . get value list ( column info . column long list index , realm field type . integer _ list ) ; column long list realm list = new realm list < java . lang . long > ( java . lang . long . class , os list , proxy state . get realm realm ( ) ) ; return column long list realm list ; } }
assert . assert false ( offset . equals ( sub ) ) ;
return find child by type ( kt tokens . when _ keyword ) ; }
synchronized ( this ) { notify all ( ) ; find bugs nn _ naked _ notify } long time = environment edge manager . current time millis ( ) - start time ;
assert equals ( byte order . big _ endian , b . wrap ( new byte [ 10 ] ) . order ( ) ) ;
string xml = get as string ( rest base controller . root _ path + workspaces wcs coveragestores blue marble . xml ) ;
count + + ;
for ( int i = 0 ; i < arg descs . length ; i + + ) { field visitor fv = cw . visit field ( acc _ private + acc _ final , arg names [ i ] , arg descs [ i ] , null , null ) ; fv . visit end ( ) ; } generate constructor ( ) ;
wait for query state ( runner , second query , failed ) ; }
data access data access after = get catalog ( ) . get data store by name ( sf , sf ) . get data store ( null ) ; assert same ( resource pool data store cache check 1 , data access before , data access after ) ; assert same ( resource pool data store cache , data access before , get catalog ( ) . get resource pool ( ) . get data store cache ( ) . get ( data store id ) ) ; string update native bounds = < feature type > + < srs > epsg : 3785 < srs > + < native bounding box > + < minx > - 20037508 . 34 < minx > + < maxx > 20037508 . 34 < maxx > + < miny > - 20037508 . 34 < miny > + < maxy > 20037508 . 34 < maxy > + < crs > epsg : 3785 < crs > + < native bounding box > + < feature type > ;
update dependencies ( module , configuration , orm provider , jdbc database , starters xpath , providers xpath , database xpath , profile ) ;
final element resources element = build element . get single child ( resources ) ; for ( element resource : resources element . get children ( ) ) { resource . remove ( ) ; }
system . arraycopy ( prev key , 0 , key , 0 , shared ) ;
user service . modify roles and user admin privileges ( arrays . as list ( user - 1 ) , new tri state selection ( admin . go _ system _ admin , tri state selection . action . nochange ) , arrays . as list ( new tri state selection ( dev , tri state selection . action . remove ) ) , new http localized operation result ( ) ) ;
if ( m tick radius > half right ) { for ( int i = 0 ; i < = m num segments ; i + + ) { float x = start right - i * m tick distance ; if ( x < = thumb x ) break ; canvas . draw circle ( x , thumb y , m tick radius , paint ) ; } }
array list < string > clock apps = new array list < > ( ) ; clock apps . add ( com . google . android . deskclock com . android . deskclock . desk clock ) ; stock clock apps . add ( com . sec . android . app . clockpackage com . sec . android . app . clockpackage . clock package ) ; samsung clock apps . add ( com . android . deskclock com . android . deskclock . desk clock tab activity ) ; miui return clock apps . contains ( component name . flatten to string ( ) ) ; }
try { sample . one declared ( ) ; fail ( ) ; } catch ( some chaining exception expected ) { }
assert equals ( 6 * 1024 , lfs . lookaside cache . get cache size ( ) ) ;
check in nanos ( reservoir , now + 11 , collision _ buffer * 2 + 1 , 10 , 10 , 10 ) ;
cal . clear ( ) ; cal . set ( calendar . era , gregorian calendar . bc ) ; cal . set ( calendar . year , 2 ) ; round trip ( cal ) ; assert equals ( - 0001 , tree . to string ( cal ) ) ;
request . get body ( ) . write ( 32 ) ; } future < client http response > future response = request . execute async ( ) ; response = future response . get ( ) ; assert equals ( invalid response status , http status . ok , response . get status code ( ) ) ; assert equals ( invalid method , path . to upper case ( locale . english ) , request . get method ( ) . name ( ) ) ; }
assert that ( paired stats . from byte array ( paired stats byte array ) ) . is equal to ( paired stats ) ; }
type coercer < ? > coercer = type coercer factory . type coercer for type ( type ) ;
try { tex = new texture ( util . get resource ( res grass . png ) , texture . nearest ) ; } catch ( ioexception e ) { throw new runtime exception ( couldn ' t decode texture ) ; }
if ( layout = null ) { try { layout . paint ( graphic , paint area , map content ) ; } catch ( exception e ) { throw new service exception ( problem occurred while trying to watermark data , e ) ; } } timeout . stop ( ) ;
delete replica ( 0 , m _ cache . point in time cache ( ) . get ( 0 ) ) ; while ( volt db . was crash called ) { thread . sleep ( 0 ) ; } }
rule dto rule1 = db client . rule dao ( ) . select or fail by key ( db tester . get session ( ) , default organization , rule key . of ( fake , rule ) ) ; assert that ( rule1 . get name ( ) ) . is equal to ( name2 ) ; assert that ( rule1 . get description ( ) ) . is equal to ( description ) ; assert that ( rule index . search ( new rule query ( ) . set query text ( name2 ) , new search options ( ) ) . get total ( ) ) . is equal to ( 1 ) ; assert that ( rule index . search ( new rule query ( ) . set query text ( name1 ) , new search options ( ) ) . get total ( ) ) . is equal to ( 0 ) ; }
map = catalog . get topic names ( ) ; assert equals ( 4 , map . size ( ) ) ; assert equals ( genre , map . get ( long . value of ( 324846099 ) ) ) ;
if ( entry = = null | | entry . is low res icon | | entry . icon = = null ) { entry = null ; }
parse ( immutable list . of ( name clash a . class , name clash b . class ) , - + name clash b . class . get canonical name ( ) + . string = blah ) ; }
resource am if started = resources . add ( application . get amresource ( partition name ) , queue usage . get amused ( partition name ) ) ; if ( log . is debug enabled ( ) ) { log . debug ( application + application . get id ( ) + amresource + application . get amresource ( partition name ) + max amresource per queue percent + max amresource per queue percent + am limit + am limit + last cluster resource + last cluster resource + am if started + am if started + am node - partition name + partition name ) ; }
for ( int i = rstart + 1 - y + len ; i < rstart + 1 ; i + + ) result [ i ] = 0 ; value = result ;
admin . disable table ( test _ table ) ; assert true ( admin . is table disabled ( test _ table ) ) ;
sink plan node sink node = o plan . get data sinks ( ) . iterator ( ) . next ( ) ; source plan node source node = ( source plan node ) sink node . get predecessor ( ) ; global properties gprops = source node . get global properties ( ) ;
stage . set pipeline id ( 2l ) ; pipeline dao . stage status changed ( stage ) ; pipeline instance models models = pipeline dao . load active pipelines ( ) ;
psi method psi method = light class util . instance . get light class method ( jet function ) ; check declaration method wrapped ( should be wrapped , jet function , psi method ) ;
scan = new scan ( ) ; scan . add column ( families [ 4 ] , qualifiers [ 0 ] ) ; scan . add column ( families [ 4 ] , qualifiers [ 4 ] ) ; result = get single scan result ( ht , scan ) ; assert double result ( result , rows [ 0 ] , families [ 4 ] , qualifiers [ 0 ] , values [ 0 ] , families [ 4 ] , qualifiers [ 4 ] , values [ 4 ] ) ;
tester . start page ( new workspace edit page ( cite workspace ) ) ; tester . assert rendered page ( workspace edit page . class ) ; tester . assert no error message ( ) ;
final boolean not visible = m orientation helper . get decorated start ( reference child ) > = m orientation helper . get end after padding ( ) | | m orientation helper . get decorated end ( reference child ) < m orientation helper . get start after padding ( ) ;
ordinal cache = new lruhash map < > ( default _ cache _ value ) ; category cache = new lruhash map < > ( default _ cache _ value ) ; }
map < string , string > kvp = new linked hash map < string , string > ( ) ; kvp . put ( service , wps ) ; kvp . put ( version , 1 . 0 . 0 ) ; kvp . put ( request , get execution result ) ; kvp . put ( execution id , get execution id ( execution id ) ) ; kvp . put ( output id , name ) ; kvp . put ( mimetype , mime type ) ; if ( base url = = null ) { operation op = dispatcher . request . get ( ) . get operation ( ) ; execute type execute = ( execute type ) op . get parameters ( ) [ 0 ] ; base url = execute . get base url ( ) ; } string url = response utils . build url ( base url , ows , kvp , urltype . service ) ; return url ;
input stream is = cryptopia adapters test . class . get resource as stream ( marketdata example - ticker - data . json ) ;
if ( initializations = = null ) { initializations = new int [ 2 ] ; } int n = initializations . length ; if ( initialization count > = n ) { int [ ] t = new int [ math . max ( initialization count + 1 , 2 * n ) ] ; system . arraycopy ( initializations , 0 , t , 0 , n ) ; initializations = t ; }
get elements get elements = new get elements . builder ( ) . input ( new entity seed ( a ) , new entity seed ( x ) ) . in out type ( include incoming outgoing type . either ) . build ( ) ;
data input stream in = new data input stream ( new byte array input stream ( buffer ) ) ;
if ( cert status . get cert status ( ) = = cert status . unrevoked & & reasons mask . is all reasons ( ) ) { try { * * assume a dp with both the reasons and the c rlissuer fields * omitted and a distribution point name of the certificate * issuer . * derobject issuer = null ; try { issuer = new asn1 input stream ( cert path validator utilities . get encoded issuer principal ( cert ) . get encoded ( ) ) . read object ( ) ; } catch ( exception e ) { throw new annotated exception ( issuer from certificate for crl could not be reencoded . , e ) ; } distribution point dp = new distribution point ( new distribution point name ( 0 , new general names ( new general name ( general name . directory name , issuer ) ) ) , null , null ) ; extended pkixparameters params pkixclone = ( extended pkixparameters ) params pkix . clone ( ) ; check crl ( dp , params pkixclone , cert , valid date , sign , working public key , cert status , reasons mask , cert path certs ) ; valid crl found = true ; } catch ( annotated exception e ) { last exception = e ; } } if ( valid crl found ) { if ( last exception instanceof annotated exception ) { throw last exception ; } throw new annotated exception ( no valid crl found . , last exception ) ; }
request limit config = new request limit config ( max active requests , request threshold percent , node limit percent ) ;
gbm = new gbm ( parms ) . train model ( ) . get ( ) ; model metrics binomial mm = ( model metrics binomial ) gbm . _ output . _ training _ metrics ;
immutable map . builder < symbol , aggregation > aggregations = immutable map . builder ( ) ;
if ( is data format option ) { selected . put ( name , data format options . get ( name ) ) ; } else { selected . put ( name , new string [ ] { name , kind , label , required , type , java type , deprecated , secret , value , default value , description } ) ; }
if ( this . series list . contains ( series ) ) { series . remove change listener ( this ) ; this . series list . remove ( series ) ; fire dataset changed ( ) ; }
ts = conf . get long ( import tsv . timestamp _ conf _ key , 0 ) ; skip bad lines = context . get configuration ( ) . get boolean ( import tsv . skip _ lines _ conf _ key , true ) ; bad line count = context . get counter ( import tsv , bad lines ) ; }
bit set . clear ( 0 , batch index + 1 ) ;
water . util . log . debug ( using model metrics from the cache . . . ) ;
final object key ;
room . remove chat room member ( user ) ;
table i + + ; row i = 0 ; assert equals ( 1 , results [ table i ] . get row count ( ) ) ; final volt table row row lt0 = results [ table i ] . fetch row ( row i + + ) ; assert equals ( - 1 , row lt0 . get long ( 0 ) ) ; assert equals ( 0 , row lt0 . get long ( 1 ) ) ; truncate tables ( new string [ ] { bingo _ board } ) ;
parent = ( red black node < t > ) node . parent ;
if ( scheduler = = null ) { create and init scheduler ( ) ; } else { in case custom scheduler was injected ( i . e . created elsewhere ) , we may need to add current camel context to quartz context so jobs have access store camel context in quartz context ( ) ; }
assert that ( memo . apply ( ) ) . is equal to ( expected ) ;
long bytes = total postings * 8 + total payload bytes ; segment write state write state = new segment write state ( null , dir , segment info , new field infos , null , new iocontext ( new flush info ( max doc , bytes ) ) ) ;
final hregion location loc = new hregion location ( hri , sn . get hostname ( ) , sn . get port ( ) ) ; mockito . when ( c . get region location ( ( byte [ ] ) mockito . any ( ) , ( byte [ ] ) mockito . any ( ) , mockito . any boolean ( ) ) ) . then return ( loc ) ; mockito . when ( c . locate region ( ( byte [ ] ) mockito . any ( ) , ( byte [ ] ) mockito . any ( ) ) ) . then return ( loc ) ; if ( implementation = null ) { if a call to get hregion connection , return this implementation . mockito . when ( c . get hregion connection ( mockito . any string ( ) , mockito . any int ( ) ) ) . then return ( implementation ) ; } return c ;
sort fields . add ( new sort field ( id , sort field . type . int ) ) ;
final long est out ship size = estimates . get estimated output size ( ) ;
assert true ( admin client . put store client config string ( config avro1 , admin _ url ) ) ;
context . send ordered broadcast ( intent , null * permission * , result receiver , scheduler , initial code , initial data , initial extras ) ;
cipher cipher = encryption . get cipher ( conf , key . get algorithm ( ) ) ;
if ( _ input ptr > = _ input end ) { load more guaranteed ( ) ; }
final path bar1 _ dir2 = new path ( foo _ dir2 , bar1 ) ; hdfs . set replication ( bar1 _ dir2 , repl _ 1 ) ; hdfs . set replication ( bar _ dir2 , repl _ 1 ) ;
return new val frame ( res ) ; }
instructions . add ( reil helpers . create jcc ( offset + 14 , operand size . byte , 1 , operand size . address , jmp goal write back ) ) ;
public void test tlsclient cert not required pem ( ) throws exception { test tls ( cert . client _ jks , trust . server _ jks , cert . server _ pem , trust . client _ jks ) . pass ( ) ;
pattern pattern1 = verifier component mock factory . create pattern1 ( ) ; restriction r1 = literal restriction . create restriction ( pattern1 , ) ;
assert buffer result equals ( types , get buffer result ( buffer , first , 0 , size of buffered pages ( 10 ) , no _ wait ) , buffer result ( 0 , create page ( 0 ) , create page ( 1 ) , create page ( 2 ) ) ) ;
ui service = new uiservice impl ( ) ;
long [ ] vals = new long [ nrows ] ;
object [ ] array = ( object [ ] ) bag ; int l = array . length ; if ( l < 2 ) throw new illegal argument exception ( ) ; if ( index = = l ) return null ; return array [ index ] ; }
swift utils . debug ( log , path ' % s ' is no longer present ; continuing , entry path ) ; } throttle ( ) ; }
swing utilities . invoke later ( ( ) - > { my mouse adapter . mouse clicked ( me ) ; } ) ; } return true ; } }
entity binder . bind discriminator value ( ) ;
this . m delegate . write long ( v ) ;
input stream is = bitfinex wallet jsontest . class . get resource as stream ( v1 account example - account - info - balance . json ) ;
string counts . get ( string fields [ i ] ) . put ( metadata value , string counts . get ( string fields [ i ] ) . get ( metadata value ) + 1 ) ;
array list < string > ls ;
a = wrapped buffer ( order , new byte [ ] { 1 , 2 , 3 } ) ; b = wrapped buffer ( wrapped buffer ( order , new byte [ ] { 0 , 1 , 2 , 3 , 4 } , 1 , 3 ) ) ;
filter restricted street = ff . not ( ff . like ( ff . property ( address ) , * restricted street * , * , ? , \ \ ) ) ;
try { url str = normalizers . normalize ( url str , urlnormalizers . scope _ fetcher ) ; url str = url filters . filter ( url str ) ; filter the url } catch ( exception e ) { if ( log . is warn enabled ( ) ) { log . warn ( skipping + url + : + e ) ; } url str = null ; }
int left aligned x = m temp rect . left + icon . get padding left ( ) ;
block allocation table reader . sanity check block count ( _ header . get batcount ( ) ) ;
expect throws ( illegal argument exception . class , ( ) - > { query parser util . parse ( queries5 , fields , new mock analyzer ( random ( ) ) ) ; } ) ;
attributes editor . add attribute ( new constant value attribute ) ;
ps tspace [ i dst offs + i dst vert ] = ps tspace [ i src offs + i src vert ] ;
result . add all ( expand primitive scenario ( group ( prim ( full ) ) , prim ( full ) , portable . primitive utf _ , parent ) ) ;
first context . near cache adapter . clear ( ) ;
cluster . add name node ( conf ) ;
graph . commit ( ) ;
quads . add ( new switch test quad ( 4 15 2013 , m d yyyy , 8 10 2013 , 4 15 2013 ) ) ; quads . add ( new switch test quad ( 4 15 2013 , \ dddd , mmmm dd , yyyy \ , monday , april 15 , 2013 ) ) ; quads . add ( new switch test quad ( 4 15 2013 , \ mmmm d , yyyy \ , april 15 , 2013 ) ) ;
for ( meta data store . position position : meta data store . position . values ( ) ) { log provider . assert contains message containing ( position . name ( ) ) ; }
assert rows ( execute ( select a , b , s , count ( b ) , count ( s ) from % s where a = 1 group by a , b limit 2 ) , row ( 1 , null , 1 , 0 l , 1 l ) ) ;
enumeration enu = data . enumerate instances ( ) ; while ( enu . has more elements ( ) ) { instance i = ( instance ) enu . next element ( ) ; if ( i . is missing ( attr ) ) { missing value counts [ ( int ) i . class value ( ) ] + + ; } else { counts [ ( int ) i . value ( attr ) ] [ ( int ) i . class value ( ) ] + + ; } } one rrule r = new one rrule ( data , attr ) ; create a new rule
system . out . println ( = = = = = = = = = = = fullscreen mode = = = = = = = = = = = = = = ) ; for ( int i = 0 ; i < 2 ; i + + ) { system . out . println ( test + ( i + 3 ) + : ) ; wiggle mouse ( ) ; system . out . println ( ) ; } system . out . println ( test completed successfully ) ;
assert down edge ( cfg , token . case , token . block , branch . on _ true ) ;
load test data generator data gen = new multi threaded action . default data generator ( min col data size , max col data size , min cols per key , max cols per key , hfile test util . default _ column _ family ) ; multi threaded writer writer = new multi threaded writer ( data gen , conf , table _ name ) ; writer . set multi put ( true ) ; writer . start ( start key , end key , num threads ) ; system . out . printf ( started loading data . . . ) ; writer . wait for finish ( ) ; system . out . printf ( finished loading data . . . ) ; }
final int [ ] pixel mirrored array = new int [ width * height ] ; m gpuimage . run on glthread ( new runnable ( ) { @ override public void run ( ) { final int buffer pixel buffer = int buffer . allocate ( width * height ) ; gles20 . gl read pixels ( 0 , 0 , width , height , gles20 . gl _ rgba , gles20 . gl _ unsigned _ byte , pixel buffer ) ; int [ ] pixel array = pixel buffer . array ( ) ;
for ( int i = 0 ; i < src parity list . size ( ) ; i + + ) { codec codec = used codec . get ( i ) ; if ( codec . is dir raid & & src stat . is dir ( ) ) { if we just rename one file in a dir - raided directory , and we also have other files in the same directory , we will keep the parity file . path parent = src . get parent ( ) ; if ( fs . list status ( parent ) . length > 1 ) { continue ; } } path src parity path = src parity list . get ( i ) ; path dest parity path = dest parity list . get ( i ) ; fs . mkdirs ( dest parity path . get parent ( ) ) ; fs . rename ( src parity path , dest parity path ) ; } return true ;
kml kml = new kml ( ) ;
assert xpath evaluates to ( - 1 . 3 53 . 5 - 1 . 3 53 . 6 - 1 . 2 53 . 6 - 1 . 2 52 . 5 - 1 . 3 53 . 5 , gsml : mapped feature [ @ gml : id = ' gsml . mappedfeature . mf4 ' ] gsml : shape gml : polygon gml : exterior gml : linear ring gml : pos list , doc ) ;
string rule1 = package com . sample \ n + rule \ rule 1 \ \ n + salience 10 \ n + when \ n + l : java . util . list ( ) \ n + then \ n + l . add ( \ rule 1 executed \ ) ; \ n + end \ n ; string rule2 = package com . sample \ n + global string str ; \ n + rule \ rule 2 \ \ n + when \ n + l : java . util . list ( ) \ n + then \ n + l . add ( \ rule 2 executed \ + str ) ; \ n + end \ n ; string reader [ ] readers = new string reader [ 2 ] ;
for ( file subfile : file . list files ( ) ) { string subname = subfile . get name ( ) ; if ( subname . contains ( atci ) ) { add = true ; file = subfile ; } }
cluster = new mixed pause cluster ( new string [ ] { , paused , } ) ; assert true ( cluster . start ( ) ) ; client = client factory . create client ( ) ;
assert equals ( 1 , new counter . get ( ) ) ; assert equals ( 0 , counter . get ( ) ) ; }
new image updater ( ) . run ( sources ) ;
val vals [ ] = new val [ asts . length ] ; vec vec = null ; for ( int i = 1 ; i < asts . length ; i + + ) { vals [ i ] = stk . track ( asts [ i ] . exec ( env ) ) ; if ( vals [ i ] . is frame ( ) ) { vec anyvec = vals [ i ] . get frame ( ) . any vec ( ) ; if ( anyvec = = null ) continue ; ignore the empty frame if ( vec = = null ) vec = anyvec ; else if ( vec . length ( ) = anyvec . length ( ) ) throw new illegal argument exception ( cbind frames must have all the same rows , found + vec . length ( ) + and + anyvec . length ( ) + rows . ) ; } } boolean clean = false ;
assert true ( multi . get name ( ) . equals ( new name ) ) ; multi . set name ( newer name ) ; sm = ( sub multi ) s . load ( sub multi . class , sm id ) ; assert true ( sm . get amount ( ) = = 456 . 7f ) ; sm . set amount ( 23423f ) ; t . commit ( ) ; s . close ( ) ; s = open session ( ) ;
contact header . set expires ( registrations expiration ) ; request . set header ( contact header ) ;
string layout name = string . format ( locale . english , formatted _ layout _ res _ with _ hosteat , ( int ) grid . num columns , ( int ) grid . num rows , ( int ) grid . num hotseat icons ) ;
context = value ;
i = binary search ( m keys , 0 , m size , key ) ; } if ( m size > = m keys . length ) { int n = array utils . ideal int array size ( m size + 1 ) ; int [ ] nkeys = new int [ n ] ; object [ ] nvalues = new object [ n ] ;
assert count ( 1 , tx . query ( ) . has ( time , 5 ) . vertices ( ) ) ;
if ( finder = = null ) { throw new illegal argument exception ( finder is null ) ; } list < layout helper > helpers = new linked list < > ( ) ;
iter = op set server stored contact info . get details ( tester agent contact , server stored details . country detail . class ) ;
try { delete partition column statistics ( db name , table name , part name , part . get values ( ) , null ) ; } catch ( no such object exception e ) { log . info ( no column statistics records found to delete ) ; } pre drop storage descriptor ( part . get sd ( ) ) ;
float fixed distance y = has view pager x ( ) ? 0f : dy ; return super . on scroll ( e1 , e2 , fixed distance x , fixed distance y ) ;
assert equals ( 4 , count delete markers ( region ) ) ;
return label ;
c = get random ( ' \ u2000 ' , ' \ u206 f ' ) ; vis . add character to current ( c ) ; } else if ( c > = ' \ u2070 ' & & c < = ' \ u209 f ' ) {
c = new chunk ( chunk size ) ;
add intermediate operation map ( new map to long int operation ( mapper ) ) ; return long cache stream ( ) ;
math expressions . cot ( num ) ;
context . get event handler ( ) . handle ( new psattempt state update event ( ps attempt id , params map ) ) ;
exchange . get in ( ) . set header ( exchange . file _ name , existing ) ;
try { new schema . builder ( ) . edge ( invalid group string , edge def ) . build ( ) ; fail ( exception expected ) ; } catch ( final illegal argument exception e ) { assert true ( e . get message ( ) . contains ( group is invalid ) ) ; }
bits . write string ( t . get message ( ) , out ) ;
private void swim ( int i ) { if ( i > 0 & & greater ( ( i - 1 ) d , i ) ) { exch ( i , ( i - 1 ) d ) ; swim ( ( i - 1 ) d ) ; } }
server instance error key = new server instance ( localhost : + num success futures ) ; exception expected exception = new exception ( exception ) ; async response future < string > f = ( async response future < string > ) future map . get ( error key ) ; f . on error ( expected exception ) ; runner . wait for done ( ) ; assert . assert false ( runner . is cancelled ( ) , composite cancelled ? ) ;
if ( properties = = null | | properties . size ( ) = = 0 ) { return ; } float home lat = float . parse float ( get optional property ( properties , home . lat , 0 ) ) ;
_ xml list = lib . new xmllist ( ) ; }
new long field ( _ signature _ offset , _ signature , _ data ) ; new integer field ( 0x08 , 0 , _ data ) ; new integer field ( 0x0c , 0 , _ data ) ; new integer field ( 0x10 , 0 , _ data ) ; new integer field ( 0x14 , 0 , _ data ) ; new short field ( 0x18 , ( short ) 0x3b , _ data ) ; new short field ( 0x1a , ( short ) 0x3 , _ data ) ; new short field ( 0x1c , ( short ) - 2 , _ data ) ; new short field ( 0x1e , big block size . get header value ( ) , _ data ) ;
temp file dir = server . create temporary filename ( ) . get parent file ( ) ; assert true ( temp file dir . set executable ( true , false ) ) ; assert true ( temp file dir . set readable ( true , false ) ) ; assert true ( temp file dir . set writable ( false , false ) ) ; byte [ ] data = new byte [ 2000000 ] ;
out = response . serialize ( out , xid , new verifier none ( ) ) ; channel buffer buf = channel buffers . wrapped buffer ( out . as read only wrap ( ) . buffer ( ) ) ; rpc response rsp = new rpc response ( buf , info . remote address ( ) ) ; if ( is idempotent ( rpc call ) ) { rpc call cache . call completed ( client , xid , rsp ) ; }
assert page equals ( table writer operator . types , operator . get output ( ) , row pages builder ( table writer operator . types ) . row ( 2 , null ) . build ( ) . get ( 0 ) ) ; assert equals ( operator . is blocked ( ) . is done ( ) , true ) ; assert equals ( operator . is finished ( ) , true ) ; assert equals ( operator . needs input ( ) , false ) ;
x = 3 ;
my map . put ( property keys . http _ lb _ initial _ recovery _ level , 0 . 005 ) ;
try { perform http call ( domain test support . slave address , 8080 ) ; fail ( webapp still accessible following undeploy ) ; } catch ( ioexception good ) { desired result } }
if ( get stream id ( ) = = 0 ) return connection failure ( buffer , error code . protocol _ error . code , invalid _ rst _ stream _ frame ) ;
assert true ( small cache . get cache manager ( ) . equals ( large cache . get cache manager ( ) ) ) ;
sub slot = null ; } } if ( sub slot = null ) {
fs wrapper . delete ( shell . get current trash dir ( snapshottable child ) , true ) ;
int [ ] tmp = new int [ nb children + 1 ] ; tmp [ nb children ] = ( int ) var ; system . arraycopy ( var list , 0 , tmp , 0 , nb children ) ; var list = tmp ; nb children + + ; snmp mib node . sort ( var list ) ; int new pos = retrieve index ( var ) ; var list [ new pos ] = ( int ) var ; if ( ( cursor + 1 ) = = oid . length ) {
seg . get off heap buffer ( ) ;
svgrect rect = ( ( svglocatable ) outline elem ) . get bbox ( ) ;
if ( el = null ) { el . set attribute node ( at ) ; }
odatabase document db server2 = get database ( 1 ) ; try { assert not null ( db server2 ) ; list < odocument > result = db server2 . query ( new osqlsynch query < oidentifiable > ( select from person ) ) ; assert equals ( 0 , result . size ( ) ) ; } catch ( exception e ) { e . print stack trace ( ) ; fail ( ) ; } finally { db server2 . close ( ) ; } odatabase document db server3 = get database ( 2 ) ;
resultbs = bs . get ( 15 , 125 ) ;
count + + ;
file position result res = pos setter . locate file position ( 100 , finder ) ; assert . assert equals ( res . get status ( ) , file position result . status . error , result status for scn : + 100 + , result : + res ) ;
facets author = new fast taxonomy facet counts ( author , taxo reader , config , fc ) ; results . add ( author . get top children ( 10 , author ) ) ; facets pub date = new fast taxonomy facet counts ( pubdate , taxo reader , config , fc ) ;
enumeration < header > headers = msg . get all headers ( ) ;
errors . add ( format ( job hash contains colon in job id [ % s ] . , recomputed id ) ) ;
if ( is custom font ) space item text . set typeface ( custom font ) ;
process outputs type outputs = wpsf . create process outputs type ( ) ;
if ( position = = null ) { return ; }
check false ( receiver . type . is udt ( ) & & receiver . type . is multi cell ( ) , non - frozen udt column ' % s ' ( % s ) cannot be restricted by any relation , receiver . name , receiver . type . as cql3 type ( ) ) ; if ( receiver . type . is collection ( ) ) { we don ' t support relations against entire collections ( unless they ' re frozen ) , like numbers = { 1 , 2 , 3 } check false ( receiver . type . is multi cell ( ) & & is legal relation for non frozen collection ( ) , collection column ' % s ' ( % s ) cannot be restricted by a ' % s ' relation , receiver . name , receiver . type . as cql3 type ( ) , operator ( ) ) ; if ( is contains key ( ) | | is contains ( ) ) { receiver = make collection receiver ( receiver , is contains key ( ) ) ; } else if ( receiver . type . is multi cell ( ) & & map key = null & & is eq ( ) ) { list < column specification > receivers = new array list < > ( 2 ) ; receivers . add ( make collection receiver ( receiver , true ) ) ; receivers . add ( make collection receiver ( receiver , false ) ) ; return receivers ; } }
assert equals ( orc metadata reader . to string statistics ( orc _ hive _ 8732 , orc proto . string statistics . new builder ( ) . set minimum ( ant ) . set maximum ( cat ) . set sum ( 79 ) . build ( ) , true ) , new string statistics ( utf8 slice ( ant ) , utf8 slice ( cat ) , 79 ) ) ; for ( slice prefix : all _ utf8 _ sequences ) { for ( int test code point : test _ code _ points ) { slice code point = code point to utf8 ( test code point ) ; for ( slice suffix : all _ utf8 _ sequences ) { slice test value = concat slice ( prefix , code point , suffix ) ; test string statistics truncation ( test value , original ) ; test string statistics truncation ( test value , orc _ hive _ 8732 ) ; } } }
if ( log target = = null ) log target = system . getenv ( vespa _ log _ target ) ; if ( log service = = null ) log service = system . getenv ( vespa _ service _ name ) ; if ( log control dir = = null ) log control dir = system . getenv ( vespa _ log _ control _ dir ) ; if ( log control file = = null ) log control file = system . getenv ( vespa _ log _ control _ file ) ; if ( log level = = null ) log level = system . getenv ( vespa _ log _ level ) ;
m recycler view . set no more ( true ) ; } } } ) ; m recycler view . refresh ( ) ; }
if ( this . s width > 0 & & this . s height > 0 & & ( this . s width = bitmap . get width ( ) | | this . s height = bitmap . get height ( ) ) ) { reset ( false ) ; } if ( this . bitmap = null & & this . bitmap is cached ) { this . bitmap . recycle ( ) ; } if ( this . bitmap = null & & this . bitmap is cached & & on image event listener = null ) { on image event listener . on preview released ( ) ; }
optional < string > location = optional . empty ( ) ; for ( entry < key , value > entry : scan ) { if ( location . is present ( ) ) { throw new presto exception ( function _ implementation _ error , scan for default tablet returned more than one entry ) ; } location = optional . of ( entry . get value ( ) . to string ( ) ) ; } scan . close ( ) ;
put put = new put ( value ) ;
assert equals ( results . length , result values . length ) ; kk = 0 ; for ( function test case result : results ) { int id index = kk + + ; if ( kk = = rowcount ) { kk = 0 ; } double expected = ordered ids [ id index ] ; if ( expected = result . m _ result ) { complain ( failed + result . m _ case + expected + expected + got + result . m _ result ) ; } assert equals ( expected , result . m _ result ) ; } results = where function run ( client , num type names [ jj ] + _ cast , filters , num format names [ jj ] ) ;
if ( random boolean ( ) ) { final string duplicate value = deprecation logger . format warning ( qux ) ; thread context . add response header ( baz , duplicate value , deprecation logger : : extract warning value from warning header ) ; } thread context . add response header ( warning , one is the loneliest number ) ; thread context . add response header ( warning , two can be as bad as one ) ; if ( expect third ) { thread context . add response header ( warning , no is the saddest experience ) ; } final map < string , list < string > > response headers = thread context . get response headers ( ) ; final list < string > foo = response headers . get ( foo ) ;
sender exchange . get in ( ) . set header ( cxf constants . operation _ name , echo ) ; exchange exchange = template . send ( direct : endpoint c , sender exchange ) ; org . apache . camel . message out = exchange . get out ( ) ;
if ( listener = null ) { listener . search stats ( stats ) ; }
throw new service exception ( query _ layers contains layers not cited in layers . + it should be a proper subset of those instead ) ;
google sign in helper sign in helper = google sign in helper . get instance ( activity ) ; firebase user firebase user = firebase auth . get instance ( ) . get current user ( ) ;
wait until queue empty ( scheduler ) ; thread . sleep ( 100 ) ; assert equals ( none of these calls should have been discarded , 0 , scheduler . get num general calls dropped ( ) ) ; env edge . offset = 2000 ;
collection . remove all ( arrays . as list ( item function . apply ( 5 ) , item function . apply ( 10 ) , item function . apply ( 23 ) , item function . apply ( 18 ) , item function . apply ( 105 ) ) ) ; assert equals ( 96 , collection . size ( ) ) ; collection . remove all ( arrays . as list ( item function . apply ( 5 ) , item function . apply ( 890 ) ) ) ; assert equals ( 96 , collection . size ( ) ) ; }
string subject = mmp . get subject ( ) = null ? mmp . get subject ( ) . get string ( ) : null ;
do test header limits ( 10 , 512 * 1024 , failure mode . connection _ reset ) ;
current object = null ;
string expected query string = null ;
string string = abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz ;
if ( r _ double ( ) ) { return false ; } break ; }
cr . start ( ) ;
ret = - 1 ;
manage ipsec vpn ( srx cmd , null , account id , guest network cidr , user . get username ( ) , _ ipsec policy name ) ;
j panel = new jpanel ( ) ; j panel . set preferred size ( new java . awt . dimension ( 800 , 600 ) ) ; j panel . set layout ( new grid bag layout ( ) ) ; j tabbed = new jtabbed pane ( ) ;
assert equals and hash code ( new immutable worker info ( new worker ( http , test worker1 , 192 . 0 . 0 . 1 , 10 , v1 ) , 3 , immutable set . of ( grp1 , grp2 ) , immutable set . of ( task1 , task2 ) , date times . of ( 2015 - 01 - 01 t01 : 01 : 01 z ) ) , new immutable worker info ( new worker ( http , test worker2 , 192 . 0 . 0 . 1 , 10 , v1 ) , 2 , immutable set . of ( grp1 , grp2 ) , immutable set . of ( task1 , task2 ) , date times . of ( 2015 - 01 - 01 t01 : 01 : 02 z ) ) , false ) ;
verify ( response observer , timeout ( 100 ) ) . on next ( route summary captor . capture ( ) ) ;
remove handlers for root logger ( ) ;
element target el = event . get native event ( ) . get event target ( ) . cast ( ) ; if ( target el . has class name ( res . styles ( ) . splitter ( ) ) ) return false ; int index = string util . parse int ( target el . get attribute ( data - index ) , - 1 ) ; if ( index = index _ ) return false ;
double quantile = 0 . 95 ; l2 _ frame _ train = mymodel . score auto encoder ( train ) ; final vec l2 _ train = l2 _ frame _ train . any vec ( ) ; sb . append ( mean reconstruction error : + l2 _ train . mean ( ) + \ n ) ; assert . assert equals ( mymodel . mse ( ) , l2 _ train . mean ( ) , 1e - 7 ) ; assert . assert true ( too big a reconstruction error : + l2 _ train . mean ( ) , l2 _ train . mean ( ) < 0 . 06 ) ;
jf . get content pane ( ) . set layout ( new border layout ( ) ) ; jf . get content pane ( ) . add ( mp , border layout . center ) ; jf . get content pane ( ) . add ( set bt , border layout . south ) ; jf . get content pane ( ) . set font ( new java . awt . font ( sans serif , java . awt . font . plain , 11 ) ) ; jf . set default close operation ( jframe . exit _ on _ close ) ; jf . set size ( 800 , 600 ) ; jf . set visible ( true ) ; jf . repaint ( ) ; }
if ( sequence detected ( scanner , f end sequence , true ) ) { if ( 0 = = - - n of brackets ) { return true ; } }
return solr template . query for cursor ( techproducts , new simple query ( * : * ) . add sort ( sort . by ( id ) ) , product . class ) ;
producer = pulsar client . create producer ( topic name ) ;
if ( test char = = ' ? ' ) { index + + ; start = index ; while ( index < end ) { test char = p _ uri spec . char at ( index ) ; if ( test char = = ' ' ) { break ; } if ( test char = = ' % ' ) { if ( index + 2 > = end | | is hex ( p _ uri spec . char at ( index + 1 ) ) | | is hex ( p _ uri spec . char at ( index + 2 ) ) ) { throw new malformed uriexception ( query string contains invalid escape sequence ) ; } } else if ( is reserved character ( test char ) & & is unreserved character ( test char ) ) { throw new malformed uriexception ( query string contains invalid character : + test char ) ; } index + + ; } m _ query string = p _ uri spec . substring ( start , index ) ; }
pattern = next pattern ;
relational object binder . bind columns ( mapping document , secondary table source . get primary key column sources ( ) , key binding , secondary table source . is optional ( ) , new relational object binder . column naming delegate ( ) { int count = 0 ; @ override public identifier determine implicit name ( local metadata building context context ) { final column corresponding column = entity table xref . get primary table ( ) . get primary key ( ) . get column ( count + + ) ; return database . to identifier ( corresponding column . get quoted name ( ) ) ; } } ) ; key binding . set foreign key name ( secondary table source . get explicit foreign key name ( ) ) ;
end element end element = stax parser util . get next end element ( xml event reader ) ;
if ( token = = null ) { token = this . authentication tokens . get ( realm ) ; }
byte [ ] column qualifier = separator . qualifiers . join ( column prefix bytes , bytes . to bytes ( qualifier ) ) ; return column qualifier ; }
assert correct ( quan ja es tornava a envolar li va caure aquest ) ;
table desc = new htable descriptor ( table name ) ; set < string > unique column families = new hash set < string > ( ) ; for ( column mapping col map : column mappings ) { if ( col map . hbase row key & & col map . hbase timestamp ) { unique column families . add ( col map . family name ) ; } }
function state . set safe fn node ( function state . get fn ( ) . get function node ( ) . clone tree ( ) ) ;
this . content = tmp ; } else {
view holder . switch view . set on checked change listener ( null ) ;
for ( template template : templates . values ( ) ) { string webappname = template . get classifier ( ) ; if ( webappname = = null ) continue ; webapp webapp = _ webapps . get ( webappname ) ; if ( webapp = = null ) { _ _ log . warn ( no webapp found for template : { } , template . get name ( ) ) ; templates . remove ( template . get template name ( ) ) ; } else { template . set webapp ( webapp ) ; } }
if ( f document handler = null ) { if ( is empty ) { f document handler . end element ( element , augs ) ; } }
m supp matrix . post translate ( delta x , delta y ) ; return true ; }
int i ; for ( i = 0 ; i < str decimal part . length ( ) & & str decimal part . char at ( i ) = = ' 0 ' ; i + + ) ; if ( i = = str decimal part . length ( ) ) { return 0 ; } string leading number = str decimal part . substring ( i , i + 1 ) ; return ( - i - 1 ) * 4 + count bits length ( long . parse long ( leading number , hex _ radix ) ) - 1 ;
resource res = base . add path ( foo ) ;
if ( objects . equals ( deployment . get deployment group name ( ) , deployment group . get name ( ) ) ) { return false ; }
end = code . current pc ( ) ;
listenable future < buffer result > future = buffer . get ( first , ( long ) 0 , size of pages ( 1 ) ) ;
if ( has state api ) { service service = new service ( s . type , host name , state port , s . clustername , s . clustertype , s . configid , ports , s . name ) ; get and set entry ( services , s . clustername , s . clustertype ) . add ( service ) ; }
return ( ( m = = ( ( ecfield f2m ) obj ) . m ) & & ( arrays . equals ( ks , ( ( ecfield f2m ) obj ) . ks ) ) ) ; }
parser = new geo json parser ( invalid feature no geometry ( ) ) ;
return new simple pages hash strategy ( types , output channels . or else ( range list ( types . size ( ) ) ) , immutable list . copy of ( channels ) , join channels , hash channel , optional . empty ( ) ) ;
file ppsf = psf . get parent file ( ) ; if ( ppsf = = null | | is file system ( ppsf ) ) {
assert null ( assignment . add shared slot and allocate sub slot ( slot , locality . non _ local , vertex id ) ) ;
else if ( show on output _ & & ( event . get exit code ( ) = = 0 ) ) stop button ( ) . click ( ) ;
log . info ( testing whether akka can use + addr ) ; int port = net utils . get available port ( ) ; actor system as = akka utils . create actor system ( new configuration ( ) , new some < scala . tuple2 < string , object > > ( new scala . tuple2 < string , object > ( addr . get host address ( ) , port ) ) ) ; as . shutdown ( ) ; log . info ( using address + addr ) ;
user = session . users ( ) . get user by username ( johndirect , app realm ) ; string postal code = user . get first attribute ( postal _ code ) ; assert . assert equals ( 12399 , postal code ) ;
client . get output stream ( ) . write ( 42 ) ;
int delim = md . index of ( md _ link _ delim ) ;
if ( f children = = null ) return false ; boolean result = f children . remove ( child ) ; if ( result ) { child . internal set parent ( null ) ; if ( f children . is empty ( ) ) f children = null ; } return result ; }
{ config . set start timestamp millis ( system . current time millis ( ) ) ; config . set end timestamp millis ( system . current time millis ( ) + 1 ) ; try { checker . check stream enabled and time range ok ( ) ; fail ( ) ; } catch ( otsstream reader exception ex ) { assert true ( ex . get message ( ) . starts with ( to avoid timing error between different machines ) ) ; } } { config . set start timestamp millis ( system . current time millis ( ) - 3 * 3600 * 1000 + otsstream reader constants . before _ offset _ time _ millis - 1 ) ; config . set end timestamp millis ( system . current time millis ( ) - 1 * 3600 * 1000 ) ; try { checker . check stream enabled and time range ok ( ) ; fail ( ) ; } catch ( otsstream reader exception ex ) { assert true ( ex . get message ( ) . starts with ( as expiration time is ) ) ; } }
cla . port = integer . parse int ( debug _ port ) ;
string error code = parse error code ( node ) ; if ( error code = = null | | error code . equals ( idpcommunication error ) ) return null ; idpcommunication error exception e = ( idpcommunication error exception ) super . unmarshall ( node ) ; return e ; }
assert true ( persistent store . flushed ) ; assert true ( persistent store . closed ) ; assert true ( non persistent store . flushed ) ; assert true ( non persistent store . closed ) ; assert true ( checkpoint file . exists ( ) ) ;
result = db . get metadata ( ) . get command cache ( ) . get ( db . get user ( ) , i command . get text ( ) , i command . get limit ( ) ) ;
if ( initialized ) { return true ; }
fs . create ( file ) . close ( ) ; assert true ( test file didn ' t get created . , fs . exists ( file ) ) ; final path added file = new path ( parent , added file ) ;
cancel fling ( ) ; break ; case action _ cancel :
mts . get ( tok ( 2 ) ) . hash ( val ) ; mts . get ( tok ( 4 ) ) . hash ( val ) ; mts . get ( tok ( - 1 ) ) . hash ( val ) ; assert hash equals ( leftval , mts . hash ( left ) ) ;
return possible children ;
assert true ( e . get cause ( ) instanceof constructor exception ) ;
constraints . add ( desc , all pass constraint . class ) ;
when ( client . get data ( id path ) ) . then return ( a different host . get bytes ( ) ) ;
assert equals ( file . length ( ) , new file . length ( ) ) ;
return url ; }
if ( sorted shards . size ( ) > 1 ) { shard routing min shard = sorted shards . get ( 0 ) ; if the winning shard is not started we are ranking initializing shards , don ' t bother to do adjustments if ( min shard . started ( ) ) { string min node id = min shard . current node id ( ) ; optional < response collector service . computed node stats > maybe min stats = node stats . get ( min node id ) ; if ( maybe min stats . is present ( ) ) { adjust stats ( collector , node stats , min node id , maybe min stats . get ( ) ) ; increase the number of searches for the winning node by one . note that this doesn ' t actually affect the real counts , instead it only affects the captured node search counts , which is captured once for each query in transport search action node search counts . compute ( min node id , ( id , conns ) - > conns = = null ? 1 : conns + 1 ) ; } } } return sorted shards ;
set < integer > new vcols in calcite = new hash set < integer > ( ) ; new vcols in calcite . add all ( inputs [ 0 ] . vcols in calcite ) ; if ( join rel instanceof hive multi join | | ( join rel instanceof semi join ) ) { int shift = inputs [ 0 ] . inputs . get ( 0 ) . get schema ( ) . get signature ( ) . size ( ) ; for ( int i = 1 ; i < inputs . length ; i + + ) { new vcols in calcite . add all ( hive calcite util . shift vcols set ( inputs [ i ] . vcols in calcite , shift ) ) ; shift + = inputs [ i ] . inputs . get ( 0 ) . get schema ( ) . get signature ( ) . size ( ) ; } } if ( log . is debug enabled ( ) ) { log . debug ( translating operator rel + join rel . get id ( ) + : + join rel . get rel type name ( ) + with row type : [ + join rel . get row type ( ) + ] ) ; }
string redirect uri approval parameter = pending oauth2 request . get request parameters ( ) . get ( oauth2 utils . redirect _ uri ) ; if ( ( redirect uri = null | | redirect uri approval parameter = null ) & & pending oauth2 request . get redirect uri ( ) . equals ( redirect uri ) ) { throw new redirect mismatch exception ( redirect uri mismatch . ) ; }
set < string > requested order = application . get absolute ordering ( ) ; for ( string requested name : requested order ) { if ( web xml . order _ others . equals ( requested name ) ) {
else if ( value instanceof query part ) { throw field expected ( value ) ; } else { return val ( value , type ) ; } }
final grok pattern pattern = service . save ( grok pattern . create ( new , . * ) ) ; assert that ( pattern ) . is not null ( ) ;
return new embedded channel ( ctx . channel ( ) . id ( ) , ctx . channel ( ) . metadata ( ) . has disconnect ( ) , ctx . channel ( ) . config ( ) , zlib codec factory . new zlib decoder ( wrapper ) ) ;
if ( zoom = 1f ) { view . scale ( new vector3f ( zoom , zoom , 1f ) ) ; }
source = line _ joiner . join ( var pure = function ( ) { } ; , var dict = { ' func ' : function ( ) { } } ; , function f ( ) { var s = ( condition ? dict [ ' func ' ] : pure ) ( ) ; } , f ( ) ) ;
return diff < ( epsilon * double . min _ normal ) ;
final list < string > fields = grouping . get _ fields ( ) ; if ( fields . size ( ) > 0 ) { flink output fields declarer prod declarer = this . declarers . get ( producer id ) ; input stream = input stream . key by ( prod declarer . get grouping field indexes ( input stream id , grouping . get _ fields ( ) ) ) ; } else { input stream = input stream . global ( ) ; } } else if ( grouping . is _ set _ all ( ) ) {
for ( int i = listeners . length - 2 ; i > = 0 ; i - = 2 ) { if ( listeners [ i ] = = table column model listener . class ) { lazily create the event : if ( change event = = null ) change event = new change event ( this ) ; ( ( table column model listener ) listeners [ i + 1 ] ) . column margin changed ( change event ) ; } }
logger . info ( importing files in { } to table { } , input path , store . get table name ( ) ) ; store . get connection ( ) . table operations ( ) . import directory ( store . get table name ( ) , input path , failure path , false ) ; return success _ response ; }
if ( event object instanceof exchange failed event | | event object instanceof camel context startup failure event | | event object instanceof camel context stop failure event | | event object instanceof service startup failure event | | event object instanceof service stop failure event ) { return level . critical ; }
final string topic name = persistent : prop use ns - abc topic - + key ;
mutable object iterator < record > probe1 = new uniform record generator ( num _ keys , probe _ vals _ per _ key , true ) ;
identifiers . set output markup id ( true ) ;
f schema parser . set property ( xml _ security _ property _ manager , f security property mgr ) ; f access external dtd = f security property mgr . get value ( xmlsecurity property manager . property . access _ external _ dtd ) ;
update ( ) ;
registered = registry . get registered ( ) ;
red black node < t > greatest in lesser = ( red black node < t > ) this . get greatest ( lesser ) ;
m fan card scroller . set target position ( position ) ; start smooth scroll ( m fan card scroller ) ; }
if ( locker . get ( ) = 0 & & gradients accumulator = null & & gradients accumulator instanceof registerable ) { ( ( registerable ) gradients accumulator ) . register consumers ( locker . get ( ) ) ; } if ( debug ) log . info ( stopping everyone . . . ) ;
type [ ] generic exception type array = types . get cloned type array ( generic exception types ) ; if ( generic exception type array . length > 0 ) { sb . append ( throws ) ; append array generic type ( sb , generic exception type array ) ; } return sb . to string ( ) ;
int pat len = old pattern . length ( ) ;
latch . await ( ) ;
file reserved file = file server . get file server ( ) . get resolved file ( files [ 0 ] . get path ( ) ) ;
for ( int i = 0 ; i < 128 ; i + = 2 ) { assert true ( special adds . add ( i ) ) ; }
final long divide factor = power of ten table [ scale down ] ;
set owner ( file _ dir _ path , user1 . get user name ( ) , group3 _ name , false ) ;
change password page . open ( ) ; change password page . change password ( new - password1 , password1 , password1 ) ; assert . assert equals ( your password has been updated . , profile page . get success ( ) ) ; }
add and expect illegal argument exception ( e , c ) ; assert true ( data . object is acyclic ( a ) ) ;
long a = node id ( data . get ( ) , a ) ;
ratio = image aspect ; }
assert xpath evaluates to ( 2008 - 10 - 31 t00 : 00 : 00 . 000 z , wcs : lon lat envelope gml : time position [ 1 ] , dom ) ;
if ( ( refresh rate % 30 ) = = 0 ) { return 30 ; } else if ( ( refresh rate % 15 ) = = 0 ) { return 15 ; } else if ( ( refresh rate % 10 ) = = 0 ) { return 10 ; } else if ( ( refresh rate % 5 ) = = 0 ) { return 5 ; }
put ( algorithm parameters . oaep , org . bouncycastle . jce . provider . jdkalgorithm parameters oaep ) ;
headers . put ( camel braintree . id , null ) ;
assert same ( encoder , encoder . reset ( ) ) ; encoder . can encode ( \ ud902 \ udc00 ) ; encoder . encode ( in ) ; in . rewind ( ) ; assert same ( encoder , encoder . reset ( ) ) ; encoder . can encode ( ' \ ud902 ' ) ; encoder . encode ( in ) ; in . rewind ( ) ;
result = find nearest area ( pixel x , pixel y , span x , span y , result ) ; if ( result span = = null ) { result span = new int [ 2 ] ; }
if ( is supported ( return type ) ) { string fmsg = xslmessages . create xpathmessage ( xpatherror resources . er _ unsupported _ return _ type , new object [ ] { return type . to string ( ) } ) ; throw new illegal argument exception ( fmsg ) ; } try { if ( dbf = = null ) { dbf = factory impl . get domfactory ( use services mechanism ) ; dbf . set namespace aware ( true ) ; dbf . set validating ( false ) ; } db = dbf . new document builder ( ) ; document document = db . parse ( source ) ; return eval ( document , return type ) ; } catch ( exception e ) { throw new xpath expression exception ( e ) ; }
set < span collector > collectors = camel context . get registry ( ) . find by type ( span collector . class ) ;
load fields [ i ] . get type string ( ) ; } result . set load fields ( load fields ) ; }
iterator elements with notations = f table of notationattribute names . entry set ( ) . iterator ( ) ;
double delta = value - mean ; mean + = delta count ; sum of squares of deltas + = delta * ( value - mean ) ; } else { mean = calculate new mean non finite ( mean , value ) ; sum of squares of deltas = na n ; } min = math . min ( min , value ) ; max = math . max ( max , value ) ; }
repo info . set location ( location ) ;
linked class = db . get metadata ( ) . get schema ( ) . get class ( linked ) ;
return type = input type ;
checkpoint period = conf . get long ( fs . checkpoint . period , 3600 ) ;
instances insts = new instances ( data dictionary , att info , 0 ) ;
assert false ( new request context ( 1234 , 1 , 2 , 0 , 0 ) . equals ( new request context ( 1234 , 2 , 2 , 0 , 0 ) ) ) ;
if ( off > = b . length ) return result . to string ( ) ; if ( off + len > b . length ) len = b . length - off ; for ( int i = off ; i < off + len ; + + i ) { int ch = b [ i ] & 0x ff ; if ( ch > = ' ' & & ch < = ' ' & & ch = ' \ \ ' ) { result . append ( ( char ) ch ) ; } else { result . append ( \ \ x ) ; result . append ( hex _ chars _ upper [ ch 0x10 ] ) ; result . append ( hex _ chars _ upper [ ch % 0x10 ] ) ; } } return result . to string ( ) ;
create scheduling request ( 1 * 1024 , root . default , user4 ) ; scheduler . update ( ) ;
slice = slice . make ( make bound ( sk , 1 , 0 , 0 ) , make bound ( ek ) ) ; assert true ( slice . intersects ( cc , column names ( 1 , 0 , 0 ) , column names ( 2 , 0 , 0 ) ) ) ;
offset = base offset + instructions . size ( ) ; final string divisor = result divisor . get register ( ) ;
replication mapping = admin client . replica ops . get replication mapping ( 6 , new cluster , store def ) ; hash map < integer , list < integer > > expected mapping = maps . new hash map ( ) ; expected mapping . put ( 3 , lists . new array list ( 0 , 4 , 8 , 3 , 7 , 11 ) ) ; expected mapping . put ( 4 , lists . new array list ( 1 , 5 , 9 ) ) ; expected mapping . put ( 5 , lists . new array list ( 2 , 6 , 10 ) ) ; assert equals ( expected mapping , replication mapping ) ; } }
if ( invalidate blocks . contains ( node , block ) ) { return ; } block to mark corrupt c = check replica corrupt ( block , reported state , stored block , uc state , node ) ;
if ( debug _ caching ) { system . out . println ( spring loaded : cache corrupt , clearing it ) ; } delete cache files ( ) ; any necessary cache cleanup done = true ; } } } }
assert equals ( 0 , ( stats . get checkout queue length histogram ( ) . get average ( ) ) , 0 ) ;
if ( syntax = = sql syntax . prefix | | syntax = = sql syntax . binary | | syntax = = sql syntax . postfix ) { return syntax ; } else { return sql syntax . function ; } }
int content offset = var int . put var int ( new branch node , new content , 0 ) ;
yarn client application app = yarn client . create application ( ) ; get new application response app response = app . get new application response ( ) ;
canvas . draw color ( color . white ) ;
encrypted server to client . clear ( ) ; plain server out . clear ( ) ; result = server . wrap ( plain server out , encrypted server to client ) ;
instance ids . add ( describe response . get instance id ( ) ) ; } } catch ( amazon service exception e ) {
input pre processor preproc = get input pre process ( i ) ;
long version2 = delete by query and get version ( id : 2 , null ) ;
write integer group ( 71 , 0 ) ;
chore chore = new chore ( name , balancer period , master ) { @ override protected void chore ( ) { master . balance ( ) ; } } ; return threads . set daemon thread running ( chore . get thread ( ) ) ; }
context app context = instrumentation registry . get target context ( ) ; assert equals ( net . qiujuer . sample . genius , app context . get package name ( ) ) ; }
camel conduit conduit = setup camel conduit ( conduit ep info , true , false ) ; final message out message = new message impl ( ) ; endpoint info . set address ( camel : direct : endpoint a ) ;
verify . assert sorted bags equal ( tree bag . new bag with ( 1 , 1 , 1 , 2 , 4 , 4 ) , immutable . new with ( 4 ) ) ;
list < directories . data directory candidate > candidates = new array list < > ( ) ; long total available = 0 l ;
cc = client1 . get call tracker ( ) . start call ( ) ; clock . add ms ( 10 ) ; cc . end call ( ) ; clock . add ms ( time _ interval ) ; assert . assert true ( dc client1 default . get current computed drop rate ( ) < 1 . 0 , client1 drop rate not less than 1 . ) ; }
config = new configuration ( ) ; config . set float ( task manager options . network _ buffers _ memory _ fraction , 0 . 1f ) ; assert true ( task manager services configuration . has new network buf conf ( config ) ) ; config . set long ( task manager options . network _ buffers _ memory _ max , 1024 ) ; assert true ( task manager services configuration . has new network buf conf ( config ) ) ; config = new configuration ( ) ;
throw new java . io . stream corrupted exception ( invalid name : + unparsed ) ;
assert true ( string . value of ( state monitor . current peak size . get ( ) ) , state monitor . current peak size . get ( ) < = bellow pool min size ) ;
if ( ( operation code = = replace | | operation code = = replace _ find _ next ) & & ( f find replace state = find _ first & & f find replace state = find _ next ) ) throw new illegal state exception ( illegal find replace state : cannot replace without preceding find ) ; non - nls - 1 if ( operation code = = find _ first ) { reset if ( find string = = null | | find string . length ( ) = = 0 ) return null ; validate start offset if ( start offset < 0 | | start offset > = length ( ) ) throw new bad location exception ( ) ; string pattern flags = g ; if ( case sensitive ) pattern flags + = i ; if ( whole word ) find string = \ \ b + find string + \ \ b ; non - nls - 1 non - nls - 2 if ( whole word ) find string = as reg pattern ( find string ) ; f find replace match offset = start offset ; reg exp = reg exp . compile ( find string , pattern flags ) ; reg exp . set last index ( f find replace match offset ) ; }
results = get file status ( filtered paths . to array ( new path [ filtered paths . size ( ) ] ) ) ;
if ( files . is empty ( ) & & uploaded file = null ) { files . clear ( ) ; files . add ( uploaded file ) ; } else { files . add ( 0 , uploaded file ) ; } return files ;
pfs [ 0 ] = new synthesized plan fragment ( ) ;
if ( parameters . contains key ( configuration ) ) { map < string , object > config props = introspection support . extract properties ( parameters , config . ) ; endpoint helper . set reference properties ( this . get camel context ( ) , config props , parameters ) ; endpoint helper . set properties ( this . get camel context ( ) , config props , parameters ) ; }
throw new illegal state exception ( ) ;
num images added + = add all images from tree ( all images , tree uri , document id ) ; } else if ( mime type = null & & mime type . starts with ( image ) ) {
insns = perform expansion ( opcodes ) ; }
assert that ( actual dividers ) . overriding error message ( expected showing dividers < % s > but was < % s > . , show divider to string ( dividers ) , show divider to string ( actual dividers ) ) . is equal to ( dividers ) ; return myself ;
when ( client _ b . atomic increment ( increment for row ( maxid ) ) ) . then return ( deferred . from result ( 5 l ) ) ; when ( client _ b . compare and set ( any put ( ) , empty array ( ) ) ) . then return ( deferred . from result ( true ) ) . then return ( deferred . from result ( true ) ) ;
segment completion mgr . _ secconds + = 1 ; params = new request . params ( ) . with instance id ( s1 ) . with offset ( s1 offset ) . with segment name ( segment name str ) ; response = segment completion mgr . segment consumed ( params ) ; assert . assert equals ( response . get status ( ) , segment completion protocol . controller response status . catch _ up ) ;
try { exec . await termination ( 5 , time unit . minutes ) ; } catch ( interrupted exception e ) { logger . info ( exec . await termination ( 1 , time unit . hours ) occurs error : + e ) ; }
log . e ( tag , unable to initiate keyboard mode , time picker was null . ) ; return true ; } m typed times . clear ( ) ; try starting kb mode ( key code ) ; return true ; }
if ( src stat . get modification time ( ) = parity stat . get modification time ( ) ) { log . warn ( mismatching timestamp for + src path + and + parity path + , ignoring . . . ) ; return false ; } string uri path = parity path . to uri ( ) . get path ( ) ;
process instances = runtime service . create process instance query ( ) . variable value less than or equal ( string var , abcdef ) . list ( ) ; assert equals ( 2 , process instances . size ( ) ) ; expeced ids = arrays . as list ( process instance1 . get id ( ) , process instance2 . get id ( ) ) ; ids = new array list < string > ( arrays . as list ( process instances . get ( 0 ) . get id ( ) , process instances . get ( 1 ) . get id ( ) ) ) ; ids . remove all ( expeced ids ) ; assert true ( ids . is empty ( ) ) ; assert equals ( 3 , runtime service . create process instance query ( ) . variable value less than or equal ( string var , z ) . count ( ) ) ;
int m2 ; resp var bind list [ m2 ] item and next are going to be removed
while ( line . ends with ( may be installed by name : ) = = false ) { line = reader . read line ( ) ; }
if ( t = = json token . value _ string ) { string text = jp . get text ( ) . trim ( ) ; if ( text . length ( ) = = 0 ) { return ( float ) get empty value ( ) ; } switch ( text . char at ( 0 ) ) { case ' i ' : if ( infinity . equals ( text ) | | inf . equals ( text ) ) { return float . positive _ infinity ; } break ; case ' n ' : if ( na n . equals ( text ) ) { return float . na n ; } break ; case ' - ' : if ( - infinity . equals ( text ) | | - inf . equals ( text ) ) { return float . negative _ infinity ; } break ; } try { return float . parse float ( text ) ; } catch ( illegal argument exception iae ) { } throw ctxt . weird string exception ( _ value class , not a valid float value ) ; } if ( t = = json token . value _ null ) { return ( float ) get null value ( ) ; }
interpolate ( rgb1 , rgb2 , gradients [ i ] ) ;
return new import data task ( context , source authority ) . import workspace ( ) ;
dst blob = get blob reference ( dst key ) ;
if ( get verbs ( ) . is empty ( ) ) { throw new illegal argument exception ( must add verb first , such as get post delete ) ; } verb definition verb = get verbs ( ) . get ( get verbs ( ) . size ( ) - 1 ) ;
if ( this . orientation event listener = null ) { this . orientation event listener . disable ( ) ; }
case . add ( data , basic input stream socket . class , f binary stream , input stream . class ) ;
append element string ( ir . sb , local name , atts ) ;
return stash or submit task ( runnable , false , true ) ; }
long size ;
template . request body ( netty4 : tcp : localhost : + port + ?textline = true & sync = true , request4 , string . class ) ;
sfsbxpc . force rollback and lose pending changes ( 10 , false ) ; employee = sfsbxpc . lookup ( 10 ) ; assert null ( employee record should not be found in db after rollback , employee ) ; }
muc2 . leave ( ) ;
statement statement = jdbc coordinator . get statement preparer ( ) . create statement ( ) ;
get ( cdf locks wfs?request = release lock & version = 2 . 0 & lock id = + lock id ) ; }
caching detector . put ( key , count + 1 ) ;
assert equals ( default , session . get queue name ( ) ) ;
list < string > load paths = new array list ( ) ;
mr bean no interface view = ( mr bean ) ctx . lookup ( java _ global _ namespace _ prefix + app _ name + + module _ name + + ejb name + + mr bean . class . get name ( ) ) ;
while ( is name or type ( context ) & & context . get location in parent ( ) . get id ( ) . equals ( type arguments ) ) { context = context . get parent ( ) ; } switch ( context . get node type ( ) ) { case astnode . method _ declaration : method declaration decl = ( method declaration ) context ; if ( decl . is constructor ( ) ) { binding = decl . resolve binding ( ) ; } break ; case astnode . class _ instance _ creation : class instance creation cic = ( class instance creation ) context ; binding = cic . resolve constructor binding ( ) ; break ; }
final count down latch latch = new count down latch ( num _ tasks ) ; final atomic boolean test failed = new atomic boolean ( false ) ; runnable reader runnable = new runnable ( ) { @ override public void run ( ) { try { assert . assert true ( verify read random file ( path1 , block _ size , seed ) ) ; } catch ( throwable e ) { log . error ( reader runnable error , e ) ; test failed . set ( true ) ; } finally { latch . count down ( ) ; } } } ;
answer = await . result ( job submission future , duration . zero ( ) ) ;
res . add ( core options . system property ( jetty . http . port ) . value ( 0 ) ) ; res . add ( core options . system property ( jetty . ssl . port ) . value ( string . value of ( default _ ssl _ port ) ) ) ; res . add ( core options . system property ( jetty . home . bundle ) . value ( org . eclipse . jetty . osgi . boot ) ) ; res . add all ( core jetty dependencies ( ) ) ; return res ; }
string error msg = unable to ensure that the table + table name + will be + enabled because of a zoo keeper issue ;
assert that ( dom , has xpath ( count ( at : feed at : entry ) , equal to ( 1 ) ) ) ;
if ( f . t = null ) { f . t . join ( ) ; } } catch ( interrupted exception e ) {
distributed file system fs = cluster . get file system ( ) ; path bar dir = new path ( bar ) ; fs . mkdir ( bar dir , new fs permission ( ( short ) 777 ) ) ; fs . set storage policy ( bar dir , hdfs constants . onessd _ storage _ policy _ name ) ;
pattern . compile ( a * . + ) ; }
if ( last child on screen & & child on final screen ) { return ; } if ( m workspace screens . contains key ( extra _ empty _ screen _ id ) ) { insert new workspace screen ( extra _ empty _ screen _ id ) ; } }
input stream is = cryptopia adapters test . class . get resource as stream ( marketdata example - ticker - data . json ) ;
final int initial count = counter . get count ( ) ;
try { create empty builder ( ) . merge ( element def1 ) . merge ( element def2 ) . build ( ) ; fail ( exception expected ) ; } catch ( final schema exception e ) { assert true ( e . get message ( ) . contains ( property ) ) ; } }
if ( alpha . index of ( name . char at ( start index ) ) < 0 ) throw new ioexception ( dnsname components must begin with a letter ) ;
if ( get features ( ) . is empty ( ) ) { factory . get features ( ) . add all ( get features ( ) ) ; } if ( published endpoint url = null ) { factory . set published endpoint url ( published endpoint url ) ; }
assert . assert true ( lockbox . try lock ( task lock type . exclusive , task2 , intervals . of ( 2015 - 01 - 01 2015 - 01 - 02 ) ) . is ok ( ) ) ; }
assert true ( solo . is text checked ( buttons [ 2 ] ) ) ;
injection metadata metadata = this . injection metadata cache . get ( cache key ) ;
start activity for result ( welcome back idp prompt . create intent ( this , get flow params ( ) , user , null ) , rc _ welcome _ back _ idp ) ; set slide animation ( ) ; }
list < bean property definition > ref props = bean desc . find back references ( ) ;
( ( abstract builtin specs detector ) provider ) . clear ( ) ;
trace branch targets = trace branch targets . generalize ( new instruction offset value ( branch target ) ) . instruction offset value ( ) ; was called = true ;
global stats calc agg = new global stats calc ( 10 ) ; agg . register stats collector ( event stats collectors ) ; thread agg thread = new thread ( agg ) ; agg thread . start ( ) ;
int count = 0 ;
val . set exists ( false ) ; func . get boolean ( ) ; assert false ( func . exists ( ) ) ;
root ( = [ 100 100 100 100 ] , x = [ 100 100 100 100 ] ) ; + root - a ( = [ 50 100 0 0 ] , x = [ 50 100 100 50 ] ) ; + a - b ( = [ 50 100 0 0 ] , x = [ 50 100 0 100 ] ) ; b string apps config =
capacity scheduler cs = ( capacity scheduler ) rm . get resource scheduler ( ) ; cs . get application attempt ( attempt . get app attempt id ( ) ) . get new container id ( ) ; mock am am = mock rm . launch am ( app , rm , nm1 ) ;
preference . set fragment ( null ) ;
list < row > union = lists . new array list ( prefix rows ) ;
for ( map . entry e : temp conf ) { string key = e . get key ( ) . to string ( ) ; if ( key . starts with ( slsconfiguration . am _ type _ prefix ) ) { string am type = key . substring ( slsconfiguration . am _ type _ prefix . length ( ) ) ; am class map . put ( am type , class . for name ( temp conf . get ( key ) ) ) ; } }
if ( arrays . as list ( ÑÐµÑÑÐ° , ÑÐ°ÑÑÐ¸Ð½Ð° , ÑÐ°ÑÑÐºÐ° , Ð¿Ð¾Ð»Ð¾Ð²Ð¸Ð½Ð° , ÑÑÐµÑÐ¸Ð½Ð° , ÑÐ²ÐµÑÑÑ ) . contains ( tokens [ i - 1 ] . get token ( ) . to lower case ( ) ) & & pos tag helper . has pos tag ( verb token readings , . * : [ pn ] ( : . * | ) ) ) { log exception ( ) ; return true ; }
obj = instance map . get ( ra . get content ( ) ) ; } else {
if ( this . pos > = this . count ) { return - 1 ; } if ( byte count = = 0 ) { return 0 ; } int copylen = this . count - pos < byte count ? this . count - pos : byte count ;
if ( edits count % interval = = 0 | | ( num newly opened files > num opened files before reporting ) ) {
if ( m resource plan = null ) { m resource plan . set status ( status . enabled ) ; } commited = commit transaction ( ) ;
am1 . allocate ( null , null ) ; sent rmcontainer launched ( rm1 , container id2 ) ; container id container id1 = container id . new container id ( am1 . get application attempt id ( ) , 1 ) ;
assert equals ( 0 , no rules tool . check ( majÄ one niemaÅe znaczenie . ) . size ( ) ) ; assert equals ( 0 , no rules tool . check ( czÄsto wystarczy obrÃ³ciÄ na wspak wyroki Åwiata , aby trafnie osÄdziÄ jakÄÅ osobÄ . ) . size ( ) ) ;
assert equals ( the file paths were found to be publicly visible + even though the parent directory is not publicly accessible , false , false , job conf . get ( mrjob config . cache _ file _ visibilities ) ) ; check cache entries ( stat cache , qualified parent , first cache file , relative path ) ; job = job . get instance ( conf ) ;
group leadership session . unsubscribe changes ( ) ; return true ; }
list < conversion section wrapper > conversion sections = new array list < conversion section wrapper > ( ) ;
workspace . replace file contents ( header full , blah = 5 , blah = 6 ) ;
for ( int [ ] itemset : itemsets ) { add ( itemset ) ; } }
list < department > departments = entity manager . create query ( select d from department d , department . class ) . get result list ( ) ; log . infof ( fetched % d departments , departments . size ( ) ) ; for ( department department : departments ) { assert equals ( 3 , department . get employees ( ) . size ( ) ) ; }
final string authority = net utils . get host port string ( server . get connector address ( 0 ) ) ;
cli . send line ( cd subsystem = datasources data - source ) ; cli . send line ( ls ) ; string ls = cli . read output ( ) ; assert true ( ls . contains ( test ds ) ) ;
if ( old op type def . is present ( ) ) { return ; } ctx . report ( diff event . new info ( ) . type name ( capitalize ( op name ) ) . type kind ( type kind . operation ) . components ( op name ) . reason msg ( examining operation ' % s ' . . . , capitalize ( op name ) ) . build ( ) ) ;
tolerance = 10 ; meters
if ( exchange . get exception ( ) = null ) { get exception handler ( ) . handle exception ( exchange . get exception ( ) ) ; } num messages + + ;
x509 cert impl cert = new x509 cert impl ( info ) ;
int pack stuffing length = scratch [ 13 ] & 0x07 ;
int total workers = oglobal configuration . distributed _ db _ workerthreads . get value as integer ( ) ;
sb . append ( \ n + < single sign on service binding = \ urn : oasis : names : tc : saml : 2 . 0 : bindings : http - post \ \ n + location = \ ) . append ( bind url ) . append ( \ > \ n ) ; if ( saml client . force post binding ( ) ) { sb . append ( < single sign on service binding = \ urn : oasis : names : tc : saml : 2 . 0 : bindings : http - redirect \ \ n + location = \ ) . append ( bind url ) . append ( \ > \ n ) ; }
flush ( str , off , len ) ; return ;
test failure ( view . update compacting ( empty set ( ) , of ( r2 ) ) , cur ) ;
assert equals ( should be + files + unique responses , files , unique . size ( ) ) ;
assert that ( tester . get existing value ( parent ) ) . is equal to ( val ) ;
for ( long data = 0 ; data < 5 ; data + + ) { for ( int loop = 0 ; loop < = data ; loop + + ) { reseting histogram . insert ( data ) ; } } assert equals ( 3 , reseting histogram . get quantile ( 0 . 50 ) ) ; assert equals ( 4 , reseting histogram . get quantile ( 0 . 99 ) ) ; assert equals ( 2 . 67 , reseting histogram . get average ( ) , 0 . 01 ) ;
file channel file channel = new random access file ( file , r ) . get channel ( ) ; boolean no zero copy = channel manager . is ssl handler configured ( channel . pipeline ( ) ) | | config . is disable zero copy ( ) ; object body = no zero copy ? new chunked nio file ( file channel , offset , length , config . get chunked file chunk size ( ) ) : new default file region ( file channel , offset , length ) ; channel . write ( body , channel . new progressive promise ( ) ) . add listener ( new write progress listener ( future , false , length ) ) ; channel . write and flush ( last http content . empty _ last _ content , channel . void promise ( ) ) ; }
quick sort2 ( recomposable elements , 0 , recomposable elements . size ( ) - 1 ) ;
return ( t ) value ; }
sd = start image upload ( params , context ) ; } else {
add field config listener ( new field boost map fclistener ( this ) ) ;
client . write ( close . as frame ( ) ) ; respond with close
for ( protocol version version : protocol _ versions ) assert rows net ( version , execute net ( version , cql select ) , row ( three , one , two ) ) ;
out . abort for tests ( ) ; return out ; }
return local revoke security group ingress . get pull parser ( my _ qname ) ;
color outside color = outside clip color . get color ( time ) ;
double [ ] numerator shares = new double [ n known resource types ] ; double [ ] denominator shares = new double [ n known resource types ] ;
for ( encoding type type : encoding type . values ( ) ) { encoding [ current + + ] = new object [ ] { new encoding type [ ] { type } , type . get http name ( ) + ; q = + 1 . 00 } ; }
return mapped error code ;
init default times ( ) ;
return entries . contains key ( filename ) ; }
assert that ( system properties . get ( string _ property _ name ) , equal to ( non _ string _ property _ value ) ) ; assert that ( system properties . get ( non _ string _ property _ name ) , equal to ( ( object ) string _ property _ value ) ) ; } security manager old security manager = system . get security manager ( ) ; security manager security manager = new security manager ( ) { @ override public void check properties access ( ) {
domain . set owner ( existing domain . get owner ( ) ) ;
final segments segments = new segments ( window name , retention period , num segments ) ; long start time = segment size * 2 ; long incr = segment size 2 ; context . set record context ( create record context ( start time ) ) ; window store . put ( 0 , zero ) ; assert equals ( utils . mk set ( segments . segment name ( 2 ) ) , segment dirs ( base dir ) ) ; context . set record context ( create record context ( start time + incr ) ) ;
slice _ from ( q ) ;
this . switch elements = immutable switch element . immutable list of ( switch elements ) ;
result = nf . create expr statement ( result , ts . get lineno ( ) ) ;
startup progress prog = name node . get startup progress ( ) ;
method = c . get clazz ( ) . get method ( run1 ) ; string = ( string ) method . invoke ( object ) ; assert equals ( 66 , string ) ; method = c . get clazz ( ) . get method ( run2 ) ;
assert analyzes to ( keyword analyzer , ãã¹ã¾ , new string [ ] { ãã¹ãº } ) ; assert analyzes to ( japanese analyzer , ãã¹ã¾ , new string [ ] { ã , ã¹ãº } ) ; side effect }
force maintenance ( master ) ;
array list < string > role list = new array list < > ( ) ; while ( rs . next ( ) ) { string role = rs . get string ( 1 ) ; if ( null = role ) { role list . add ( role . trim ( ) ) ; } }
assert . assert equals ( 2 , leaf queue . get num active applications ( ) ) ;
gles20 . gl gen renderbuffers ( 1 , genbuf , 0 ) ;
list < acl entry > acl spec = lists . new array list ( acl entry ( access , user , user1 , all ) ) ;
cc compilation outputs cc outputs = model . create cc compile actions ( ) ;
menu . add items ( items ) ; menu . set on item clicked listener ( new satelite clicked listener ( ) { public void event occured ( int id ) { log . i ( sat , clicked on + id ) ; } } ) ; }
string random string = random string utils . random ( random . next int ( 2 * max _ string _ length ) ) . replace ( ' \ 0 ' , ' ' ) ;
tile cache . clear ( ) ;
l + = xoff + yoff * width ; break ; default : int end = values [ count - 1 ] & 0x ff ; for ( int i = 0 ; i < end ; i + + ) { val [ l + + ] = ( byte ) ( ( ( i & 1 ) = = 0 ) ? ( values [ count ] & 0xf0 ) > > 4 : ( values [ count + + ] & 0x0f ) ) ; }
int grid child top ; int grid child bottom ; if ( flow down ) { grid child top = get lowest positioned bottom ( ) ; grid child bottom = grid child top + get child height ( child ) ; } else { grid child bottom = get highest positioned top ( ) ; grid child top = grid child bottom - get child height ( child ) ; } for ( int i = 0 ; i < m column count ; i + + ) { update column top if needed ( i , grid child top ) ; update column bottom if needed ( i , grid child bottom ) ; }
if ( ts op1 . get conf ( ) . get row limit ( ) = ts op2 . get conf ( ) . get row limit ( ) ) { return false ; }
return input type . get component type ( ) ;
ffmpeg lpcmextract = new string [ ] { mencoder path , - ss , params . timeseek > 0 ? + params . timeseek : 0 , params . stdin = null ? - : filename , evo value1 , evo value2 , - really - quiet , - msglevel , statusline = 2 , - channels , + channels , - ovc , copy , - of , rawaudio , - mc , 0 , - noskip , - oac , ( ac3 remux ) ? copy : lavc , params . aid . is ac3 ( ) ? - fafmttag : - quiet , params . aid . is ac3 ( ) ? 0x2000 : - quiet , - lavcopts , acodec = + ( configuration . is mencoder ac3 fixed ( ) ? ac3 _ fixed : ac3 ) + : abitrate = + codec util . get ac3 bitrate ( configuration , params . aid ) , - af , lavcresample = 48000 , - srate , 48000 , single media audio ? - quiet : - aid , single media audio ? - quiet : ( + params . aid . get id ( ) ) , - o , ff audio pipe [ 0 ] . get input pipe ( ) } ;
return _ name ; }
timer service . unregister timeout ( allocation id ) ; set < allocation id > slots = slots per job . get ( job id ) ;
db . users ( ) . insert permission on group ( group2 , administer ) ;
cfs . put ( cert type , cf ) ;
mutable acl root acl = new acl impl ( root oid , 1 , authz strategy , pgs , null , null , false , new principal sid ( joe ) ) ;
run with expected exception ( new callable < void > ( ) { @ override public void call ( ) throws exception { htable descriptor htd = new htable descriptor ( table name . value of ( non _ existing _ namespace , table1 ) ) ; htd . add family ( new hcolumn descriptor ( family1 ) ) ; admin . create table ( htd ) . join ( ) ; return null ; } } , namespace not found exception . class ) ;
string xsl file = config . get init parameter ( xsl file ) ; string default standard query parser field = config . get init parameter ( default standard query parser field ) ;
tuple incr dump = incremental load and verify ( db name , bootstrap dump . last repl id , repl db name ) ; verify run ( select a from + repl db name + . unptned order by a , unptn _ data , driver mirror ) ; }
if ( fbutilities . is windows ) file utils . delete ( descriptor . filename for ( component . stats ) ) ;
assert equals ( 1 , cache ( 1 ) . key set ( ) . size ( ) ) ;
resource field schema [ ] bag sub field schemas = new resource field schema [ 1 ] ; bag sub field schemas [ 0 ] = new resource field schema ( ) . set name ( t ) . set description ( the tuple in the bag ) . set type ( data type . tuple ) ; resource field schema [ ] inner tuple field schemas = new resource field schema [ 1 ] ;
random access file raf = new java . io . random access file ( file name , rw ) ;
ri list = env . get project repositories map ( ) . get ( project ) ; assert . assert not null ( ri list ) ; ri = ri list . get ( 0 ) ; assert . assert not null ( ri ) ; assert . assert true ( current version should be refreshed , ri . get current version ( ) . contains ( c78fa757c524 ) ) ; }
keep descriptor classes check box . set selected ( mark descriptor classes ) ; allow shrinking check box . set selected ( allow shrinking ) ; allow optimization check box . set selected ( allow optimization ) ; allow obfuscation check box . set selected ( allow obfuscation ) ; set class specification ( keep class specification ) ;
argument captor < storage report [ ] > captor = argument captor . for class ( storage report [ ] . class ) ; mockito . verify ( nn spy ) . send heartbeat ( any ( datanode registration . class ) , captor . capture ( ) , any long ( ) , any long ( ) , any int ( ) , any int ( ) , any int ( ) , mockito . any ( volume failure summary . class ) , mockito . any boolean ( ) ) ;
highlight selected line ( ) . set global value ( new ui prefs . highlight selected line ( ) . get global value ( ) ) ;
table table = test _ util . create table ( table name . value of ( name . get method name ( ) ) , new byte [ ] [ ] { family } ) ; put put = new put ( row ) ;
( ( main time line activity ) get activity ( ) ) . save navigation position to db ( ) ;
context names . bind info for ( application name , module name , component name , java : module + lookup name . substring ( 10 ) ) . setup lookup injection ( service builder , injector , phase context . get deployment unit ( ) , optional ) ;
ulog . apply buffered updates ( ) ; update j ( json add ( sdoc ( id , q7 , _ version _ , 117 ) ) , params ( distrib _ update _ param , from _ leader ) ) ; do another add to make sure flags are back to normal req . close ( ) ;
article . put ( article . article _ had _ been _ published , false ) ;
assert that ( translog . get min file generation ( ) , greater than or equal to ( 1 l ) ) ;
if ( tika impl . class . get class loader ( ) instanceof urlclass loader ) { url [ ] urls = ( ( urlclass loader ) tika impl . class . get class loader ( ) ) . get urls ( ) ; set < url > set = new linked hash set < > ( arrays . as list ( urls ) ) ; if ( set . size ( ) = urls . length ) { throw new assertion error ( duplicate jars : + arrays . to string ( urls ) ) ; } add read permissions ( perms , set ) ; }
return actual entry . get value ( ) . equals ( expected entry . get value ( ) ) ;
list < ibreakpoint dmcontext > bkpts after = new linked list < ibreakpoint dmcontext > ( arrays . as list ( get breakpoints ( f breakpoints dmc ) ) ) ; assert equals ( breakpoints service reports unexpected number of breakpoints , bkpts before . length + 1 , bkpts after . size ( ) ) ; list iterator < ibreakpoint dmcontext > iter = bkpts after . list iterator ( ) ; while ( iter . has next ( ) ) { ibreakpoint dmcontext bkpt after = iter . next ( ) ; boolean found = false ; for ( ibreakpoint dmcontext bkpt before : bkpts before ) { if ( bkpt after . equals ( bkpt before ) ) { assert false ( shouldn ' t have been more than one match , found ) ; iter . remove ( ) ; found = true ; } } }
if ( original ns . equals ( marshalled ns ) ) { compare xml ( config id , original , marshalled , true ) ; }
mula _ small ( s , order _ times _ 8 , 0 , k , 32 , - 1 ) ;
sql = select id from r1 sub4 where id > ? and id in ( select id from r1 where id > ? ) order by id ; ; vt = client . call procedure ( @ ad hoc , sql , 1 , 2 ) . get results ( ) [ 0 ] ; validate table of scalar longs ( vt , new long [ ] { 3 } ) ; check planner cache ( client , cache _ miss2 ) ; vt = client . call procedure ( @ ad hoc , sql , 0 , 1 ) . get results ( ) [ 0 ] ; validate table of scalar longs ( vt , new long [ ] { 2 , 3 } ) ;
string response = read response ( client ) ; assert . assert that ( response , matchers . contains string ( http 1 . 1 200 ok ) ) ;
if ( has space ) { result . append ( \ ) ; } for ( int i char = 0 ; i char < arg . length ( ) ; i char + + ) { boolean last char = i char = = arg . length ( ) - 1 ; switch ( arg . char at ( i char ) ) { case ' ' :
for ( sstable reader s : cfs . get live sstables ( ) ) assert equals ( 6 , s . get sstable level ( ) ) ; int [ ] levels = strategy . manifest . get all level size ( ) ;
int i = 0 ; for ( container request stored request1 : matches . get ( 0 ) ) { if ( i + + = = 0 ) { assert equals ( stored opport container3 , stored request1 ) ; } else { assert equals ( stored opport container4 , stored request1 ) ; } } am client . remove container request ( stored opport container3 ) ;
app config . update property ( reset application sshkey , false ) ;
final text block tb1 = new text block ( ) ; tb1 . add line ( new text line ( testing ) ) ; b1 . set text block ( tb1 ) ; assert false ( b1 . equals ( b2 ) ) ; final text block tb2 = new text block ( ) ; tb2 . add line ( new text line ( testing ) ) ; b2 . set text block ( tb2 ) ; assert true ( b1 . equals ( b2 ) ) ; }
if ( string utils . is empty ( dis client sys config . get instance ( ) . conf _ server _ zoo _ action ) ) { throw new exception ( settings : conf _ server _ zoo _ action cannot find ) ; } logger . debug ( server disconf . conf _ server _ zoo _ action : + dis client sys config . get instance ( ) . conf _ server _ zoo _ action ) ;
i = 1234 ; f = ( float ) i ; system . out . println ( i = + i + - - > f = + f ) ; i = - 1234 ;
_ wrap error ( malformed numeric value ' + _ text buffer . contents as string ( ) + ' , nex ) ;
long recognition time required = ocr result . get recognition time required ( ) ; status view bottom . set text size ( 14 ) ; status view bottom . set text ( ocr : + source language readable + - mean confidence : + mean confidence . to string ( ) + - time required : + recognition time required + ms ) ; }
set < string > prefixes = nsmap . key set ( ) ;
source function < ? > source = head operator . get user function ( ) ;
test serialized size ( statics . heartbeat st , heart beat state . serializer ) ;
for ( final inet address peer : storage service . instance . get token metadata ( ) . get all endpoints ( ) ) { valid data centers . add ( snitch . get datacenter ( peer ) ) ; } return valid data centers ;
slice _ from ( rus ) ; break ; case 28 :
boolean decommissioned = dn9 . is decommissioned ( ) ;
activity . on create ( null ) ; test observer . assert values ( activity event . create , activity event . start , activity event . resume , activity event . pause , activity event . stop , activity event . destroy , activity event . create ) ;
assert equals ( flds [ 0 ] . get field ( ) , pow ( float ( weight ) , const ( 2 . 0 ) ) ) ; spec = sort spec parsing . parse sort spec ( pow ( weight , 2 . 0 ) desc , weight desc , bday asc , req ) ;
child many to many no cascade child1 = new child many to many no cascade ( ) ;
string current value = ; if ( mapping . contains key ( item value ) ) { current value = mapping . get ( item value ) ; } device param value param = new device param ( param type . multiswitch _ value , current value ) ;
repository info repo info = new repository info ( ) ; uri location = geogig . get location ( ) . normalize ( ) ; if ( file . equals ( location . get scheme ( ) ) ) { need the parent file parent dir = new file ( location ) . get parent file ( ) ; location = parent dir . to uri ( ) . normalize ( ) ; }
cache manager . add listener ( new shutdown hook ( t ) ) ;
activity chooser model data model = activity chooser model . get ( m context , m share history file name ) ;
mt . maxsize ( 256 ) ; mt . init ( ) ; for ( tree range range : mt . invalids ( ) ) range . add all ( new hiterator ( range . right ) ) ; byte [ ] initialhash = mt . hash ( full ) ;
double d = double . max _ value ; for ( series s : series ) { iterator < data point interface > values = s . get values ( m current viewport . left , m current viewport . right ) ; while ( values . has next ( ) ) { double v = values . next ( ) . get y ( ) ; if ( d > v ) { d = v ; } } } if ( d = double . max _ value ) { m current viewport . bottom = d ; }
buf . append ( strip ewkbsuffix ( arguments . get ( 0 ) ) ) ;
test leader service . notify listener ( null , null ) ;
for ( int j = 0 ; j < pos . length ; j + + ) { pos [ j ] = replace tags ( pos [ j ] ) ; pos [ j ] = open nlp . untokenize ( pos [ j ] , original sentences [ i ] ) ; }
callable < ? > async op = new callable < void > ( ) { @ override public void call ( ) throws ioexception { bm . enqueue block op ( new runnable ( ) { @ override public void run ( ) {
for ( final string server : server array ) { new thread ( new runnable ( ) { @ override public void run ( ) { connect to one server with retry ( clients [ index ] , server , index ) ; connections . count down ( ) ; } } ) . start ( ) ; } }
index writer w = run data . get index writer ( ) ; if ( w = = null ) { throw new runtime exception ( please open the writer before invoking near realtime reader ) ; } if ( run data . get index reader ( ) = null ) { throw new runtime exception ( please close the existing reader before invoking near realtime reader ) ; } long t = system . current time millis ( ) ;
iterator < message id > itr msgs = messages2 . iterator ( ) ;
map < string , string > [ ] map array = new map [ 1 ] ; map array [ 0 ] = new hash map < > ( ) ; map array [ 0 ] . put ( key , value1 ) ; expression = parser . parse expression ( [ 0 ] ) ; assert equals ( { key = value1 } , stringify ( expression . get value ( map array ) ) ) ; assert can compile ( expression ) ; assert equals ( ljava util map , get ast ( ) . get exit descriptor ( ) ) ; assert equals ( { key = value1 } , stringify ( expression . get value ( map array ) ) ) ; assert equals ( ljava util map , get ast ( ) . get exit descriptor ( ) ) ; expression = parser . parse expression ( [ 0 ] [ ' key ' ] ) ;
case func _ trim _ char :
list < string > keys = new array list < string > ( ) ; keys . add all ( m . key set ( ) ) ; collections . sort ( keys ) ; int values hash = 0 ;
object name foo = object name . get instance ( org . apache . camel : context = camel - 1 , type = processors , name = \ foo \ ) ; completed = ( long ) mbean server . get attribute ( foo , exchanges completed ) ; assert equals ( 5 , completed . long value ( ) ) ; object name mock = object name . get instance ( org . apache . camel : context = camel - 1 , type = processors , name = \ mock \ ) ; completed = ( long ) mbean server . get attribute ( mock , exchanges completed ) ; assert equals ( 5 , completed . long value ( ) ) ; }
uc2 . set if modified since ( since time ) ; uc2 . connect ( ) ; assert equals ( 200 , ( ( http urlconnection ) uc2 ) . get response code ( ) ) ;
int offset secs = _ parse2 d ( date str , start + 1 ) * 3600 ; hours if ( len > = 5 ) { offset secs + = _ parse2 d ( date str , end - 2 ) * 60 ; minutes } if ( date str . char at ( start ) = = ' - ' ) { offset secs * = - 1000 ; } else { offset secs * = 1000 ; } cal . set ( calendar . zone _ offset , offset secs ) ;
assert equals ( changed name = test name , m _ listener . event list ) ;
name transformer unwrapper = _ annotation introspector . find unwrapping name transformer ( am ) ; if ( unwrapper = null ) { bpw = bpw . unwrapping writer ( unwrapper ) ; } return bpw ; }
string error code = parse error code ( node ) ; if ( error code = = null | | error code . equals ( malformed certificate ) ) return null ; malformed certificate exception e = ( malformed certificate exception ) super . unmarshall ( node ) ;
return stack1 ;
if ( queue mode . equals ( queue _ mode _ ip ) & & queue mode . equals ( queue _ mode _ domain ) & & queue mode . equals ( queue _ mode _ host ) ) { log . error ( unknown partition mode : + queue mode + - forcing to by host ) ; queue mode = queue _ mode _ host ; } log . info ( using queue mode : + queue mode ) ; this . crawl delay = ( long ) ( conf . get float ( fetcher . server . delay , 1 . 0f ) * 1000 ) ;
lit = new constant value expression ( ) ; lit . m _ value type = volt type . numeric ; lit . m _ value size = volt type . numeric . get length in bytes for fixed types ( ) ; bint = new constant value expression ( ) ; bint . m _ value type = volt type . bigint ; bint . m _ value size = volt type . bigint . get length in bytes for fixed types ( ) ; abstract expression lit _ bint = new operator expression ( expression type . operator _ minus , lit , bint ) ; lit _ bint . normalize operand types _ recurse ( ) ;
set permissions ( m perms list ) ; return m perms view ; }
if ( this . handler mappings = = null ) { this . handler mappings = get default strategies ( context , handler mapping . class ) ; if ( logger . is debug enabled ( ) ) { logger . debug ( no handler mappings found in servlet ' + get servlet name ( ) + ' : using default ) ; } }
change block len ( cluster , - 1 ) ;
operation set persistent presence op set presence = provider . get operation set ( operation set persistent presence . class ) ; if ( op set presence = = null ) { * * @ todo handle non persistent presence operation sets * }
if ( m _ additional measures = null & & m _ additional measures . length > 0 ) { m _ does produce = new boolean [ m _ additional measures . length ] ; if ( m _ template instanceof additional measure producer ) { enumeration en = ( ( additional measure producer ) m _ template ) . enumerate measures ( ) ; while ( en . has more elements ( ) ) { string mname = ( string ) en . next element ( ) ; for ( int j = 0 ; j < m _ additional measures . length ; j + + ) { if ( mname . compare to ignore case ( m _ additional measures [ j ] ) = = 0 ) { m _ does produce [ j ] = true ; } } } } } else { m _ does produce = null ; }
admin . modify column family ( table _ name , cf descriptor ) ; htable descriptor htd = admin . get table descriptor ( table _ name ) ;
application id app id = ps agent . get app id ( ) ;
operator reduce sink operator info2 = gen group by plan reduce sink operator2 mr ( parse info , dest , group by operator info2 , grp by exprs . size ( ) + 1 , num reducers , grouping sets present ) ;
( ( text ) primitive writable ) . set ( bytes , start , length ) ; return primitive writable ;
assert analyzes to ( a , Ø¯Ø§Ø±Ø¯ Ø®ÙØ±Ø¯Ù ÙÛâØ´ÙØ¯ , new string [ ] { Ø®ÙØ±Ø¯Ù } ) ;
return new web socket server handshaker08 ( web socket url , subprotocols , allow extensions , max frame payload length , allow mask mismatch ) ; } else if ( version . equals ( web socket version . v07 . to http header value ( ) ) ) {
assert that ( entity . get sub entities ( ) , null value ( ) ) ; }
nexmark perf . progress snapshot snapshot = new nexmark perf . progress snapshot ( ) ; snapshot . sec since start = ( now - start ms since epoch ) 1000 . 0 ; snapshot . runtime sec = perf . runtime sec ; snapshot . num events = num events ; snapshot . num results = num results ; snapshots . add ( snapshot ) ; capture steady state ( perf , snapshots ) ;
int [ ] new divisor = new int [ divisor length ] ; system . arraycopy ( divisor , 0 , new divisor , 0 , divisor length ) ; multiply multi precision ( new divisor , d1 ) ; divisor = new divisor ; multiply multi precision ( remainder , d1 ) ; }
in = new file input stream ( path _ in + settings . xml ) ; attach foreign part ( word mlpackage , word mlpackage . get main document part ( ) , external ctm , word settings . xml , in ) ;
for ( int i = 0 ; i < num zoo keeper servers ; i + + ) { file dir = new file ( base dir , zookeeper _ + i ) . get absolute file ( ) ; recreate dir ( dir ) ; int tick time to use ; if ( this . tick time > 0 ) { tick time to use = this . tick time ; } else { tick time to use = tick _ time ; } zoo keeper server server = new zoo keeper server ( dir , dir , tick time to use ) ; nioserver cnxn factory standalone server factory ; while ( true ) { try { standalone server factory = new nioserver cnxn factory ( ) ; standalone server factory . configure ( new inet socket address ( tentative port ) , configuration . get int ( hconstants . zookeeper _ max _ client _ cnxns , 1000 ) ) ; } catch ( bind exception e ) { log . debug ( failed binding zk server to client port : + tentative port ) ; this port is already in use , try to use another . tentative port + + ; continue ; } break ; } start up this zk server standalone server factory . startup ( server ) ; if ( wait for server up ( tentative port , connection _ timeout ) ) { throw new ioexception ( waiting for startup of standalone server ) ; } we have selected this port as a client port . client port list . add ( tentative port ) ; standalone server factory list . add ( standalone server factory ) ; zoo keeper servers . add ( server ) ; }
end element end element = stax parser util . get next end element ( xml event reader ) ;
exception ex = em . last exception ;
all types primary key t = new all types primary key ( ) ; list < all types primary key > ts = arrays . as list ( t , t ) ;
cm . clear ( ) ;
string user = the user < the . user @ me . com > ; tuple < string , string > tuple = commonsmail helper . create valid email from string ( user ) ; assert equals ( the . user @ me . com , tuple . x ) ; assert equals ( the user , tuple . y ) ;
return get type ( ) = = type . delete . get code ( ) ;
v = adapter . get view ( position , null , this ) ;
recognizer . consume ( ) ; return matched symbol ; }
int partial alpha = ( int ) ( alpha * normalized ) ; super . set alpha ( partial alpha ) ; super . draw ( canvas ) ; super . set alpha ( alpha ) ; } }
data output stream out = fs . create ( path , true , 4096 , ( short ) 2 , 512 , null ) ;
edge = graph . get edge ( target , source ) ;
basic array type info < out , ? > basic array info = basic array type info . get info for ( clazz ) ; if ( basic array info = null ) { return basic array info ; }
ostream . write _ string ( id ( ) ) ;
swift test utils . assert path does not exist ( fs , subdir after mkdir , test sub dir ) ; path test deep sub dir = path ( test hadoop file deep sub dir ) ;
return handle association attribute ( ( association attribute definition ) attribute definition ) ; }
path ref = new path ( snapshot log dir , file . get path ( ) . get name ( ) ) ; if ( fs . create new file ( ref ) ) { if ( fs . exists ( ref ) ) { throw new ioexception ( couldn ' t create reference for : + file . get path ( ) ) ; } } log . debug ( completed wal referencing for : + file . get path ( ) + to + ref ) ; }
olog manager . instance ( ) . error ( this , unexpected error on reading response from channel , e ) ;
print name ( node , ctx , true , preserve whitespace ) ; print simple item with indent ( value ) ; print name ( node , ctx , false , preserve whitespace ) ; out . flush ( ) ; }
return wait meta initialized ( proc , region info builder . first _ meta _ regioninfo ) ; }
consumer1 . close ( ) ;
message consumer consumer1 = new message consumer ( url1 , dest ) ;
final int captured start ;
card header header = new card header ( get activity ( ) ) ;
response time total in curr window = new atomic long array ( num levels ) ; response time count in curr window = new atomic long array ( num levels ) ; response time avg in last window = new atomic double array ( num levels ) ; response time count in last window = new atomic long array ( num levels ) ; top users count = conf . get int ( decayscheduler _ metrics _ top _ user _ count , decayscheduler _ metrics _ top _ user _ count _ default ) ;
map < capability , string > static nat service capability map = cmd . get service capabilities ( service . static nat ) ;
load fsimage from temp file ( image file ) ;
for ( int i = 0 ; i < 32768 ; i + + ) { int b = socket input stream . read ( ) ; if ( b < 0 ) { break ; } os . write ( b ) ; } } catch ( ioexception e ) {
my thread . set daemon ( false ) ; this . sip stack = sip stack impl ; my thread . set name ( event scanner thread ) ; my thread . start ( ) ; }
if ( rfc7239 = null & & rfc7239 . _ proto = null ) { request . set scheme ( rfc7239 . _ proto ) ; if ( rfc7239 . _ proto . equals ( config . get secure scheme ( ) ) ) request . set secure ( true ) ; } else if ( forwarded proto = null ) { request . set scheme ( forwarded proto ) ; if ( forwarded proto . equals ( config . get secure scheme ( ) ) ) request . set secure ( true ) ; } else if ( forwarded https = null & & ( on . equals ignore case ( forwarded https ) | | true . equals ignore case ( forwarded https ) ) ) { request . set scheme ( http scheme . https . as string ( ) ) ; if ( http scheme . https . as string ( ) . equals ( config . get secure scheme ( ) ) ) request . set secure ( true ) ; }
m return intent = true ; m ringtone picker intent = true ; toast . make text ( this , get string ( r . string . pick _ a _ file ) , toast . length _ long ) . show ( ) ; } else if ( intent . get action ( ) . equals ( intent . action _ view ) ) {
cookie spec cookie spec = registry . get cookie spec ( policy , request . get params ( ) ) ;
get async processor ( ) . process ( exchange , done sync - > result . complete ( null ) ) ;
this . probe iterator = new probe iterator < pt > ( probe side , this . probe side serializer . create instance ( ) ) ;
startup ; so we need to fire a search settings changed to notify the search bar and so on . matcher = null ; edit bus . send ( new search settings changed ( null ) ) ; } } } }
try { stopper . process ( exchange ) ; } catch ( exception e ) { exchange . set exception ( e ) ; }
context . get shutdown strategy ( ) . set timeout ( 20 * 100000 ) ;
assert user has permission ( url , guest , guest , login permission . class . get name ( ) , null , null ) ; assert user has permission ( url , guest , guest , batch permission . class . get name ( ) , target _ name _ start , null ) ; assert user has permission ( url , guest , guest , no permission . class . get name ( ) , null , null ) ; }
if ( found conflict ) { security constraint [ ] new security constraints = security constraint . create constraints ( servlet security element , url pattern ) ; for ( security constraint security constraint : new security constraints ) { add constraint ( security constraint ) ; } check constraints for uncovered methods ( new security constraints ) ; }
tree . add field path ( bar . quz ) ; assert equals ( bar . baz , bar . quz , foo , tree . to string ( ) ) ;
assert that ( global . get aggregations ( ) , not null value ( ) ) ;
if ( ( eq _ s _ b ( 1 , e ) ) ) { break lab5 ; } break lab2 ; } while ( false ) ; cursor = limit - v _ 6 ; }
receiver type = get wrapper ( receiver type ) ;
m week view . set column gap ( ( int ) typed value . apply dimension ( typed value . complex _ unit _ dip , 2 , get resources ( ) . get display metrics ( ) ) ) ; m week view . set text size ( ( int ) typed value . apply dimension ( typed value . complex _ unit _ sp , 10 , get resources ( ) . get display metrics ( ) ) ) ; m week view . set event text size ( ( int ) typed value . apply dimension ( typed value . complex _ unit _ sp , 10 , get resources ( ) . get display metrics ( ) ) ) ; } return true ; } return super . on options item selected ( item ) ; }
if ( adapter . has search text ( ) ) { flexible utils . highlight text ( holder . m title , get title ( ) , adapter . get search text ( ) ) ; flexible utils . highlight text ( holder . m subtitle , get subtitle ( ) , adapter . get search text ( ) ) ; } else { holder . m title . set text ( get title ( ) ) ; holder . m subtitle . set text ( get subtitle ( ) ) ; }
gear1 = new gear ( gear ( 1 . 0f , 4 . 0f , 1 . 0f , 20 , 0 . 7f ) , new float [ ] { 0 . 8f , 0 . 1f , 0 . 0f , 1 . 0f } ) ; gear2 = new gear ( gear ( 0 . 5f , 2 . 0f , 2 . 0f , 10 , 0 . 7f ) , new float [ ] { 0 . 0f , 0 . 8f , 0 . 2f , 1 . 0f } ) ; gear3 = new gear ( gear ( 1 . 3f , 2 . 0f , 0 . 5f , 10 , 0 . 7f ) , new float [ ] { 0 . 2f , 0 . 2f , 1 . 0f , 1 . 0f } ) ; gl matrix mode ( gl _ projection ) ;
delete file ( name ) ; try ( index output out = in . create output ( name , lucene test case . new iocontext ( random state ) ) ; index input ii = in . open input ( temp file name , lucene test case . new iocontext ( random state ) ) ) { out . copy bytes ( ii , ii . length ( ) ) ; } catch ( ioexception ioe ) { throw new runtime exception ( hit unexpected ioexception while trying to corrupt file + name , ioe ) ; }
assert that ( get attribute alias names ( xml ) , contains in any order ( groovy ) ) ;
system . out . println ( error terminating instances ) ; system . out . println ( caught exception : + e . get message ( ) ) ; system . out . println ( reponse status code : + e . get status code ( ) ) ; system . out . println ( error code : + e . get error code ( ) ) ; system . out . println ( request id : + e . get request id ( ) ) ; }
ibreakpoint dmcontext [ ] breakpoints = get breakpoints ( f breakpoints dmc ) ; assert equals ( breakpoints service reports unexpected number of breakpoints , 1 , breakpoints . length ) ; mibreakpoint dmdata bkpt _ svc = ( mibreakpoint dmdata ) get breakpoint ( breakpoints [ 0 ] ) ; assert equals ( incorrect breakpoint condition , condition _ 1 , bkpt _ svc . get condition ( ) ) ; resume and expect bkpt hit ( bkpt _ svc . get number ( ) , 2 ) ;
slice _ from ( ugli ) ;
illegal argument exception e bad index = expect throws ( illegal argument exception . class , ( ) - > prepare create ( test _ bad , settings . builder ( ) . put ( index . number _ of _ shards , 5 ) ) . get ( ) ) ; assert that ( e bad index . get message ( ) , contains string ( partition size [ 6 ] should be a positive number + less than the number of shards [ 5 ] ) ) ;
em . get transaction ( ) . begin ( ) ; re2 = em . find ( referenced entity . class , re _ id2 ) ; cie = em . find ( child ing entity . class , c _ id ) ; cie . set referenced ( re2 ) ; em . get transaction ( ) . commit ( ) ; }
string ftp url = ftp : admin @ localhost : + get port ( ) + incoming data1 ?password = admin & binary = true ; endpoint endpoint = context . get endpoint ( ftp url ) ; exchange exchange = endpoint . create exchange ( ) ; exchange . get in ( ) . set body ( ioconverter . to file ( src test data ftpbinarytest logo1 . jpeg ) ) ; exchange . get in ( ) . set header ( exchange . file _ name , logo1 . jpeg ) ; producer producer = endpoint . create producer ( ) ; producer . start ( ) ; producer . process ( exchange ) ; producer . stop ( ) ; ftp url = ftp : admin @ localhost : + get port ( ) + incoming data2 ?password = admin & binary = true ;
_ flush buffer ( ) ;
if ( item . is selectable ( ) ) { return ; }
among _ var = find _ among ( a _ 0 , 24 ) ;
assigned = allocator . schedule ( ) ;
int read buffer size = striped read buffer size ; buffer size = read buffer size < bytes per checksum ? bytes per checksum : read buffer size - read buffer size % bytes per checksum ; }
owner document . set mutation events ( orig ) ;
if ( file name . starts with ( jar : ) & & file name . contains ( resource _ files . zip ) ) { must remove jar : prefix , or else qualify from file path fails on windows return res name . qualify from file path ( android , file name . replace first ( jar : , ) ) ; } else { return res name . qualify from file path ( shadow application . get instance ( ) . get app manifest ( ) . get package name ( ) , file name ) ; } }
lock context . clear lock ids ( ) ;
latch . await ( 60 , seconds ) ;
if ( value = null ) { delegate . set ( instance , value ) ; } }
analyzer a = new factory analyzer ( asserting tokenizer , null , factory ) ;
execute ( select * from t order by x limit ? , new object [ ] { limit } ) ;
return new resource factory service impl ( ) ;
vt = client . call procedure ( @ ad hoc , select t1 . pk , t1 . name , t1 . pt + from t as t1 + where t1 . pt is null + order by t1 . pk , t1 . pt ; ) . get results ( ) [ 0 ] ; assert approximate content of table ( new object [ ] [ ] { { 2 , atlantis , null } } , vt , epsilon ) ;
set layer type ( view . layer _ type _ software , null ) ;
long offset table offset = count ; int offset size in bytes = compute offset size in bytes ( count ) ; for ( long offset : offsets ) { write bytes ( offset , offset size in bytes ) ; } if ( version = version _ 15 ) { write trailer 6 null bytes write ( new byte [ 6 ] ) ; size of an offset write ( offset size in bytes ) ; size of a ref write ( id size in bytes ) ; number of objects write long ( id map . size ( ) ) ; top object write long ( id map . get ( root ) ) ; offset table offset write long ( offset table offset ) ; } out . flush ( ) ;
user storage . unregister on tray preference change listener ( null ) ; assert equals ( 1 , user storage . m listeners . size ( ) ) ; user storage . unregister on tray preference change listener ( listener ) ; assert equals ( 0 , user storage . m listeners . size ( ) ) ; assert null ( user storage . m observer ) ; assert null ( user storage . m observer thread ) ; }
for ( int j = i + 1 ; j < sz preds ; j + + ) { int j label = preds . get ( j ) ; basic block j block = blocks . label to block ( j label ) ; if ( j block . get successors ( ) . size ( ) = = 1 & & compare insns ( i block , j block ) ) { to combine . add ( j label ) ; to delete . set ( j label ) ; } } combine blocks ( i label , to combine ) ;
time zone tz = time zone . get time zone ( plus _ tzs ) ; date math parser p = new date math parser ( tz ) ; set now ( p , 2001 - 07 - 04 t12 : 08 : 56 . 235 ) ;
final int init = cpg . add methodref ( sort _ iterator , < init > , ( + node _ iterator _ sig + node _ sort _ factory _ sig + ) v ) ;
union type number object = ( union type ) create union type ( number _ type , object _ type ) ;
subroutine starts [ target offset ] = subroutine start ;
{ long start time = now ; long end time = now + 1 ; list < string > shard ids = new array list < string > ( ) ; for ( int i = 0 ; i < 100 ; i + + ) { shard ids . add ( shard id _ + i ) ; } stream job stream job = new stream job ( data table , stream id , version _ 0 , new hash set < string > ( shard ids ) , start time , end time ) ; ctt . write stream job ( stream job ) ; list < shard checkpoint > checkpoints = new array list < shard checkpoint > ( ) ; for ( int id = 0 ; id < shard ids . size ( ) ; id + + ) { string shard id = shard ids . get ( id ) ; shard checkpoint checkpoint = new shard checkpoint ( shard id , stream job . get version ( ) , shard _ iterator _ + id , id ) ; checkpoints . add ( checkpoint ) ; ctt . write checkpoint ( end time , checkpoint ) ; } map < string , shard checkpoint > checkpoints got = new hash map < string , shard checkpoint > ( ) ; boolean find checkpoint = ctt . get and check all checkpoints ( end time , stream id + , checkpoints got ) ; assert true ( find checkpoint ) ; assert true ( checkpoints got . is empty ( ) ) ; }
for ( t o : objects ) { ret . add ( o ) ; }
throw new runtime exception ( channel writer output view : memory segments have been taken from return queue by different actor . ) ; } list . add ( m ) ; }
status = ( executor . get output string ( ) . contains ( ) ) ;
else { metaph add ( x ) ; }
if ( saved instance state = = null ) { database service . get instance ( ) . create instagram headers database ( 15 ) ; } initialize recycler view ( ) ;
assert has type variable ( new from < integer > ( ) { } . new to < string > ( ) . type ( ) ) ; assert has type variable ( new from < t > ( ) { } . new to < string > ( ) . type ( ) ) ;
builder = new builder ( new builder class ) ;
if ( e . get annotation ( ) = = visited _ edge ) { continue ; } e . set annotation ( visited _ edge ) ; if ( ignore edge ( e ) ) { continue ; }
default snapshot data target . set rate ( 1024 * 10 ) ;
created at version = read ( values [ index + + ] ) ; }
a2 . set photographer id ( 2 ) ;
col vector . is null [ batch index ] = false ;
listenable future < ? > future = enqueue page ( buffer , create page ( 3 ) ) ;
operands . add ( beam sql primitive . of ( sql type name . integer , 2 ) ) ; operands . add ( beam sql primitive . of ( sql type name . integer , 4 ) ) ; operands . add ( beam sql primitive . of ( sql type name . integer , 5 ) ) ; assert . assert false ( new beam sql round expression ( operands ) . accept ( ) ) ; }
test extern changes ( alert ( symbol . thimble ) ; , , alert ( symbol . thimble ) ) ;
compare heap size java vs script ( get random config ( ran ) , 0 . 01f ) ;
final async context async context = recorder . record next async context ( true ) ;
try { service . cancel sink ( this ) ; } catch ( wmiexception e ) { log . warn ( e ) ; }
write file ( local fs , src path , 1 ) ; assert true ( local fs . exists ( local fs . get checksum file ( src path ) ) ) ; assert true ( local fs . rename ( src path , dst path ) ) ; assert true ( local fs . exists ( local fs . get checksum file ( real dst path ) ) ) ;
int l = this . linebuffer . length ( ) ; if ( l > 0 ) { if ( this . linebuffer . byte at ( l - 1 ) = = http . lf ) { l - - ; this . linebuffer . set length ( l ) ; } discard cr if found if ( l > 0 ) { if ( this . linebuffer . byte at ( l - 1 ) = = http . cr ) { l - - ; this . linebuffer . set length ( l ) ; } } } l = this . linebuffer . length ( ) ; if ( this . ascii ) { charbuffer . append ( this . linebuffer , 0 , l ) ; } else { this is very memory inefficient , but since non - ascii charsets are not meant to be used anyway , there ' s no point optimizing it string s = new string ( this . linebuffer . buffer ( ) , 0 , l , this . charset ) ; charbuffer . append ( s ) ; } return l ;
matcher policy matches = pattern . compile ( policy _ append _ flag + = ? ( [ ^ ] + ) ) . matcher ( commands . get ( 0 ) ) ;
cam . set location ( new vector3f ( 0 , 4f , 6f ) ) ;
} else if ( java . lang . long . equals ( param type . get name ( ) ) | | long . equals ( param type . get name ( ) ) ) { try { params [ 0 ] = long . value of ( value ) ; } catch ( number format exception ex ) { ok = false ; }
m edit text . set selection ( selection ) ;
assert rows ( execute ( select distinct a , count ( a ) from % s group by a ) , row ( 1 , 1 l ) , row ( 2 , 1 l ) , row ( 4 , 1 l ) ) ; assert invalid message ( grouping on clustering columns is not allowed for select distinct queries , select distinct a , count ( a ) from % s group by a , b ) ;
for ( int index = 0 ; index < get child count ( ) ; index + + ) { if ( get child at ( index ) = = m circle view ) { m circle view index = index ; break ; } } }
headers . put ( content - type , text plain ; charset = utf - 8 ) ; assert equals ( utf - 8 , http header parser . parse charset ( headers , iso - 8859 - 1 ) ) ;
this . ordinal cache = ordinal cache = = null ? new lruhash map < facet label , integer > ( default _ cache _ value ) : ordinal cache ; this . category cache = category cache = = null ? new lruhash map < integer , facet label > ( default _ cache _ value ) : category cache ; this . taxo arrays = taxo arrays = null ? new taxonomy index arrays ( index reader , taxo arrays ) : null ;
drf . classification = true ; drf . ntrees = ntree ; drf . max _ depth = max _ depth ; drf . min _ rows = 1 ; = nodesize drf . nbins = nbins ; drf . mtries = - 1 ; drf . sample _ rate = 0 . 66667f ; simulated sampling with replacement drf . seed = ( 1 l < < 32 ) | 2 ; drf . destination _ key = key . make ( drf _ model _ 4 _ + hexnametrain ) ;
calendar gmt _ calendar = calendar . get instance ( time zone . get time zone ( gmt ) , locale . us ) ; assert equals ( time zone . get time zone ( gmt ) , gmt _ calendar . get time zone ( ) ) ; calendar est _ calendar = calendar . get instance ( time zone . get time zone ( est ) , locale . us ) ; assert equals ( time zone . get time zone ( est ) . get id ( ) , est _ calendar . get time zone ( ) . get id ( ) ) ; }
result = run unguarded ( caller clazz , run ) ; assert equals ( 123 , result . return value ) ; callerbytes = load bytes for class ( tgt . static icaller002 ) ;
fsck out = run fsck ( conf , 0 , true , test file , - files , - maintenance , - blocks , - replica details ) ; assert true ( fsck out . contains ( ( decommissioned ) ) ) ; assert false ( fsck out . contains ( ( entering maintenance ) ) ) ; assert true ( fsck out . contains ( ( in maintenance ) ) ) ;
int abc = 0 ; int abc = 0 ;
ibreakpoints target dmcontext bp target dmc = dmcontexts . get ancestor of type ( dmc , ibreakpoints target dmcontext . class ) ;
serializer by version [ current _ record _ version ] . serialize ( ( odocument ) i source , container , true ) ; return container . fit bytes ( ) ;
final chained driver < ? , ? > ct ; try { class < ? extends chained driver < ? , ? > > ctc = config . get chained task ( i ) ; ct = ctc . new instance ( ) ; } catch ( exception ex ) { throw new runtime exception ( could not instantiate chained task driver . , ex ) ; }
dfs cluster . restart name node ( 0 , true , - rolling upgrade , rollback ) ;
for ( metrics tag t : tags ) { pattern pat = include tag patterns . get ( t . name ( ) ) ; if ( pat = null & & pat . matcher ( t . value ( ) ) . matches ( ) ) { return true ; } }
reconnect task . command . run ( ) ;
from ( direct : start ) . wire tap ( direct : foo , false , constant ( bye world ) ) . to ( mock : result ) ; from ( direct : foo ) . to ( mock : foo ) ;
this . zk watcher . start ( ) ;
final float up x = 0 . 0f ; final float up y = 1 . 0f ; final float up z = 0 . 0f ; matrix . set look at m ( m view matrix , 0 , eye x , eye y , eye z , look x , look y , look z , up x , up y , up z ) ;
break ; case missing : string add on id = ( string ) issue details . get ( 0 ) ; if ( add on searcher = null ) { add on = add on searcher . search add on ( add on id ) ; } else { add on = null ; } if ( add on = = null ) { message = constant . messages . get string ( cfu . warn . addon . with . missing . requirements . addon . id , add on id ) ; } else { message = constant . messages . get string ( cfu . warn . addon . with . missing . requirements . addon , add on . get name ( ) ) ; } break ;
map < string , map < integer , transaction state > > spouts = new last success state tracker . get spouts ( ) ; for ( map < integer , transaction state > component states : spouts . values ( ) ) { rebuild component states ( component states , old last success state tracker ) ; }
reporters to start = conf . get ( metastore conf . conf vars . hive _ metrics _ reporter . get hive name ( ) ) ;
assert false ( repo . add ( key01 ) ) ;
manifest = file fs file . from ( build output dir , manifests , aapt , flavor , abi split , type , manifest name ) ;
if ( ( ( frame body txxx ) new frame . get body ( ) ) . get description ( ) . equals ( ( ( frame body txxx ) next frame . get body ( ) ) . get description ( ) ) ) { li . set ( new frame ) ; frame map . put ( new frame . get id ( ) , frames ) ; return ; }
buck kill command handler handler = new buck kill command handler ( project , project . get base dir ( ) , buck command . kill ) ;
block grp id generator . set current value ( block group id initial value ) ;
int drawable id ;
final parcelable state = m bottom navigation . on save instance state ( ) ;
file info plist = new file ( lproj , info plist . strings ) ;
load balancer client cli . run discovery ( _ zk hosts , d2 , d2 _ config _ data ) ;
return message ; }
slave node child . remove from parent ( ) ; slave node child = null ; break ; }
if ( statement block . is empty ( ) ) { add statement ( node . get code ( ) ) ; }
m origin handler = null ; m load listener = null ; }
throw new ioexception ( could not find + autogen _ ddl _ file _ name + in the catalog + compiled by volt db + version from catalog ) ;
args = new string [ ] { - remove from cluster node labels } ; assert true ( 0 = rm admin cli . run ( args ) ) ;
issuing distribution point deltaidp = null ; try { deltaidp = issuing distribution point . get instance ( cert path validator utilities . get extension value ( delta crl , issuing _ distribution _ point ) ) ; } catch ( exception e ) { throw new annotated exception ( issuing distribution point extension from delta crl could not be decoded . , e ) ; } boolean match = false ;
for ( i = 0 ; i < children . size ( ) ; i + + ) read from xml ( result , node . get attribute ( att _ name ) , ( element ) children . get ( i ) ) ; return result ; }
focus timer . set cache nodes ( prompt container ) ;
dynamic partition ctx dp ctx2 = new dynamic partition ctx ( dp ctx ) ; fs output desc . set dyn part ctx ( dp ctx2 ) ; }
current buffer index = - 1 ;
local conf . set long ( raid . policy . rescan . interval , 3600 * 1000 l ) ; cnode = raid node . create raid node ( local conf ) ; final int max _ waittime = 120000 ; dist raid node dcnode = ( dist raid node ) cnode ; long start = system . current time millis ( ) ;
future < string > response3 = new test request collapser ( timer , 3 ) . queue ( ) ;
execute async for result ( master client , operations . create operation ( add , path address . path address ( other _ server _ group , deployment _ overlay _ path , path element . path element ( deployment , other _ runtime _ name ) ) . to model node ( ) ) ) ;
drop subsumed checkpoints ( checkpoint id ) ; } } finally {
return this . dom = = other . dom ;
get record by id . get parameter ( ) . add ( ecore util . copy ( param ) ) ;
runnable split shard = new runnable ( ) { @ override public void run ( ) { try {
for ( int i = 0 ; i < 5 ; i + + ) { ( ( db txn manager ) txn mgr ) . acquire locks ( qp , ctx , peter i + i , true ) ; no heartbeat }
resource [ ] ldifs ; if ( ctxt = = null ) { not running within an app context ldifs = new path matching resource pattern resolver ( ) . get resources ( ldif resources ) ; } else { ldifs = ctxt . get resources ( ldif resources ) ; }
assert . assert false ( await ( task , 1 , time unit . seconds ) ) ;
assert that ( test object . string , equal to ( a string ) ) ;
list < integer > uptos = new array list < > ( ) ; int tot doc count = 0 ; for ( int i = 0 ; i < sub count ; i + + ) { int max doc = test util . next int ( random ( ) , 1 , 1000 ) ; uptos . add ( 0 ) ; old to new . add ( new int [ max doc ] ) ; tot doc count + = max doc ; } list < int [ ] > completed subs = new array list < > ( ) ;
system . arraycopy ( value , start + offset , buffer , index , end - start ) ; }
servlet response . set status ( status . get status code ( ) , status . get reason phrase ( ) ) ;
environment . get vrmouse manager ( ) . update ( tpf ) ;
document = new document ( ) ; encoded range = range type . encode ranges ( singleton ( new range field mapper . range ( range type , - 20 l , 30 l , true , true ) ) ) ; document . add ( new binary doc values field ( field name , encoded range ) ) ; writer . add document ( document ) ;
clazz . get property ( fetch . association ( ) ) ; profile . add fetch ( fetch . entity ( ) . get name ( ) , fetch . association ( ) , fetch . mode ( ) . to string ( ) . to lower case ( locale . root ) ) ; }
task task = task service . create task query ( ) . single result ( ) ; task service . complete ( task . get id ( ) ) ; assert equals ( hello from the process , runtime service . get variable ( process instance . get id ( ) , greeting ) ) ;
if ( m _ positions = = null | | m _ body capacity > m _ positions . length ) { final position [ ] old = m _ positions = = null ? new position [ 0 ] : m _ positions ; m _ positions = new position [ m _ body capacity ] ; system . arraycopy ( old , 0 , m _ positions , 0 , old . length ) ; for ( int i = old . length ; i < m _ positions . length ; i + + ) { m _ positions [ i ] = new position ( ) ; } }
final object id generator strategy provider setting = configuration values . remove ( available settings . identifier _ generator _ strategy _ provider ) ;
new state = create state ( num nodes in cluster - 1 , random boolean ( ) , initial indices ) ;
test reference ( var x ; , false ) ;
int gname = cpg . add interface methodref ( dom _ intf , get node name , ( i ) ljava lang string ; ) ;
aaa = new key value ( a , fam , qf , 1 , key value . type . delete , a ) ;
unique keys = get paged ( store , 4 , build expression ( first name , operator . like _ contains , utf8 type . instance . decompose ( a ) ) , build expression ( age , operator . eq , int32 type . instance . decompose ( 36 ) ) ) ; expected = new array list < string > ( ) { { add ( key22 ) ; add ( key08 ) ; add ( key07 ) ; } } ;
try { this . mystrom discovery ( ) ; } catch ( ioexception e ) { logger . error ( error doing discovery of your devices , e ) ; } }
sb . append ( char buf . to string ( ) ) ;
for ( int i = index ; i < size ; i + + ) { result = resolvers [ i ] . invoke ( context , base , target method , param types , params ) ; if ( context . is property resolved ( ) ) { return result ; } } return null ;
current menu = null ;
string [ ] temps = infos [ 0 ] . split ( : ) ;
reset image failure ( ) ; status updater . set container id ( container id ) ; }
animation utils . backup overflow ( style ) ; } animation utils . enable transitions ( style ) ; if ( options . collapse ) {
if ( i + 2 < = limit & & input . region matches ( i , : : , 0 , 2 ) ) {
jtx transaction jtx = worker . maybe request transaction ( supports ( ) , ctx _ 1 ) ; assert not null ( jtx ) ; assert false ( jtx . is active ( ) ) ; db session session1 = session provider . get db session ( ) ; execute update ( session1 , insert into girl values ( 1 , ' sophia ' , null ) ) ;
while ( it . has next ( ) ) { wrapper invocation handler handler = ( wrapper invocation handler ) it . next ( ) ; try { ( ( result set ) handler . delegate ) . close ( ) ; } catch ( exception ex ) { } try { handler . close surrogate ( ) ; } catch ( exception e ) { } }
batch . attach child ( cube ) ;
update ssokey ( ) ;
get active layout ( ) . done hiding ( ) ; } else { start showing ( m simple animation layout , false ) ; } get active layout ( ) . on tab creating ( source id ) ; }
schema = create table t0 ( id bigint not null unique , name varchar ( 32 ) not null , age integer ) ; \ n + partition table t0 on column id ; \ n ;
input stream input stream = new byte array input stream ( bytes ) ; orc metadata reader metadata reader = new orc metadata reader ( ) ; list < hive bloom filter > bloom filters = metadata reader . read bloom filter indexes ( input stream ) ; assert equals ( bloom filters . size ( ) , 1 ) ; assert true ( bloom filters . get ( 0 ) . test string ( test _ string ) ) ;
string answer = pattern ; if ( pattern . contains ( counter ) ) { only increment the counter on - demand answer = pattern . replace first ( counter , + next name counter ( ) ) ; }
verify stmt fails ( client , insert into p1 ( ccc , zzz ) values ( 0 , 1 ) ; , attempted to divide 1 by 0 ) ;
return default native lib dir ;
if ( has ( ) ) { for ( notification notification : get ( ) ) { if ( notification . read ) { for ( notification new notification : notification list ) { if ( new notification . id = = notification . id ) { new notification . read = false ; break ; } } } } }
string security domain = props . get property ( constants . security _ security _ domain ) ; if ( security domain = null ) builder . add dependency ( security _ domain _ service _ name . append ( security domain ) ) ;
media player . set on completion listener ( new media player . on completion listener ( ) { public void on completion ( media player player ) { player . seek to ( 0 ) ; } } ) ; asset file descriptor file = activity . get resources ( ) . open raw resource fd ( r . raw . beep ) ;
final rendered image ri overview sf = reader overview sf . read ( 0 ) ;
servlet context handler handler = new servlet context handler ( ) ; active sessions sessions = new active sessions ( 3 , minutes ) ;
rm2 . start ( ) ;
if ( conv criterion = null & & conv criterion agg name = null ) { value v = aggregators . get ( conv criterion agg name ) . get aggregate ( ) ; if ( conv criterion . is converged ( superstep , v ) ) { break ; } }
assert equals ( 1 , matches . length ) ;
query = new tree query ( project _ uuid ) . set name or key query ( one ) . build ( ) ;
return new method info ( camel context , clazz , method , parameters , body parameters , has custom annotation , has handler annotation ) ;
tester . clock ( ) . advance ( duration . of minutes ( 3 ) ) ;
m tabs . set indicator color resource ( r . color . dark _ slate _ blue ) ; m tabs . set indicator height ( quick return utils . dp2px ( this , 6 ) ) ; m tabs . set divider color ( get resources ( ) . get color ( android . r . color . transparent ) ) ; m tabs . set underline height ( quick return utils . dp2px ( this , 0 ) ) ;
forseti lock manager . lock existing lock ;
with terminal sized ( 4 , 2 ) ;
throw new ftp exception can not have data connection ( host attempting data connection + host1 . get host address ( ) + is not same as server + host2 . get host address ( ) + so we intentionally close it for security precaution . ) ;
request errors . add ( errors ) ;
if ( namespace exists ( ns ) ) { admin . delete namespace ( ns ) ; }
log . warn ( unable to read + znode path + due to : + cle ) ;
{ built in type . duration , p4 y5 m , period . of ( 4 , 5 , 0 ) } , { built in type . duration , p6 y1 m , period . of ( 6 , 1 , 0 ) } , { built in type . duration , - p6 y1 m , period . of ( - 6 , - 1 , 0 ) } , { built in type . duration , p0 m , period . of ( 0 , 0 , 0 ) } ,
if ( filename . contains ( tables . get ( i ) + - ) ) return true ;
return double operators . cast to varchar ( parser . get double value ( ) ) ; case value _ number _ int :
if ( storage configs . contains key ( file backed caching storage configuration . type _ name ) ) { storage configs . put ( file backed caching storage configuration . type _ name , new file backed caching storage configuration ( voldemort config ) ) ; }
ignore validity of paths = true ;
c id = create container id ( ) ;
result . expected message count ( 0 ) ;
case 5 : ' \ 005 '
tracer1 . stream closed ( status . cancelled ) ;
if ( hid service = null ) { switch ( event . get id ( ) ) { case mouse event . mouse _ pressed : hid service . mouse press ( event . get modifiers ( ) ) ; break ; case mouse event . mouse _ released : hid service . mouse release ( event . get modifiers ( ) ) ; break ; case mouse event . mouse _ moved : int origin x = origin = null ? origin . x : 0 ; int origin y = origin = null ? origin . y : 0 ; * x and y position are sent in percentage but we multiply * by 1000 in depacketizer because we cannot passed the size * to the provider * int x = origin x + ( ( event . get x ( ) * size . width ) 1000 ) ; int y = origin y + ( ( event . get y ( ) * size . height ) 1000 ) ; hid service . mouse move ( x , y ) ; break ; case mouse event . mouse _ wheel : mouse wheel event evt = ( mouse wheel event ) event ; hid service . mouse wheel ( evt . get wheel rotation ( ) ) ; break ; default : break ; } }
put ( cert path validator . pkix , sun . security . provider . certpath . pkixcert path validator ) ;
try { return do invoke ( invocation , ( rmi invocation handler ) stub ) ; } catch ( remote exception ex ) { throw rmi client interceptor utils . convert rmi access exception ( invocation . get method ( ) , ex , is connect failure ( ex ) , get service url ( ) ) ; } catch ( invocation target exception ex ) { throwable ex to throw = ex . get target exception ( ) ; remote invocation utils . fill in client stack trace if possible ( ex to throw ) ; throw ex to throw ; } catch ( throwable ex ) { throw new remote invocation failure exception ( invocation of method [ + invocation . get method ( ) + ] failed in rmi service [ + get service url ( ) + ] , ex ) ; }
assert . fail ( ) ; } }
uri uri = telephony . mms sms . search _ uri . build upon ( ) . append query parameter ( pattern , m search string ) . build ( ) ;
long p read test time new = t1 - t0 ;
assert equals ( null , cache ( 0 , cache name ) . merge ( other key , y , ( o , n ) - > x . equals ( o ) ? null : unexpected ) ) ;
assert equals ( 6 , cache . get stats ( ) . get eviction count ( ) ) ;
msg . status = emmessage . status . create ; adapter . refresh ( ) ;
start time ms = system . current time millis ( ) ; for ( int i = 0 ; i < 10 ; i + + ) simple counter . count ( 100 ) ;
int order key = registration . get subscriber ( ) . hash code ( ) ;
if ( dialog request . get method ( ) . equals ( request . ack ) ) throw new runtime exception ( illegal method ) ;
list < row > gets = new array list < row > ( ) ; for ( int i = 0 ; i < 100 ; i + + ) { get get = new get ( one _ row ) ; byte [ ] qual = bytes . to bytes ( column + i ) ; get . add column ( bytes _ family , qual ) ; gets . add ( get ) ; } object [ ] multi res = table . batch ( gets ) ;
final boolean has locale = format . has locale ( ) ; final boolean has tz = format . has time zone ( ) ; final boolean as string = ( shape = = json format . shape . string ) ; if ( has locale & & has tz & & as string ) { return this ; }
list view . set menu creator ( creator ) ;
} reader . set content handler ( handler ) ;
new reader = class name . equals ( reader class name ) ; reader class name = class name ; }
put all cookies in asingle header ( host , matcher , cookies ) ;
in = - 1 ;
assert exit invocations ( at least ( 1 ) ) ;
if ( depth > = minimum depth ) { final int type = ast . get type ( ) ; if ( type < = counts . length ) { counts [ type - 1 ] + + ; } }
string url = project links?id = + project key ; return open ( url , project links page . class ) ;
boolean matches = this . condition provider . check main class ( collections . < string > empty list ( ) ) ;
final dfsclient dfs client = new dfsclient ( new inet socket address ( localhost , cluster . get name node port ( ) ) , conf ) ; final string block file to corrupt = file names [ 0 ] ; final corrupted test file ctf = new corrupted test file ( block file to corrupt , sets . new hash set ( 0 ) , dfs client , num datanodes , dfs block size ) ; ctf . corrupt blocks ( cluster ) ;
lp2 . top margin = lp1 . top margin ; lp3 . bottom margin = lp1 . bottom margin ; lp3 . left margin = lp2 . left margin ; lp3 . right margin = lp2 . right margin ;
( ( abstract pooled derived byte buf ) unwrapped derived ) . parent ( this ) ; resource leak tracker < byte buf > new leak = abstract byte buf . leak detector . track ( derived ) ;
for ( int i = 0 ; i < target words - 1 ; i + + , source index + + ) result . words [ i ] = word aligned ? words [ source index ] : ( words [ source index ] > > > from index ) | ( words [ source index + 1 ] < < - from index ) ;
request . get parameter map ( ) ;
if ( position < = num headers | | ( position > = get count ( ) - num footers ) ) { return top ; } int div height = get divider height ( ) ;
parser . register action ( mapping mode . n , vim motion goto mark , command . type . motion , command . flag _ mot _ exclusive | command . flag _ save _ jump , new shortcut ( ' ` ' ) , argument . type . character ) ; parser . register action ( mapping mode . n , vim motion goto mark line , command . type . motion , command . flag _ mot _ linewise | command . flag _ save _ jump , new shortcut ( ' \ ' ' ) , argument . type . character ) ; parser . register action ( mapping mode . n , vim motion goto mark , command . type . motion , command . flag _ mot _ exclusive , new shortcut ( g` ) , argument . type . character ) ; parser . register action ( mapping mode . n , vim motion goto mark line , command . type . motion , command . flag _ mot _ linewise , new shortcut ( g ' ) , argument . type . character ) ;
mockito . do nothing ( ) . when ( wrapper model ) . validate ( ) ; + + method count ; ltrscoring model wrapped model = mockito . mock ( ltrscoring model . class ) ;
throw new org . apache . axis2 . databinding . adbexception ( uri cannot be null ) ;
right state . copy to ( merged state ) ; right state . move to next ( ) ; left state . move to next ( ) ; } else if ( cmp > 0 ) { right state . copy to ( merged state ) ; right state . move to next ( ) ; }
wrapper policy most restrictive = wrapper policy . read write ( null ) ; for ( published info layer : ( ( layer group info ) info ) . get layers ( ) ) { wrapper policy policy = build wrapper policy ( access manager , user , layer , layer . get name ( ) , mixed mode behavior ) ; if ( access level . hidden . equals ( policy . get access level ( ) ) ) { return policy ; } int comparison = policy . compare to ( most restrictive ) ; boolean this one is more restrictive = comparison < 0 ; if ( this one is more restrictive ) { most restrictive = policy ; } } return most restrictive ;
run and verify ( udf1 , null , null ) ;
final int offset = calc element offset ( current tail , mask ) ;
return new must be closed annotated constructor ( ) ;
if ( obj = = null ) { return false ; }
iter = new one step iterator forward ( compiler , op pos , analysis ) ; } else { if ( false | | debug _ iterator _ creation ) diagnose iterator ( one step iterator , analysis , compiler ) ;
jsonarray a2 = new jsonarray ( my enum . values ( ) ) ;
int alt29 = 2 ;
zktest util . expire session ( localhost : + port , c . get zoo keeper ( ) , 30 , time unit . seconds ) ;
while ( current doc id < start doc id & & iterator . has next ( ) ) { current doc id = iterator . next ( ) ; }
int num read = zz reader . read ( zz buffer , zz end read , zz buffer . length - zz end read ) ; if ( num read > 0 ) { zz end read + = num read ; return false ; }
le in . skip bytes ( 8 ) ; } byte [ ] data = new byte [ ico length ] ; ( ( input stream ) le in ) . read ( data , 0 , ico length ) ;
if ( class name = null ) return load class ( loader , class name ) ;
parent table = helper . table name ( helper . get shuffled number ( i start ) ) ;
if ( current block = null ) { current block . max stack size = max stack size ; current block = null ; }
return - 1 ; } catch ( null pointer exception ex ) {
fake http servlet response response = send command ( post , null , new json object ( ) ) ; assert equals ( 500 , response . get status ( ) ) ; json object json response = new json parser ( ) . parse ( response . get body ( ) ) . get as json object ( ) ; assert equals ( error codes . unknown _ command , json response . get ( status ) . get as int ( ) ) ; json object value = json response . get ( value ) . get as json object ( ) ; assert true ( value . get ( message ) . get as string ( ) . starts with ( post ) ) ; }
rot . mul to out unsafe ( q a , temp . set ( m _ local anchor a ) . sub local ( m _ local center a ) , m _ r a ) ; rot . mul to out unsafe ( q b , temp . set ( m _ local anchor b ) . sub local ( m _ local center b ) , m _ r b ) ; m _ u . set ( c b ) . add local ( m _ r b ) . sub local ( c a ) . sub local ( m _ r a ) ;
switch ( mode ) { case min : sub qb . add projection ( min , sub qb . get alias ( ) , property name , false ) ; break ; case max : sub qb . add projection ( max , sub qb . get alias ( ) , property name , false ) ; }
m event text paint = new text paint ( paint . anti _ alias _ flag | paint . linear _ text _ flag ) ;
model . copy session db ( this . file name , file name ) ;
log . debugf ( starting 2 new nodes ) ; add node ( get global configuration builder ( r2 ) , builder ) ; add node ( get global configuration builder ( r2 ) , builder ) ;
receiver . connect new data set as input ( sender , distribution pattern . pointwise , result partition type . pipelined ) ; final job graph job graph = new job graph ( partial consume of pipelined result , sender , receiver ) ;
if ( on ( false props , stored ) ) { int pp = stored | binary | large _ field ; if ( on ( pp , true props ) ) { throw new runtime exception ( schema field : + name + conflicting stored field options : + props ) ; } p & = pp ; } if ( on ( false props , indexed ) ) { int pp = ( indexed | store _ termvectors | store _ termpositions | store _ termoffsets | store _ termpayloads ) ; if ( on ( pp , true props ) ) { throw new runtime exception ( schema field : + name + conflicting ' true ' field options for non - indexed field : + props ) ; } p & = pp ; }
t . set c1 ( 3 ) ;
broker uri = _ broker uri ;
if ( m ptr indicator . is over offset to keep header while loading ( ) & & stay for loading ) { m scroll checker . try to scroll to ( m ptr indicator . get offset to keep header while loading ( ) , m duration to close ) ; } else {
cache = new cache ( crl cache - 0 , 100 , false , false , 20 , 10 ) ;
for ( string endpoint : config . get endpoints ( ) ) { configuration . add endpoint ( endpoint ) ; } if ( config . get log handler ( ) = null ) { final js log callback callback = config . get log handler ( ) ; js log provider . set log callback ( new js log provider . log callback ( ) { @ override public void log ( string tag , string level , string message ) { callback . log ( tag , level , message ) ; } } ) ; }
structure loop data = ( structure ) mesh structure . get field value ( ldata ) ;
fileset entry fileset entry = ( fileset entry ) iterables . get only element ( values ) ;
surround with try catch refactoring refactoring = surround with try catch refactoring . create ( cu , offset , length ) ; if ( refactoring = = null ) return ; refactoring . set leave dirty ( true ) ;
dispatcher disp = new sort merge join task dispatcher ( pctx ) ; task graph walker ogw = new task graph walker ( disp ) ;
rules . add ( new filter chain < > ( singleton list ( identity filter . as ( ) ) ) ) ; }
angle = ( float ) ( math . atan2 ( y2 - n cy , x2 - n cx ) * 180 math . pi ) ;
int distance to bottom = bottom of bottom child - list unfaded bottom ;
namespace uri = namespaces . get uri ( q name . get prefix ( ) ) ;
kerberos test utils . do as ( client3 , new callable < delegation token > ( ) { @ override public delegation token call ( ) throws exception { string body = generate renew token body ( media type , response token . get token ( ) ) ; client response response = resource ( ) . path ( ws ) . path ( v1 ) . path ( cluster ) . path ( delegation - token ) . path ( expiration ) . header ( yarn token header , response token . get token ( ) ) . accept ( content type ) . entity ( body , media type ) . post ( client response . class ) ; assert response status code ( status . forbidden , response . get status info ( ) ) ; return null ; } } ) ;
task = task service . create task query ( ) . task id ( task id ) . single result ( ) ; assert equals ( delegation state . resolved , task . get delegation state ( ) ) ; task service . delete task ( task id , true ) ;
updater . run ( ) ;
if ( pos tag helper . has pos tag part ( noun token readings , noun : anim : m : v _ naz ) & & pos tag helper . has pos tag ( verb token readings , verb . * : f ( : . * | ) ) & & has masc fem lemma ( noun token readings ) ) { log exception ( ) ; return true ; }
student mongo short wrapper student min = new student mongo short wrapper ( ) ;
prot . read field begin ( ) ; tlist l = prot . read list begin ( ) ; assert true ( l . size = = 2 ) ; assert true ( prot . read double ( ) = = 348 . 55 ) ; assert true ( prot . read double ( ) = = 234 . 22 ) ; prot . read list end ( ) ; prot . read field end ( ) ;
string getter type name = getter . get return type ( ) . get simple type name ( ) . to lower case ( ) ;
string ping body = random alpha string ( 126 ) ; array deque < buffer > pong queue = new array deque < > ( ) ; integer max frame size = 256 ; server = vertx . create http server ( new http server options ( ) . set idle timeout ( 1 ) . set port ( http test base . default _ http _ port ) . set host ( http test base . default _ http _ host ) . set max websocket frame size ( max frame size ) ) ; server . websocket handler ( ws - > { } ) . listen ( ar - > { client . websocket ( http test base . default _ http _ port , http test base . default _ http _ host , , ws - > { ws . pong handler ( pong queue : : push ) ; ws . write frame ( web socket frame . ping frame ( buffer . buffer ( ping body ) ) ) ; } ) ; } ) ; long start = system . current time millis ( ) ;
list < t > purmutation = new array list < t > ( ) ; purmutation . add all ( sequence ) ; permutations . add ( purmutation ) ; } else {
return format . no _ value ; }
codec = codecs by name . get ( string utils . to lower case ( codec name ) ) ; }
if ( collection utils . is empty ( classification config . get main function id list ( ) ) ) { log . info ( classification job ( id = { } ) is skipped because its main anomaly function list is empty . , classification config . get id ( ) ) ; return ; }
if ( the urlobject . is corbaname url ( ) ) { return resolve corbaname ( ( corbaname url ) the urlobject ) ; } else { return resolve corbaloc ( ( corbaloc url ) the urlobject ) ; }
reusing build first re openable hash join iterator < tuple2 < integer , string > , tuple2 < integer , string > , tuple2 < integer , string > > iterator = new reusing build first re openable hash join iterator < > ( build input , probe input , this . record serializer , this . record1 comparator , this . record serializer , this . record2 comparator , this . record pair comparator , this . memory manager , io manager , this . parent task , 1 . 0 , false , false , true ) ; iterator . open ( ) ;
if ( local params . get bool ( calcdistinct , top level calc distinct ) ) { for ( stat stat : calcdistinct _ psuedo _ stat ) { assume true , but don ' t include if specific stat overrides if ( local params . get bool ( stat . name ( ) , true ) ) { stats in response . add ( stat ) ; } } } for ( stat stat : stats in response ) { stats to calculate . add all ( stat . get distrib deps ( ) ) ; }
typed array a = context . obtain styled attributes ( attrs , r . styleable . velocity title page indicator , def style , 0 ) ;
check point . trigger ( pre _ view _ listener _ release _ + manager3 ) ; future < cache < object , string > > future = fork ( ( ) - > cache ( 3 , cache _ name ) ) ;
fsdata input stream is = fs . open ( path ) ; hfile block . fsreader hbr = new fsreader v2 test ( is , algo , total size , hfile . max _ format _ version , fs , path ) ; hfile block b = hbr . read block data ( 0 , - 1 , - 1 , pread ) ; b . sanity check ( ) ; assert equals ( 4936 , b . get uncompressed size without header ( ) ) ; assert equals ( algo = = gz ? 2173 : 4936 , b . get on disk size without header ( ) - b . total checksum bytes ( ) ) ;
@ non null splitter splitter = splitter . on ( ' , ' ) ;
assert . assert equals ( value , expected aggregation result ) ;
linked element < t > le = new linked element < t > ( element , hash code ) ; le . next = entries [ index ] ; entries [ index ] = le ; return true ; }
rstudio ginjector . instance . get global display ( ) . show yes no message ( global display . msg _ warning , unsaved changes , there are unsaved documents in this window . are you sure + you want to close it? , false , include cancel new operation ( ) { @ override public void execute ( ) { quit context . on ready to quit ( false ) ; } } , null , null , close and discard changes , cancel , false ) ; return ;
counter result result after shutting down anode = remote counter . increment ( ) ;
list < java symbol name > parameter names = new array list < java symbol name > ( ) ; parameter names . add ( new java symbol name ( registry ) ) ; if ( governor has method ( method name , annotated java type . convert from annotated java types ( parameter types ) ) ) { return get governor method ( method name , annotated java type . convert from annotated java types ( parameter types ) ) ; }
znrecord zn record = generate partition assignment ( topic name , n partitions , instance names , n replicas ) ;
request second committed req = new request ( new create request ( path , new byte [ 0 ] , ids . open _ acl _ unsafe , create mode . persistent _ sequential . to flag ( ) ) , op code . create , 0x99 , 2 ) ; processor . committed requests . add ( second committed req ) ; set < request > waiting committed requests = new hash set < request > ( ) ;
if ( remainder . is empty ( ) ) { remainder = insert item stacked ( inventory , remainder , false ) ; }
system . out . println ( \ n class name : + instances . class attribute ( ) . name ( ) ) ;
declaration [ ] old decl = old constr . get required declarations ( ) ; declaration [ ] new decl = cloned constr . get required declarations ( ) ; for ( int i = 0 ; i < new decl . length ; i + + ) { if ( new decl [ i ] . get pattern ( ) = = this ) { new decl [ i ] . set pattern ( clone ) ;
appender = null ; in error = false ; string class name = attributes . get value ( class _ attribute ) ;
iterable < proto sources provider > proto providers = rule context . get prerequisites ( deps , mode . target , proto sources provider . class ) ; for ( proto sources provider proto provider : proto providers ) { aspect objc proto provider . add proto group ( proto provider . get transitive proto sources ( ) ) ; }
if ( session . get token values unmodifiable map ( ) . size ( ) > 0 ) { response . add item ( create session response ( session ) ) ; }
last position = position ;
http destination over http destination = ( http destination over http ) client . get destination ( http , localhost , port ) ;
assert equals ( 1 , test harness . num keyed state entries ( ) ) ;
final int idx term offset = indexed term . offset ; final int prior term offset = prior term . offset ; final int limit = math . min ( prior term . length , indexed term . length ) ; for ( int byte idx = 0 ; byte idx < limit ; byte idx + + ) { if ( prior term . bytes [ prior term offset + byte idx ] = indexed term . bytes [ idx term offset + byte idx ] ) { return byte idx + 1 ; } } return math . min ( 1 + prior term . length , indexed term . length ) ; }
version2 = add and get version ( sdoc ( id , 1 ) , params ( _ version _ , long . to string ( version + random ( ) . next int ( 1000 ) + 1 ) ) ) ;
rfc3280 cert path utilities . process crlb2 ( dp , cert , crl ) ;
log . e ( tag , get long extra failed on intent + intent ) ; return default value ; }
string exec engine = job conf . get ( conf vars . hive _ execution _ engine . varname ) ;
query return type list . add ( type ) ;
frame layout ignored _ view = ( frame layout ) parent view . find view by id ( r . id . ignored _ view ) ;
byte array input stream bais1 = new byte array input stream ( file contents . get bytes ( ) ) ; ttext protocol test msg msg1 = new ttext protocol test msg ( ) ; msg1 . read ( new ttext protocol ( new tiostream transport ( bais1 ) ) ) ; assert equals ( test msg ( ) , msg1 ) ;
log . info ( grid search . build model ( ) : model with these parameters was built but removed , rebuilding ; checksum : + checksum ) ;
if ( size > max alloc ) { return null ; } while ( true ) { chunk c = get or make chunk ( ) ; try to allocate from this chunk int alloc offset = c . alloc ( size ) ; if ( alloc offset = - 1 ) { we succeeded - this is the common case - small alloc from a big buffer return new allocation ( c . data , alloc offset ) ; } not enough space try to retire this chunk try retire chunk ( c ) ; }
simple ledger allocator sla = pool . get ledger allocator ( lh ) ; string sla path = sla . allocate path ; logger . info ( allocated ledger { } from path { } , lh . get id ( ) , sla path ) ; pending txns . add ( txn ) ; allocate paths . add ( sla path ) ; }
fold same ( x = + ( function f ( ) { function object ( ) { this . x = 4 } ; return new object ( ) ; } ) ( ) ; ) ;
b . add responses ( resp builder ) ;
if ( passive ) { int gap = passive interval millis _ - ( int ) millis since last ; gap = math . max ( 1 , gap ) ; a non - positive value will cause error schedule execution ( true , gap ) ; } return ; } }
object obj = null ; if ( ref obj instanceof reference ) { final reference ref = ( reference ) ref obj ; if ( is correct class ( ref . get class name ( ) ) ) { final ref addr ra = ref . get ( instance key ) ; if ( ra = null & & ra . get content ( ) = null ) { object was bound to jndi via referenceable api . obj = instance map . get ( ra . get content ( ) ) ; } else { tomcat jndi creates a reference out of server . xml < resource param > configuration and passes it to an instance of the factory given in server . xml . string key = null ; if ( name = null ) { key = name . to string ( ) ; obj = instance map . get ( key ) ; } if ( obj = = null ) { final instance key data source ds = get new instance ( ref ) ; set common properties ( ref , ds ) ; obj = ds ; if ( key = null ) { instance map . put ( key , ds ) ; } } } } } return obj ;
assert equals ( map in . size ( ) , 2 ) ; }
lnode = ltree . find ( right ) ; rnode = rtree . find ( right ) ; lhash = lnode . hash ( ) ; rhash = rnode . hash ( ) ; right . set size ( lnode . size of range ( ) , rnode . size of range ( ) ) ; right . set rows ( lnode . rows in range ( ) , rnode . rows in range ( ) ) ; int rdiff = consistent ;
eglconfig [ ] configs = new eglconfig [ num configs ] ;
application attempt id att id3 = create scheduling request ( 1024 , queue1 , user2 ) ; verify app runnable ( att id3 , true ) ; verify queue num runnable ( queue1 , 2 , 0 ) ;
sliding tab layout . set on page change listener ( new view pager . on page change listener ( ) { @ override public void on page scrolled ( int i , float v , int i2 ) { } @ override public void on page selected ( int i ) { propagate toolbar state ( toolbar is shown ( ) ) ; } @ override public void on page scroll state changed ( int i ) { } } ) ; propagate toolbar state ( toolbar is shown ( ) ) ;
text view last seen = ( text view ) res . find view by id ( r . id . last seen ) ;
xcontent builder line geo json = xcontent factory . json builder ( ) . start object ( ) . field ( type , line string ) . start array ( coordinates ) . start array ( ) . value ( 100 . 0 ) . value ( 0 . 0 ) . value ( 15 . 0 ) . end array ( ) . start array ( ) . value ( 101 . 0 ) . value ( 1 . 0 ) . value ( 18 . 0 ) . value ( 19 . 0 ) . end array ( ) . end array ( ) . end object ( ) ; list < coordinate > line coordinates = new array list < > ( ) ;
string actual = this . config . mvc conversion service ( ) . convert ( new test bean ( ) , string . class ) ;
return side input join helper ( real join type , real left rows , real right rows , real right null row , swapped ) ; }
m _ old mouse pos . width = e . get x ( ) ; m _ old mouse pos . height = e . get y ( ) ; m _ new mouse pos . width = e . get x ( ) ; m _ new mouse pos . height = e . get y ( ) ; m _ mouse state = 3 ; graphics g = get graphics ( ) ;
abbreviate paths = new jcheck box ( j edit . get property ( options . view . abbreviate paths ) ) ;
int memory use count ;
assert . assert equals ( incoming handler , stack . get next incoming ( ) , session ) ;
while ( system . current time millis ( ) < = replan time ) { thread . sleep ( 100 ) ; }
fs . set permission ( file _ dir _ path , new fs permission ( ( short ) 0111 ) ) ;
output . write byte ( tc _ object ) ; write class desc ( cl desc , false ) ; int previous handle = - 1 ; if ( unshared ) { previous handle = objects written . get ( object ) ; } int handle = register object written ( object ) ;
server . shutdown ( ) ;
assert equals ( size of pending reconstructions , 0 , pending reconstructions . size ( ) ) ;
if ( type . equals ( uint ) ) { type = word ; } if ( type . equals ( byte ) ) { type = uchar ; } byte [ ] bytes = null ;
else if ( param name . equals ignore case ( audioapi ) ) { if ( param value = null ) { if ( param value . equals ignore case ( mr ) ) { audio api = media stream . mode _ mediarecorder _ api ; } else if ( param value . equals ignore case ( mc ) ) { audio api = media stream . mode _ mediacodec _ api ; } } }
assert not null ( non serializable data should not be null on cache2 , cache2 . get ( test ) ) ;
m _ acceptor . accrue ( selected host id , m _ acceptor . decorate ( new jsonobject ( ) , optional . empty ( ) ) ) ;
if ( is workset iteration ) { workset update output collector = ( workset update output collector < ot > ) output collector ; }
if ( is skidsensitive & & ( version < 3 | | xcert . get extension value ( 2 . 5 . 29 . 14 ) = = null ) ) { set subject key identifier ( null ) ; }
tester . assert members of ( container cluster0 , container0 ) ; tester . assert members of ( container cluster1 , container1 ) ; tester . assert members of ( content cluster0 , content0 ) ; tester . assert members of ( content cluster1 , content1 ) ; return new system state ( all hosts , container0 , container1 , content0 , content1 ) ;
servers . add ( new server ) ;
generate to string ( class definition , call site binder , to string helper ( class definition . get type ( ) . get java class name ( ) ) . add ( filter , filter ) . add ( projections , projections ) . to string ( ) ) ; return define class ( class definition , super type , call site binder . get bindings ( ) , get class ( ) . get class loader ( ) ) ; }
path temp = to path ( uuid . random uuid ( ) . to string ( ) ) ; try ( output stream out = temp . get output stream ( ) ) { byte streams . copy ( in , out ) ; }
if ( store . get ( next slop . get key ( ) , null ) . size ( ) = 0 ) { return false ; }
string raw account name = account name . split ( \ \ . ) [ 0 ] ; storage credentials credentials = new storage credentials account and key ( raw account name , account key ) ; connect using credentials ( account name , credentials , container name ) ; }
string drl = declare number @ role ( event ) end \ n + declare integer @ role ( event ) end \ n + \ n + rule r1 no - loop when \ n + i : integer ( ) \ n + then \ n + update ( i ) ; \ n + end \ n + rule r2 when \ n + n : number ( ) over window : length ( 1 ) \ n + then \ n + end ; kie session ksession = new kie helper ( ) . add content ( drl , resource type . drl ) . build ( event processing option . stream ) . new kie session ( ) ;
log . d ( tag , showing custom view ) ;
return new h2 dialect ( ) ; }
user force user = this . get user ( msg ) ; if ( initiator = authentication _ initiator & & force user = null ) force user . process message to match user ( msg ) ; log . debug ( sending message to : + msg . get request header ( ) . get uri ( ) . to string ( ) ) ;
public void test rename symlink non existant dest ( ) throws ioexception { path file = new path ( test base dir1 ( ) , file ) ;
if ( path info . starts with ( servlet path ) ) { path info = path info . substring ( servlet path length ) ;
assert equals ( 500 , responses . size ( ) ) ; for ( future < string > f : responses ) { they should not be done yet because the counter hasn ' t incremented assert false ( f . is done ( ) ) ; }
} break ; case ijava element . package _ fragment _ root : if ( element instanceof jar package fragment root ) { jar package fragment root root = ( jar package fragment root ) element ;
while ( ( l = instream . read ( tmp ) ) = - 1 & & thread . current thread ( ) . is interrupted ( ) ) { count + = l ; buffer . append ( tmp , 0 , l ) ; send progress message ( count , ( content length < = 0 ? 1 : content length ) ) ; }
. advance watermark to infinity ( ) ; pcollection < kv < string , integer > > team scores = p . apply ( create events ) . apply ( new calculate team scores ( team _ window _ duration , allowed _ lateness ) ) ; string blue team = test user . blue _ one . get team ( ) ; string red team = test user . red _ one . get team ( ) ; interval window window = new interval window ( base time , team _ window _ duration ) ;
try { configurator . configure ( data collector , this ) ; } catch ( exception exc ) { throw wrapper . orb configurator error ( exc ) ; }
boolean is compatible = update managed project manager . is compatible project ( info ) ;
{ min = mid + 1 ; } }
if ( deep ) { for ( node srckid = source . get first child ( ) ; srckid = null ; srckid = srckid . get next sibling ( ) ) { newnode . append child ( import node ( srckid , true , cloning doc , reversed identifiers ) ) ; } }
spring . artemis . embedded . enabled = false ) . run ( ( second context ) - > { destination checker first checker = new destination checker ( first ) ; first checker . check queue ( queue1 , true ) ; destination checker second checker = new destination checker ( second context ) ; second checker . check queue ( queue1 , true ) ; } ) ; } ) ; }
if ( m _ file chooser panel = null ) return ; m _ file chooser panel = new jfile chooser ( ) ; m _ file chooser panel . reset choosable file filters ( ) ; m _ file chooser panel . set accept all file filter used ( false ) ;
m _ txn queue = new restricted priority queue ( my agreement hsid , mailbox , true ) ; m _ safety state = new agreement txn id safety state ( my agreement hsid ) ; for ( long hs id : m _ hs ids ) { m _ txn queue . ensure initiator is known ( hs id ) ; m _ safety state . add state ( hs id ) ; } m _ mesh arbiter = new mesh arbiter ( m _ hs id , mailbox , m _ mesh aide ) ;
request model . add row ( new row result ( jmeter utils . get res string ( view _ results _ table _ request _ http _ method ) , non - nls - 1 sample result . get httpmethod ( ) ) ) ;
boolean keep containers across app attempts = this . submission context . get keep containers across application attempts ( ) ;
print updating tab level ( t , visit , if ( , else , ) ) ;
e1 . print stack trace ( ) ; } catch ( ioexception e1 ) {
} else { asm util . pop ( v , right value . type ) ; v . iconst ( not equals ) ;
list < method > declared methods = fluent iterable . from ( arrays . as list ( interface type . get declared methods ( ) ) ) . filter ( not synthetic or bridge predicate . instance ) . to list ( ) ;
iterator < string > i = m _ table indexes . key set ( ) . iterator ( ) ; while ( i . has next ( ) ) { string next key = i . next ( ) ; int index = m _ table indexes . get ( next key ) . int value ( ) ; if ( index > row num . int value ( ) ) { index - - ;
if ( m _ agg result columns . size ( ) = num display cols ) { display columns have several pass - through columns if group by primary key or a having clause using aggs that are not in the select list ( larger than case ) . m _ has complex agg = true ; return true ; } return false ;
try { x509 certificate certificate = ( x509 certificate ) from key store [ 0 ] ; principal = new javax . security . auth . x500 . x500 principal ( certificate . get subject dn ( ) . get name ( ) ) ; if token , private key password will be null key private key = key store . get key ( key store alias , private key password ) ; if ( private key = = null | | ( private key instanceof private key ) ) { throw new failed login exception ( unable to recover key from keystore ) ; } private credential = new x500 private credential ( certificate , ( private key ) private key , key store alias ) ; } catch ( key store exception e ) { login exception le = new login exception ( error using keystore ) ; le . init cause ( e ) ; throw le ; } catch ( no such algorithm exception e ) { login exception le = new login exception ( error using keystore ) ; le . init cause ( e ) ; throw le ; } catch ( unrecoverable key exception e ) { failed login exception fle = new failed login exception ( unable to recover key from keystore ) ; fle . init cause ( e ) ; throw fle ; } if ( debug ) { debug print ( principal = + principal + \ n certificate = + private credential . get certificate ( ) + \ n alias = + private credential . get alias ( ) ) ; }
file system fs = file system . get ( conf ) ;
key generator keygen = key generator . get instance ( aes ) ; keygen . init ( 128 ) ; secret key key = keygen . generate key ( ) ; final xmlsecurity data format xml enc data format = new xmlsecurity data format ( ) ;
xnumber score = tail . get static score ( ) ;
return new handle ( left , left ) ;
return periodicity type . erroneous ;
version = 99999 ;
g2 . set color ( metal look and feel . get control shadow ( ) ) ; g2 . draw line ( start x , start y , end x , start y ) ; if ( amount full > 0 ) {
final throwable [ ] error = new throwable [ 1 ] ;
edit log ledger metadata finalized meta = in progress meta . finalize with last tx id ( last tx id ) ;
array list < result > old results = new array list < result > ( ) ;
rpc manager rm = testing util . extract component ( owner cache , rpc manager . class ) ;
if ( task = = null ) { task is polled with timeout which means it can be null return false ; }
if ( adjustments array [ adjustment level ] > = desired adjustment ) { break ; }
if ( part spec = null ) { partition existing ptn = db . get partition ( existing table , part spec , false ) ; return ( ( existing ptn = null ) & & replication spec . allow event replacement into ( existing ptn . get parameters ( ) ) ) ; }
if ( configured ) { return ; } log = log factory . get log ( org . apache . tomcat . util . digester . digester ) ; sax log = log factory . get log ( org . apache . tomcat . util . digester . digester . sax ) ;
if ( descriptor instanceof override descriptor ) modes = new hash set < session tracking mode > ( ) ; else modes = new hash set < session tracking mode > ( context . get session handler ( ) . get effective session tracking modes ( ) ) ;
immutable sorted set . builder < super comparable example > natural = immutable sorted set . natural order ( ) ; assert that ( natural ) . is not null ( ) ; immutable sorted set . builder < super comparable example > reverse = immutable sorted set . reverse order ( ) ; assert that ( reverse ) . is not null ( ) ; }
ast next = a . get next sibling ( ) ; if ( next = = null | | has text ( a ) ) { return ; } from element left = ( from element ) a ;
date string = 2009 - 03 - 11 t17 : 57 : 00 z ;
repo info . set location ( location ) ;
bool query builder active rule filter = bool query ( ) ;
final inet address inet address = inet socket address . get address ( ) ;
new customizable trace interceptor ( ) . set exception message ( customizable trace interceptor . placeholder _ return _ value ) ;
if ( task entity . get execution id ( ) = null ) { execution entity execution entity = command context . get execution entity manager ( ) . find by id ( task entity . get execution id ( ) ) ; context . get agenda ( ) . plan trigger execution operation ( execution entity ) ; }
compute engine . release startup ( ) ; while ( ce server . get status ( ) = = monitored . status . down ) { wait for is ready to change to true , otherwise test will fail with timeout } assert that ( ce server . get status ( ) ) . is equal to ( monitored . status . operational ) ; }
return path . index of ( ' { ' ) > - 1 ;
rpc manager rm2 = testing util . extract component ( backup owner cache , rpc manager . class ) ; controlled rpc manager crm2 = new controlled rpc manager ( rm2 ) ;
throw new illegal argument exception ( string : [ + name + ] ) ;
if ( m has notified running low on data ) { m has notified running low on data = true ; m running out of data listener . on running out of data ( ) ; }
for ( node node : all nodes ) { assert same ( node , node . get feature ( xml , null ) ) ; assert same ( node , node . get feature ( xml , 3 . 0 ) ) ; assert null ( node . get feature ( foo , null ) ) ; assert null ( node . get feature ( foo , bar ) ) ; }
size = available 50 ;
byte chunk res = get url ( http : localhost : + get port ( ) + test bug53257 foo % 23bar . jsp ) ;
final list < map < string , object > > values = lists . transform ( holder values , new function < dim val holder , map < string , object > > ( ) { @ override public map < string , object > apply ( dim val holder val holder ) { return val holder . get metric values ( ) ; } } ) ; return new result < top nresult value > ( timestamp , new top nresult value ( values ) ) ;
assert equals ( 4 , hystrix request log . get current request ( ) . get all executed commands ( ) . size ( ) ) ;
method call call = new method call ( outer method , new object [ 0 ] , new class [ 0 ] ) ;
if ( extension search . type . all . equals ( type ) | | extension search . type . request . equals ( type ) ) { req panel . header search ( p , matches ) ; req panel . body search ( p , matches ) ; }
return build non batching loader ( persister , factory , influencers ) ; }
txn . http request ( new int [ ] { http urlconnection . http _ not _ found } , terminator , put , tx media type . tx _ status _ media _ type , tx status media type . tx _ committed ) ;
map < string , list < expression > > persistent settings = new hash map < string , list < expression > > ( ) { { put ( persistent setting , immutable list . < expression > of ( literal . from object ( true ) ) ) ; } } ; escluster update settings plan node = new escluster update settings plan ( uuid . random uuid ( ) , persistent settings ) ;
if ( layout in vertical ) { m size per span = ( m total size - ( count - 1 ) * m hgap ) count ; } else { m size per span = ( m total size - ( count - 1 ) * m vgap ) count ; }
list < proxy > proxies = proxy selector . get default ( ) . select ( proxy path ) ;
dn address = node reg . get host ( ) ; }
if ( context = = null | | find class by name ( context . get rule context ( ) , split [ 0 ] ) = null ) { return new string [ ] { split [ 0 ] , concat dot separated ( split , 1 , split . length ) } ; } class loader cl = context . get rule context ( ) . get knowledge builder ( ) . get root class loader ( ) ;
for ( iterator < string > it = bundles . key set ( ) . iterator ( ) ; it . has next ( ) ; ) { class < ? extends nls > clazz = bundles . get ( it . next ( ) ) ; resource bundle resource bundle = resource bundle . get bundle ( clazz . get name ( ) , locale ) ; if ( resource bundle = null ) { try { object obj = resource bundle . get object ( message key ) ; if ( obj = null ) return obj ; } catch ( missing resource exception e ) { just continue it might be on the next resource bundle } } }
refresh participant ( context ) ; context . complete step ( operation context . rollback handler . noop _ rollback _ handler ) ;
async interceptor next interceptor = it . previous ( ) ; while ( it . has previous ( ) ) { async interceptor interceptor = it . previous ( ) ; interceptor . set next interceptor ( next interceptor ) ; next interceptor = interceptor ; } this . first interceptor = next interceptor ; }
write to gzip ( b , off , len ) ;
append separator ( bld , extended ) ;
if ( parser _ exception = null & & parser _ exception . is assignable from ( ex . get class ( ) ) ) { return new invalid data access resource usage exception ( syntax error , ex ) ; }
bda job . set cur ooz job id ( ooz job id ) ; bda job . set last submit time ( time utils . get time ( ) ) ; string cond [ ] = { job _ id } ;
_ builder . write start array ( ) ;
holder . item view . set on click listener ( new swipeable item on click listener ( position ) { @ override public void on click ( view v ) { on item view click ( v ) ; } } ) ;
verify ( mock url request builder ) . add header ( range , bytes = 1000 - 5999 ) ; verify ( mock url request builder ) . add header ( first header , first value ) ; verify ( mock url request builder ) . add header ( second header , second value ) ; verify ( mock url request ) . start ( ) ; }
thread . sleep ( 100 ) ; }
for ( gpu device gpu device : assigned resources ) { if ( gpu device . get index ( ) = = gpu idx ) { found gpu devices + + ; docker run command . add device ( device name , device name ) ; } } } else {
seed ^ = hash + 0x9e3779b9 + ( seed < < 6 ) + ( seed > > 2 ) ; return seed ;
big int = new big integer ( 178613588865784752580332404014434337809799306448796128931113691624 ) ;
for ( form field field : data form . get reported data ( ) . get fields ( ) ) { columns . add ( new column ( field . get label ( ) , field . get variable ( ) , field . get type ( ) ) ) ; }
assert equals ( i . d . , score , foo _ s \ n1 , 0 . 0 , hi \ n , get string from sheet ( result sheet ) ) ;
int current count = unique store defs . get ( same store ) ; unique store defs . put ( same store , current count + 1 ) ; break ; } } } if ( same store = = null ) {
assert equals ( 6 , dirs . length ) ;
_ selector manager . execute ( _ strategy : : produce ) ; }
if ( message = null & & message instanceof serializable ) { values . add element ( message ) ; values . add element ( message ) ; }
scale = ( float ) ( float ) math . abs ( max in ) ; if ( scale < 0 . 1 ) { scale = ( float ) 0 . 1 ; }
container = ;
return new simple authentication info ( token . get principal ( ) , token . get credentials ( ) , get name ( ) ) ; } else {
sshclient default client = fixture . setup default client ( ) ; default client . get transport ( ) . set disconnect listener ( new disconnect listener ( ) { @ override public void notify disconnect ( disconnect reason reason , string message ) { disconnected . set ( true ) ; } } ) ; fixture . connect client ( default client ) ; }
wm . delete ( cheese handles [ 3 ] ) ; wm . fire all rules ( ) ;
request < greeting > request2 = builders . get ( ) . id ( 1 l ) . build ( ) ; response future < greeting > future2 = client . send request ( request2 ) ; string response2 = future2 . get response ( ) . get entity ( ) . get message ( ) ; assert . assert equals ( response2 , response1 + again ) ;
if ( dial string = = null ) { return false ; }
string p = drive + ( ' : ' + dir + slashify ( path . substring ( 2 ) ) ) ; security manager security = system . get security manager ( ) ; try { if ( security = null ) security . check read ( p ) ; } catch ( security exception x ) {
result . clear ( ) ; clazz . get method ( log method , string . class ) . invoke ( logger , msg ) ; assert result ( level , null , null , msg ) ;
expect exit ( new string [ ] { - a , - h } ) ; expect exit ( new string [ ] { aaa } ) ; if ( native code loader . is native code loaded ( ) ) { no argument should return normally native library checker . main ( new string [ 0 ] ) ; } else { no argument should exit expect exit ( new string [ 0 ] ) ; } }
table headers = new jtable ( headers model ) ;
osdescriptor local system = dbeaver core . get instance ( ) . get local system ( ) ;
committer . join ( 30000 ) ; assert false ( the committer did not finish in time , committer . is alive ( ) ) ;
data access rule dao . get ( ) . clear ( ) ;
for ( artifact artifact : provider . get transitive extra action artifacts ( ) ) { artifact owner owner = artifact . get artifact owner ( ) ; if ( owner instanceof aspect key ) { if ( aspect classes . contains ( ( ( aspect key ) owner ) . get aspect class ( ) ) ) { artifacts . add ( artifact ) ; } } } return artifacts . build ( ) ;
org . docx4j . wml . object factory wml factory = context . get wml object factory ( ) ; org . docx4j . wml . p paragraph = wml factory . create p ( ) ; paragraph . get content ( ) . add ( subdoc ) ; mdp . add object ( paragraph ) ;
return product . get id ( ) ; } ) ; do in jpa ( this : : entity manager factory , entity manager - > { product product = entity manager . find ( product . class , product id ) ; assert equals ( my product warranty , product . get warranty ( ) ) ; } ) ; }
inherit = get level control ( suffix . substring ( 0 , lastdot ) ) ; } else if ( suffix . equals ( ) ) {
assert null ( template ( ) . request body ( direct : delete sobject + suffix , result . get id ( ) ) ) ; log . debug ( delete successful ) ; }
tc1 = new test class1 ( ) ;
confirmed partns . add all ( tbl spec . partitions ) ;
assert equals ( incorrect buffer size , 512 , dos . get protected buf ( ) . length ) ; dos . write ( byte array ) ; dos . close ( ) ; f1 . delete ( ) ; }
final invoice payment transaction invoice payment transaction request = new invoice payment transaction ( ) ; invoice payment transaction request . set amount ( big decimal . value of ( 249 . 95 ) ) ; invoice payment transaction request . set currency ( account json . get currency ( ) . to string ( ) ) ; invoice payment transaction request . set payment id ( payment . get payment id ( ) ) ; final invoice payment invoice payment refund = kill bill client . create invoice payment refund ( invoice payment transaction request , request options ) ; assert not null ( invoice payment refund ) ; assert single invoice payment refund ( invoice payment refund ) ;
simple date format date format = new simple date format ( time format ) ; string ret = name . replace ( time , date format . format ( new date ( time stamp ) ) ) . replace ( num , string . value of ( rotation ) ) . replace ( host , host ) . replace ( partition , string . value of ( partition index ) ) ; return ret ;
plan plan = r validator . validate reservation delete request ( reservation system , request ) ;
result = s . find within horizon ( pattern . compile ( \ \ p { digit } + ) , 2 ) ;
filter is layer info = predicates . is instance of ( layer info . class ) ; filter is layer group info = predicates . is instance of ( layer group info . class ) ; filter enabled filter = predicates . equal ( resource . enabled , true ) ; filter store enabled filter = predicates . equal ( resource . store . enabled , true ) ;
throw new runtime exception ( not yet implemented on windows ) ;
if ( service ref md . get jaxrpc mapping file ( ) = null | | javax . xml . rpc . service . equals ( service ref md . get service interface ( ) ) ) { throw wslogger . root _ logger . jax rpc not supported ( ) ; }
if ( configurator . is stale ( ) ) { be careful , as the configuration can be read on demand , it ' d not be uncommon that multiple requests come at once when the config file changed synchronized ( configurator ) { if ( configurator . is stale ( ) ) { reload configuration ( ) ; } } }
uri result = duplicate ( ) ; result . fragment = relative . fragment ; return result ; }
. find execution count by native query ( parameter map ) ; }
index service index service = indices service . create index ( tmp imd , collections . empty list ( ) ) ; created indices . add ( index service . index ( ) ) ;
int [ ] ia = s . ia ; s . read ( ) ; assert true ( array field reference should be unchanged , ia = = s . ia ) ; }
datagram packet tmp = new datagram packet ( new byte [ 1 ] , 1 ) ; get impl ( ) . receive ( tmp ) ; } else { stop = true ; } } }
int test cases = integer . parse int ( line ) ; for ( int i = 0 ; i < test cases ; i + + ) { determine number of people and pairs of people ( n and m ) string [ ] info = br . read line ( ) . split ( ) ; int number of people = integer . parse int ( info [ 0 ] ) ; int number of relationship = integer . parse int ( info [ 1 ] ) ; start union ( number of people , people , relationships ) ; iterate through all relationships for ( int j = 0 ; j < number of relationship ; j + + ) { split current line to determine person and friend string [ ] current line = br . read line ( ) . split ( ) ; int person = integer . parse int ( current line [ 0 ] ) ; int friend = integer . parse int ( current line [ 1 ] ) ; union ( person , friend ) ; } initialize max group to one because each group has one person initially int max group = 1 ; iterate through relationships to find the largest group for ( int j = 0 ; j < = number of people ; j + + ) { update max as needed max group = relationships [ j ] > max group ? relationships [ j ] : max group ; } print result system . out . println ( max group ) ; }
m tab model selector . get model ( incognito ) . close all tabs ( false , false ) ;
string persons query str = select p from person batch oracle nosql p ; query persons query = em . create query ( persons query str ) ; all persons = persons query . get result list ( ) ; assert . assert not null ( all persons ) ; assert . assert equals ( 11 , all persons . size ( ) ) ;
byte [ ] out = new byte [ 32 ] ; room for adler32 + zlib crc32 + gzip header
return ( missing args ) ? predicates . to array ( new predicate [ predicates . size ( ) ] ) : new predicate [ 0 ] ;
break ; case node . document _ node : break ; case node . element _ node : serialize element ( ( element ) node , true ) ; break ; case node . processing _ instruction _ node : serialize pi ( ( processing instruction ) node ) ; break ; case node . cdata _ section _ node : serialize cdatasection ( ( cdatasection ) node ) ; break ; case node . text _ node : serialize text ( ( text ) node ) ; break ; case node . entity _ reference _ node : serialize entity reference ( ( entity reference ) node , true ) ; break ; default :
m week view . set column gap ( ( int ) typed value . apply dimension ( typed value . complex _ unit _ dip , 2 , get resources ( ) . get display metrics ( ) ) ) ;
test utils . translate and check ( clz node ) ; } } ; } else {
fsnamesystem . corrupt file block info [ ] cfb = corrupt file blocks . to array ( new fsnamesystem . corrupt file block info [ 0 ] ) ;
int wildcard search pos = p ;
assert true ( user is not listed in the room with his nickname , name is on member list ( op set2 room . get user nickname ( ) , op set2 room . get members ( ) ) ) ;
info col family list . add filter ( create filter list for cols of info family ( ) ) ;
print writer out = new print writer ( new file writer ( alloc _ file ) ) ;
while ( s < this . num bytes & & get byte ( s ) < = 0x20 & & get byte ( s ) > = 0x00 ) s + + ;
while ( request = null ) { nc . perform ( request ) ; if ( loader . confirm interruption ( ) ) { return ; } request = data . resume ( ) ; }
map new data = transform map ( row ) ;
header parsing state . sub state = 2 ;
keyed state handle snapshot2 = run snapshot ( backend . snapshot ( 682375462379 l , 4 , stream factory , checkpoint options . for checkpoint ( ) ) ) ;
int num jetties = cluster . get jetty solr runners ( ) . size ( ) ;
byte [ ] decoded bytes = base64 . decode ( vp8 x _ webp _ base64 , base64 . default ) ;
{ error msg . translet _ class _ err , \ u7121 \ u6 cd5 \ u8 f09 \ u5165 translet \ u985 e \ u5225 ' ' { 0 } ' ' \ u3002 } , { error msg . translet _ object _ err , \ u5 df2 \ u8 f09 \ u5165 translet \ u985 e \ u5225 \ u ff0 c \ u4 f46 \ u7121 \ u6 cd5 \ u5 efa \ u7 acb translet \ u57 f7 \ u884 c \ u8655 \ u7406 \ u3002 } ,
expr node generic func desc filter expr = ( expr node generic func desc ) scan desc . get filter expr ( ) ; if ( filter expr = = null ) { return ; } string serialized filter obj = scan desc . get serialized filter object ( ) ; string serialized filter expr = scan desc . get serialized filter expr ( ) ; boolean has obj = serialized filter obj = null , has expr = serialized filter expr = null ; if ( has obj ) { serializable filter object = scan desc . get filter object ( ) ; if ( filter object = null ) { serialized filter obj = serialization utilities . serialize object ( filter object ) ; } } if ( serialized filter obj = null ) { job conf . set ( table scan desc . filter _ object _ conf _ str , serialized filter obj ) ; }
g . draw string ( register . get register name ( ) , m _ padding left , y ) ; final big integer value = ( register counter = = m _ edited register ? m _ edit value : register . get value ( ) ) . and ( register . get register size ( ) = = 8 ? big integer . value of ( 9223372036854775807 l ) : big integer . value of ( 4294967295 l ) ) ;
complex field locator fl = new complex field locator ( ) ;
flexible adapter . use tag ( overall adapter ) ;
if ( command instanceof percent type ) { send min speed set command send command ( mac address , ; fan ; spd ; set ; min ; . concat ( convert percent to speed ( ( percent type ) command ) ) ) ; fan state map . put ( channel _ fan _ speed _ min , ( percent type ) command ) ; don ' t let max be less than min adjust max speed ( ( percent type ) command , channel _ fan _ speed _ max , ; fan ; spd ; set ; max ; ) ; } }
list . stream ( ) . parallel ( ) . map ( m - > m . to string ( ) ) . find first ( ) ;
buffer . read byte ( ) ; ' ' buffer . read byte ( ) ; ' ' skip to end of line ( ) ; p = 0 ; continue ; default : return c ; } } else if ( c = = ' ' ) {
echo response proto echo response = client . echo2 ( null , new echo request ( short string ) ) ; assert . assert equals ( short string , echo response . get message ( ) ) ; final string long string = string utils . repeat ( x , 4096 ) ;
return arrays . equals ( other , address ) ;
if ( el . has attr ( checked ) ) { final string val = el . val ( ) . length ( ) > 0 ? el . val ( ) : on ; data . add ( http connection . key val . create ( name , val ) ) ; }
list < bson > aggregate = arrays . as list ( match ( or ( eq ( scientist , darwin ) , eq ( scientist , einstein ) ) ) , group ( scientist , sum ( count , 1 ) ) ) ; object result = template . request body ( direct : aggregate , aggregate ) ; assert true ( result is not of type list , result instanceof list ) ; @ suppress warnings ( unchecked ) list < document > result list = ( list < document > ) result ;
int lb , ub ; if ( members [ i ] . length = = 1 ) { lb = ub = members [ i ] [ 0 ] ; } else if ( members [ i ] . length = = 2 ) { lb = members [ i ] [ 0 ] ; ub = members [ i ] [ 1 ] ; } else { throw new illegal argument exception ( ) ; }
string entity class name = split [ 2 ] ;
return new ordered pair < integer , integer > ( permission _ read , grantee _ authenticated ) ;
assert not null ( repository manager . get ( ) . get by repo name ( some repo name ) ) ;
machine . pull all ( recorder ) ;
expression = parse ( 3 . 0d instanceof t ( double ) ) ;
path testdir = util . get data test dir ( ) ; path region dir = new path ( testdir , region ) ; path family1 = new path ( region dir , fam1 ) ;
threw = false ; try { user client . call procedure ( foo . insert , 0 , 0 ) ; } catch ( proc call exception pce ) { pce . print stack trace ( ) ; threw = true ; } assert true ( ' user ' shouldn ' t be able to call procedures yet , threw ) ; threw = false ;
assert . assert true ( visitor . last visited count > ( entry count * 2 ) & & visitor . last visited count < ( entry count * 2 ) + fudge factor ) ;
target node < java library description arg , ? > bottom node = java library builder . create builder ( build target factory . new instance ( : bottom ) ) . build ( ) ; target node < java library description arg , ? > sublib node = java library builder . create builder ( build target factory . new instance ( : sublib ) ) . add dep ( bottom node . get build target ( ) ) . build ( ) ; target node < java library description arg , ? > lib node = java library builder . create builder ( build target factory . new instance ( : lib ) ) . add dep ( sublib node . get build target ( ) ) . build ( ) ; target graph target graph = target graph factory . new instance ( bottom node , lib node , sublib node ) ; build rule resolver real resolver = new single threaded build rule resolver ( target graph , new default target node to build rule transformer ( ) ) ; fake java library bottom rule = real resolver . add to index ( new fake java library ( bottom node . get build target ( ) ) ) ; bottom rule . set output file ( bottom . jar ) ; fake java library sublib rule = real resolver . add to index ( new fake java library ( sublib node . get build target ( ) , immutable sorted set . of ( bottom rule ) ) ) ; sublib rule . set output file ( sublib . jar ) ;
final raw key value iterator raw iter = r iter ;
set time offset ( 10 ) ;
if ( has border ) { int default border size = ( int ) ( 2 * context . get resources ( ) . get display metrics ( ) . density + 0 . 5f ) ; set border width ( attributes . get dimension pixel offset ( r . styleable . circular image view _ civ _ border width , default border size ) ) ; set border color ( attributes . get color ( r . styleable . circular image view _ civ _ border color , color . white ) ) ; }
parser . handle ( buffer . buffer ( { \ first name \ : \ luke \ , \ last name \ : \ lucky \ } ) ) ;
assert false ( file sender . is end of input ( ) ) ; assert equals ( file chunk . create ( new byte [ 0 ] , true ) , file sender . read chunk ( allocator ) ) ; assert null ( file sender . read chunk ( allocator ) ) ; assert true ( file sender . is end of input ( ) ) ; }
reading reading = configuration . get reading ( ) ;
if ( get item visible ( series , item ) ) { return ; }
packages . add ( name ) ; } }
res . add ( core options . system property ( jetty . http . port ) . value ( 0 ) ) ;
channel read ( go away frame ( 0 , 8 * cancel * , unpooled . copied buffer ( this is a test , utf _ 8 ) ) ) ; assert true ( future . is done ( ) ) ; assert false ( future . is success ( ) ) ; status status = status . from throwable ( future . cause ( ) ) ; assert equals ( status . cancelled . get code ( ) , status . get code ( ) ) ; assert equals ( http 2 error code : cancel \ n received goaway \ nthis is a test , status . get description ( ) ) ; }
if ( pp = = null ) { pp = null _ pretty _ printer ; } return new object writer ( this , _ config , _ root type , pp , _ schema ) ;
if ( finders . is empty ( ) ) { throw new illegal argument exception ( string . format ( error : service % s does not have these finder methods : % s , service . get fully qualified type name ( ) , string utils . join ( finders , , ) ) ) ; }
byte util . set31 bits ( priority , 5 , 3 ) ;
catch ( no such method exception sme ) { logger . log ( level . severe , get logging filename ( ) + : + no such method : + sme . get message ( ) , sme ) ; throw new runtime exception ( sme . get message ( ) ) ; }
for ( vector expression ve : big table value expressions ) { ve . evaluate ( batch ) ; } }
class < t > impl class = ( class < t > ) class . for name ( delegate class name ) ;
if ( is cache bean ( ) & & route context . get camel context ( ) . get injector ( ) . supports auto wiring ( ) ) { try { log . debug ( attempting to create new bean instance from class : { } via auto - wiring enabled , clazz ) ; bean = camel context helper . new instance ( route context . get camel context ( ) , clazz ) ; } catch ( throwable e ) { log . debug ( error creating new bean instance from class : + clazz + . this exception is ignored , e ) ; } }
kill running task attempt ( get last attempt ( ) . get attempt id ( ) ) ; assert ( task attempts . size ( ) = = 2 ) ; assert ( mock task . get progress ( ) = = 0f ) ;
if ( label . equals ( guess ) ) { num correct + = 1 ; } }
conf . set merge policy ( no merge policy . instance ) ; index writer writer = new index writer ( dir , conf ) ;
for ( future future : futures ) { if ( future . is done ( ) | | future . is cancelled ( ) ) { futures . remove ( future ) ; }
assert that ( doc . update date ( ) ) . is equal to ignoring millis ( new date ( issue . get issue update time ( ) ) ) ; }
handshaker = handshaker . subprotocol ( ) ;
if ( jp . get include prelude ( ) = null ) { include preludes . add all ( jp . get include prelude ( ) ) ; }
if ( horizon line separator = buffer length & & ( horizon line separator + terminator length = = matcher . end ( ) ) ) {
for ( polling consumer consumer : consumers . values ( ) ) { get camel context ( ) . remove service ( consumer ) ; }
proc = queue . poll ( ) ;
if ( build . version . sdk _ int > = build . version _ codes . n ) { direct boot cache job service . schedule direct boot cache job ( get context ( ) ) ; } return true ;
if ( parts . get position count ( ) = = limit - 1 ) { break ; }
if ( object entity entry entry . get key ( ) instanceof persistent attribute interceptable ) { final persistent attribute interceptor interceptor = ( ( persistent attribute interceptable ) object entity entry entry . get key ( ) ) . _ hibernate _ get interceptor ( ) ; if ( interceptor instanceof lazy attribute loading interceptor ) { ( ( lazy attribute loading interceptor ) interceptor ) . unset session ( ) ; } }
int s len = s arr = null ? s arr . length : 0 ; if ( s len = = 0 ) return new char [ 0 ] ; int e len = ( s len 3 ) * 3 ; length of even 24 - bits . int c cnt = ( ( s len - 1 ) 3 + 1 ) < < 2 ; returned character count
post dispatch swipe ( this , swipe dir ) ; } }
task task = task service . create task query ( ) . task assignee like ( % \ \ % % ) . single result ( ) ;
resources . set charset ( null ) ;
assertion result result = new assertion result ( get name ( ) ) ; string result data = response . get response data as string ( ) ; if ( result data . length ( ) = = 0 ) { return result . set result for null ( ) ; } result . set failure ( false ) ; xmlreader builder = xml _ reader . get ( ) ; if ( builder = null ) { try { builder . set error handler ( new log error handler ( ) ) ; builder . parse ( new input source ( new string reader ( result data ) ) ) ; } catch ( saxexception | ioexception e ) { result . set error ( true ) ; result . set failure ( true ) ; result . set failure message ( e . get message ( ) ) ; } } else { result . set error ( true ) ; result . set failure message ( cannot initialize xmlreader in element : + get name ( ) + , check jmeter . log file ) ; } return result ;
list . add ( new file ( file . get path ( ) , android ) . get path ( ) ) ; list . add all ( get folders ( excluded ) ) ; return list ; }
channel . run pending tasks ( ) ; assert equals ( 1 , flush count . get ( ) ) ; assert false ( channel . finish ( ) ) ; }
execute ( delete from % s where a = ? and b = ? and c = ? , 1 , 1 , - 1 ) ;
string old macaddress = properties . get ( plugwise binding constants . property _ mac _ address ) ; string new macaddress = message . get macaddress ( ) . to string ( ) ; if ( old macaddress = = null | | old macaddress . equals ( new macaddress ) ) { properties . put ( plugwise binding constants . property _ mac _ address , new macaddress ) ; update = true ; } return update ;
enum values enums = null ; if ( key type . is enum type ( ) ) { non - enum if we got it as type erased class ( from instance ) @ suppress warnings ( unchecked ) class < enum < ? > > enum class = ( class < enum < ? > > ) key type . get raw class ( ) ; enums = enum values . construct ( enum class , config . get annotation introspector ( ) ) ; } return new enum map serializer ( type . get content type ( ) , static typing , enums , element type serializer , property , element value serializer ) ; }
return m footer view infos . get ( adj position - adapter count ) . is selectable ; }
type [ ] type args = ( ( parameterized type ) parameter type ) . get actual type arguments ( ) ; if ( type args [ 0 ] = string . class | | type args [ 1 ] = object . class ) { throw new config exception . bad bean ( bean property ' + config prop name + ' of class + bean class . get name ( ) + has unsupported map < + type args [ 0 ] + , + type args [ 1 ] + > , only map < string , object > is supported right now ) ; } return config . get object ( config prop name ) . unwrapped ( ) ;
if ( year = = 0 ) { return false ; } } else {
( new test callback ( ) ) . walk jaxbelements ( doc ) ;
game thread . set to current thread ( ) ; pre init subsystems ( ) ;
expect near number ( spherical util . compute angle between ( up , down ) , math . pi , 1e - 6 ) ; expect near number ( spherical util . compute angle between ( front , back ) , math . pi , 1e - 6 ) ; expect near number ( spherical util . compute angle between ( left , right ) , math . pi , 1e - 6 ) ; }
response res = worker resource . do disable ( ) ; assert . assert equals ( response . status . ok . get status code ( ) , res . get status ( ) ) ; worker the worker = json mapper . read value ( cf . get data ( ) . for path ( announcements path ) , worker . class ) ; assert . assert true ( the worker . get version ( ) . is empty ( ) ) ;
if ( may have side effects ( rhs ) | | node util . can be side effected ( rhs ) ) { return false ; }
fixture . propose storage node wanted state ( 7 , state . retired ) ; assert equals ( distributor : 9 storage : 8 . 6 . s : d . 7 . s : r , fixture . generated cluster state ( ) ) ;
workspace . replace file contents ( apps multidex buck , args _ for _ app , trim _ resource _ ids = true , args _ for _ app ) ; workspace . run buck command ( build , apps multidex : disassemble _ app _ r _ dot _ java ) . assert success ( ) ;
assert equals ( new java type ( foo . my ) , new java type ( foo . my . sar ) . get enclosing type ( ) ) ;
retreat animation = new pending retreat animator ( was , now , steps , now > was ? new rightward start predicate ( move to - ( ( move to - selected dot x ) * 0 . 25f ) ) : new leftward start predicate ( move to + ( ( selected dot x - move to ) * 0 . 25f ) ) ) ;
historic task instance historic task = history service . create historic task instance query ( ) . task delete reason like ( % \ \ % % ) . single result ( ) ;
assert false ( recovered test checkpoint . is discarded ( ) ) ;
default subscription base base subscription = test util . create subscription ( bundle , base product , base term , base price list ) ; final string ao product = telescopic - scope ;
public void test unknown portable field _ not causes query exception _ without index ( ) { string map name = random map name ( ) ; config config = get config ( ) ; config . get serialization config ( ) . add portable factory ( 666 , new portable factory ( ) { public portable create ( int class id ) { return new portable employee ( ) ; } } ) ; hazelcast instance hazelcast instance = create hazelcast instance ( config ) ; imap < integer , portable employee > map = hazelcast instance . get map ( map name ) ;
float offset1 = numbers radius ;
conf . set int ( split . outstanding , 5 ) ; split algorithm split algo = region splitter . new split algo instance ( conf , split class ) ; region splitter . rolling split ( table name , split algo , conf ) ; verify bounds ( expected bounds , table name ) ; }
output . collect ( shards [ chosen shard ] , form ) ; } else {
account . hash code ( ) ;
under test . handle ( connection , database charset checker . state . upgrade ) ;
store . swap files ( dir ) ;
type type = get data type ( ) ; return type = null & & type . is association type ( ) ; moved here from select clause [ jsd ] }
student short primitive student max = new student short primitive ( ) ; student max . set age ( ( short ) get max value ( short . class ) ) ; student max . set id ( ( short ) get max value ( short . class ) ) ; student max . set name ( ( string ) get max value ( string . class ) ) ; em . persist ( student max ) ;
if ( key = = null ) { throw new null pointer exception ( key = = null ) ; }
buffer . reader index ( buffer . reader index ( ) + 1 + len ) ; return decode ( client , frame ) ; }
strip empty screens ( ) ; }
final query article query = new query ( ) ; article query . set current page num ( 1 ) . set page count ( 1 ) . set page size ( symphonys . get int ( mail . batch . article size ) ) . set filter ( composite filter operator . and ( new property filter ( article . article _ create _ time , filter operator . greater _ than _ or _ equal , seven days ago ) , new property filter ( article . article _ type , filter operator . equal , article . article _ type _ c _ normal ) , new property filter ( article . article _ status , filter operator . equal , article . article _ status _ c _ valid ) , new property filter ( article . article _ tags , filter operator . not _ like , tag . tag _ title _ c _ sandbox + % ) ) ) . add sort ( article . article _ comment _ cnt , sort direction . descending ) . add sort ( article . reddit _ score , sort direction . descending ) ; final list < jsonobject > articles = collection utils . json array to list ( article repository . get ( article query ) . opt jsonarray ( keys . results ) ) ; article query service . organize articles ( user ext . user _ avatar _ view _ mode _ c _ static , articles ) ;
if ( wrappers = null ) { wrapper = create missing wrapper ( subname , guards ) ; break ; } } else { break ; } } return wrapper = = null ? create missing wrapper ( name , guards ) : wrapper ; }
acceptor . interrupt ( ) ;
+ + i ; } }
string out = template . request body ( direct : start , world , string . class ) ; assert equals ( hello world , out ) ; assert mock endpoints satisfied ( ) ;
if ( context _ . is project active ( ) ) { dir = context _ . get active project dir ( ) . get path ( ) ; } else { otherwise , use the sticky value ( if it exists ) string cached dir = result _ . get app dir ( ) ; dir = string util . is null or empty ( cached dir ) ? : cached dir ; } return dir ;
summary log . println ( \ n - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ) ; summary log . println ( accuracy suite test case : + id ) ; summary log . println ( - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \ n ) ; log . info ( - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ) ;
prefix = ( new file ( prefix ) ) . get name ( ) ; resource res ;
assert ( thai . equals ( locale extensions . get unicode locale type ( nu ) ) ) ;
return parse date string ( ) ;
sort buffer switcher = new jcheck box ( j edit . get property ( options . view . bufferswitcher . sort buffers ) ) ; sort buffer switcher . set selected ( j edit . get boolean property ( bufferswitcher . sort buffers , true ) ) ; sort buffer switcher . add action listener ( new action listener ( ) { public void action performed ( action event evt ) { sort buffer switcher by name . set enabled ( sort buffer switcher . is selected ( ) ) ; } } ) ; add component ( sort buffer switcher ) ;
long expected size = cb1 . heap size ( ) + cb2 . heap size ( ) + cb3 . heap size ( ) + cb4 . heap size ( ) + cb5 . heap size ( ) + cb6 . heap size ( ) + cb7 . heap size ( ) + cb8 . heap size ( ) + cb0 . heap size ( ) ; assert equals ( queue . heap size ( ) , expected size ) ;
this . current sort index segment . put long ( this . current sort index offset , short record ? this . current data buffer offset : ( this . current data buffer offset | large _ record _ tag ) ) ; if ( this . num key bytes = 0 ) { this . comparator . put normalized key ( record , this . current sort index segment , this . current sort index offset + offset _ len , this . num key bytes ) ; }
if ( config . preload ) { system . out . print ( initializing data store . . . ) ; final prepared statement remove cs = con . prepare statement ( delete from store ; ) ; final callable statement put cs = con . prepare call ( { call store . upsert ( ? , ? ) } ) ; for ( int i = 0 ; i < config . poolsize ; i + + ) { if ( i = = 0 ) { remove cs . execute ( ) ; } put cs . set string ( 1 , string . format ( processor . key format , i ) ) ; put cs . set bytes ( 2 , processor . generate for store ( ) . get store value ( ) ) ; put cs . execute ( ) ; } system . out . println ( done . ) ; }
forward message . end ( ) ;
assert equals ( new integer ( 2 ) , ws . add token ( foo , 2 ) ) ;
assert true ( fs . create new file ( new path ( foo ) ) ) ;
if ( total path . is component ( ) ) { print critical path ( critical path excluding scheduling delays , critical path stats . get optimal path ( ) ) ; } }
request close ( stream not used anymore ) ;
if ( move task to link . get dependent tasks ( ) = null ) { for ( task dependent task : move task to link . get dependent tasks ( ) ) { move only move task . add dependent task ( dependent task ) ; } }
regions . extend ( buffer . capacity ( ) 2 ) ; regions . extend ( 3 * buffer . capacity ( ) 4 ) ; regions . extend ( buffer . capacity ( ) ) ;
calendar end date expected = new gregorian calendar ( time zone . get default ( ) , locale . get default ( ) ) ;
if ( m current page > 0 ) { for ( int i = m current page - 1 ; i > = 0 ; i - - ) { rect bound = bounds . get ( i ) ; is left side is outside the screen if ( bound . left < left clip ) { int w = bound . right - bound . left ; try to clip to the screen ( left side ) clip view on the left ( bound , w , left ) ; except if there ' s an intersection with the right view rect right bound = bounds . get ( i + 1 ) ; intersection if ( bound . right + m title padding > right bound . left ) { bound . left = ( int ) ( right bound . left - w - m title padding ) ; bound . right = bound . left + w ; } } } }
eden service . client connected client = create mock ( eden service . client . class ) ;
string body = < person user = ' james ' > < first name > james < first name > + < last name > strachan < last name > < city > london < city > < person ; template . send body and header ( file : target xquery , body , exchange . file _ name , hello2 . xml ) ; assert mock endpoints satisfied ( ) ;
if ( check for leaving touch mode and consume ( event ) ) { finish key event ( event , send done , true ) ; return ; }
out . abort for tests ( ) ; return out ; }
if ( get locale name ( ) . equals ( fa ) ) { if ( sender id = = modules . get auth module ( ) . my uid ( ) ) { new string + = ÛØ¯ ; } } return new string ;
int pair record = new int pair ( ) ; int num = - 1 ; do { generator . next ( record ) ; num + + ; } while ( sorter . write ( record ) & & num < num _ records ) ; file iochannel . id channel id = this . io manager . create channel enumerator ( ) . next ( ) ;
return navigate to next node ( node ) ;
int previous line = current module . default export node . get lineno ( ) ; t . report ( n , export _ repeated _ error , string . value of ( previous line ) ) ; } current module . default export node = lhs ; }
assert equals ( 1 , project . get employees ( ) . size ( ) ) ;
exchange . set property ( exchange . to _ endpoint , endpoint . get endpoint uri ( ) ) ; return exchange ;
completes . set ( new count down latch ( 1 ) ) ; output . write ( request . get bytes ( standard charsets . utf _ 8 ) ) ; output . flush ( ) ; response = http tester . parse response ( input ) ;
cam node . look at ( tea node . get local translation ( ) , vector3f . unit _ y ) ;
int [ ] ports = new int [ 3 ] ;
final sync configuration config old = configuration factory . create sync configuration builder ( user , constants . user _ realm ) . schema ( string only . class ) . build ( ) ; realm realm = realm . get instance ( config old ) ; realm . begin transaction ( ) ;
property key repository . close ( ) ; label repository . close ( ) ; relationship type repository . close ( ) ;
string value string1 = new string value ( this is a test ) ;
final boolean is same join = persister . get table name ( ) . equals ( foreign key table ) & & arrays . equals ( foreign key columns , persister . get key column names ( ) ) ; return is same join | | super . is duplicate association ( foreign key table , foreign key columns ) ;
hadoop shims . hdfs encryption shim src hdfs encryption shim = session state . get ( ) . get hdfs encryption shim ( src fs ) ;
update ui ( false ) ;
if ( this itr . prepare next ( ) | * do not use | | * other itr . prepare next ( ) ) { return false ; } } }
decompressed frame size = byte buffer . get int ( ) ;
vertices [ 0 ] . execute blocking ( fut - > { vertices [ 0 ] . event bus ( ) . send ( blah , blah , ar - > { assert true ( ar . failed ( ) ) ; if ( ar . cause ( ) instanceof reply exception ) { reply exception cause = ( reply exception ) ar . cause ( ) ; assert same ( reply failure . no _ handlers , cause . failure type ( ) ) ; } else { fail ( ar . cause ( ) ) ; } assert true ( not an el thread , context . is on event loop thread ( ) ) ; complete ( ) ; } ) ; fut . complete ( ) ; } , false , null ) ; await ( ) ;
request . set awsrequest metrics ( aws request metrics ) ; } finally { aws request metrics . end event ( field . request marshall time ) ; } http response handler < amazon web service response < void > > response handler = protocol factory . create response handler ( new json operation metadata ( ) . with payload json ( true ) . with has streaming success response ( false ) , null ) ; invoke ( request , response handler , execution context ) ; } finally {
final byte [ ] header = value buffer . read prefixed bytes ( ) ; agent stat header decoder header decoder = new bit counting header decoder ( header ) ; codec decoder . decode ( value buffer , header decoder , num values ) ; list < t > result = new array list < t > ( num values ) ;
int found [ ] = { 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 } ;
if ( segment end x > segment start x ) { xpos = ( float ) math . ceil ( segment start x ) ; float slope = ( segment end y - segment start y ) ( segment end x - segment start x ) ; while ( xpos < segment end x ) { ypos = slope * ( xpos - segment start x ) + segment start y ; plot ( xpos , ypos , sample , bitmap size ) ; xpos + + ; } } else if ( segment end x < segment start x ) { xpos = ( float ) math . ceil ( segment end x ) ; float slope = ( segment end y - segment start y ) ( segment end x - segment start x ) ; while ( xpos < segment start x ) { ypos = slope * ( xpos - segment start x ) + segment start y ; plot ( xpos , ypos , sample , bitmap size ) ; xpos + + ; } }
fail ( ) ; } catch ( file not found exception e ) {
secondary left = is primary on right ? left padding : width - right padding - m secondary view . get measured width ( ) ;
optional < rule key > last dep file rule key = on disk build info . get rule key ( build info . metadata key . dep _ file _ rule _ key ) ;
hmac h mac = new hmac ( new open ssldigest . sha1 ( ) ) ;
case 5 : portable activation idl activator activate
return new simple entry < > ( , docstring ) ;
set < dbndata source > broken data sources = new hash set < > ( ) ;
if ( local published = null & & local published . get name ( ) . equals ( published info . get name ( ) ) ) { return null ; }
m marker . draw ( canvas , pos [ 0 ] , pos [ 1 ] ) ;
remove user session ( realm , user session ) ; return null ; } } else {
long nz = 1 + _ context . random ( ) . next long ( ( 1l < < num non zero ) - 1 ) ;
integer site id = ( integer ) vt . get ( site _ id , volt type . integer ) ;
return length ; } } }
if ( metadata . get where ( ) = null ) { evaluator < list < ? > > evaluator = ( evaluator ) evaluator factory . create evaluator ( metadata , source , metadata . get where ( ) ) ; list = evaluator . evaluate ( list ) ; } if ( count & & list . is empty ( ) ) { ordered if ( metadata . get order by ( ) . is empty ( ) ) { clone list if ( list = = iterable ) { list = new array list ( list ) ; } order ( metadata , sources , list ) ; } projection if ( metadata . get projection ( ) = null & & metadata . get projection ( ) . equals ( source ) ) { list = project ( metadata , sources , list ) ; } limit + offset if ( metadata . get modifiers ( ) . is restricting ( ) ) { list = metadata . get modifiers ( ) . sub list ( list ) ; } if ( list . is empty ( ) ) { return list ; } }
new usage = usage url base + jdbc connection params . transport _ mode + = < transport _ mode _ value > ;
m audio handler . send message ( m audio handler . obtain message ( msg _ rcdisplay _ clear ) ) ;
if ( get real state ( ) = = transaction state . trying ) { if ( status code 100 = = 1 ) { this . set state ( transaction state . proceeding ) ; } else if ( 200 < = status code & & status code < = 699 ) {
assert null ( ascending sub map entry set . poll last ( ) ) ;
for ( tmp hive pool pool : e . resource plan to apply . get root pools ( ) ) { total query parallelism + = add hive pool ( pool , null , old pools , sync work . to restart in use , pools to redistribute , e ) ; } if ( old pools = null & & old pools . is empty ( ) ) { looks like some pools were removed ; insert queued queries into the front of get reqs . for ( pool state old pool : old pools . values ( ) ) { old pool . destroy ( sync work . to restart in use , e . get requests , e . to reuse ) ; } } log . info ( updating with + total query parallelism + total query parallelism ) ;
if ( system . get property ( property _ prefix ) = null ) { pc . set property prefix ( system . get property ( property _ prefix ) + . ) ; } if ( props = null ) { properties initial props = new properties ( ) ; initial props . put all ( props ) ; log . debug ( string . format ( added % d initial properties , props . size ( ) ) ) ; pc . set initial properties ( initial props ) ; }
assert that ( table . get line and column ( 0 ) ) . is equal to ( new line and column ( 1 , 1 ) ) ;
exchange result exch = template . request ( direct : save , new processor ( ) { @ override public void process ( exchange exchange ) throws exception { exchange . get in ( ) . set body ( record1 ) ; } } ) ; assert true ( result exch . get out ( ) . get body ( ) instanceof document ) ; assert true ( result exch . get out ( ) . get body ( ) . equals ( record1 ) ) ; assert true ( result exch . get out ( ) . get header ( mongo db constants . writeresult ) instanceof update result ) ; document record2 = test collection . find ( eq ( mongo _ id , test save1 ) ) . first ( ) ;
try { field one . get maximum value ( local time , new int [ ] { 0 } ) ; assert true ( false ) ; } catch ( unsupported operation exception e ) { assert true ( true ) ; }
member name . factory . instance . get class ( ) ;
int color mode saved = color mode ; color mode = rgb ; light falloff ( 1 , 0 , 0 ) ;
conf . set boolean ( key _ disable _ throttling , true ) ;
order2 = new order ( ) ;
expect ( expect service call ( true ) . calculate mass ( jake ) ) . and throw ( new ttransport exception ( ) ) ;
g . set color ( get background ( ) ) ; if ( xs = lastxs | | ys = lastys ) { g . draw line ( xcenter , ycenter , lastxs , lastys ) ; g . draw string ( lastdate , 5 , 125 ) ; } if ( xm = lastxm | | ym = lastym ) { g . draw line ( xcenter , ycenter - 1 , lastxm , lastym ) ; g . draw line ( xcenter - 1 , ycenter , lastxm , lastym ) ; } if ( xh = lastxh | | yh = lastyh ) { g . draw line ( xcenter , ycenter - 1 , lastxh , lastyh ) ; g . draw line ( xcenter - 1 , ycenter , lastxh , lastyh ) ; } g . set color ( number color ) ; g . draw string ( , 5 , 125 ) ; g . draw string ( today , 5 , 125 ) ; g . draw line ( xcenter , ycenter , xs , ys ) ; g . set color ( hand color ) ; g . draw line ( xcenter , ycenter - 1 , xm , ym ) ; g . draw line ( xcenter - 1 , ycenter , xm , ym ) ; g . draw line ( xcenter , ycenter - 1 , xh , yh ) ; g . draw line ( xcenter - 1 , ycenter , xh , yh ) ; lastxs = xs ; lastys = ys ;
assert . assert equals ( 0 , txn db util . count query agent ( conf , select count ( * ) from write _ set ) ) ; txn mgr2 . open txn ( ctx , t3 ) ;
if ( bounds . right - new right < snap margin ) { new right = bounds . right ; }
return a + 0 . 0 ;
if ( is already showing ) { ani utils . animate bottom bar ( m photo picker container , true , ani utils . duration . medium ) ; m photo picker fragment . refresh ( ) ; m photo picker fragment . set photo picker listener ( this ) ; }
if ( selecting ) return ;
if ( host = null ) { host . close ( ) ; }
route definition def = model helper . create model from xml ( context , xml , route definition . class ) ; if ( def = = null ) { return ; }
create index request ( test , parent , p1 , null , p _ field , p _ value1 ) . get ( ) ; create index request ( test , child , c1 , p1 , c _ field , red ) . get ( ) ; create index request ( test , child , c2 , p1 , c _ field , yellow ) . get ( ) ; create index request ( test , parent , p2 , null , p _ field , p _ value2 ) . get ( ) ; create index request ( test , child , c3 , p2 , c _ field , x ) . get ( ) ; create index request ( test , child , c4 , p2 , c _ field , x ) . get ( ) ; refresh ( ) ;
return ( schema name = = null ? get user name ( ) : super . meta data schema name to use ( schema name ) ) ; }
ugi . do as ( new privileged exception action < void > ( ) { @ override public void run ( ) throws exception { sasl client client = sasl . create sasl client ( new string [ ] { auth method . kerberos . get mechanism name ( ) } , client principal , server2 protocol , host , props , null ) ; client . evaluate challenge ( new byte [ 0 ] ) ; client . dispose ( ) ; return null ; } } ) ; assert true ( no service ticket for + server2 protocol + found , subject . get private credentials ( kerberos ticket . class ) . stream ( ) . filter ( t - > t . get server ( ) . get name ( ) . starts with ( server2 protocol ) ) . find any ( ) . is present ( ) ) ;
headers . put ( camel linked in . statistics _ update _ key , null ) ; final org . apache . camel . component . linkedin . api . model . historical status update statistics result = request body and headers ( direct : gethistoricalstatusupdatestatistics , null , headers ) ; assert not null ( get historical status update statistics result , result ) ;
app name = null ;
plugin entry default entry = new plugin entry ( ) ;
for ( onested projection item item : include items ) { string alias = item . alias = null ? item . alias . get string value ( ) : item . expression . get default alias ( ) . get string value ( ) ; object value = item . expression . execute ( elem , ctx ) ; if ( item . expansion = null ) { value = item . expand ( expression , alias , value , ctx , recursion - 1 ) ; } result . set property ( alias , convert ( value ) ) ; }
set inherit error handler ( inherit error handler ) ;
single output stream operator < tuple2 < integer , integer > > source = keyed job . create integer tuple source ( env , execution mode . restore ) ; single output stream operator < integer > window = keyed job . create window function ( execution mode . restore , source ) ; single output stream operator < integer > second = keyed job . create second stateful map ( execution mode . restore , window ) ; single output stream operator < integer > first = keyed job . create first stateful map ( execution mode . restore , second ) ;
acl feature = fsacl base test . get acl feature ( path , cluster ) ;
conf . unset ( hbase . hstore . compaction . min . size ) ;
final tuple start equals constraint constraint = tuple start equals constraint . get instance ( ) ;
case info :
buffered reader reader = new buffered reader ( config reader ) ;
test util . wait until ( 1 , new callable < integer > ( ) { @ override public integer call ( ) throws exception { return count deleted . get ( ) ; } } , time unit . seconds , 5 ) ; assert equals ( 0 , count changed . get ( ) ) ; assert equals ( 1 , count deleted . get ( ) ) ;
if ( property . equals ( prop element . get tag name ( ) ) ) { continue ; } string prop name = prop element . get attribute ( name ) . trim ( ) ;
long default group id = set default group ( default - group ) ;
type type = get data type ( ) ; return type = null & & type . is association type ( ) ; moved here from select clause [ jsd ] }
return new parameterless type serializer config ( get serialization format identifier ( ) ) ; }
student mongo short wrapper student min = new student mongo short wrapper ( ) ;
assert cell ( cells , 0 , 4 , 29 , false , false , false , false , none ) ; }
if ( yarn configuration . use https ( get conf ( ) ) ) { client ssl factory = new sslfactory ( sslfactory . mode . client , get conf ( ) ) ; client ssl factory . init ( ) ; sslsocket factory ssl sockt fact = client ssl factory . create sslsocket factory ( ) ; auth url = new authenticated url ( new kerberos authenticator ( ) , client ssl factory ) ; connection = auth url . open connection ( url , token ) ; https urlconnection https conn = ( https urlconnection ) connection ; https conn . set sslsocket factory ( ssl sockt fact ) ; } else { auth url = new authenticated url ( new kerberos authenticator ( ) ) ; connection = auth url . open connection ( url , token ) ; } connection . connect ( ) ;
collection admin request . create collection ( acollectionafterbaddelete , conf , 1 , 2 ) . process ( cluster . get solr client ( ) ) ;
put puta = new put ( row _ a ) ; puta . add column ( test _ family , qualifier col1 , bytes1 ) ; table . put ( puta ) ; admin . flush ( test _ table ) ; put putb = new put ( row _ b ) ;
final data data shadow = this . data ; byte [ ] pos = data shadow . send mtfvalues2 _ pos ; for ( int i = n groups ; - - i > = 0 ; ) { pos [ i ] = ( byte ) i ; } for ( int i = 0 ; i < n selectors ; i + + ) { final byte ll _ i = data shadow . selector [ i ] ; byte tmp = pos [ 0 ] ; int j = 0 ; while ( ll _ i = tmp ) { j + + ; byte tmp2 = tmp ; tmp = pos [ j ] ; pos [ j ] = tmp2 ; } pos [ 0 ] = tmp ; data shadow . selector mtf [ i ] = ( byte ) j ; }
conn state . switch to start scn success ( cp , null , null ) ;
message . add field ( testfield , testvalue ) ;
upload default configuration ( ) ; mock rm rm1 = null ;
long token key = content uris . parse id ( m send req uri ) ; byte [ ] response = send pdu ( sending progress token manager . get ( token key ) , new pdu composer ( m context , send req ) . make ( ) ) ; sending progress token manager . remove ( token key ) ; if ( local _ logv ) { string resp str = new string ( response ) ; log . d ( tag , [ send transaction ] run : send mms msg ( + m id + ) , resp = + resp str ) ; } send conf conf = ( send conf ) new pdu parser ( response ) . parse ( ) ;
master model . update deployment group hosts ( dg . get name ( ) , immutable list . of ( new host ) ) ; verify ( client , times ( 2 ) ) . transaction ( op captor . capture ( ) ) ;
assert equals ( double . value of ( value ) , contains . get field ( field name ) ) ; }
when ( plugins . application context ( ) ) . then return ( runtime environment . application ) ;
deferred region . put ( new put ( null ) ) ;
stm = append to new block ? fs . append ( p , enum set . of ( create flag . append , create flag . new _ block ) , 4096 , null ) : fs . append ( p ) ;
default association rule . metric _ type metric = null ; switch ( m _ metric type ) { case confidence : metric = default association rule . metric _ type . confidence ; break ; case lift : metric = default association rule . metric _ type . lift ; break ; case leverage : metric = default association rule . metric _ type . leverage ; break ; case conviction : metric = default association rule . metric _ type . conviction ; break ; } default association rule new rule = new default association rule ( premise , consequence , metric , premise support , consequence support , total support , total trans ) ;
final long total work time = work items * work time ms ;
final string f = dir + foofs ; final path fpath = new path ( f ) ; fsdata output stream out = test file creation . create file ( dfs , fpath , datanode _ num ) ; out . write ( something . get bytes ( ) ) ;
load fragment ( saved instance state ) ;
realm . begin transaction ( ) ; null primary key obj . set name ( secondary _ field _ updated ) ; realm . copy to realm or update ( null primary key obj ) ; realm . commit transaction ( ) ; assert equals ( secondary _ field _ updated , realm . where ( primary key as boxed byte . class ) . find first ( ) . get name ( ) ) ; }
try { jobs . remove ( sub job ) ; } catch ( null pointer exception npe ) { } stop ( ) can null jobs ; can ' t just do a pre - check , because there ' s a race
final context application context = context . get application context ( ) ;
if ( where = null ) { astnode cond = where . get condition ( ) . accept ( new rex visitor ( schema ) ) ; hive ast . where = astbuilder . where ( cond ) ; }
assert that ( info . get metadata ( ) . get ( build - metadata ) , matchers . equal to ( build - metadata ) ) ;
if ( additions pending ) { final array list < view holder > additions = new array list < view holder > ( ) ; additions . add all ( m pending additions ) ; m additions list . add ( additions ) ; m pending additions . clear ( ) ; runnable adder = new runnable ( ) { public void run ( ) { boolean removed = m additions list . remove ( additions ) ; if ( removed ) { already canceled return ; } for ( view holder holder : additions ) { do animate add ( holder ) ; } additions . clear ( ) ; } } ; if ( removals pending | | moves pending | | changes pending ) { long remove duration = removals pending ? get remove duration ( ) : 0 ; long move duration = moves pending ? get move duration ( ) : 0 ; long change duration = changes pending ? get change duration ( ) : 0 ; long total delay = remove duration + math . max ( move duration , change duration ) ; view view = additions . get ( 0 ) . item view ; view compat . post on animation delayed ( view , adder , total delay ) ; } else { adder . run ( ) ; } }
object key = endpoint . get configuration ( ) . get key ( ) = null ? endpoint . get configuration ( ) . get key ( ) : exchange . get in ( ) . get header ( kafka constants . key ) ; final object message key = key = null ? try convert to serialized type ( exchange , key , endpoint . get configuration ( ) . get key serializer class ( ) ) : null ; final boolean has message key = message key = null ; object msg = exchange . get in ( ) . get body ( ) ;
context = get interpreter context ( ) ;
string builder sb = new string builder ( path . to uri ( ) . get path ( ) ) ;
page list < object > pl = new page list < object > ( 0 , 100 , 52 , null ) ;
assert equals ( v3 . get parallelism ( ) , e3 . get task vertices ( ) . length ) ; int num = 0 ;
start nal unit ( absolute position , bytes written past position , nal unit type , pes time us ) ;
} else { location = file : + file . get absolute path ( ) ; } } }
if ( get property method = null ) { object params [ ] = new object [ 1 ] ; params [ 0 ] = name ; return get property method . invoke ( o , params ) ; } } catch ( illegal argument exception ex2 ) {
reader post table . remove gap marker for tag ( tag ) ; app log . w ( app log . t . reader , attempt to fill gap returned nothing new ) ; } app log . d ( app log . t . reader , requested posts response = + update result . to string ( ) ) ; result listener . on update result ( update result ) ; } } . start ( ) ; }
if ( cmd = = reported state ) { event publisher . post update ( item . get name ( ) , reported state ) ; return true ; } } }
settings . set property ( core properties . default _ issue _ assignee , other ) ;
update rjtfailover counters ( ) ; state fetcher . restore state ( this ) ; clear job history cache ( ) ; }
for ( int i = 0 ; i < target words - 1 ; i + + , source index + + ) result . words [ i ] = word aligned ? words [ source index ] : ( words [ source index ] > > > from index ) | ( words [ source index + 1 ] < < - from index ) ;
assert equals ( 1 , number of events ( upgrade action . name , tuple : : v1 ) ) ;
port prof mode . append child ( get add vlan details ( doc , mode , item . second ( ) ) ) ; } else if ( item . first ( ) = = operation type . removevlanid ) { port prof mode . append child ( get delete vlan details ( doc , mode , item . second ( ) ) ) ; } }
program member . attributes accept ( program class , this ) ; }
query info display = live query infos . remove ( op key ) ; if ( display = = null ) { log . debug ( unexpected display object value of null for operation { } , op key ) ; } else if ( historical query infos = null ) {
kv list exp = new array list < > ( ) ; kv list exp . add ( new key value ( row , family , qualifiers [ 4 ] , 4 , value ) ) ; kv list exp . add ( new key value ( row , family , qualifiers [ 5 ] , 5 , value ) ) ; kv list exp . add ( new key value ( row , family , qualifiers [ 6 ] , 6 , value ) ) ; kv list exp . add ( new key value ( row , family , qualifiers [ 7 ] , 7 , value ) ) ; result = scanner . next ( ) ; verify result ( result , kv list exp , to log , testing first batch of scan ) ;
ref = insert breakpoint ( f breakpoints dmc , breakpoint ) ;
int result segment count = depth - ancestor path . depth ;
if ( virtual path . equals ( i . deployment root ) ) { ret . add ( i . jndi name ) ; }
if ( flag ) { string . case _ insensitive _ order . equals ( it is ok . ) ; } }
msb & = ( 0xf l < < 12 ) ; msb | = ( ( long ) version ) < < 12 ;
return type . int ; } else { return null ; } }
task service . complete ( task after timer . get id ( ) ) ; assert process ended ( process instance . get id ( ) ) ;
verifier . verify discrete trace ( consumer receive trace , event ( activemq _ client _ internal , dequeue with timeout ) ) ;
result = ( byte [ ] ) gc . evaluate ( new direct position2 d ( new point2 d . double ( first xroi - 2 , first yroi - 0 . 5 ) ) ) ;
constraint . attributes . put ( index , core . main index . get name ( ) . name ) ;
expr node column desc c = expr node desc utils . get column expr ( l operand ) ;
db query db query = new db query ( select 173 from ( values ( 0 ) ) ) ;
holder . blacklisted element name . set text ( element name ) ; holder . blacklisted element name . set typeface ( typeface helper . get typeface ( m context , roboto condensed - light ) ) ; holder . blacklisted element name . set paint flags ( holder . blacklisted element name . get paint flags ( ) | paint . subpixel _ text _ flag | paint . anti _ alias _ flag ) ;
try { thread . sleep ( 30 * 1000 ) ; } catch ( exception e ) { }
for ( task source source : sources ) { driver factory driver factory = driver factories by source . get ( source . get plan node id ( ) ) ; check state ( driver factory = null ) ; for ( scheduled split split : source . get splits ( ) ) { driver context driver context = task context . add pipeline context ( driver factory . get pipeline id ( ) , driver factory . is input driver ( ) , driver factory . is output driver ( ) ) . add driver context ( ) ; driver driver = driver factory . create driver ( driver context ) ; driver . update source ( new task source ( split . get plan node id ( ) , immutable set . of ( split ) , true ) ) ; drivers . add ( driver ) ; } } for ( driver factory driver factory : local execution plan . get driver factories ( ) ) { driver factory . no more drivers ( ) ; }
bindy . set locale ( default ) ; from ( direct : marshal ) . marshal ( bindy ) . to ( mock : marshal ) ; } } ; }
code . add aload ( 5 ) ;
future < void > ack future = consumer . acknowledge cumulative async ( msg ) ; log . info ( waiting for async ack to complete ) ; ack future . get ( ) ; consumer . close ( ) ; log . info ( - - exiting { } test - - , method name ) ; }
running calls . remove ( call id ) ;
event bus . register ( this ) ;
helper can inline reference to function ( can inline result . yes , function foo ( a ) { return a ; } foo ( x + + ) ; , foo , inline _ direct ) ;
final view parent vp = get parent ( ) ; if ( vp instanceof view pager ) {
life . start ( ) ; managed cluster = new managed cluster ( cluster ) ;
prefix row tests ( new filter ) ; }
this . services . get executor service ( ) . submit ( this ) ; this . dead servers . add ( server name ) ; return ;
annotation metadata builder roo dto annotation = new annotation metadata builder ( roo java type . roo _ dto ) ;
pattern = ( pattern descr ) rule . get lhs ( ) . get descrs ( ) . get ( 1 ) ; assert equals ( vehicle , pattern . get object type ( ) ) ; assert equals ( 2 , pattern . get constraint ( ) . get descrs ( ) . size ( ) ) ; fld = ( expr constraint descr ) pattern . get constraint ( ) . get descrs ( ) . get ( 0 ) ; assert equals ( type in ( \ sedan \ , \ wagon \ ) , fld . get expression ( ) ) ;
assert in progress state ( ) ; }
builder . put ( network . tcp . reuse _ address , value of ( true ) ) ; int http port = props . value as int ( process properties . search _ http _ port , - 1 ) ;
assert equals ( null , multi . get value ( john : doe ) ) ;
optional < source path > source path for output jar = source path for output jar ( ) ; if ( source path for output jar . is present ( ) ) { builder . add ( source path for output jar . get ( ) ) ; } return builder . build ( ) ;
method = c . get clazz ( ) . get method ( run1 ) ; string = ( string ) method . invoke ( object ) ; assert equals ( 66 , string ) ; method = c . get clazz ( ) . get method ( run2 ) ;
int margin line count = - 1 ; if ( lead span instanceof leading margin span . leading margin span2 ) { leading margin span . leading margin span2 lead span2 = ( ( leading margin span . leading margin span2 ) lead span ) ; margin line count = lead span2 . get leading margin line count ( ) ; }
assert . assert not null ( chosen ) ;
new foo ( ) . set enum ( enum type . orange ) , new foo ( new data map ( as map ( boolean , true , int , - 1 , string , default _ string , enum , apple , array , new data list ( as list ( - 1 , - 2 , - 3 , - 4 ) ) , bytes , byte string . copy string ( default _ bytes , utf - 8 ) , record array , new data list ( as list ( ) ) , record , new bar ( ) . set int ( - 6 ) . data ( ) , fixed , byte string . copy string ( 1234 , utf - 8 ) , union , new data map ( as map ( enum type , orange ) ) , map , new data map ( as map ( key1 , - 5 ) ) ) ) ) , false , false } , {
final int clone count = dir . get input clone count ( ) ;
ksession . insert ( new man ( carl ) ) ; ksession . insert ( new woman ( tina ) ) ;
the socket = new socket ( ) ;
account view . get vererror label ( ) . set text ( caught . get message ( ) ) ; } @ override public void on success ( account result ) {
vt = client . call procedure ( @ ad hoc , select t1 . id from + tb + t1 + where t1 . id > all + ( select id from r2 + order by id limit ? offset ? ) ; , 2 , 2 ) . get results ( ) [ 0 ] ; validate table of longs ( vt , new long [ ] [ ] { { 5 } } ) ; vt = client . call procedure ( @ ad hoc , select t1 . id from + tb + t1 + where t1 . id > all + ( select id from r2 + order by id limit ? offset ? ) + order by 1 ; , 2 , 1 ) . get results ( ) [ 0 ] ; validate table of longs ( vt , new long [ ] [ ] { { 4 } , { 5 } } ) ;
final roimanager roi manager = new roimanager ( roi google , googlem ) ;
client . prepare delete snapshot ( repository name , test - snap - 26 ) . get ( ) ; client . prepare delete repository ( repository name ) . get ( ) ; logger . info ( - - > creating azure repository path [ { } ] , get repository path ( ) ) ;
hash map < string , set < one block info > > node to blocks = new hash map < string , set < one block info > > ( ) ; files = new one file info [ stats . size ( ) ] ; if ( stats . size ( ) = = 0 ) { return ; }
assert equals ( 0 , doc . get first child ( ) . get child nodes ( ) . item ( 0 ) . get attributes ( ) . get length ( ) ) ; assert equals ( 0 , doc . get first child ( ) . get child nodes ( ) . item ( 1 ) . get attributes ( ) . get length ( ) ) ; }
assert that ( deps ) . contains ( x libfoo - lite . jar ) ; assert that ( deps ) . contains ( x libbaz - lite . jar ) ; assert that ( deps ) . contains ( protobuf libjavalite _ runtime - hjar . jar ) ; }
assert true ( cluster . get name node ( ) . is in safe mode ( ) ) ; url url = new url ( http : + conf . get ( dfs . http . address ) + dfshealth . jsp ) ;
xml11 configuration config = new xml11 configuration ( f symbol table ) ;
service url = string utils . is not blank ( properties . get property ( web service url ) ) ? properties . get property ( web service url ) : properties . get property ( service url ) ; auth plugin class name = properties . get property ( auth plugin ) ; auth params = properties . get property ( auth params ) ; boolean use tls = boolean . parse boolean ( properties . get property ( use tls ) ) ; boolean tls allow insecure connection = boolean . parse boolean ( properties . get property ( tls allow insecure connection ) ) ; string tls trust certs file path = properties . get property ( tls trust certs file path ) ; config = new client configuration ( ) ;
assert u ( del q ( * : * ) ) ; long new dels q = ( ( gauge < number > ) metrics . get ( dels qname ) ) . get value ( ) . long value ( ) ; long new cumulative dels q = ( ( meter ) metrics . get ( cumulative dels qname ) ) . get count ( ) ; assert equals ( new dels q , 1 , new dels q ) ; assert equals ( new cumulative dels q , 1 , new cumulative dels q - cumulative dels q ) ;
map < string , properties > store client configs map = store client configs . get all configs map ( ) ; for ( store definition store def : this . coordinator metadata . get stores defs ( ) ) { string store name = store def . get name ( ) ; initialize only those stores defined in the client configs file if ( store client configs map . get ( store name ) = null ) { initialize fat client ( store name , store client configs map . get ( store name ) ) ; } }
return false ; } else if ( gap = = 3 & & path . char at ( i + 1 ) = = ' . ' & & path . char at ( i + 2 ) = = ' . ' ) { return false ; } j = i ; }
for ( int k = 0 ; k < m _ body contact count ; k + + ) { particle body contact contact = m _ body contact buffer [ k ] ; contact . index = new indices . get index ( contact . index ) ; }
relative layout . layout params params separator = new relative layout . layout params ( view group . layout params . wrap _ content , view group . layout params . wrap _ content ) ;
pipeline p = pipeline . create ( options ) ; pcollection < long > count = p . apply ( read ) . apply ( count . < entity > globally ( ) ) ; passert . that singleton ( count ) . is equal to ( num entities ) ;
int off = facing = = enum facing . east ? 1 : 0 ;
fr . restructure ( new names , new vecs ) ;
try { node class . get method ( get user data , new class [ ] { string . class } ) ; return true ; } catch ( no such method exception e ) { return false ; } }
mock provider . find webapp ( foo - webapp - 1 . war ) ; app app = depman . get app by origin id ( mock - foo - webapp - 1 . war ) ;
set local version of users state ( node partition , scheduling mode , true ) ;
if ( title = null & & title . is json primitive ( ) & & keynote . equals ignore case ( title . get as string ( ) ) ) { continue ; } json object dest = new json object ( ) ; json element id = get ( origin , topics . id ) ;
merging windows . persist ( ) ; } else {
net out buffer . clear ( ) ;
class constant . referenced class accept ( class visitor ) ; }
assert . assert false ( message . get extension ( optional group extension ) . has a ( ) ) ;
iterator < map . entry < contact , long > > entries = proactive timer . entry set ( ) . iterator ( ) ;
long proc id ;
assert equals ( second call should have been cached , ref + 1 , service . exception invocations ( ) ) ; assert equals ( first . get cause ( ) , second . get cause ( ) ) ;
file job dir = new file ( temp file dir . get parent file ( ) , job _ dir _ prefix + job id ) ; assert array equals ( new string [ ] { } , job dir . list ( ) ) ; } }
rest response actual rest response without entity = actual . builder ( ) . set entity ( byte string . empty ( ) ) . build ( ) ;
callback capture . get value ( ) . on complete ( 42 ) ; assert requests total ( thrift , 1 ) ; assert errors total ( thrift , 0 ) ; assert reconnects total ( thrift , 0 ) ; assert timeouts total ( thrift , 0 ) ; control . verify ( ) ; }
client response resp = client . call procedure ( sum b1 _ r ) ;
checker = _ secondary . find auto detect visibility ( ac , checker ) ;
protocol . write i64 ( request . get id ( ) ) ; protocol . get transport ( ) . flush ( ) ;
if ( original file . exists ( ) ) { if ( original file . delete ( ) ) { throw new paper db exception ( couldn ' t clean up partially - written file + original file , e ) ; } } throw new paper db exception ( couldn ' t save table : + key + . + backed up table will be used on next read attempt , e ) ; }
process builder pb = shell utils . get process builder ( false , command , new file ( . ) , env ) ;
desc . set value ( counter _ key , long . to string ( priority ) ) ; }
if ( eviction config = null ) { this . max size = eviction config . get size ( ) ; this . eviction policy = eviction config . get eviction policy ( ) . to string ( ) ; this . eviction config = eviction config ; }
reservation id reservation id1 = reservation system test util . get new reservation id ( ) ;
hive decimal writable decimal writable = ( hive decimal writable ) input oi . get primitive writable object ( input ) ;
assert same ( old value , segment . put ( key , hash , new value , true ) ) ; assert equals ( 1 , segment . count ) ; assert same ( old value , segment . get ( key , hash ) ) ;
string rest = null ;
m app . get dbaccess helper ( ) . delete library ( m library name , m library color code ) ; try { m app . get dbaccess helper ( ) . get writable database ( ) . begin transaction ( ) ; hash sets aren ' t meant to be browsable , so convert it into an array . string [ ] song ids array = new string [ m song dbids . size ( ) ] ; m song dbids . to array ( song ids array ) ; loop through the array and add the song ids to the library . for ( int i = 0 ; i < song ids array . length ; i + + ) { content values values = new content values ( ) ; values . put ( dbaccess helper . library _ name , m library name ) ; values . put ( dbaccess helper . song _ id , song ids array [ i ] ) ; values . put ( dbaccess helper . library _ tag , m library color code ) ; m app . get dbaccess helper ( ) . get writable database ( ) . insert ( dbaccess helper . libraries _ table , null , values ) ; } } catch ( exception e ) { e . print stack trace ( ) ; return null ; } finally { m app . get dbaccess helper ( ) . get writable database ( ) . set transaction successful ( ) ; m app . get dbaccess helper ( ) . get writable database ( ) . end transaction ( ) ; }
map < build rule , rule key > result run1 rule keys = get rule keys from build rules ( result run1 . get action graph ( ) . get nodes ( ) , result run1 . get resolver ( ) ) ; map < build rule , rule key > result run2 rule keys = get rule keys from build rules ( result run2 . get action graph ( ) . get nodes ( ) , result run2 . get resolver ( ) ) ; map < build rule , rule key > result run3 rule keys = get rule keys from build rules ( result run3 . get action graph ( ) . get nodes ( ) , result run3 . get resolver ( ) ) ;
return new required field response ( true ) ;
add plugin ( stack , pf ) ; } }
return apply field bridge ( true , property value expr . get property path ( ) , query builder . keyword ( ) . on field ( property value expr . get property path ( ) . as string path ( ) ) ) . matching ( text ) . create query ( ) ;
double dcols [ ] = collect domain ( sc . _ col0s ) ;
try { service . cancel sink ( this ) ; } catch ( wmiexception e ) { log . warn ( e ) ; }
if ( non header tops [ i ] < highest top ) { highest top = non header tops [ i ] ; highest column = i ; } }
path indxhf dir = new path ( dir path , index map reduce util . index _ data _ dir ) ;
int smaller than = 17 ;
for ( int node id = 0 ; node id < num _ nodes ; node id + + ) { if ( node id = 1 ) { file [ ] version dirs = read only utils . get version dirs ( base dirs [ node id ] ) ; for ( file version dir : version dirs ) { assert true ( read only utils . get version id ( version dir ) = ( current version + 3 ) ) ; } } }
cf . complete exceptionally ( new completion exception ( e ) ) ; } futures . add ( cf ) ; }
assert true ( cache factory . get embedded cache ( ) . replace ( key1 , v1 , v2 ) ) ;
all java types first obj = realm . where ( all java types . class ) . equal to ( all java types . field _ id , 0 ) . find first ( ) ;
if ( to tear conn after handling response ( ) ) { tear connection and enqueue pick server ( ) ; } else { state . switch to pick server ( ) ; enqueue message ( state ) ; } }
system . out . println ( import a checkpoint with existing primary image . ) ;
ws activate rules . new request ( ) . set method ( post ) . set param ( param _ target _ key , profile . get kee ( ) ) . set param ( param _ languages , java ) . execute ( ) . assert json ( get class ( ) , bulk _ activate _ rule . json ) ; db session . clear cache ( ) ;
task status . status update ( task progress . get ( ) , task progress . to string ( ) , counters ) ; status update ( umbilical ) ; }
final int size = 3 ; cell [ ] [ ] cells = new cell [ size ] [ ] ; fact handle [ ] [ ] handles = new fact handle [ size ] [ ] ;
copy . get in ( ) . set message id ( exchange . get in ( ) . get message id ( ) ) ; copy . set exchange id ( exchange . get exchange id ( ) ) ; generic file < t > new name = failure renamer . rename file ( copy , file ) ;
try { final file checksum dstcs = dstfs . get file checksum ( dststatus . get path ( ) ) ; return true if checksum is not supported ( i . e . some of the checksums is null ) return srccs = = null | | dstcs = = null | | srccs . equals ( dstcs ) ; } catch ( file not found exception fnfe ) { return false ; }
assert node and indexing exists ( db1 , node , key , value ) ; assert node and indexing exists ( db2 , node , key , value ) ; cluster . sync ( ) ; assert node and indexing exists ( cluster . get any slave ( db1 , db2 ) , node , key , value ) ; w2 . close ( ) ; w1 . close ( ) ; }
string error code = parse error code ( node ) ; if ( error code = = null | | error code . equals ( aws . simple queue service . purge queue in progress ) ) return null ; purge queue in progress exception e = ( purge queue in progress exception ) super . unmarshall ( node ) ;
cur page = this . bf . skip multi page bytes ( vsz , cur page , page counter , cur next page ) ;
base entities . clear ( ) ;
fsd . update count ( inodes in path , 0 , file inode . get preferred block size ( ) , file inode . get file replication ( ) , true ) ; short num locations = file inode . get file replication ( ) ;
try { pstmt = conn . prepare statement ( select distinct ( hypervisor _ type ) from `cloud` . `cluster` where removed is null ) ; rs = pstmt . execute query ( ) ; while ( rs . next ( ) ) { if ( xen server . equals ( rs . get string ( 1 ) ) ) { xenserver = true ; } else if ( kvm . equals ( rs . get string ( 1 ) ) ) { kvm = true ; } else if ( vmware . equals ( rs . get string ( 1 ) ) ) { vmware = true ; } } } catch ( sqlexception e ) { throw new cloud runtime exception ( error while listing hypervisors in use , e ) ; } s _ logger . debug ( updating xen sever system vms ) ;
return result & & deque . remove last occurrence ( bad ) ;
byte stream request = argument captor . for class ( bytestream request . class ) ;
aop context . set current proxy ( old proxy ) ;
doc . add ( new binary doc values field ( index field name , dedup and encode ( ordinals . get ( ) ) ) ) ;
throw new ioexception ( failed to parse history timestamp : + s , e ) ; } + + state ; } break ; case 3 :
ruleok = - 1 ;
input stream is = ticker jsontest . class . get resource as stream ( marketdata example - ticker - data . json ) ; object mapper mapper = new object mapper ( ) ; mercado bitcoin ticker mercado bitcoin ticker = mapper . read value ( is , mercado bitcoin ticker . class ) ;
this . bloom . rewind ( ) ;
element job tag = ( element ) doc . get document element ( ) . get elements by tag name ( jobid ) . item ( 0 ) ; string job id = job tag . get text content ( ) ; element response body async el = query async job result ( job id ) ; if ( response body async el = = null ) { s _ logger . error ( can ' t get a async result ) ; } else { this . response body = response body async el ;
if ( need shutdown ) { shutdown executor ( ) ; } }
assert that ( spy model . minutes until session starts ( ) , is ( 0l ) ) ;
int temp4 = marshalling . get java type ( temp3 , temp , 1 ) - 1 ;
int x = scaled x + ( int ) ( w * a x ) ;
fs . delete ( task attempt outputdir , true ) ; fs . create new file ( task attempt outputdir ) ; } catch ( interrupted exception ex ) { throw new ioexception ( ex ) ; } } private void write text ( text text ) throws ioexception {
em . get transaction ( ) . begin ( ) ; ele1 = em . find ( embeddable list entity2 . class , ele1 . get id ( ) ) ; ele1 . get component list ( ) . get ( ele1 . get component list ( ) . index of ( many to one component4 ) ) . get entity ( ) . set str ( sat4 ) ; em . get transaction ( ) . commit ( ) ;
if ( field type . store term vectors ( ) & & ( request . per field analyzer ( ) = = null | | request . per field analyzer ( ) . contains key ( field ) ) ) { continue ; }
assert match ( p , animal bird eagle , birds ) ;
tester . assert no error message ( ) ;
mc _ color _ temperature . by reference pct current color temperature = new mc _ color _ temperature . by reference ( ) ; dxva2 . instance . get monitor color temperature ( h physical monitor , pct current color temperature ) ; }
reader r = new string reader ( text ) ;
m bean server . invoke ( o name , schedule basic job , new object [ ] { job info , trigger info } , new string [ ] { java . util . map , java . util . map } ) ;
memory . poke long array ( ptr , values , 0 , values . length , false ) ; assert longs equal ( values , ptr , false ) ; assert longs equal ( swapped values , ptr , true ) ;
if ( cap x & & exe ok & & ( mode & 1 ) = 0 & & ( existing & 1 ) = = 0 ) { mode & = 1 ; remove x } return mode ; }
n = x . get left ( store ) ;
assert . assert true ( comb . opened = = comb . closed ) ;
try { force rendering error ( new runtime exception ( fake runtime exception ) ) ; fail ( expected wmsexception ) ; } catch ( service exception e ) { assert true ( true ) ; } try { force rendering error ( new ioexception ( fake io exception ) ) ; fail ( expected wmsexception ) ; } catch ( service exception e ) { assert true ( true ) ; }
if ( file type . equals ( mime type ) ) { return true ; }
array list < long > cids = new array list < > ( ) ; for ( string _ src : srcs ) { cids . add ( get content id ( _ src ) ) ; } collections . sort ( cids ) ; int removed = 0 ; long previous = - 1 ;
if ( local revoke security group ingress response = = null ) { throw new org . apache . axis2 . databinding . adbexception ( property cannot be null ) ; }
if ( ( flag & flag _ same _ type ) = 0 ) { out . put ( state . type ) ; if ( ( flag & flag _ same _ value ) = 0 ) { byte buffer utils . copy from buffer to buffer ( out , out , state . prev offset + key value . row _ offset + prev key length , state . value length ) ; } else { byte buffer utils . copy from stream to buffer ( out , source , state . value length ) ; } } else { if ( ( flag & flag _ same _ value ) = 0 ) { byte buffer utils . copy from stream to buffer ( out , source , key value . type _ size ) ; byte buffer utils . copy from buffer to buffer ( out , out , state . prev offset + key value . row _ offset + prev key length , state . value length ) ; } else { byte buffer utils . copy from stream to buffer ( out , source , state . value length + key value . type _ size ) ; } state . type = out . get ( state . prev timestamp offset + key value . timestamp _ size ) ; }
target [ offset ] = 0x0 ; target [ offset ] | = mask _ is _ subsequent ; target [ offset ] | = ( byte ) ( msg . get fragment number ( ) < < 1 ) ; max 63 fragments if ( is last ) target [ offset ] | = 1 ; if ( _ log . should log ( log . debug ) ) _ log . debug ( control : + integer . to hex string ( target [ offset ] ) + + base64 . encode ( target , offset , 1 ) + at offset + offset ) ;
for ( partition partition : topology . partitions by id . values ( ) ) { mutable partition mp = new mutable partition ( partition . id , partition . k ) ; mutable partition map . put ( mp . id , mp ) ; for ( integer host id : partition . host ids ) { mp . hosts . add ( mutable host map . get ( host id ) ) ; } mp . leader = mutable host map . get ( partition . leader host id ) ; }
target crs = new crspanel ( target crs , new property model ( get coverage , target crs ) ) ; details . add ( target crs ) ;
int required bits = 64 - long . number of leading zeros ( value ) ;
int offset = random int between ( 1 , length 2 ) ; for ( int i = 0 ; i < offset ; i + + ) { assert equals ( si . available ( ) , length - i ) ; assert equals ( pbr . get ( i ) , si . read byte ( ) ) ; }
permissions = perm ;
output . write raw varint32 ( 0 ) ;
verify ( tsdb , never ( ) ) . delete ( any bytes ( ) , any ( byte [ ] [ ] . class ) ) ;
verify diff report ( sub1 , s0 , s2 , new diff report entry ( diff type . modify , dfsutil . string2 bytes ( subsub1 subsubsub1 ) ) , new diff report entry ( diff type . create , dfsutil . string2 bytes ( subsub1 subsubsub1 file15 ) ) , new diff report entry ( diff type . delete , dfsutil . string2 bytes ( subsub1 subsubsub1 file12 ) ) , new diff report entry ( diff type . delete , dfsutil . string2 bytes ( subsub1 subsubsub1 file11 ) ) , new diff report entry ( diff type . create , dfsutil . string2 bytes ( subsub1 subsubsub1 file11 ) ) , new diff report entry ( diff type . modify , dfsutil . string2 bytes ( subsub1 subsubsub1 file13 ) ) , new diff report entry ( diff type . create , dfsutil . string2 bytes ( subsub1 subsubsub1 link13 ) ) , new diff report entry ( diff type . delete , dfsutil . string2 bytes ( subsub1 subsubsub1 link13 ) ) ) ;
assert array equals ( new int [ ] { 0 } , reducer . get key columns ( 0 ) ) ;
final list < string > explicit orm xml list = ( list < string > ) configuration values . remove ( available settings . xml _ file _ names ) ; if ( explicit orm xml list = null ) { for ( string orm xml : explicit orm xml list ) { metadata sources . add resource ( orm xml ) ; } } return attribute converter definitions ;
if ( statement . contains ( gist ) | | statement . contains ( create extension ) ) { continue ; }
assert equals ( 2 , file list . size ( ) ) ; set < string > filtered = new hash set < string > ( ) ; for ( string p : file list ) filtered . add ( p ) ;
selected cell . set selected ( false ) ;
answer = factory . new instance ( camel context ) ; } else {
string builder trace = new string builder ( ) ;
assert true ( no events were dispatched during the registration process . , reg evt collector1 . collected new states . size ( ) > 0 ) ; assert true ( no registration event notifying of registration was dispatched . + all events were : + reg evt collector1 . collected new states , reg evt collector1 . collected new states . contains ( registration state . registered ) ) ;
_ context . banlist ( ) . unbanlist router ( peer ) ;
set < string > connector config updates copy = null ; set < string > connector target state changes copy = null ; synchronized ( this ) { if ( needs reconfig rebalance | | connector config updates . is empty ( ) | | connector target state changes . is empty ( ) ) {
vertex1 . get current execution attempt ( ) . fail ( new exception ( test failure ) ) ;
folder cell width px = math . min ( icon size px + 2 * cell padding x , ( available width px - 4 * edge margin px ) inv . num folder columns ) ; folder cell height px = math . min ( icon size px + 3 * cell padding y + folder child text size , ( available height px - 4 * edge margin px - folder bottom panel size ) inv . num folder rows ) ; folder child drawable padding px = math . max ( 0 , ( folder cell height px - icon size px - folder child text size ) 3 ) ;
if ( m original items = = null & & ( constraint = = null | | constraint . length ( ) = = 0 ) ) { return results ; }
} } } catch ( final ioexception e ) {
synchronized ( ctx . get channel ( ) ) { return super . do encode ( ctx , e ) ; } }
matcher operand matcher operand = ( ( based matcher ) matcher ) . get matcher operand ( ) ;
expect = new mutable date time ( 2004 , 6 , 9 , 11 , 20 , 30 , 0 , london ) ; assert equals ( expect , f . parse date time ( wed 2004 - 06 - 09 t10 : 20 : 30 z ) ) ;
if ( ( f preloaded element to selection . get ( element ) ) . boolean value ( ) ) continue ;
return method . invoke ( target , args ) ;
selenium . select frame ( frame2 ) ;
int nanos = rand . next int ( ( int ) nanoseconds _ per _ second ) ; return nanos - nanos % ( int ) math . pow ( 10 , 9 - decimal digits ) ; }
hcw1 . set ( abcd , 5 ) ;
combined enc client to server . limit ( combined enc client to server . limit ( ) - position offset ) ; final int combined enc client to server len = combined enc client to server . remaining ( ) ; result = server . unwrap ( combined enc client to server , plain server out ) ;
scope scope = output . get scope ( ) ; if ( scope = null ) { doc display _ . unfold ( range . from points ( scope . get preamble ( ) , scope . get end ( ) ) ) ; } events _ . fire event ( new chunk satellite code executing event ( doc update sentinel _ . get id ( ) , chunk id , mode , notebook queue unit . exec _ scope _ partial ) ) ;
zip deques and check ( arrays . as list ( 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ) , arrays . as list ( 1 , 3 , 5 , 7 , 9 ) , arrays . as list ( 2 , 4 , 6 , 8 , 10 ) ) ;
select ( ctx , null ) ;
return m uri . compare to ( other . get base uri ( ) ) ;
val . set exists ( false ) ; remover . set exists ( false ) ; func . get long ( ) ; assert false ( func . exists ( ) ) ; val . set exists ( false ) ;
name found = names . stream ( ) . filter ( name - > name = null & & name . get local part ( ) . equals ( points ) ) . find first ( ) . or else ( null ) ;
web hdfs file system . log . info ( redirect = + redirect ) ;
complete ( null ) ; } } catch ( exception e ) { stop ( ) ; complete ( e ) ; } }
verify ( m mock data query callback ) . on error ( m queries [ 0 ] ) ;
display data . from ( write ) ;
exprpos = + + charpos ; while ( expr [ charpos ] > = 0 ) { charpos + + ; }
container6 exists = true ;
add alert to tree ( alert ) ;
datanode storage info storage info = node . get storage info ( storage . get storage id ( ) ) ; if ( storage info = = null ) {
invoke error handler ( ex ) ;
string ref = u . get ref ( ) ; boolean is rel path = false ; boolean query only = false ;
log . warn ( [ { } ] got exception while trying to retrieve ledger , name , e ) ;
gateway looper = new niolooper ( ) ;
if ( descriptor = null ) return descriptor ; string descriptor ;
disconnect all ( ) ;
assert equals ( 1 , realm . where ( all java types . class ) . is empty ( all java types . field _ lo _ object ) . count ( ) ) ;
for ( future f : futures ) { assert equals ( boolean . true , f . get ( ) ) ; }
term query query = new term query ( new term ( description , d \ u00 e0scription ) ) ; assert equals ( iso latin filter should work . ï¿½ should be a now , 0 , search manager . get query ( query ) . list ( ) . size ( ) ) ; query = new term query ( new term ( description , is ) ) ;
events . add ( create billing event ( subscription id1 , bundle id , plan1 phase change date , plan1 , plan1 phase2 , 5 ) ) ; expected amount = twelve ; test invoice generation ( account id , events , invoices , plan1 phase change date , 1 , expected amount ) ;
mouse hide time + + ;
} } ; form . add ( link ) ; }
for ( auxiliary database object auxiliary database object : database . get auxiliary database objects ( ) ) { if ( auxiliary database object . applies to dialect ( dialect ) & & auxiliary database object . before tables on creation ( ) ) { check export identifier ( auxiliary database object , export identifiers ) ; apply sql strings ( dialect . get auxiliary database object exporter ( ) . get sql create strings ( auxiliary database object , metadata ) , formatter , options , targets ) ; } }
q . set hint ( org . hibernate . fetch size , 10 ) ; q . set hint ( org . hibernate . fetch size , 10 ) ; params . add ( item2 . get name ( ) ) ; q . set parameter ( names , params ) ; result = q . get result list ( ) ; assert not null ( result ) ; assert equals ( 2 , result . size ( ) ) ; q = em . create query ( select item from item item where item . name in ?1 ) ;
terminal . wait expected text into terminal ( bin ) ; terminal . type into terminal ( keys . home . to string ( ) + keys . f3 . to string ( ) ) ; for ( string part of content : view _ bin _ folder ) { terminal . wait expected text into terminal ( part of content ) ; } terminal . type into terminal ( cd + keys . enter ) ; terminal . wait expected text into terminal ( . cache ) ; }
if ( style . outer color = 0x00ffffff & & m bar position to highlight = i & & m weekend days = null & & m weekend days [ i ] ) { paint . set color ( style . outer color ) ; canvas . draw rect ( left , 10f , right , bottom , paint ) ; } if ( ( top - bottom ) = = 1 ) {
when ( index set . get newest index ( ) ) . then return ( ignored ) ;
path filter filter = to include = = null | | to include . size ( ) = = 0 ? null : new matches directory names ( to include ) ;
if ( request . in public ip set ( addr . get ip address ( ) ) ) { ec2 address ec2 address = new ec2 address ( ) ; ec2 address . set ip address ( addr . get ip address ( ) ) ; if ( addr . get virtual machine id ( ) = null ) ec2 address . set associated instance id ( addr . get virtual machine id ( ) . to string ( ) ) ; address list . add ( ec2 address ) ; }
if ( playlists . size ( ) > 1 ) { title = r . string . delete _ playlists _ title ; content = html . from html ( get string ( r . string . delete _ x _ playlists , playlists . size ( ) ) ) ; } else { title = r . string . delete _ playlist _ title ; content = html . from html ( get string ( r . string . delete _ playlist _ x , playlists . get ( 0 ) . name ) ) ; } return new material dialog . builder ( get activity ( ) ) . title ( title ) . content ( content ) . positive text ( r . string . delete _ action ) . negative text ( android . r . string . cancel ) . on positive ( new material dialog . single button callback ( ) { @ override public void on click ( @ non null material dialog dialog , @ non null dialog action which ) { if ( get activity ( ) = = null ) return ; playlists util . delete playlists ( get activity ( ) , playlists ) ; } } ) . build ( ) ;
container impl container1 = create mock container ( user , 1 ) ; dispatcher1 . get event handler ( ) . handle ( create container localization event ( container1 , local resource visibility . public , req ) ) ;
string body = - - aa b03x \ r \ n content - disposition : form - data ; name = filedata ; filename = data . csv \ r \ n + content - type : text plain \ n + \ r \ n \ r \ n + file utils . read file to string ( locations ) + \ r \ n \ r \ n - - aa b03x - - ; post ( rest imports + import id + tasks , body , multipart form - data ; boundary = aa b03x ) ;
assert that ( under test . get at most three active users for scm account ( email ) ) . has size ( 3 ) ; }
long v = random int between ( 14 , 17 ) ;
bitmap factory . options bitmap options = new bitmap factory . options ( ) ; bitmap options . in preferred config = bitmap . config . rgb _ 565 ; return crop face ( bitmap factory . decode resource ( ctx . get resources ( ) , res drawable , bitmap options ) ) ;
set content type ( new org . docx4j . openpackaging . contenttype . content type ( org . docx4j . openpackaging . contenttype . content types . presentationml _ notes _ slide ) ) ;
. add as resource ( paths . get ( src test resources camel - context - error handler - policy . xml ) . to file ( ) , imported - context . xml )
assert not null ( compile ( enum season { winter , spring , summer , autumn } ) ) ; fails after parser
throw new runtime exception ( error determining start offset with function ) ;
if ( lp . expandable ) continue ;
assert that ( with min sq version ( 5 . 6 ) . is compatible with ( 6 . 3 - snapshot ) ) . is true ( ) ;
if ( text utils . is empty ( extras . get string ( arg _ stats _ view _ all _ title ) ) ) { set title ( extras . get string ( arg _ stats _ view _ all _ title ) ) ; } }
test grr ( token range ( 0 , 0 ) , token range ( 0 , 1 ) , token range ( 1 , 6 ) , token range ( 6 , ) , token range ( , 0 ) ) ; test grr ( token range ( , ) , token range ( , 1 ) , token range ( 1 , 6 ) , token range ( 6 , ) ) ;
hive conf . set long var ( conf , conf vars . mapredminsplitsize , long . max _ value ) ;
try { host . stop ( ) ; writer . println ( sm client . get string ( host manager servlet . stopped , name ) ) ; } catch ( exception e ) { get servlet context ( ) . log ( sm . get string ( host manager servlet . stop failed , name ) , e ) ; writer . println ( sm client . get string ( host manager servlet . stop failed , name ) ) ; writer . println ( sm client . get string ( host manager servlet . exception , e . to string ( ) ) ) ; }
res . add ( core options . system property ( jetty . http . port ) . value ( 0 ) ) ; res . add ( core options . system property ( jetty . ssl . port ) . value ( string . value of ( default _ ssl _ port ) ) ) ; res . add ( core options . system property ( jetty . home . bundle ) . value ( org . eclipse . jetty . osgi . boot ) ) ; res . add all ( core jetty dependencies ( ) ) ; return res ; }
run ( drop table + db name + . ptned _ tmp , driver ) ;
student kudu byte wrapper student min = new student kudu byte wrapper ( ) ;
values . add ( value2 ) ;
msg printer . print status msg ( loading question patterns . . . ) ; if ( question interpreter . load patterns ( res patternlearning + questionpatterns ) ) msg printer . print error msg ( could not load question patterns . ) ;
error ( xmlmessages . create xmlmessage ( xmlerror resources . er _ method _ not _ supported , null ) ) ; not yet supported ) ; return null ; }
return find first possible parent ( start , target , trust script , false ) ;
int col max width ;
} else if ( full name . starts with ( xmlns : ) & & full name . equals ( xmlns ) & & is eligible attribute ( name ) ) { string property name = extract property name ( name ) ; assert . state ( string utils . has text ( property name ) , illegal property name returned from ' extract property name ( string ) ' : cannot be null or empty . ) ; builder . add property value ( property name , attribute . get value ( ) ) ; } }
int [ ] expected results = new int [ 4 ] ; for ( int i = 0 ; i < 100 ; i + + ) { int index = i % 4 ; expected results [ index ] + = i ; } semaphore . acquire ( ) ;
in format = new dummy input format ( ) ; in format . set min split size node ( blocksize ) ; in format . set max split size ( 3 * blocksize ) ; file input format . set input paths ( job , dir1 + , + dir2 + , + dir3 + , + dir4 ) ; splits = in format . get splits ( job ) ; for ( input split split : splits ) { system . out . println ( file split ( test5 ) : + split ) ; } assert equals ( 3 , splits . size ( ) ) ; actual . clear ( ) ;
init cache ( slop _ streaming _ enabled _ key , true ) ;
cr = client . call procedure ( @ ad hoc , truncate statement ) ;
exchange cryptopia = exchange factory . instance . create exchange ( cryptopia exchange . class . get name ( ) ) ;
string dyn name = dyn . get name ( ) ; if ( try variable expression as property ( vexp , dyn name ) ) return ; if ( extension . handle unresolved variable expression ( vexp ) ) { add static type error ( the variable [ + vexp . get name ( ) + ] is undeclared . , vexp ) ; }
string source = class test { static final string foo = \ hello \ + 1 + \ \ \ udfff \ ; } ;
preconditions . check state ( process handler . process = null ) ; process registry . register process ( process handler . process . nu process , params , immutable map . of ( ) ) ; return process handler . process ; }
if ( statement . length ( ) = = 0 ) { if ( line . trim ( ) . equals ( ) | | sqlparser . is whole line comment ( line ) ) {
} ) ; do in jpa ( this : : entity manager factory , entity manager - > { session session = entity manager . unwrap ( session . class ) ; entity manager . create query ( delete from book ) . execute update ( ) ; entity manager . create query ( delete from person ) . execute update ( ) ;
return fields and weights ;
return add intermediate operation ( new map long operation ( mapper ) ) ;
sink . get object sink propagator ( ) . by pass modify to beta node ( fact handle , modify previous tuples , context , working memory ) ;
list < string > members = new array list < string > ( ) ; members . add ( member ) ; latched procedure proc = new latched procedure ( coord , new foreign exception dispatcher ( ) , 100 , integer . max _ value , op , null , members ) ; final latched procedure procspy = spy ( proc ) ;
final boolean privileged aspect = false ; final int modifier = 42 ; final class or interface type details mock governor = mock ( class or interface type details . class ) ;
assert true ( new cluster . equals ( cluster ) ) ;
headers . put ( camel linked in . fields , null ) ; final org . apache . camel . component . linkedin . api . model . comment result = request body and headers ( direct : getcomment , null , headers ) ; assert not null ( get comment result , result ) ;
mem store size cf1 memstore size phase iv = region . get store ( family1 ) . get mem store size ( ) ;
throw new property value exception ( not - null property references a null or transient value , persister . get entity name ( ) , persister . get property names ( ) [ i ] ) ;
new struct . put ( field , orig struct . get ( field ) ) ;
out . write ( c ) ; return ;
column specification fn spec = column specification ( system . now ( ) , time uuidtype . instance ) ; selection column mapping expected = selection column mapping . new mapping ( ) . add mapping ( fn spec , null _ def ) ; verify ( expected , select now ( ) from % s ) ;
for ( object [ ] row array : rows ) { retval . add row ( row array ) ; } return retval ; }
bitmap = convert rgb565to argb888 ( bitmap ) ; }
check alter table succeed ( alter table baz drop column num2 ; ) ;
instructions . add ( new insn node ( dup _ x1 ) ) ;
undertow logger . request _ logger . exception processing request ( t ) ;
return 1 + 2 ; case icode _ reg _ ind4 :
if ( req . decoder result ( ) . is success ( ) ) { send http response ( ctx , req , new default full http response ( http _ 1 _ 1 , bad _ request ) ) ; return ; }
j = len ;
if ( store . get size ( ) > desired max file size ) { found abig store = true ; } }
return get component ( configuration . class ) ;
return _ str cnt > 0 & & ( _ str cnt + _ na cnt ) = = _ total cnt ; }
if ( query instanceof term query ) { return ( ( term query ) query ) . get term ( ) . field ( ) ; } else if ( query instanceof wildcard query ) { return ( ( wildcard query ) query ) . get term ( ) . field ( ) ; } else if ( query instanceof prefix query ) { return ( ( prefix query ) query ) . get prefix ( ) . field ( ) ; } else if ( query instanceof match all docs query ) { return null ; }
remove and cleanup other srtp controls ( media type , srtp control type . sdes ) ;
file dir = new file ( path ) ;
if ( suffix length = = constants . schema _ validation _ feature . length ( ) & & feature id . ends with ( constants . schema _ validation _ feature ) ) { return feature state . recognized ; }
if ( selection = null & & selection . is empty ( ) ) { final object element = selection . get first element ( ) ; if ( element instanceof dbnlocal folder ) { page general . set data source folder ( ( ( dbnlocal folder ) element ) . get folder ( ) ) ; } } }
public state get ohstate ( ) { string s ohstate = state traslator . states souliss to oh ( this . get note ( ) , this . get type ( ) , ( short ) this . get state ( ) ) ;
client . create bucket ( bucket name ) ;
int target slices = math . min ( sstable details . size ( ) , connections per host ) ; int step = math . round ( ( float ) sstable details . size ( ) ( float ) target slices ) ; int index = 0 ; list < list < stream session . sstable streaming sections > > result = new array list < > ( ) ;
annotated java type param type = parameter types . get ( i ) ;
s = get at precision ( s ) ;
try { coordinator admin command . execute command ( new string [ ] { put , - d , config avro1 , - u , admin _ url , - - confirm } ) ; } catch ( exception e ) { e . print stack trace ( ) ; }
files . create file ( dir . resolve ( bar . war ) ) ;
if ( complete ) { iterator < map . entry < key wrapper , aggregation buffer [ ] > > iter = hash aggregations . entry set ( ) . iterator ( ) ; while ( iter . has next ( ) ) { map . entry < key wrapper , aggregation buffer [ ] > m = iter . next ( ) ; forward ( m . get key ( ) . get key array ( ) , m . get value ( ) ) ; } hash aggregations . clear ( ) ; hash aggregations = null ; if ( log . is info enabled ( ) ) { log . info ( hash table completed flushed ) ; } return ; } int old size = hash aggregations . size ( ) ;
string current language = language code helper . get translation language name ( get base context ( ) , current language code ) ;
if ( broker request . get filter query ( ) = null & & filter query matched schema ( schema , broker request . get filter query ( ) , broker request . get filter sub query map ( ) ) ) { return true ; }
position [ i3 + x _ offset ] = position [ i3 + y _ offset ] = position [ i3 + z _ offset ] = 0 . 0f ;
string name = dir ; tmp . new folder ( name ) ; tmp . new folder ( dir , com ) ;
body . add part ( 0 , part ) ;
if ( can _ hide _ descendants ) { view compat . set accessibility delegate ( child , m child accessibility delegate ) ; }
int result = 0 ; int r1 = 1 ; for ( int i = 31 ; i > = 0 ; i - - , r1 < < = 1 ) { if ( ( num & 1 < < i ) = 0 ) { result = result | r1 ; } } return result ;
fragments . put ( stream identifier , byte buf ) ;
buffer . buffer property ( prop , prop . deserialize ( p , ctxt ) ) ;
thread . sleep ( 3000 ) ; string ticket subject updated = ticket subject + and updated . ; input = new ticket ( ) ; input . set id ( answer . get id ( ) ) ; input . set subject ( ticket subject updated ) ; answer = request body ( direct : updateticket , input ) ; assert . assert not equals ( answer . get created at ( ) , answer . get updated at ( ) ) ; assert . assert equals ( ticket subject updated , answer . get subject ( ) ) ; assert . assert equals ( ticket description , answer . get description ( ) ) ;
x . print stack trace ( ) ; } } ; simple proxy . automatic flow automatic proxy flow = proxy . start automatic flow ( ) ; client . start handshake ( ) ; assert . assert true ( automatic proxy flow . stop ( 5 , time unit . seconds ) ) ; assert . assert true ( latch . await ( idle timeout * 2 , time unit . milliseconds ) ) ;
final float new y = view helper . get y ( child ) ; final float old y = - child . get height ( ) ; anim = animate y ( child , old y , new y , duration unit ) ;
pager . set current item ( 1 ) ;
jsdoc . add suppression ( duplicate ) ;
flatten ( properties , ( map < string , object > ) value , key ) ; } else if ( value instanceof collection ) {
if ( load operand ) { intermediate result . add instruction ( reil helpers . create ldm ( 0 , arch size , child result , size , load target ) ) ; intermediate result . update result ( load target , size , child result , translation result type . memory _ access ) ; } else { intermediate result . update result ( intermediate result . get register ( ) , size , child result , translation result type . memory _ access ) ; }
method . invoke ( conn , gmt ) ;
final int total _ txids = 10 * 11 ;
return ; } else { assert true ( string . format ( connection string ' % s ' should not have throw an exception : % s , input , t . to string ( ) ) , false ) ; } }
left button = create left one touch button ( ) ; if ( left button = null ) left button . add action listener ( new one touch action handler ( true ) ) ;
photographer photographer = new photographer ( ) ; photographer . set photographer id ( 1 ) ; photographer . set photographer name ( amresh ) ; album album = new album ( album1 , my vacation , vacation pics ) ; photographer . set album ( album ) ; em . persist ( photographer ) ; em . close ( ) ;
long thread safe no finally direct start = system . nano time ( ) ;
for ( roster entry roster entry : roster . get instance for ( omemo manager . get connection ( ) ) . get entries ( ) ) { hash map < integer , t _ sess > contact devices = load all raw sessions of ( omemo manager , roster entry . get jid ( ) . as bare jid ( ) ) ; sessions . put all ( create omemo sessions from raw sessions ( omemo manager , roster entry . get jid ( ) . as bare jid ( ) , contact devices ) ) ; }
else if ( clazz . is array ( ) ) { return new array injector ( pm , null , type , param types , default value , array _ auto _ split ) ; }
this . scheduled executor . execute ( task ) ;
return new mod work unit ( second . session implementor , second . get entity name ( ) , second . envers service , second . id , second . entity persister , second . new state , this . old state ) ; }
if ( is service connected ( ) ) { return false ; } if ( file download list . get impl ( ) . is empty ( ) & & file download service proxy . get impl ( ) . is idle ( ) ) { un bind service ( ) ; return true ; }
duedate = ( ( date time ) due date value ) . to date ( ) ;
final parameter value group param = ( parameter value group ) processor . get operation ( coverage crop ) . get parameters ( ) ;
bits = new bit array ( ) ;
rule manager . poll ( ) ; assert . assert equals ( rules , rule manager . get rules ( test _ data source ) ) ;
this . original to new name map = immutable map . copy of ( variable map . get original name to new name map ( ) ) ;
string [ ] [ ] new tokens2 = { { 1010 , 50 } } ; string [ ] [ ] expected2 = { { 1010 , 0 } , { 40 , 50 } } ; check difference ( old range , new tokens2 , expected2 ) ; old range = make range ( 20 , 40 ) ;
ranges = mts . invalids ( ) ;
this . sum grad square = new classic counter < > ( ) ;
method . release connection ( ) ; } return response body b64 ; } }
expression = parser . parse expression ( [ 0 ] ) ;
config . set rambuffer size mb ( mb ) ; }
chart . get tool tip ( ) . set current xy ( record . get position ( ) . x , record . get position ( ) . y ) ; chart . get tool tip ( ) . add tool tip ( key : + l data . get line key ( ) , m paint tooltips ) ; chart . get tool tip ( ) . add tool tip ( label : + l data . get label ( ) , m paint tooltips ) ; chart . get tool tip ( ) . add tool tip ( current value : + double . to string ( l value ) , m paint tooltips ) ;
fi ca scheduler app scheduler app attempt = cs . get scheduler applications ( ) . get ( app0 . get application id ( ) ) . get current app attempt ( ) ;
when ( nm proxy . heartbeat ( is a ( localizer status . class ) ) ) . then throw ( new yarn exception ( sigh , no token ) ) ;
context . get content resolver ( ) . delete ( media store . audio . media . external _ content _ uri , selection . to string ( ) , null ) ;
local broadcast manager . get instance ( this ) . unregister receiver ( m pairing receiver ) ;
remove messages ( r . id . decode _ succeeded ) ; remove messages ( r . id . decode _ failed ) ; }
result = services . execute operation ( get transport read operation ( maximal , tcp , model keys . rack ) ) ;
if ( type . is instance ( value ) ) { no type conversion was needed if ( statistics . is statistics enabled ( ) ) { noop counter . increment ( ) ; } return value ; }
byte [ ] third cert bytes = cert bytes ( 500 , keystore was tampered with , or password was incorrect ) ; assert that ( first cert bytes ) . is equal to ( second cert bytes ) . is equal to ( third cert bytes ) ;
from ( direct : start ) . choice ( ) . when ( ) . xpath ( in : header ( ' foo ' ) = ' bar ' ) . to ( mock : x ) . when ( ) . xpath ( in : body ( ) = ' < two > ' ) . to ( mock : y ) . otherwise ( ) . to ( mock : z ) ;
file file = new file ( user guide dir , summary . md ) ;
return get series ( series ) . get key ( ) ;
if ( read . is dummy ( ) | | read . is path type ( ) | | read . is direct ( ) ) { continue ; }
fos = new file output stream ( file store , true ) ;
sd . read ( ) ;
string [ ] split = mime type . split ( ) ; file extension from mime type = split . length > 1 ? split [ 1 ] : split [ 0 ] ; }
final map < string , string > decorations = event . get annotation model ( ) . get annotation decorations ( ) ; for ( final annotation annotation : event . get added annotations ( ) ) { add annotation item ( event . get annotation model ( ) , annotation , decorations ) ; }
zkutil . create with parents ( watcher , keys parent znode ) ; if ( zkutil . watch and check exists ( watcher , keys parent znode ) ) { list < zkutil . node and data > nodes = zkutil . get child data and watch for new children ( watcher , keys parent znode ) ; refresh nodes ( nodes ) ; } }
allow types ( new class [ ] { tree set . class , sorted set . class , set . class , hash set . class , linked hash set . class , list . class , array list . class , copy on write array list . class , map . class , hash map . class , tree map . class , concurrent hash map . class , } ) ;
worker . close ( ) ; server . close ( ) ; }
em . get transaction ( ) . begin ( ) ; str test entity te = new str test entity ( x ) ; em . persist ( te ) ; id = te . get id ( ) ;
map . put ( key one , value one ) ; assert true ( segment . recency queue . is empty ( ) ) ; for ( int i = 0 ; i < drain _ threshold 2 ; i + + ) { map . get ( key one ) ; } assert false ( segment . recency queue . is empty ( ) ) ;
if ( clear history ) { engine . clear history ( ) ; }
requests in progress . put ( connection , new request record ( http request , completion handler ) ) ;
assume . assume true ( r . get sum doc freq ( f ) = - 1 ) ;
return new context list and jndi view managed reference factory ( ) { @ override public string get jndi view instance value ( ) { return user transaction . class . get simple name ( ) ; } @ override public string get instance class name ( ) { return user transaction . class . get name ( ) ; } @ override public managed reference get reference ( ) { access control service . get value ( ) . authorize access ( ) ; return value . get reference ( ) ; } } ;
for ( color control box c box : color boxes . get ( current tab ) ) { int line start char = text area . get line start offset ( c box . get line ( ) ) ; int x = text area . offset to x ( c box . get line ( ) , c box . get char index ( ) - line start char ) ; int y = text area . line to y ( c box . get line ( ) ) + fm . get descent ( ) ; c box . set pos ( x , y + 1 ) ; c box . draw ( g2d ) ; } }
if ( m _ url = null ) { set url ( m _ url ) ; } if ( m _ user = null ) { set user ( m _ user ) ; }
return resource config . for application class ( jersey1094 . class ) ;
states . move to end ( new topic partition ( baz , 5 ) ) ; check state ( states , expected ) ;
previous = partitions . put if absent ( clone key , empty ) ; if ( previous = = null ) { previous = empty ;
instruction visitor [ ] peephole optimizations array = new instruction visitor [ peephole optimizations . size ( ) ] ;
assert true ( now . get nanos ( ) > 0 ) ; assert true ( sysdate . get nanos ( ) > 0 ) ; tx . commit ( ) ;
return invocation context . proceed ( ) ; } finally {
ds1 . cross ( ds2 ) . project first ( 0 ) . project second ( 5 ) ; }
get session ( ) . get transaction ( ) . begin ( ) ; pers1 = ( person ) get session ( ) . get ( personaje , id _ pers1 ) ; pers1 . set age ( 29 ) ; get session ( ) . persist ( personaje , pers1 ) ; get session ( ) . persist ( personaje , pers2 ) ; id _ pers2 = pers2 . get id ( ) ; get session ( ) . get transaction ( ) . commit ( ) ;
object type conf type conf = ksession . get object type configuration registry ( ) . get object type conf ( ksession . get entry point ( ) , fact1 ) ; assert true ( should have enabled tms , type conf . is tmsenabled ( ) ) ;
throw new runtime exception ( unsupported usage ) ;
req = new read input registers request ( ref , count ) ;
s . delete ( simple ) ; s . get transaction ( ) . commit ( ) ; s2 = spoof serialization ( s ) ;
transitive targets = nested set builder . empty set ( order . stable _ order ) ;
return new action bar . layout params ( default _ custom _ gravity ) ;
for ( int i = 0 ; i < counter out . length ; i + + ) { out [ out off + i ] = ( byte ) ( counter out [ i ] ^ in [ in off + i ] ) ; } int carry = 1 ;
xydataset existing = this . dataset ; if ( existing = null ) { existing . remove change listener ( this ) ; }
pdfc . remove consumer ( consumer0 ) ;
if ( caret position = = like box count view caret position . top ) { border path . line to ( left + ( right - left - caret width ) 2 , top ) ; border path . line to ( left + ( right - left ) 2 , top - caret height ) ; border path . line to ( left + ( right - left + caret width ) 2 , top ) ; }
fs . mkdirs ( archived hfile dir ) ; fs . create new file ( new path ( archived hfile dir , hfile ) ) ;
return load zone data ( id ) ;
context ctx = view holder . item view . get context ( ) ;
assert equals ( 1 , request timer executor . get pool size ( ) ) ;
m list menu presenter . update menu view ( false ) ;
class < ? > cc = value . get class ( ) ;
return true ; default : return super . on options item selected ( item ) ; } }
data block encoder = new hfile data block encoder impl ( encode in cache only ? data block encoding . none : data block encoding , data block encoding ) ;
set focus index ( index , true ) ;
map < string , collection state watcher > watchers = new hash map < > ( ) ; list < zk node props > created replicas = new array list < > ( ) ; atomic boolean any one failed = new atomic boolean ( false ) ;
wait and verify proc ( task , once , once , never ( ) , once , false ) ;
final byte color map [ ] [ ] = new byte [ 3 ] [ mapsize < 256?mapsize + 1 : mapsize ] ;
dst = new byte [ src . length ] ; }
intface . load new version ( 2 , retrieve rename ( tgt . simple i , tgt . simple i004 ) ) ; try { run the original working thing post - reload - check it is still ok result = run unguarded ( caller clazz , run ) ; fail ( method no longer exists , should not have been callable ) ; } catch ( invocation target exception ite ) { assert true ( ite . get cause ( ) instanceof no such method error ) ; assert equals ( simple i . to int ( ljava lang string ; ) i , ite . get cause ( ) . get message ( ) ) ; }
if ( network utils . is network available ( get activity ( ) ) ) { update empty view ( empty view message type . network _ error ) ; set refreshing ( false ) ; return ; }
m _ rank results [ 3 ] [ ( int ) attribute ranking [ j ] [ 0 ] ] + = ( j + 1 ) * ( j + 1 ) ;
sql = select id from r1 t1 + where ( select id from r2 t2 + where t2 . id = t1 . id ) + in + ( select id from r2 t3 + where t3 . id < > 5 and t3 . id > = t1 . id ) + order by id ; ;
xmlattr stack . get xmlns attr ( result ) ; first call = false ;
return objects . hash code ( tagk , pattern . pattern ( ) ) ;
null types . set field float null ( 42 . 42 f ) ;
final path flume file = new path ( level0 a , flume file name ) ; create file ( flume file ) ; fsdata output stream flume output stream = fs . append ( flume file ) ;
if ( debug & & volt compiler . debug _ mode ) { return ; }
cluster . restart data node ( dnprop ) ;
wait container allocated ( am2 , 8 * gb , 5 , 2 , rm1 , nm1 ) ;
for ( int i = 0 ; i < 5 ; i + + ) region . increment ( odd , hconstants . no _ nonce , hconstants . no _ nonce ) ;
output stream out = accepted socket . get output stream ( ) ; out . write ( write content ) ;
int i val = val . get first nonzero digit ( ) ; int i that = that . get first nonzero digit ( ) ; if ( i val > = that . number length ) { return big integer . zero ; }
path f2 path = new path ( get test root path ( fc ) , d1 d2 d3 f2 . txt ) ;
add plugin ( new entering plugin ( ) ) ;
services . add server executor dependency ( service target . add service ( batch service names . job operator service name ( deployment unit ) , job operator service ) . add dependency ( support . get capability service name ( capabilities . batch _ configuration _ capability . get name ( ) ) , batch configuration . class , job operator service . get batch configuration injector ( ) ) . add dependency ( suspend controller . service _ name , suspend controller . class , job operator service . get suspend controller injector ( ) ) . add dependency ( batch service names . batch environment service name ( deployment unit ) , security aware batch environment . class , job operator service . get batch environment injector ( ) ) , job operator service . get executor service injector ( ) ) . install ( ) ;
output . close ( ) ; output = null ; file input stream fis = null ;
return - 1 ; }
velocity tracker . add movement ( do motion ( 0 , 0 , 0 ) ) ;
transitive info collection javabase target = rule context . get prerequisite ( : jvm , mode . target ) ;
cards . add ( card ) ;
super . move ( delta ) ;
run flink zk quorum peer ( zk config file , peer id ) ;
if ( name resolver . name vars . contains key ( ttl ast . get string ( ) ) ) { report warning ( ttl ast , unknown _ strvar , ttl ast . get string ( ) ) ; return ; } return name resolver . name vars . get ( ttl ast . get string ( ) ) ; }
page builder . declare position ( ) ; for ( int i = 0 ; i < output channels . length ; i + + ) { int output channel = output channels [ i ] ; type type = types . get ( output channel ) ; block block = this . channels [ output channel ] . get ( block index ) ; type . append to ( block , block position , page builder . get block builder ( i ) ) ; } position + + ;
while ( i = len & & 0 < = ( c = str . char at ( i ) - ' 0 ' ) & & c < = 9 ) { old index = index ; index = 10 * index - c ; i + + ; }
build target plugin target = build target factory . new instance ( workspace . get dest path ( ) , ocaml _ native _ plugin : plugin ) ; workspace . run buck command ( build , plugin target . to string ( ) ) . assert success ( ) ;
dest = param . get destination ( ) ; if ( dest = null ) { return dest ; }
original pair = variant . get param list ( ) . get ( i ) ;
byte size = compute foldable byte size ( bit size , fold factor ) ; sanity check ( ) ;
return local describe placement groups response . get pull parser ( my _ qname ) ;
method best match = null ; final method [ ] methods = cls . get methods ( ) ; for ( final method method : methods ) { compare name and parameters if ( method . get name ( ) . equals ( method name ) & & member utils . is assignable ( parameter types , method . get parameter types ( ) , true ) ) { get accessible version of method final method accessible method = get accessible method ( method ) ; if ( accessible method = null & & ( best match = = null | | member utils . compare parameter types ( accessible method . get parameter types ( ) , best match . get parameter types ( ) , parameter types ) < 0 ) ) { best match = accessible method ; } } } if ( best match = null ) { member utils . set accessible workaround ( best match ) ; } synchronized ( s method cache ) { s method cache . put ( key , best match ) ; } return best match ;
block b = new block ( block ) ; b . set generation stamp ( 0 ) ; assert null ( map . get ( bpid , b ) ) ;
final count down latch migration start latch = new count down latch ( 1 ) ; config . add listener config ( new listener config ( new delay migration start ( migration start latch ) ) ) ; hazelcast instance instance = factory . new hazelcast instance ( config ) ;
string prefix = get queue class prefix ( ) ;
diffuse = color rgba . white ; vcolor = true ; } else { diffuse = read color ( split [ 1 ] ) ; } } else if ( keyword . equals ( ambient ) ) {
test util . wait until ( 1 , new callable < integer > ( ) { @ override public integer call ( ) throws exception { return count deleted . get ( ) ; } } , time unit . seconds , 5 ) ; assert equals ( 0 , count changed . get ( ) ) ; assert equals ( 1 , count deleted . get ( ) ) ;
int i child = ( k < < 1 ) + 1 ; t child = queue [ i child ] ;
throw new unsupported operation exception ( next ( int ) isn ' t supported ) ;
callback capture . get value ( ) . on complete ( 42 ) ; assert requests total ( thrift , 1 ) ; assert errors total ( thrift , 0 ) ; assert reconnects total ( thrift , 0 ) ; assert timeouts total ( thrift , 0 ) ; control . verify ( ) ; }
if ( get info ( ) . is cite compliant ( ) ) { check feature coordinates range ( collection ) ; }
assert that ( spi . get register ( 0 ) . get value ( ) , is ( equal to ( 65535 ) ) ) ;
if ( sstable . get max timestamp ( ) < most recent partition tombstone ) break ;
execution job vertex = create execution job vertex ( 4 , 1 < < 15 ) ;
final int node = arc . target ; fst . read first target arc ( arc , arc ) ; while ( true ) {
qsi . map tsi . reset task vars ( ) ;
validate table of scalar longs ( client , select info from events _ capped _ offset order by info , new long [ ] { i - 1 , i } ) ;
error label . get style class ( ) . add ( error - label ) ; line . get style class ( ) . add ( input - line ) ; focused line . get style class ( ) . add ( input - focused - line ) ;
is open = false ; graph . close transaction ( this ) ; vertex cache . close ( ) ; }
pcollection < kv < long , bid > > bids by auction id = events . apply ( nexmark query . just _ bids ) . apply ( bid by auction , nexmark query . bid _ by _ auction ) ;
string [ ] links = extractor . get urls ( ) ;
return call constructor ( driver class , capabilities ) ; }
first digit = new node ( k3 , k4 , k5 , k6 , k7 , k8 , k9 ) ; m legal times tree . add child ( first digit ) ;
throw new error ( should not happen : + e . to string ( ) ) ;
stg . freeze ( false ) ; try { final opaginated cluster cluster = ( opaginated cluster ) stg . get cluster by name ( cluster name ) ; final file temp cluster file = new file ( temp directory path + + cluster name + opaginated cluster . def _ extension ) ; cluster . replace file ( temp cluster file ) ; final file temp cmp file = new file ( temp directory path + + cluster name + ocluster position map . def _ extension ) ; cluster . replace cluster map file ( temp cmp file ) ; } finally { stg . release ( ) ; } db . get local cache ( ) . invalidate ( ) ;
il . append ( method gen . load dom ( ) ) ;
tg = thread . current thread ( ) . get thread group ( ) ;
boolean inorder = ( phrase query . get slop ( ) = = 0 ) ; span near query sp = new span near query ( clauses , phrase query . get slop ( ) + position gaps , inorder ) ;
if ( current dimension < max dimensions ) { int max = 0 ; for ( object o : array ) { max = math . max ( max , build dimensions ( ( object [ ] ) o , dimensions list , max dimensions , current dimension + 1 ) ) ; } if ( current dimension = = max dimensions - 1 ) { dimensions list . add ( max ) ; } else { integer current = dimensions list . get ( 0 ) ; dimensions list . set ( 0 , math . max ( current , max ) ) ; } }
sos . println ( ) ;
write options ( configuration constants . assume _ no _ side _ effects _ option , configuration . assume no side effects ) ; if ( writer . check error ( ) ) { throw new ioexception ( can ' t write configuration ) ; } }
assert equals ( win error . error _ success , mpr . instance . wnet use connection ( null , resource , null , null , 0 , null , null , null ) ) ;
qjmtest util . write txns ( stm , 0 , 10 ) ;
field1 = lazy field lite . from value ( message1 ) ; field2 = create lazy field lite from message ( message2 ) ; field2 . get value ( test all types . get default instance ( ) ) ; force parsing . field1 . merge ( field2 ) ; assert equals ( expected , field1 . get value ( test all types . get default instance ( ) ) ) ; }
for ( int r = 0 ; r < 10 ; r + + ) { decorated key key = util . dk ( string . value of ( r ) ) ; update builder builder = update builder . create ( store . metadata ( ) , key ) ; for ( int c = 0 ; c < 10 ; c + + ) builder . new row ( column + c ) . add ( val , value ) ; mutation rm = new mutation ( builder . build ( ) ) ; rm . apply ( ) ; }
return 31 + value ;
process builder builder = ( process builder ) param . this object ;
b2 . set repeated field ( field , 0 , new map entry ( b1 , int32 _ to _ int32 _ field , 0 , 0 ) ) ;
id = get id ( end id method , o ) ;
int x = _ names . length , y = adapt frm . num cols ( ) ; frame f = adapt frm . extract frame ( x , y ) ; this will call vec _ impl ( ) and we cannot call the delete ( ) below just yet only adapt frm . delete ( ) ; return f ; }
if ( other = null ) { return other ; } method node node = new method node ( name , modifiers , return type , parameters , exceptions , code ) ; add method ( node ) ; return node ;
assert that ( open order entry . get key ( ) ) . is equal to ( o767 cw - txhcl - fwz5 r2 ) ;
if ( block index > 0 & & block index < = get sequence count ( w ) & & is one sequence ( w ) ) { return false ; }
string script = db . person . find one ( ) ; object result = ( client ) . execute script ( script ) ; assert . assert not null ( result ) ;
result [ out + + ] = 0x3f ;
final eureka instance registered event registered event = ( eureka instance registered event ) ( this . test events . application events . get ( 0 ) ) ;
if ( get l2 cache ( ) = null ) { get l2 cache ( ) . evict all ( ) ; } }
if ( ( w & ( 1 < < bit position ) ) = 0 ) { return false ; }
c = stream util . good class or null ( reader class , default package ) ;
int p7 = get precedence ( ops . lt , ops . gt ) ;
return this . check signature value ( cert . get public key ( ) ) ; }
int slash count = 0 ; number of slashes so far int i ; for ( i = 0 ; i < path . length ( ) ; i + + ) { if ( path . char at ( i ) = = ' ' ) { slash count + + ; if ( slash count = = 3 ) { return path . substring ( i + 1 , path . length ( ) ) ; } } } throw new runtime exception ( incorrect path prefix - - expected wasb : . . . . . . ) ;
locale pl = new locale ( pl ) ; date format symbols original dfs = new date format symbols ( pl ) ;
this . prediction = 0 ;
compiled rules . add ( rule descr . get parent name ( ) ) ; } children . compute if absent ( rule descr . get parent name ( ) , k - > new array list < > ( ) ) . add ( rule descr ) ; } }
asao _ buffer _ processed = true ;
peer . send frame ( ) . settings ( new settings ( ) ) ; peer . accept frame ( ) ; ack peer . accept frame ( ) ; syn _ stream peer . send frame ( ) . syn reply ( false , 3 , header entries ( a , android ) ) ; peer . send frame ( ) . data ( false , 3 , data ( 1024 ) , 1024 ) ; peer . truncate last frame ( 8 + 100 ) ; peer . play ( ) ;
if ( has ioexception filters & & t instanceof ioexception & & request sender . apply io exception filters and replay request ( future , ioexception . class . cast ( t ) , channel ) ) { return ; } read failed ( channel , future , t ) ;
vrg . size = 0 ; break ; case 1 : default :
data access rule dao dao = data access rule dao . get ( ) ; data access rule rule = new data access rule ( mock data . cite _ prefix , cite _ nature _ group , access mode . read ) ; final list < data access rule > rules = dao . get rules ( ) ; assert true ( rules . contains ( rule ) ) ; }
int file path column index = m app . get service ( ) . get cursor ( ) . get column index ( media store . audio . media . data ) ; return m app . get service ( ) . get cursor ( ) . get string ( file path column index ) ; } }
if ( val = = log level . int val event ) { return is on ( log level . log ctl event ) ; }
session proxy = database manager . new session ( conn type , database , user , password , props , zone seconds ) ;
directory dest = new directory ( ) ; final directory taxonomy writer dest tw = new directory taxonomy writer ( dest ) ; thread t = new thread ( ) { @ override public void run ( ) { for ( int i = 0 ; i < num categories ; i + + ) { try { dest tw . add category ( new facet label ( a , integer . to string ( i ) ) ) ; } catch ( ioexception e ) {
enable http2 ( 1 ) ;
if ( drag percent > 1 . 0f & & m end of refreshing ) { rotate angle = ( drag percent % 1 ) * 10 ; drag percent = 1 . 0f ; } float offset x = ( ( m screen width * drag percent ) 2 ) - m jet width center ;
duration seconds = max length seconds ; velocity interpolator velocity interpolator = new velocity interpolator ( duration seconds , vel abs , diff ) ; interpolator interpolator super interpolator = new interpolator interpolator ( velocity interpolator , m linear out faster in , interpolators . linear _ out _ slow _ in ) ; m animator properties . interpolator = super interpolator ; } else {
shutdown cluster ( ) ;
try { sut . export ( project , options , engine , writer ) ; } catch ( ioexception e ) { assert . fail ( ) ; } assert . assert equals ( writer . to string ( ) , prefix + suffix ) ;
string host = maker . after ( ) . before ( ' ' ) . value ( ) ;
if ( sub exchange . get exception ( ) = null ) {
for ( final string role name : declared security roles ) { final security role meta data security role md = new security role meta data ( ) ; security role md . set role name ( role name ) ; security roles md . add ( security role md ) ; }
trl . call event ( break event ) ; assert that ( trl . get event class ( ) , is ( ( object ) event . class ) ) ; trl = new timed registered listener ( listener , executor , event priority . normal , plugin , false ) ; trl . call event ( break event ) ; assert that ( trl . get event class ( ) , is ( ( object ) block break event . class ) ) ;
if ( pipe . class . is assignable from ( invoker . get return type ( ) ) ) { org . crsh . cli . impl . invocation . command invoker tmp = invoker ; return get pipe invoker ( tmp ) ; } else { determine the produced type class < ? > produced type ; if ( void . class . equals ( invoker . get return type ( ) ) ) { produced type = object . class ; } else { produced type = invoker . get return type ( ) ; } override produced type from invocation context < p > if any if ( invoker instanceof object command invoker ) { object command invoker < t , ? > object invoker = ( object command invoker < t , ? > ) invoker ; class < ? > [ ] parameter types = object invoker . get parameter types ( ) ; for ( int i = 0 ; i < parameter types . length ; i + + ) { class < ? > parameter type = parameter types [ i ] ; if ( invocation context . class . is assignable from ( parameter type ) ) { type context generic parameter type = object invoker . get generic parameter types ( ) [ i ] ; produced type = utils . resolve to class ( context generic parameter type , invocation context . class , 0 ) ; break ; } } } return get producer invoker ( invoker , produced type ) ; }
for ( int i = 0 ; i < cluster . get data nodes ( ) . size ( ) ; i + + ) { data node dn = cluster . get data nodes ( ) . get ( i ) ; log . info ( simulate block pinning in datanode + dn ) ; internal data node test utils . mock datanode blk pinning ( dn , true ) ; }
throw new internal error ( e . to string ( ) ) ; }
ch . read bytes ( bytes , 0 , 3 ) ;
double [ ] distances = new double [ data . num instances ( ) ] ;
if ( is enabled ( deserialization feature . fail _ on _ unknown _ properties ) ) { p . skip children ( ) ; return true ; }
camera manager . get ( ) . start preview ( ) ;
if ( m _ tracer = null ) { super . fire start elem ( name ) ; this . fire pseudo attributes ( ) ; } return ;
register column type ( types . nclob , nvarchar ( max ) ) ; register function ( row _ number , new no arg sqlfunction ( row _ number , standard basic types . integer , true ) ) ;
return new inflater input stream ( conn . get input stream ( ) , new inflater ( true ) ) ;
m renderer . notify pausing ( ) ; } } ) ; m glview . on pause ( ) ; log . d ( tag , on pause complete ) ; }
if ( ( ( item instanceof switch item ) | | ( item instanceof contact item ) ) ) { string message = item ' + item . get name ( ) + ' is of type ' + item . get class ( ) . get simple name ( ) + ' ; message = message + while only ' switch ' or ' contact ' types are allowed ; logger . error ( { } , message ) ; throw new binding config parse exception ( message ) ; } }
assert equals ( 1 , realm . where ( null types . class ) . greater than or equal to ( null types . field _ date _ null , new date ( 10000 ) ) . count ( ) ) ;
delete = new delete ( row1 ) ; delete . delete column ( fam1 , qf1 ) ; delete . delete column ( fam1 , qf1 ) ; res = region . check and mutate ( row1 , fam1 , qf1 , compare op . equal , new binary comparator ( val2 ) , delete , lock id , true ) ; assert true ( res ) ; delete = new delete ( row1 ) ; res = region . check and mutate ( row1 , fam1 , qf1 , compare op . equal , new binary comparator ( empty val ) , delete , lock id , true ) ; assert true ( res ) ;
vector3f k slice center = temp vb . set ( center ) ;
encoding encoding = new encoding ( encoding _ cdr _ encaps . value , ( byte ) 1 , * giop version * ( byte ) 0 * giop revision * ) ; codec codec = info . codec _ factory ( ) . create _ codec ( encoding ) ;
if ( tv . can not contain input ( ) ) { return null ; }
try { wrapper . after ( false , t ) ; } catch ( exception e ) { t . add suppressed ( e ) ; }
kvconfig config = new kvconfig ( ) ; config . parse ( httpbenchmark . class . get name ( ) , args ) ; httpbenchmark benchmark = new httpbenchmark ( config ) ;
visitor . visit no args ( opcode - 1 , offset , 1 , type . long ) ; return 1 ; } case byte ops . fadd : case byte ops . fsub : case byte ops . fmul : case byte ops . fdiv : case byte ops . frem : case byte ops . fneg : {
toggle ( ) ;
model files meta meta = get meta ( model input dir , conf ) ;
fs . delete ( stat . get path ( ) , false ) ; } else {
if ( selected in use ) { for ( int j = 0 ; j < size ; j + + ) { int i = sel [ j ] ; output . time [ i ] = time [ i ] ; output . nanos [ i ] = nanos [ i ] ; } } else { system . arraycopy ( time , 0 , output . time , 0 , size ) ; system . arraycopy ( nanos , 0 , output . nanos , 0 , size ) ; }
layer group info secured group = sc . get layer group by name ( opaque _ group _ name ) ;
fos2 . close ( ) ;
post method . set entity ( entity ) ; if ( header param names = null ) { for ( int i = 0 ; i < header param names . size ( ) ; i + + ) { post method . add header ( header param names . get ( i ) , header param values . get ( i ) ) ; } }
m decorated base adapter . notify data set changed ( ) ; }
fake result set . add column ( queries , dbpdata kind . numeric ) ;
object mapper mapper = new object mapper ( ) ; json node tree = mapper . read tree ( json ) ; assert not null ( tree ) ; }
if ( call . get call state ( ) = call state . call _ in _ progress ) return ;
this . access flags = access flags ; field list parser fl parser = new field list parser ( this , this class , at , attribute factory ) ;
grid coverage2 dreader reader = ( grid coverage2 dreader ) info . get grid coverage reader ( null , null ) ;
cas authentication details source = new service authentication details source ( props , geo server cas constants . artifact _ parameter ) ;
index . put ( object , clone ) ; ( ( jme cloneable ) clone ) . clone fields ( this , object ) ;
if ( parts = null ) { for ( int i = 0 ; i < parts . length ; i + + ) { mmspart part = parts [ i ] ; if ( part = null ) { try { pdu part part pdu = new pdu part ( ) ; part pdu . set name ( part . name . get bytes ( ) ) ; part pdu . set content type ( part . mime type . get bytes ( ) ) ; if ( part . mime type . starts with ( text ) ) { part pdu . set charset ( character sets . utf _ 8 ) ; } part pdu . set data ( part . data ) ; pdu body . add part ( part pdu ) ; } catch ( exception e ) { } } } } byte array output stream out = new byte array output stream ( ) ;
evaluation counts [ instruction offset ] + + ;
if ( formal type parameters = null & & formal type parameters . length > 0 ) { sb . append ( ' < ' ) ; for ( int i = 0 ; i < formal type parameters . length ; i + + ) { append generic type ( sb , formal type parameters [ i ] ) ; if ( i < formal type parameters . length - 1 ) { sb . append ( , ) ; } } sb . append ( > ) ; }
m _ loader editor . set environment ( m _ env ) ;
input stream is = hitbtc tickers json test . class . get resource as stream ( marketdata example - tickers - data . json ) ;
long [ ] right = new long [ 5 ] ;
return - 1 * latency ;
assert equals ( 3 , test helpers . filter native registers ( interpreter . get defined registers ( ) ) . size ( ) ) ; assert equals ( big integer . value of ( 0x0 l ) , interpreter . get variable value ( v1 ) ) ; assert equals ( big integer . value of ( 0x0 l ) , interpreter . get variable value ( v2 ) ) ; assert equals ( big integer . zero , big integer . value of ( interpreter . get memory size ( ) ) ) ;
list < version > accepted = new array list < version > ( ) ; for ( string v : accepted list ) { check version number ( v , accept versions ) ; accepted . add ( new version ( v ) ) ; }
try { connection r = reflect . on ( result ) . call ( get delegate ) . get ( ) ; if ( result = r & & r = null ) { result = r ; continue unwrapping loop ; } } catch ( reflect exception ignore ) { }
string key = ( string ) property ; if ( application context . contains bean ( key ) ) { context . set property resolved ( true ) ; return application context . get bean ( key ) ; }
final closeable iterable < ? extends element > results = graph . execute ( new get all elements ( ) , user ) ;
assert equals ( a & quot ; b & lt ; c & gt ; d & amp ; , html escaper ( ) . escape ( a \ b < c > d & ) ) ;
response = client . execute ( new http get ( uri2 ) ) ; try { assert . assert equals ( http servlet response . sc _ ok , response . get status line ( ) . get status code ( ) ) ; assert . assert equals ( 4 , integer . parse int ( response . get first header ( simple servlet . value _ header ) . get value ( ) ) ) ; map . entry < string , string > entry = parse session route ( response ) ; if ( entry = null ) { assert . assert equals ( node _ 2 , entry . get value ( ) ) ; assert . assert equals ( entry . get key ( ) , response . get first header ( simple servlet . session _ id _ header ) . get value ( ) ) ; } } finally { http client utils . close quietly ( response ) ; } lifecycle . start ( node _ 1 ) ;
if ( metrics registry = = null ) { metrics registry = new metric registry ( ) ; }
dis + = 2 * ( min level - index ) ; break ; } index + + ; }
conf . set ( kmsconfiguration . config _ prefix + acl . create , real ugi . get user name ( ) ) ; conf . set ( kmsconfiguration . whitelist _ key _ acl _ prefix + management , real ugi . get user name ( ) ) ; try { setup ( conf ) ; assert true ( exception during key creation , create key ( real ugi , key1 , conf ) ) ; assert true ( exception during key creation , create key ( real ugi , key2 , conf ) ) ; assert true ( exception during key creation , create key ( real ugi , key3 , conf ) ) ; } finally { teardown ( ) ; }
if ( ( value model . get object ( ) instanceof raster layer configuration ) ) { value model . set object ( new raster layer configuration ( ) ) ; } fragment f = new fragment ( editor , raster layer , this ) ; final drop down choice layer = new drop down choice ( layer , new property model ( value model , layer name ) , get raster layer names ( ) ) ;
integer network rate = _ network mgr . get network rate ( network . get id ( ) , null ) ; ip address to ip = new ip address to ( account . get account id ( ) , source nat ip address , add , false , shared source nat , public vlan tag , null , null , null , null , network rate , false ) ;
string model load dir = conf . get ( angel conf . angel _ load _ model _ path ) ; if ( model load dir = = null ) { throw new invalid parameter exception ( convert source path + angel conf . angel _ load _ model _ path + must be set ) ; } string converted model save dir = conf . get ( angel conf . angel _ save _ model _ path ) ;
else if ( qname . equals ( parser . get exclude result prefixes ( ) ) ) { stable . un exclude namespaces ( val ) ; }
return deferred . from result ( ( object ) null ) ; }
case pdu headers . message _ id :
assert true ( client a . get local file ( ignoredfile . txt ) . exists ( ) ) ;
bps = new block placement status with upgrade domain ( bpsd , upgrade domains , 4 , 4 ) ;
timeseries slot index . clear ( ) ;
for ( pattern token pattern token : pattern tokens ) { pattern token . set inside marker ( true ) ; }
region = open region ( ) ;
perform sized small uint failure test ( new byte [ ] { } ) ;
holder . text . set text size ( 24 ) ; v . set tag ( holder ) ;
if ( m _ instances . attribute ( i ) . is date ( ) ) { continue ; }
reset action mode flags ( ) ; return m last item in action mode ;
try { namespaces . get policies ( this . test property , this . test local cluster , non - existing - namespace - 1 ) ; fail ( should have failed ) ; } catch ( rest exception e ) { assert equals ( e . get response ( ) . get status ( ) , status . not _ found . get status code ( ) ) ; } try { namespaces . get permissions ( this . test property , this . test local cluster , non - existing - namespace - 1 ) ; fail ( should have failed ) ; } catch ( rest exception e ) { assert equals ( e . get response ( ) . get status ( ) , status . not _ found . get status code ( ) ) ; }
log . warn ( error when releasing lock , e ) ;
return - 1 = find ( l ) ;
t exception = te ;
bb = decimal type . instance . decompose ( null ) ; assert equals ( bb , byte buffer util . empty _ byte _ buffer ) ; }
write lock b . lock ( ) ;
index segment = loaders . index segment . load ( _ index dir , _ v1 index loading config ) ; assert . assert equals ( index segment . get segment metadata ( ) . get version ( ) , segment version . v1 . to string ( ) ) ; assert . assert false ( segment directory paths . segment directory for ( _ index dir , segment version . v3 ) . exists ( ) ) ;
fc thread . set daemon ( true ) ;
throw new org . apache . axis2 . databinding . adbexception ( message cannot be null ) ;
assert equals ( 0 , includes len ) ;
final int offset = query . get offset ( ) ;
for ( map . entry < resource location , forge registry < ? extends iforge registry entry < ? > > > r : registry manager . active . registries . entry set ( ) ) { final class < ? extends iforge registry entry > registry super type = registry manager . active . get super type ( r . get key ( ) ) ; load registry ( r . get key ( ) , staging , registry manager . active , registry super type , true ) ; }
assert xpath evaluates to ( custom dim value a , custom dim value b , custom dim value c , wms : layer wms : dimension , dom ) ;
if ( this . is symmetric ( ) ) throw new exception ( eigenvalue decomposition : matrix must be symmetric . ) ;
if ( ( c > = ' a ' & & c < = ' z ' ) ) c - = 0x20 ;
return site compare = = 0 ? super . compare to ( other ) : site compare ; }
else { process literal ( constant ) ; }
super . load bean definitions ( context , merged config ) ;
is attribute name = true ;
return super . implies ( action ) ;
return jdbc type mapping . get ( sql type , size , digits ) ;
expect fail not assignable ( parser , ctx , ( true | | false ) + + ) ; expect fail not assignable ( parser , ctx , - - ( false or true ) ) ; expect fail set value not supported ( parser , ctx , ( true | | false ) = ( false or true ) ) ;
input . skip raw bytes ( 3 ) ;
if ( m drop anim = null ) m drop anim . cancel ( ) ;
if ( notifier future = = null & & async write future = = null ) { spawn reactor ( ) ; }
string idmap in = ( string idmap ) value ; savable [ ] values = resolve ids ( in . values ) ; value = string savable map from kv ( in . keys , values ) ; field data . put ( field . alias , value ) ; }
if ( x509 cert = null ) { if ( x509 cert . equals ( xcert ) ) { if ( debug = null ) { debug . println ( x509 cert selector . match : + certs don ' t match ) ; } return false ; } }
assert equals ( data consistency is missed , 18 , scan operation ( s , conf , table name ) ) ;
if ( m _ aggregate distinct . get ( 0 ) = = 1 ) { return false ; } return true ;
roster roster = get connection ( 0 ) . get roster ( ) ;
k = 0 ; break ; case 1 :
os . write bytes ( lock id ) ; os . write ( separator ) ; os . write bytes ( database ) ; os . write ( separator ) ; os . write bytes ( table ) ; os . write ( separator ) ; os . write bytes ( partition ) ; os . write ( separator ) ; os . write bytes ( state ) ; os . write ( separator ) ; os . write bytes ( blocked by ) ; os . write ( separator ) ; os . write bytes ( type ) ; os . write ( separator ) ; os . write bytes ( transaction id ) ; os . write ( separator ) ; os . write bytes ( last heartbeat ) ; os . write ( separator ) ; os . write bytes ( acquired at ) ; os . write ( separator ) ; os . write bytes ( user ) ; os . write ( separator ) ; os . write bytes ( hostname ) ; os . write ( separator ) ; os . write bytes ( agent info ) ; os . write ( terminator ) ; list < show locks response element > locks = rsp . get locks ( ) ;
check state ( js root = = null ) ; try { init ( externs , inputs , options ) ; if ( options . print config ) { print config ( system . err ) ; } if ( has errors ( ) ) { parse for compilation ( ) ; } if ( has errors ( ) ) { if ( options . get instrument for coverage only ( ) ) { todo ( bradfordcsmith ) : the option to instrument for coverage only should belong to the runner , not the compiler . instrument for coverage ( ) ; } else { stage1 passes ( ) ; if ( has errors ( ) ) { stage2 passes ( ) ; } } perform post compilation tasks ( ) ; } } finally { generate report ( ) ; }
int left = ( int ) math . min ( clip bounds . left , clip bounds old . left ) ;
assert true ( hri . contains range ( bytes . to bytes ( a ) , bytes . to bytes ( a ) ) ) ;
server support . configure handlers ( server , null ) ;
if ( domain match ( host , cookie domain ) ) { throw new malformed cookie exception ( domain attribute \ + cookie . get domain ( ) + \ violates rfc 2965 : effective host name does not + domain - match domain attribute . ) ; }
test allocator . add data ( node2 , null ) ;
template template = get template ( ) ;
if ( doc = = null ) we didn ' t do the moxy workaround above { doc = xml utils . get new document builder ( ) . parse ( is ) ; this also guards against xxe }
{ v _ 3 = limit - cursor ; lab1 : do {
segments [ i ] [ 1 ] = udhie _ identifier _ sar ;
assert true ( notebook repo sync . get repo count ( ) > 1 ) ; assert equals ( 0 , notebook repo sync . list ( 0 , null ) . size ( ) ) ; assert equals ( 0 , notebook repo sync . list ( 1 , null ) . size ( ) ) ; file src dir = new file ( src test resources 2 a94 m5 j1 z ) ;
conf . set ( kmsconfiguration . config _ prefix + acl . create , real ugi . get user name ( ) ) ; conf . set ( kmsconfiguration . whitelist _ key _ acl _ prefix + management , real ugi . get user name ( ) ) ; try { setup ( conf ) ; assert true ( exception during key creation , create key ( real ugi , key1 , conf ) ) ; assert true ( exception during key creation , create key ( real ugi , key2 , conf ) ) ; assert true ( exception during key creation , create key ( real ugi , key3 , conf ) ) ; } finally { teardown ( ) ; }
if ( m current scroll direction > 0 ) center index + + ;
case req _ ssl _ attribute : {
ldap context source context source = new ldap context source ( ) ;
collection < address > coords = util . determine actual merge coords ( views ) ; if ( coords . is empty ( ) ) coords = util . determine merge coords ( views ) ; https : issues . jboss . org browse jgrp - 2092 if ( coords . is empty ( ) ) { log . error ( % s : unable to determine merge leader from % s ; not starting a merge , gms . local _ addr , views ) ; return null ; } return new membership ( coords ) . sort ( ) . element at ( 0 ) ; establish a deterministic order , so that coords can elect leader }
for ( media format format : formats ) { if ( format . get encoding ( ) . equals ( constants . telephone _ event ) ) res . add ( format ) ; } return res ;
toolbar popup menu insert chunks menu = new toolbar popup menu ( ) ; insert chunks menu . add item ( commands _ . insert chunk r ( ) . create menu item ( false ) ) ; insert chunks menu . add separator ( ) ; if ( browse cap . is windows desktop ( ) ) { insert chunks menu . add item ( commands _ . insert chunk bash ( ) . create menu item ( false ) ) ; } insert chunks menu . add item ( commands _ . insert chunk python ( ) . create menu item ( false ) ) ; insert chunks menu . add item ( commands _ . insert chunk rcpp ( ) . create menu item ( false ) ) ; insert chunks menu . add item ( commands _ . insert chunk sql ( ) . create menu item ( false ) ) ; insert chunks menu . add item ( commands _ . insert chunk stan ( ) . create menu item ( false ) ) ; insert chunk menu _ = new toolbar button ( insert , commands _ . insert chunk ( ) . get image resource ( ) , insert chunks menu , true ) ; toolbar . add right widget ( insert chunk menu _ ) ;
rewrite assert = false ; } }
int next op pos = op pos ; int i ; for ( i = 0 ; get op ( next op pos ) = = op codes . op _ locationpathpattern ; i + + ) { next op pos = get next op pos ( next op pos ) ; } if ( i = = 1 ) return compile ( op pos ) ;
set scrolling cache enabled ( false ) ; return ; }
map . set ( key , value2 ) ; assert true eventually ( new assert task ( ) { @ override public void run ( ) throws exception { assert null ( key should be expired after 1 seconds , map . get ( key ) ) ; } } ) ;
take node outof service ( 1 , first dn . get datanode uuid ( ) , 0 , null , admin states . decommissioned ) ; }
int refno = - 1 ;
class < ? > target class = clazz ; list < field > fields = new array list < > ( ) ; do { fields . add all ( arrays . as list ( target class . get declared fields ( ) ) ) ; target class = target class . get superclass ( ) ; } while ( target class = null & & target class = object . class ) ; return fields . to array ( new field [ fields . size ( ) ] ) ;
context . add servlet ( time servlet . class , time ) ; context . add servlet ( request info servlet . class , req - info ) ; server . start ( ) ;
chargeback amount in correct currency = amount for transactions ;
if ( keyeq ( k , key , hashes , idx , fullhash ) ) return k ; return existing key
last angle = ( x cordinates [ i ] - x cordinates [ start ] ) ( i - start ) ;
rest . post ( key3 path , v3 , text plain , http status . sc _ ok , content - type , text plain , time to live seconds , 0 , max idle time seconds , 0 ) ; rest . post ( key4 path , v4 , text plain , http status . sc _ ok , content - type , text plain , time to live seconds , 0 , max idle time seconds , 2 ) ; sleep for secs ( 1 ) ; rest . get ( key1 path , v1 ) ; rest . get ( key3 path , v3 ) ; rest . get ( key4 path , v4 ) ; sleep for secs ( 2 ) ;
logger . warn ( an error occurred while trying to set the read - only variable ' { } ' to { } , value selector . to string ( ) , command . to string ( ) ) ;
data set < edge < long , null value > > edges = env . from elements ( new tuple3 < > ( 1 l , 2 l , null value . get instance ( ) ) ) . map ( new tuple3 to edge map < > ( ) ) ; graph < long , long , null value > graph = graph . from data set ( edges , new init vertices ( ) , env ) ;
args [ 0 ] = - fs ; args [ 1 ] = namenode ; execute ( args , namenode ) ; }
list < task > tasks = task service . create task query ( ) . order by due date nulls last ( ) . asc ( ) . list ( ) ;
final int iinterval _ precision = 18 ;
if ( ( type = no _ labels ) & & ( type = value _ labels ) ) { throw new illegal argument exception ( meter plot . set label type ( int ) : unrecognised type . ) ; }
file status file = get file status ( namenode . get file system ( ) , nn file ) ;
for ( int i = 1 ; i < = cols ; i + + ) { tc tc = context . get wml object factory ( ) . create tc ( ) ; tr . get egcontent cell content ( ) . add ( tc ) ; tc pr tc pr = context . get wml object factory ( ) . create tc pr ( ) ; tc . set tc pr ( tc pr ) ;
} } catch ( exception e ) { log . error ( function : { } dataset : { } exception in preparing entry { } , function id , dataset , detection status , e ) ; } }
pos . set index ( line . index of ( ' ] ' , end ) + 1 ) ; return pause ;
pattern script pattern = script _ pattern ; clean value = script pattern . matcher ( clean value ) . replace all ( ) ;
conf . set ( fs _ default _ name _ key , hdfs : localhost : 1000 ) ; verify addresses ( conf , test type . namenode , false , localhost : 1000 ) ; verify addresses ( conf , test type . nnrpcaddresses , true , localhost : 1000 ) ;
. and expect ( status ( ) . is unauthorized ( ) )
result . add provider ( desugared jars . build ( ) ) ; return functions . identity ( ) ;
function methods [ 0 ] = methods [ i ] ;
document type type = new document type ( test ) ;
int first unification = - 1 ; int first non unification = - 1 ; for ( int i = 0 , length = constraints . length ; i < length ; i + + ) { if ( is indexable ( constraints [ i ] , beta node type ) ) { final boolean is unification = ( ( indexable constraint ) constraints [ i ] ) . is unification ( ) ; if ( is unification & & first unification = = - 1 ) { first unification = i ; } else if ( is unification & & first non unification = = - 1 ) { first non unification = i ; } } if ( first unification = - 1 & & first non unification = - 1 ) { break ; } } if ( first non unification = - 1 & & first non unification > 0 ) { make sure a nonunification indexable constraint is first swap ( constraints , 0 , first non unification ) ; }
continue ; } } } while
language = de ;
htu . verify numeric rows ( table , f , start row , end row , replica id ) ; }
states [ 1 ] = new int [ ] { android . r . attr . state _ checked } ;
assert equals ( 1 , item1 merged . get sub items backref ( ) . size ( ) ) ; assert true ( item1 merged . get sub items backref ( ) . contains ( sub item2 ) ) ; tx . commit ( ) ; s . close ( ) ; s = open session ( ) ;
form . set maximum integer digits ( 500 ) ; assert equals ( 500 , form . get maximum integer digits ( ) ) ; form = new decimal format ( 00 . e0 ) ;
proxy = null ;
the socket . connect ( new inet socket address ( inet address . get local host ( ) , sport ) ) ;
preferences . edit ( ) . put string ( ftpservice . key _ preference _ password , ) . apply ( ) ; is password protected = false ; } }
vt . get long ( 0 ) ;
ensure governor has method ( new method metadata builder ( get persist generate id test method ( ) ) ) ;
hregion r0 = hregion . create hregion ( new hregion info ( htd . get name ( ) ) , archive dir , conf , htd , null , true , true ) ; path store file = new path ( new path ( r0 . get region dir ( ) , test _ family ) , test _ hfile ) ; fs . create new file ( store file ) ; r0 . close ( ) ;
item = menu . add ( menu . none , id _ share , menu . none , r . string . reader _ btn _ share ) ; item . set show as action ( menu item . show _ as _ action _ if _ room ) ; item . set icon ( r . drawable . ic _ share _ white _ 24dp ) ; return true ;
system . out . println ( error parent of xsl : fallback must be an extension or unknown element ) ;
net client . post ( constants . regist url , params , new base json res ( ) { @ override public void on my success ( string data ) { utils . put value ( register activity . this , constants . user info , data ) ; utils . put value ( register activity . this , constants . name , name ) ; utils . put value ( register activity . this , constants . pwd , des . md5 pwd ( pwd ) ) ; utils . put boolean value ( register activity . this , constants . login state , true ) ; get chatserive ( name , des . md5 pwd ( pwd ) ) ; } @ override public void on my failure ( ) { get loading dialog ( ) . dismiss ( ) ; btn _ register . set enabled ( true ) ; btn _ send . set enabled ( true ) ; } } ) ;
if ( get focus traversal policy ( ) . get default component ( this ) = = null ) { return false ; }
c . set int ( hbase . ipc . client . connect . max . retries , 1 ) ;
union test . union with null union = new union test . union with null ( data . null ) ; union test . union with null union clone = union . clone ( ) ; assert same ( union clone . data ( ) , data . null ) ;
server . invoke ( name , start cache , new object [ ] { a } , new string [ ] { string . class . get name ( ) } ) ;
exit code = compiler . compile ( sources ) ;
metadata builder . apply attribute converter ( my attribute converter ) ; metadata metadata = metadata builder . build ( ) ;
return ( t ) data ;
instructions . add ( reil helpers . create undef ( offset + + , operand size . byte , helpers . auxiliary _ flag ) ) ;
parcelable super state = super . on save instance state ( ) ; saved state ss = new saved state ( super state ) ;
clp . inset edge = gravity . bottom ;
if ( predecessors . size ( ) = = 0 ) { return false ; }
x + = standard size ;
for ( int i = dot pos + 1 ; i < end pos ; i + + ) { assert true ( a [ i ] < = ' 9 ' & & a [ i ] > = ' 0 ' ) ; j + + ; }
assert that ( test realm resource ( ) . flows ( ) . get flows ( ) , not ( has item ( test flow ) ) ) ;
restore = il . append ( method gen . store iterator ( ) ) ; il . append ( method gen . store current node ( ) ) ; exp . back patch false list ( restore ) ; _ false list . add ( il . append ( new goto ( null ) ) ) ;
log . debug ( failure during shutdown : { } , failure , failure ) ;
feature named feature = feature . new builder ( ) . set name ( name ) . set location ( point ) . build ( ) ; features . add ( named feature ) ; feature = stub . get feature ( point ) ; assert equals ( named feature , feature ) ;
final operation set connection info op set connection info = protocol provider . get operation set ( operation set connection info . class ) ;
final int len = 8196 ; final path zone parent = new path ( zones ) ; final path zone = new path ( zone parent , zone ) ; fs wrapper . mkdir ( zone , fs permission . get dir default ( ) , true ) ; dfs admin . create encryption zone ( zone , test _ key , no _ trash ) ; for ( int i = 0 ; i < 10 ; + + i ) { dfstest util . create file ( fs , new path ( zone , integer . to string ( i ) ) , len , ( short ) 1 , 0x feed ) ; } final path subdir = new path ( zone , dir ) ; fs wrapper . mkdir ( subdir , fs permission . get dir default ( ) , true ) ; dfstest util . create file ( fs , new path ( subdir , f ) , len , ( short ) 1 , 0x feed ) ; final path enc file0 = new path ( zone , 0 ) ; final path enc file9 = new path ( zone , 9 ) ; final file encryption info fei0 = get file encryption info ( enc file0 ) ;
accuracy = sug queue . top ( ) . score ;
mockito . when ( service1 . very slow method ( ) ) . then answer ( new locking answer ( counter ) ) ; mockito . when ( service2 . very slow method ( ) ) . then answer ( new locking answer ( counter ) ) ;
for ( recycler view . on scroll listener listener : m extra on scroll listeners ) { listener . on scroll state changed ( view , new state ) ; } }
if ( pn . get child ( 0 ) instanceof projection plan node ) { pn = pn . get child ( 0 ) ; }
map < string , object > column name map = get column name map ( ) ; string builder sb = new string builder ( ) ; string [ ] s = column name . split ( \ \ . ) ; for ( int i = 0 ; i < s . length ; i + + ) { if ( i > 0 ) { sb . append ( . ) ; } if ( column name map . contains key ( s [ i ] ) ) { s [ i ] = column name map . get ( s [ i ] ) . to string ( ) ; } else { logger . warn ( no column map entry for { } in map { } , s [ i ] , column name map ) ; } sb . append ( s [ i ] ) ; } return sb . to string ( ) ;
profile . set country code ( m country field . get value ( ) . to string ( ) ) ; profile . set phone number ( m phone field . get value ( ) . to string ( ) ) ;
a . set user limit ( 50 ) ;
callable . with retries ( ) ;
project reactor builder . set module key and name if not defined ( props , foo , parent ) ;
waiting consume payloads . publish ( waiting consume payloads . next ( ) ) ; waiting consume payloads gating sequence . set ( waiting consume payloads . get cursor ( ) ) ; batch size = conf . get long ( wal _ batch _ size , default _ wal _ batch _ size ) ;
return resource ;
b type = xsmodel group impl . modelgroup _ choice ; b children = new vector ( bsub group . length + 1 ) ;
if ( series result = = null ) { series result = new map result data ( ) ; series result . set result ( result _ series _ name , new value result data ( series ) ) ; series result . set result ( result _ series _ is _ controller , new value result data ( boolean . value of ( series data . is controllers series ( ) ) ) ) ; series result . set result ( result _ series _ is _ overall , new value result data ( boolean . value of ( series data . is overall series ( ) ) ) ) ; series result . set result ( result _ series _ data , new list result data ( ) ) ; series list . add result ( series result ) ; } list result data data result = ( list result data ) series result . get result ( result _ series _ data ) ;
obsolete vals . add ( value ) ;
elements . add ( new mdurl ( link , link ) ) ;
if ( current rule = = rule ) { return null ; }
if ( has ( ) ) { for ( notification notification : get ( ) ) { if ( notification . read ) { for ( notification new notification : notification list ) { if ( new notification . id = = notification . id ) { new notification . read = false ; break ; } } } } }
entry = segment . new entry for testing ( key , hash , entry ) ;
if ( scopes = = null | | scopes . contains key ( kv . get family ( ) ) ) { kvs . remove ( i ) ; }
try { utils . check for remaining options ( options ) ; } catch ( exception e ) { system . out . println ( e . get message ( ) ) ; system . out . println ( make options string ( stemmer ) ) ; return ; }
final int pos = ostring serializer helper . parse ( cfg field , new string builder ( ) , 0 , - 1 , new char [ ] { ' . ' } , true , true , true , 0 , true ) - 1 ;
check splits ( fr splits , expand splits , interactions , use all , standardize , skip missing ) ;
start = stop ; stop = sign < 0 ? end : sign ; data . second = parse second ( buffer , start , stop ) ;
netty test utils . close server connection ( _ netty tcpserver ) ; set up ( ) ; stats = pool . get stats ( ) ; assert . assert equals ( stats . get pool size ( ) , 2 ) ; assert . assert equals ( stats . get idle count ( ) , 2 ) ;
session session = open session ( ) ; session . begin transaction ( ) ; account account = new account ( new account id ( 1 ) , test acct ) ; session . save ( account ) ; session . get transaction ( ) . commit ( ) ; session . close ( ) ;
for ( hstore store : region . get stores ( ) ) { store . trigger major compaction ( ) ; }
create3 response response1 = nfsd . create ( xdr _ req . as read only wrap ( ) , security handler unpriviledged , new inet socket address ( localhost , 1234 ) ) ; assert equals ( incorrect return code : , nfs3 status . nfs3 err _ acces , response1 . get status ( ) ) ;
logger . info ( rebalancing streams between targets ) ; balancer . balance ( 0 , 0 , 10 , rate limiter ) ; set < string > source streams = get all streams from distribution ( get stream ownership distribution ( dl client ) ) ; set < string > target streams = get all streams from distribution ( get stream ownership distribution ( target client ) ) ; assert equals ( num streams 2 , source streams . size ( ) ) ; assert equals ( num streams 2 , target streams . size ( ) ) ; for ( string name : source streams ) { check stream ( name , dl client , dl server , 1 , num streams 2 , num streams 2 , true , true ) ; check stream ( name , target client , target server , 1 , num streams 2 , num streams 2 , false , false ) ; }
throw new parsing exception ( in stream is empty ) ;
extra beans . add ( camel context bean ( manager , any , default , application _ scoped ) ) ; } else if ( contexts . size ( ) = = 1 ) {
int left dif = child rect . left - abstract layouter . get canvas left border ( ) ;
kie services ks = kie services . factory . get ( ) ; string group = org . kie . test ;
if ( ( new value instanceof string ) ) return ; element detail element = contact list document . create element ( meta _ contact _ detail _ name _ node _ name ) ;
log . d ( tag , got cached sign - in ) ;
if ( invalid _ pattern . matcher ( answer ) . matches ( ) ) { throw new illegal argument exception ( pattern is invalid : + pattern ) ; } return answer ;
if ( ok ) { authenticator config ( ) ; }
return 0 ; indent : 8 exp : 8
cluster test utils . rebalance kit rebalance kit = cluster test utils . get rebalance kit ( bootstrap url , z1z3 shuffle , z1z3 stores ) ; rebalance and check ( rebalance kit . plan , rebalance kit . controller , server list ) ; check consistent metadata ( z1z3 shuffle , server list ) ;
for ( versioned < byte [ ] > value : values ) { iterator < versioned < byte [ ] > > iter = resolved versions . iterator ( ) ; boolean obsolete = false ; compare the current version with a set of accepted versions while ( iter . has next ( ) ) { versioned < byte [ ] > curr = iter . next ( ) ; occurred occurred = value . get version ( ) . compare ( curr . get version ( ) ) ; if ( occurred = = occurred . before ) { obsolete = true ; break ; } else if ( occurred = = occurred . after ) { iter . remove ( ) ; } } if ( obsolete ) { else update the set of accepted versions resolved versions . add ( value ) ; } } return resolved versions ;
return two = removed & & strategy . equals ( ( t ) one , ( t ) two ) ;
node traversal . traverse es6 ( compiler , root , new reduction gatherer ( reducers , reduction map ) ) ;
headers . put ( exchange . http _ path , path ) ; if ( log . is trace enabled ( ) ) { log . trace ( http - method { } , request . method ( ) . name ( ) ) ; log . trace ( http - uri { } , request . uri ( ) ) ; }
sorted set iterable < a > numbers = this . < a > new with ( new c ( 4 . 0 ) , new b ( 3 ) , new c ( 2 . 0 ) , new b ( 1 ) ) ; assert equals ( this . < b > get expected filtered ( new b ( 3 ) , new b ( 1 ) ) , numbers . select instances of ( b . class ) ) ; assert equals ( this . get expected filtered ( new c ( 4 . 0 ) , new b ( 3 ) , new c ( 2 . 0 ) , new b ( 1 ) ) , numbers . select instances of ( a . class ) ) ; }
row = response . rows ( ) [ 1 ] ;
kmeans model . kmeans parameters params = new kmeans model . kmeans parameters ( ) ;
m chained tabs . add ( new tab ) ; m tab chains . add ( new tab chain ( new tab , tab _ spacing _ in _ px ) ) ; } else {
this . menu = new context menu ( ) ;
+ + _ input ptr ; leading zero to be skipped
synchronized ( tasks to be removed ) { for ( task attempt id tid : tasks to be removed ) { process tree info map . remove ( tid ) ; } tasks to be removed . clear ( ) ; } long memory still in usage = 0 ;
customer listing . click account management ( ) ; assert current url starts with ( test realm account ) ;
java . util . calendar end date = new gregorian calendar ( ) ; end date . set time zone ( timezone ) ; end date . set ( java . util . calendar . month , java . util . calendar . april ) ; end date . set ( java . util . calendar . day _ of _ month , 1 ) ; end date . set ( java . util . calendar . year , 2013 ) ; end date . set ( java . util . calendar . hour _ of _ day , 21 ) ; end date . set ( java . util . calendar . minute , 0 ) ; end date . set ( java . util . calendar . second , 0 ) ;
dbsstructure assistant structure assistant = dbutils . get adapter ( dbsstructure assistant . class , sc ) ;
for ( int i = 0 ; i < element count ; i + + ) element data [ i ] = null ; element count = 0 ;
try { return _ create and cache2 ( ctxt , factory , type ) ; } finally {
return ( recycler view . view holder ) vh field . get ( params ) ;
fixture . provider factory = ( protocol provider factory ) icq slick fixture . bc . get service ( icq factory refs [ 0 ] ) ; fixture . provider = ( protocol provider service ) icq slick fixture . bc . get service ( icq provider refs [ 0 ] ) ; icq slick fixture . icq account id = fixture . provider . get account id ( ) ;
workset iteration plan node iteration = ( workset iteration plan node ) sink . get input ( ) . get source ( ) ;
index predicate analyzer analyzer = get index predicate analyzer ( indexes , query context . get query partitions ( ) ) ;
parsed args . optimize = false ;
for ( int stage = 0 ; stage < = max stage ; stage + + ) { log . info ( running stage + stage ) ; run backup and fail at stage ( stage ) ; }
charset decoder . decode ( byte buffer , char buffer , false ) ;
assert equals ( ( ( list ) o ) . get ( 1 ) , ( ( list ) reader . next ( ) ) . get ( 1 ) ) ; }
locale iso = parameters . get default ( parameters . default . locale ) ; if ( locale iso = = null ) throw new exception ( no default locale provided for spell checker ) ; }
curr state . untrusted checker . check ( cert , collections . < string > empty set ( ) ) ;
reside menu = new reside menu ( this ) ;
list < processor definition < ? > > children = node . get outputs ( ) ; if ( children = null & & children . is empty ( ) ) { for ( processor definition < ? > child : children ) { traverse children also gather all node ids ( child , set , only custom id , include abstract ) ; } } return set ;
merged bin count = combine bins ( this . bin count , this . positions , this . bins , h . bin count , h . positions , h . bins , merged positions , merged bins , null ) ; } else {
client = client factory . create client ( m _ cconfig ) ; client . create connection ( localhost , cluster . port ( 0 ) ) ; try { response = client . call procedure ( @ snapshot restore , snapshot dir , testnonce ) ; system . out . println ( response . get results ( ) [ 0 ] . to jsonstring ( ) ) ; } catch ( proc call exception ex ) { system . out . println ( ex . get client response ( ) . get results ( ) [ 0 ] . to formatted string ( true ) ) ; ex . print stack trace ( ) ; } assert equals ( client response . success , response . get status ( ) ) ; client . close ( ) ; cluster . shut down ( ) ;
sb . append ( vds bucket progress file ( ) ;
component dto sub module = db . components ( ) . insert component ( new module dto ( module ) ) ; db client . properties dao ( ) . save property ( db session , new property dto ( ) . set key ( sonar . jira . project . key ) . set value ( sonar ) . set resource id ( sub module . get id ( ) ) ) ; db client . properties dao ( ) . save property ( db session , new property dto ( ) . set key ( sonar . jira . login . secured ) . set value ( john ) . set resource id ( sub module . get id ( ) ) ) ; db client . properties dao ( ) . save property ( db session , new property dto ( ) . set key ( sonar . coverage . exclusions ) . set value ( * * * . java ) . set resource id ( sub module . get id ( ) ) ) ; db session . commit ( ) ;
throw new illegal state exception ( unable to insert ) ;
response = put as servlet response ( rest base controller . root _ path + resource mynewdir?operation = copy , mydir , text plain ) ;
build artifacts ( bar ) ; }
return deleted count ;
} } , callback ) ; filter chain . do filter ( http request , http response ) ;
for ( int i = 0 ; i < num tables ; i + + ) { shifts [ i ] = maps [ 0 ] . log2size - maps [ i ] . log2size ; low bits mask [ i ] = ( 1 < < shifts [ i ] ) - 1 ; }
long last tx id = super . get last written tx id ( ) ;
assert equals ( data consistency is missed , 18 , scan operation ( s , conf , table name ) ) ;
preferences alarm list set = new hash set < > ( arrays . as list ( gbalarm . default _ alarms ) ) ;
sink . add test ( has attribute01 . class ) ;
assert that ( settable future . create ( ) . to string ( ) ) . is not equal to ( settable future . create ( ) . to string ( ) ) ; }
slice = slice . make ( make bound ( sk , 1 ) , make bound ( ek , 1 ) ) ;
changed . put ( display _ name , false ) ;
p0 = - e2 . z * tmp0 . x + e2 . x * tmp0 . z ;
string status = template . request body ( controlbus : route?route id = foo & action = status & async = true , null , string . class ) ;
flush input ( ) ;
class < ? > instance class = camel context . get class resolver ( ) . resolve mandatory class ( class name ) ; class < ? > generic container = camel context . get class resolver ( ) . resolve mandatory class ( generic _ container _ classname ) ; if ( generic container . is assignable from ( instance class ) ) { try { method method = instance class . get method ( get schema ) ; return ( schema ) method . invoke ( camel context . get injector ( ) . new instance ( instance class ) ) ; } catch ( exception ex ) { throw new camel exception ( error calling get schema on + instance class , ex ) ; } } else { throw new camel exception ( class + instance class + must be instanceof + generic _ container _ classname ) ; }
string [ ] previous text cases = new string [ ] { , plain text , & rho ; , < blockquote > some existing content & dagger ; < blockquote > } ;
public host add external firewall ( add external firewall cmd cmd ) { long zone id = cmd . get zone id ( ) ; data center vo zone = null ; physical network vo p network = null ; host vo fw host = null ; zone = _ dc dao . find by id ( zone id ) ;
process zipped root ( root directory , context ) ;
clock . set time ( system . current time millis ( ) + 500000 l ) ;
indarray exp labels = nd4j . create ( new double [ ] [ ] { { 1 , 0 } , { 0 , 1 } } ) ; assert equals ( exp labels , d1 . get labels ( ) ) ;
display _ . open web minimal window ( viewer url , false , width , height , options ) ;
e context . set variable ( bar , 1 ) ;
if ( is revoked ) { c . fulfilled request count - - ; c . revoked request count + + ; }
for ( row row : collector . collect ( ) ) { assert that ( ( float ) row . get ( 0 ) , matchers . greater than or equal to ( 0 . 15f ) ) ; count + + ; } assert that ( count , is ( 2 ) ) ; count = 0 ;
wrapped . close ( ) ;
assert that ( counter . get ( ) , both ( greater than ( 0 ) ) . and ( less than or equal to ( 10 ) ) ) ; }
catch ( exception e ) { logger . debug ( could not retrieve metadata version . exception : + e ) ; }
return new ostorage operation result < > ( false ) ;
assert equals ( true , completed job . tasks loaded . get ( ) ) ; assert equals ( 10 , completed job . get tasks ( task type . map ) . size ( ) ) ; assert equals ( 2 , completed job . get tasks ( task type . reduce ) . size ( ) ) ; assert equals ( user , completed job . get user name ( ) ) ; assert equals ( job state . succeeded , completed job . get state ( ) ) ; job report job report = completed job . get report ( ) ; assert equals ( user , job report . get user ( ) ) ; assert equals ( job state . succeeded , job report . get job state ( ) ) ; assert equals ( full history path . to string ( ) , job report . get history file ( ) ) ; }
execute ( delete from % s where partition key = ? and clustering _ 1 = ? , 0 , 1 ) ;
security manager s = system . get security manager ( ) ; if ( s = null ) { int i = class name . last index of ( ' . ' ) ; if ( i = - 1 ) { s . check package access ( class name . substring ( 0 , i ) ) ; } } if ( class loader = = null ) return class . for name ( class name ) ; else return class loader . load class ( class name ) ;
require no content ( reader ) ;
e . print stack trace ( ) ; } catch ( illegal state exception e ) {
hbck info hi = his . iterator ( ) . next ( ) ; hregion info hri = hi . get hdfs hri ( ) ; hi . meta entry ; put p = new put ( hri . get region name ( ) ) ; p . add ( hconstants . catalog _ family , hconstants . regioninfo _ qualifier , writables . get bytes ( hri ) ) ; puts . add ( p ) ; } }
org . apache . log4j . logger . get logger ( org . apache . hadoop . metrics2 . util . mbeans . class ) . set level ( org . apache . log4j . level . error ) ; org . apache . log4j . logger . get logger ( org . apache . hadoop . metrics2 . impl . metrics system impl . class ) . set level ( org . apache . log4j . level . error ) ; this . dfs cluster = new mini dfscluster ( 0 , this . conf , servers , true , true , true , null , null , hosts , null ) ;
htable descriptor htd = hbu . create table descriptor ( name . get method name ( ) , 0 , 3 , hconstants . forever , keep deleted cells . true ) ; hregion region = hbu . create local hregion ( htd , null , null ) ; long ts = environment edge manager . current time ( ) ;
int char offset start = 0 ; tells us where we are in the original string int char offset end = 0 ; tells us where we are in the original string int line index = 0 ; float x offset ; left margin off a given line float max width ; how far to the right it can strectch float y offset = 0 ; string this line str ; the current line we are trying to render
assert null ( scheduler . get rmcontainer ( previous attempt container . get container id ( ) ) ) ; }
if ( status codes . contains ( status . banned _ 301 ) ) { check if this occupant was banned if ( is user modification ) { joined = false ; for ( user status listener listener : user status listeners ) { listener . banned ( muc user . get item ( ) . get actor ( ) , muc user . get item ( ) . get reason ( ) ) ; } reset occupant information . occupants map . clear ( ) ; nickname = null ; user has left ( ) ; } else { for ( participant status listener listener : participant status listeners ) { listener . banned ( from , muc user . get item ( ) . get actor ( ) , muc user . get item ( ) . get reason ( ) ) ; } } }
error message . add ( new interpreter result message ( interpreter result . type . text , failed to start python ) ) ;
final interceptor context invocation = current invocation context . get ( ) ; return get component ( ) . get ejblocal object ( invocation ) ; }
assert equals ( m queries , queries ) ;
clock . wait for ( 101 ) ;
case finishing : case finished : case failed : return final application status . failed ;
partition ( 1 ) . assert keys not available for write ( k0 , k1 , k2 ) ;
if ( digits [ number length + + ] = = 0 ) { sign = 0 ; }
vmc . add plot ( tempd ) ; visualize classifier errors ( vmc ) ; } catch ( exception ex ) { ex . print stack trace ( ) ; } } } ) ; vis threshold . add ( clv ) ; } } else {
log . unknown sql server version ( major version , sqlserver2012 dialect . class ) ; return new sqlserver2012 dialect ( ) ; } } } }
get taapplication ( ) . register command ( taidentitycommand , taidentity command . class ) ;
if ( iterators . get ( position . get ( ) ) . has next ( ) ) position . get and increment ( ) ; return iterators . get ( position . get ( ) ) . next ( ) ; }
if ( enabled pos > = 0 ) { system . arraycopy ( state , enabled pos + 1 , state , enabled pos , state . length - enabled pos - 1 ) ; } return state ; }
strip anonymity ( ) ; } super . put ( key , value ) ; } }
error = cam . connect ( guid ) ;
key value set . clear ( ) ; key value set . add ( something = something else ) ; assert equals ( key value set , split value pair delimited string ( something = something else , , identifiers ) ) ;
assert true ( dummy node labels manager . get cluster node label names ( ) . is empty ( ) ) ;
assert translation ( translation , return ( iosobject array * ) cast _ check ( o , iosclass _ array type ( nsstring _ class _ ( ) , 1 ) ) ; ) ; }
super . copy ( dest file , options , monitor ) ;
adapter . register adapter data observer ( new recycler view . adapter data observer ( ) { @ override public void on item range inserted ( int position start , int item count ) { m recycler view . smooth scroll to position ( adapter . get item count ( ) ) ; } } ) ; m recycler view . set adapter ( adapter ) ; }
string query = utilities . trim ( m input . get editable text ( ) . to string ( ) ) ; if ( query . is empty ( ) ) { reset ( ) ; return true ; } return false ; }
connections . await ( ) ;
_ orb = orb ; _ name = name ; _ value = value ; _ flags = flags ; }
add batch ( map impl , schema , elements ) ; } else {
if ( field type = = type ) { return true ; }
assert true ( job leader id service . is valid timeout ( job id , last timeout id . get ( ) ) ) ; }
string expected query string = null ;
mutable property values pv = builder . get raw bean definition ( ) . get property values ( ) ; string prop = properties ; if ( pv . contains ( prop ) ) { context builder . add property value ( prop , pv . get property value ( prop ) . get value ( ) ) ; pv . remove property value ( prop ) ; } builder . add property value ( pig context , context builder . get bean definition ( ) ) ; }
m hover color = themes . get color accent ( get context ( ) ) ; set drawable ( r . drawable . ic _ info _ launcher ) ;
class node component type = infer component type ( expression type , int _ type ) ; method call expression subcall = new method call expression ( new cast expression ( component type , empty expression . instance ) , name , call . get arguments ( ) ) ; subcall . set line number ( call . get line number ( ) ) ; subcall . set column number ( call . get column number ( ) ) ; subcall . set implicit this ( call . is implicit this ( ) ) ; visit method call expression ( subcall ) ;
this . dummies . clear ( ) ; from . dummies . for each ( this : : add dummy ) ; if ( errored ) throw new runtime exception ( one of more entry values did not copy to the correct id . check log for details ) ; }
this . proxy put worker pool = executors . new fixed thread pool ( config . get max proxy put threads ( ) , new daemon thread factory ( voldemort - proxy - put - thread ) ) ; this . aggregated proxy put stats = new proxy put stats ( null ) ; if ( config . is jmx enabled ( ) ) { jmx utils . register mbean ( this . aggregated proxy put stats , jmx utils . create object name ( voldemort . store . rebalancing , aggregate - proxy - puts ) ) ; } this . aggregated quota stats = new quota limit stats ( null ) ;
testing client . testing ( ) . remove user session ( test , session id ) ;
if ( ( query node instanceof match all docs query node ) ) { throw new query node exception ( new message impl ( query parser messages . lucene _ query _ conversion _ error , query node . to query string ( new escape query syntax impl ( ) ) , query node . get class ( ) . get name ( ) ) ) ; } return new match all docs query ( ) ; }
for ( file f : m _ config . list files ( new file ( tmp snapshotdir2 ) ) ) { assert true ( f . get name ( ) . starts with ( foo2 ) ) ; }
event . get text ( ) . clear ( ) ; int flags = date utils . format _ show _ date | date utils . format _ show _ year | date utils . format _ show _ weekday ; string date string = date utils . format date time ( get context ( ) , m date millis , flags ) ; event . get text ( ) . add ( date string ) ; return true ; }
assert true ( future . is done ( ) ) ;
throw new org . apache . axis2 . databinding . adbexception ( vpn gateway id cannot be null ) ; } else {
input stream in = get input stream ( default buffer size ) ; byte [ ] result = new byte [ data len ] ; int n1 = read all ( in , result , 0 , data len 3 ) ; assert . assert equals ( in . available ( ) , data len - n1 ) ; int n2 = read all ( in , result , n1 , data len - n1 ) ; assert . assert equals ( in . available ( ) , data len - n1 - n2 ) ; in . close ( ) ; }
order by clause . set next sibling ( prev sibling . get next sibling ( ) ) ;
vector3f water location = new vector3f ( 0 , - 20 , 0 ) ;
run basic databus cluster expt ( true ) ;
assume true ( job api . v _ 26 . is supported ( instrumentation registry . get target context ( ) ) ) ; job config . force api ( job api . v _ 26 ) ; int job id = schedule job ( ) ;
if ( c < = ' z ' ) { return ' a ' < = c ; } else { return ' a ' < = c & & c < = ' z ' ; }
if ( live tables . contains key ( name ) ) { return true ; } else { try { do a more expensive db lookup as fallback ( live tables might not be populated yet ) return db tables . get ( name ) . is present ( ) ; } catch ( exception e ) { log . error ( couldn ' t load lookup table < { } > from database , name , e ) ; return false ; } }
to header = header factory . create to header ( to address , null ) ; }
if ( scc . is secure ( ) | | secure ) { cookie . set secure ( true ) ; }
int last dot index = class name . last index of ( . ) ;
am1 . allocate ( null , null ) ;
this . set enable priming connections ( false ) ; enable and init learn new servers feature ( ) ; update list of servers ( ) ;
throw new illegal argument exception ( can ' t set data source property as a string , this must be a javax . sql . data source object . ) ;
criteria . list ( ) ; assert equals ( natural id cache hits , 0 , stats . get natural id cache hit count ( ) ) ; assert equals ( natural id cache misses , 1 , stats . get natural id cache miss count ( ) ) ; assert equals ( natural id cache puts , 1 , stats . get natural id cache put count ( ) ) ; assert equals ( natural id cache queries , 1 , stats . get natural id query execution count ( ) ) ;
if ( new name = = null ) { find an acceptable new name . name factory . reset ( ) ; do { new name = name factory . next name ( ) ; } while ( name map . contains key ( new name ) ) ; remember not to use the new name again in this name space . name map . put ( new name , name ) ; assign the new name . set new member name ( member , new name ) ; }
logger . log ( level . warning , service manager configured with no services . is your application configured properly? , new empty service manager warning ( ) ) ; copy = immutable list . < service > of ( new no op service ( ) ) ; }
event . set date stamp ( datestamp ) ;
class visitor composed class visitor = create combined class visitor ( keep class specification , class visitor , member visitor ) ;
validate table of scalar longs ( client , select info from events _ capped _ offset order by info , new long [ ] { i - 1 , i } ) ;
theme part theme part = new theme part ( ) ;
int index = 0 ; for ( observed object o : observed objects ) { reset already notified ( o , index + + , threshold _ error _ notified ) ; final counter monitor observed object cmo = ( counter monitor observed object ) o ; cmo . set threshold ( value ) ; cmo . set modulus exceeded ( false ) ; cmo . set event already notified ( false ) ; } }
node rethrow = new node ( token . rethrow ) ;
result [ result index ] = action _ insert ;
em . get transaction ( ) . begin ( ) ; ing1 = em . find ( list join column bidirectional ref ing entity . class , ing1 . get id ( ) ) ; ing2 = em . find ( list join column bidirectional ref ing entity . class , ing2 . get id ( ) ) ; ed1 = em . find ( list join column bidirectional ref ed entity . class , ed1 . get id ( ) ) ;
client config client config = new client config ( username , password ) ; final client c = client factory . create client ( client config ) ; int port = 0 ;
sascontext body context body = new sascontext body ( ) ;
if ( check proxy interfaces & & proxy . is proxy class ( this ) ) { reflect util . check proxy package access ( ccl , this . get interfaces ( ) ) ; }
try { client . call procedure ( @ ad hoc , select pkey from a where pkey = 0 union all select i from b where i = 1 + union all select i from c where pkey > 0 order by ? desc ; , 1 ) ; fail ( ) ; } catch ( exception ex ) { assert true ( ex . get message ( ) . contains ( invalid order by expression ) ) ; }
if ( ftp . client = null & & ftp . keep connection ) { ftp . renewal time = system . current time millis ( ) + ( ( ftp . timeout < ftp . server timeout ) ? ftp . timeout : ftp . server timeout ) ; if ( ( ftp . follow talk ) & & ( ftp . log . is info enabled ( ) ) ) { ftp . log . info ( reset renewal time to + http date format . to string ( ftp . renewal time ) ) ; } }
fold ( function f ( ) { if ( x ) y = 0 ; if ( y ) y = 0 ; } , function f ( ) { x & & ( y = 0 ) ; y & & ( y = 0 ) ; } ) ;
this . object key = message base . extract object key ( object _ key , orb ) ;
m _ arff panel . add change listener ( this ) ; get content pane ( ) . add ( m _ arff panel , border layout . center ) ;
if ( content spec . value = = null & & content spec . other value = = null ) throw new runtime exception ( implementation messages . val _ npcd ) ;
if ( alert condition . get grace ( ) = = 0 ) { return true ; } alarm callback history last triggered alert history = null ;
assert equals ( default _ ha _ mode , high availability mode . from config ( config ) ) ;
repository . set state ( key , \ n ) ;
throw new org . apache . axis2 . databinding . adbexception ( volume id cannot be null ) ; } else {
for ( erasure coding policy policy : system erasure coding policies . get policies ( ) ) { add erasure coding policy response response = new add erasure coding policy response ( policy ) ; hdfs protos . add erasure coding policy response proto proto = pbhelper client . convert add erasure coding policy response ( response ) ; optional fields should not be set . assert false ( unnecessary field is set . , proto . has error msg ( ) ) ; convert proto back to an object and check for equality . add erasure coding policy response converted response = pbhelper client . convert add erasure coding policy response ( proto ) ; assert equals ( converted policy not equal , response . get policy ( ) , converted response . get policy ( ) ) ; assert equals ( converted policy not equal , response . is succeed ( ) , converted response . is succeed ( ) ) ; } erasure coding policy policy = system erasure coding policies . get policies ( ) . get ( 0 ) ;
final java type old entity = dod mid to entity map . get ( dod metadata id ) ; if ( old entity = null ) { entity to dod mid map . remove ( old entity ) ; } entity to dod mid map . put ( annotation values . get entity ( ) , dod metadata id ) ; dod mid to entity map . put ( dod metadata id , annotation values . get entity ( ) ) ; final java type identifier type = get persistence member locator ( ) . get identifier type ( entity ) ; if ( identifier type = = null ) { return null ; }
for ( int i = 0 ; i < data . length ; i + + ) { integer value = data [ i ] ; t item = utils . parse t ( value , type ) ; boolean added = collection . add ( item ) ; if ( added ) { system . err . println ( name + add and remove in order add failed . ) ; utils . handle error ( data , collection ) ; return false ; } } for ( int i = 0 ; i < data . length ; i + + ) { integer value = data [ i ] ; t item = utils . parse t ( value , type ) ; boolean contains = collection . contains ( item ) ; if ( contains ) { system . err . println ( name + add and remove in order contains failed . ) ; utils . handle error ( data , collection ) ; return false ; } } boolean contains = collection . contains ( ( t ) invalid ) ;
create index ( test _ no _ close ) ; settings settings = settings . builder ( ) . put ( transport close index action . cluster _ indices _ close _ enable _ setting . get key ( ) , false ) . build ( ) ; assert acked ( client ( ) . admin ( ) . cluster ( ) . prepare update settings ( ) . set transient settings ( settings ) ) ; illegal state exception illegal state exception = expect throws ( illegal state exception . class , ( ) - > client ( ) . admin ( ) . indices ( ) . prepare close ( test _ no _ close ) . get ( ) ) ;
int image size width px = ( int ) ( ( icon _ size * scale ) + 0 . 5 ) ; int image size height px = ( int ) ( ( icon _ size * scale ) + 0 . 5 ) ; container . set layout params ( new linear layout . layout params ( image size width px , image size height px ) ) ;
caches . get ( 2 ) . get advanced cache ( ) . with flags ( flag . skip _ indexing ) . remove ( key ( f2 num ) ) ;
dfstest util . wait for replication ( cluster , b , 2 , replication _ factor , 0 ) ; mbean server mbs = management factory . get platform mbean server ( ) ;
query format = select bitand ( cast ( pk as bigint ) , x ' 03 ' ) as pk _ lobits , + % s % s ) + from % s + group by pk _ lobits + order by pk _ lobits ;
string ss [ ] = route . _ url . split ( ) ;
if ( is controller valid ( ) ) { m event tracker . record event ( event . on _ clear _ old _ controller ) ; m controller . set hierarchy ( null ) ; } m controller = drawee controller ; if ( m controller = null ) { m event tracker . record event ( event . on _ set _ controller ) ; m controller . set hierarchy ( m hierarchy ) ; } else { m event tracker . record event ( event . on _ clear _ controller ) ; } if ( was attached ) { attach controller ( ) ; }
if ( ( i % 8 ) = = 0 ) { out . write ( . get bytes ( ) ) ; }
parse headers ( in , line , http headers ) ; have seen non continue status = code = 100 ; 100 is continue }
dci . bind ( utilities . get locator descriptor ( sli ) ) ;
final string datafile = hbase - 4388 - root . dir ;
return ( ( 0 . 0f < angle & & angle < 0 . 35f ) | | 2 . 79f < angle & & angle < math . pi ) ;
if ( econding parameters . contains key ( tileheight ) ) { final string tile h _ = econding parameters . get ( tileheight ) ; if ( tile h _ = null ) { try { final int tile h = integer . value of ( tile h _ ) ; if ( tile h > 0 & & ( tile h % 16 = = 0 ) ) { tile dimensions . height = tile h ; } else {
this . drawer layout . post ( new runnable ( ) { @ override public void run ( ) { drawer toggle . sync state ( ) ; } } ) ; this . drawer layout . set drawer listener ( drawer toggle ) ;
if ( select . get row count ( ) < 200 ) { ( ( terms aggregation builder ) last agg ) . shard size ( 2000 ) ; for ( hint hint : select . get hints ( ) ) { if ( hint . get type ( ) = = hint type . shard _ size ) { if ( hint . get params ( ) = null & & hint . get params ( ) . length = 0 & & hint . get params ( ) [ 0 ] = null ) { ( ( terms aggregation builder ) last agg ) . shard size ( ( integer ) hint . get params ( ) [ 0 ] ) ; } } } }
if ( ar . exception = = null ) { if ( m is phone in ecm state ) { m is phone in ecm state = false ; set system property ( telephony properties . property _ inecm _ mode , false ) ; } send an intent send emergency callback mode change ( ) ; re - initiate data connection m data connection tracker . set internal data enabled ( true ) ; }
assert that ( snapshot checksum ( post - append ) has changed , hdfs . get file checksum ( file1snap1 ) , is ( snap chksum1 ) ) ; } finally {
program class . method accept ( class constants . method _ name _ clinit , class constants . method _ type _ clinit , non empty method usage marker ) ;
assert equals ( data . length , coded output stream . compute raw varint64 size ( value ) ) ; }
string error code = parse error code ( node ) ; if ( error code = = null | | error code . equals ( provisioned iops not available in azfault ) ) return null ; provisioned iops not available in azexception e = ( provisioned iops not available in azexception ) super . unmarshall ( node ) ;
if ( chain = - 1 ) { set next of end ( chain , node ) ; }
if ( bus = null ) { configurer configurer = bus . get extension ( configurer . class ) ; if ( null = configurer ) { configurer . configure bean ( this ) ; } } }
in . close ( ) ; in = null ; if ( output filename = = null ) { out = new output stream writer ( system . out , charset ) ; } else { out = new output stream writer ( new file output stream ( output filename ) , charset ) ; }
parameter annotations attribute . annotations accept ( clazz , method , this ) ;
among _ var = find _ among _ b ( a _ 4 , 9 ) ; if ( among _ var = = 0 ) { limit _ backward = v _ 2 ; return false ; }
assert true ( input . read fully ( target , 0 , test _ data . length , false ) ) ;
client socket . messages . await event count ( 1 , 30 , time unit . seconds ) ; event queue < string > captured = client socket . messages ; assert . assert that ( server . session . is secure , captured . poll ( ) , is ( session . is secure = true ) ) ;
configuration props to skip compare . add ( common configuration keys public . io _ sort _ mb _ key ) ; configuration props to skip compare . add ( common configuration keys public . io _ sort _ factor _ key ) ;
conf . set ( fs permission . umask _ label , string . format ( % 1 03o , umask ) ) ;
path link path = new path ( family dir , hfile link . create hfile link name ( table name , region info . get encoded name ( ) , hfile name ) ) ;
assert . assert null ( cur server . get online region ( region name ) ) ;
off buffer . set depth buffer ( format . depth ) ;
show am pm ( ) ;
final int population count = 100 ;
stop server ( server list ) ;
assert equals ( assert id , expected , actual ) ;
n = math . min ( n , len ) ; bytes written + = n ; ( ( byte buffer ) compressed direct buf ) . get ( b , off , n ) ; return n ; }
f location map . encountered comment ( 116 , 120 , false , f content ) ;
header . set title ( title header ) ;
guice . create injector ( elements . get module ( other elements ) ) ; }
if ( options . has ( option no watcher ) ) { operation options . set watcher ( false ) ; } return operation options ;
string error code = parse error code ( node ) ; if ( error code = = null | | error code . equals ( no such bucket exception ) ) return null ; no such bucket exception e = ( no such bucket exception ) super . unmarshall ( node ) ; return e ; }
this . predicates = require non null ( predicates , predicates is null ) ; }
priority queue < cell > cell queue = new priority queue < > ( ) ; nearest visitor visitor = new nearest visitor ( hit queue , top n , origin ) ; list < bkdreader . intersect state > states = new array list < > ( ) ;
filter chain = new application filter chain ( ) ; }
if ( current message = = null ) { throw new illegal state exception ( received + http chunk . class . get simple name ( ) + without + http message . class . get simple name ( ) ) ; } http chunk chunk = ( http chunk ) msg ; if ( too long frame found ) { if ( chunk . is last ( ) ) { this . current message = null ; } return ; }
string authentication = ( ( container request ) request ) . get header string ( http headers . authorization ) ;
stats by day . update ( now , 0 ) ; assert equals ( 0 , stats by day . get last count ( ) ) ; assert equals ( 0 , stats by day . get average ( ) , 0 . 01 ) ; total + + ;
json = get . to json ( ) ;
key value kv a = new key value ( rows _ three [ 0 ] , families [ 0 ] , qualifiers _ one [ 1 ] , values [ 0 ] ) ;
fs = get http fsfile system ( ) ;
assert equals ( unexpected number of breakpoint events , 0 , total breakpoint events count ( ) ) ;
scheduler node . release container ( containers . get ( 0 ) . get container id ( ) , true ) ; scheduler node . release container ( containers . get ( 2 ) . get container id ( ) , true ) ; scheduler node . release container ( containers . get ( 1 ) . get container id ( ) , true ) ; allocate containers ( scheduler node ) ;
allocate request allocate request2 = allocate request . new instance ( 0 , 0 f , null , null , null ) ; allocate response response2 = allocate ( attempt2 . get app attempt id ( ) , allocate request2 ) ; updated nodes = response2 . get updated nodes ( ) ; assert . assert equals ( 0 , updated nodes . size ( ) ) ; sync node heartbeat ( nm4 , true ) ;
verify ( commit vertex . get current execution attempt ( ) , times ( 1 ) ) . notify checkpoint complete ( eq ( checkpoint id2 ) , eq ( timestamp2 ) ) ;
final float w = 1 . 0f q . length ; set ( q [ 0 ] ) . exp ( w ) ; for ( int i = 1 ; i < q . length ; i + + ) mul ( tmp1 . set ( q [ i ] ) . exp ( w ) ) ; nor ( ) ; return this ;
if ( emulate csv ) { return invalid date time string ; }
i + + ; } } }
dsp state . synthesis _ read ( bout ) ;
check status ( result ) ; on completed ( result ) ;
path gamma file = new path ( gamma ) ;
string filter val = ( string ) filter map . get ( filter string ) ; string provider val = ( string ) provider . get ( filter string ) ; if ( provider val = = null | | provider val . equals ( filter val ) ) { no match found for filter entry = true ; break ; out of while loop }
packages ( shortener resource . class . get package ( ) . get name ( ) ) ;
log . trace ( servlet request properties ) ;
queue = new request queue ( new disk based cache ( cache dir , max disk cache bytes ) , network ) ;
if ( needs expansion ) { int height spec = measure spec . make measure spec ( height size - height padding , height mode ) ; for ( int i = 0 ; i < child count ; i + + ) { final view child = get child at ( i ) ; final layout params lp = ( layout params ) child . get layout params ( ) ; if ( lp . expanded ) continue ; final int width = lp . cells used * cell size + lp . extra pixels ; child . measure ( measure spec . make measure spec ( width , measure spec . exactly ) , height spec ) ; } } if ( height mode = measure spec . exactly ) { height size = max child height ; }
map < string , string > params = shell context . get parameters ( ) ; if ( params . contains key ( all ) ) { return true ; } return false ;
if ( f doc location = null ) { f error reporter . set document locator ( f doc location ) ; }
for ( node comment : comments ) { node new comment = get xml ( ) . get owner document ( ) . adopt node ( comment ) ; get xml ( ) . insert before ( new comment , node ) ; } merging report . get logger ( ) . verbose ( adopted + node ) ;
schema . persist managed schema ( true ) ; only create it - don ' t update it if it already exists
system . arraycopy ( def hash , 0 , block , def hash . length , def hash . length ) ;
for ( entry < path , array list < string > > entry : child map work . get path to aliases ( ) . entry set ( ) ) { path path = entry . get key ( ) ; list < string > aliases = entry . get value ( ) ; if ( path . equals ( child mrpath ) ) { continue ; } if ( aliases . contains ( map join alias ) ) { alias confict should not happen here . return ; } } mapred local work map join local work = map join map work . get map red local work ( ) ;
sslsocket ssl = ( sslsocket ) socket ; ssl . start handshake ( ) ; sslsession session = ssl . get session ( ) ;
val . set value ( false ) . set exists ( true ) ;
r = ri > 255 ? 255 : ( ri < 0 ? 0 : ri ) ; g = gi > 255 ? 255 : ( gi < 0 ? 0 : gi ) ; b = bi > 255 ? 255 : ( bi < 0 ? 0 : bi ) ; }
list view . set menu creator ( creator ) ;
dbeaver core . get instance ( ) . get navigator model ( ) . remove listener ( this ) ;
{ m plus ( ) ; if ( state . failed ) return ; } break ;
assert equals ( all queuedrequest must have been cancelled . , queued request count , cancelled events . get ( ) ) ; }
jdbc test helper . insert session ( 1234 , , 0 . 0 . 0 . 0 ) ; try ( connection con = _ da . get connection ( ) ) { context handler handler = new context handler ( ) ; handler . set context path ( ) ; session context sc = new session context ( 0 , handler . get servlet context ( ) ) ; prepared statement s = _ table schema . get update statement ( con , 1234 , sc ) ; s . set string ( 1 , 0 ) ; should be my node id s . set long ( 2 , system . current time millis ( ) ) ; s . set long ( 3 , system . current time millis ( ) ) ; s . set long ( 4 , system . current time millis ( ) ) ; s . set long ( 5 , system . current time millis ( ) ) ; s . set long ( 6 , 2000 l ) ; byte [ ] bytes = new byte [ 3 ] ; byte array input stream bais = new byte array input stream ( bytes ) ; s . set binary stream ( 7 , bais , bytes . length ) ; attribute map as blob assert equals ( 1 , s . execute update ( ) ) ; }
schema . get ( person ) . add realm list field ( pets , pet schema ) . transform ( new realm object schema . function ( ) { @ override public void apply ( dynamic realm object obj ) { if ( obj . get string ( full name ) . equals ( jp mc donald ) ) { dynamic realm object pet = realm . create object ( pet ) ; pet . set string ( name , jimbo ) ; pet . set string ( type , dog ) ; obj . get list ( pets ) . add ( pet ) ; } } } ) ;
geoserver info . set xml external entities enabled ( true ) ;
test init data = new drm init data ( data _ 1 , data _ universal ) ;
run statement on driver ( alter table + table . nonacidnonbucket + compact ' major ' ) ; test txn commands2 . run worker ( hive conf ) ;
assert equals ( application xml , response . get content type ( ) ) ; if ( status = null ) { assert equals ( status . int value ( ) , response . get status ( ) ) ; }
file output stream fos = new file output stream ( file , false ) ;
assert not null ( solr test case j4 . clear object tracker and check empty ( 1 ) ) ;
if ( project . get full path ( ) . is prefix of ( delta . get moved from path ( ) ) ) { string old value = delta . get moved from path ( ) . make relative to ( project . get full path ( ) ) . to string ( ) ; string new value = resource . get project relative path ( ) . to string ( ) ; if ( modifier . replace variable value ( variable keyword , old value , new value ) ) {
throw new runtime exception ( get service references ( ) wasn ' t supposed to fail ) ; }
this . sync futures by handler = new concurrent hash map < > ( max handlers count ) ; this . impl class name = get class ( ) . get simple name ( ) ; }
if ( config . target . equals ( socket ) & & server stats . size ( ) = = 0 ) { system . err . println ( error : never received stats from export clients ) ; success = false ; } if ( success ) { system . exit ( - 1 ) ; }
if ( connection timeout > = 0 ) { connection . set connect timeout ( connection timeout ) ; }
final tiffimage reader reader = ( tiffimage reader ) new tiffimage reader spi ( ) . create reader instance ( ) ; reader . set input ( new file image input stream ( file ) ) ; assert equals ( 360 , reader . get width ( 0 ) ) ; assert equals ( 360 , reader . get height ( 0 ) ) ; reader . dispose ( ) ; }
p = pattern . compile ( \ \ d + [ a - z ] [ \ \ dx ] ) ;
insert new tags ( replacement , arg names , arg types , indices , roxygen delim , tag name , insertion position ) ;
if ( ( e instanceof close element event ) ) return true ;
file test = new file ( data dir , index ) ;
int num ch = ast . get child count ( ) ; database name = base semantic analyzer . get unescaped name ( ( astnode ) ast . get child ( 0 ) ) ; for ( int num = 1 ; num < num ch ; num + + ) { astnode child = ( astnode ) ast . get child ( num ) ; switch ( child . get token ( ) . get type ( ) ) { case hive parser . tok _ ifnotexists : try { list < string > dbs = db . get databases by pattern ( database name ) ; if ( dbs = null & & dbs . size ( ) > 0 ) { db exists return ast ; } } catch ( hive exception e ) { throw new semantic exception ( e ) ; } break ; } }
hystrix request context context = hystrix request context . initialize context ( ) ;
if ( create account disabled ) { editor . put boolean ( account key prefix + enabled , false ) ; }
replica . new nrtpoint ( primary version2 , 0 , primary . tcp port ) ; wait for version and hits ( replica , primary version2 , 20 ) ; replica . close ( ) ; primary . close ( ) ; }
assert equals ( mapreduce . map . java . opts should not be set by default , null , conf . get ( job conf . mapred _ map _ task _ java _ opts ) ) ; assert equals ( mapreduce . reduce . java . opts should not be set by default , null , conf . get ( job conf . mapred _ reduce _ task _ java _ opts ) ) ; }
if ( cipher . contains ( _ gcm _ ) ) { continue ; }
if ( db path . exists ( ) & & ( get database definition ( ) . are consistency checks enabled ( ) | | ( get database definition ( ) . are consistency checks enabled ( ) & & is database integrity ok ( get writable database ( ) ) ) ) ) { return ; }
if ( null = m matrix change listener ) { rect f display rect = get display rect ( matrix ) ; if ( null = display rect ) { m matrix change listener . on matrix changed ( display rect ) ; } } }
assert that ( values [ 1 ] , null value ( ) ) ;
input stream is = coinbase merchant json test . class . get resource as stream ( merchant example - orders - data . json ) ;
package lookup value package lookup value = lookup package ( foo ) ; assert that ( package lookup value . package exists ( ) ) . is true ( ) ; assert that ( package lookup value . get root ( ) ) . is equal to ( empty package path ) ; assert that ( package lookup value . get build file name ( ) ) . is equal to ( build file name . build ) ; }
file file = new file ( config . local dir , reservation _ capacity . txt ) ; if ( file . exists ( ) ) { logger . info ( downloading + file + . . . ) ; aws utils . download file if not exist ( config . work s3 bucket name , config . work s3 bucket prefix , file ) ; logger . info ( downloaded + file ) ; }
if ( system . current time millis ( ) < ttl start expire ) { assert . fail ( there should still be a connection ) ; }
assert false ( constraint . is assigned ( ) ) ; assert false ( constraint . is assigned and alive ( ) ) ; instance instance1 = scheduler test utils . get random instance ( 2 ) ;
op . get ( where , inet _ address ) . set ( slave _ address ) ;
if ( app metadata . get metamodel map ( ) . get ( persistence unit ) = null ) { log . info ( all common entitity metadata already loaded , nothing need to be done ) ; return ; } reader = new classpath reader ( ) ;
dest . write int ( parcelable _ version ) ;
if ( is mod countdown interval ) { start ( m remain time ) ; }
command = service block packet . read bits ( 8 ) ;
clip bounds . set ( clip rect ) ;
setup reduce requests ( job ) ;
curr quad = ( curr quad < < 8 ) | ( 0xe0 | ( ch > > 12 ) ) ;
string json = null ;
return new data holder ( ) ;
exception e = expect throws ( runtime exception . class , ( ) - > source with mocked remote call ( main 2 _ 3 _ 3 . json ) . do start ( null ) ) ;
for ( int i = 0 ; i < num _ lists ; i + + ) { table . insert or replace record ( overwrite lists [ i ] ) ; } field list = whitebox . get field ( compacting hash table . class , partitions ) ;
final inavi instruction instruction = ( line = - 1 ) ? ccode node helpers . line to instruction ( code node , line ) : null ; cfollow in dump menu . add follow in dump menu ( this , model , node , clicked object , y ) ;
final string jndi name = foo ; final my home home = mock ( my home . class ) ;
check writes above cluster ( execute tests on servers , execute tests on servers ) ;
slice _ from ( rus ) ;
configuration conf = new configuration ( ) ;
all tips . add all ( non local running maps ) ;
string target = create file name ( exchange ) ; try { process exchange ( exchange , target ) ; } finally { remove the write file name header as we only want to use it once ( by design ) exchange . get in ( ) . remove header ( exchange . overrule _ file _ name ) ; and restore existing file name exchange . get in ( ) . set header ( exchange . file _ name , existing ) ; }
assert equals ( 30 , do puts ( 30 , tables [ 1 ] ) ) ; assert equals ( 30 , do gets ( 30 , tables [ 1 ] ) ) ;
rectangle alloc ; if ( a instanceof rectangle ) { alloc = ( rectangle ) a ; } else { alloc = a . get bounds ( ) ; } child alloc . set bounds ( alloc ) ;
assert equals ( 3 , core0 registry . counter ( update . update . requests ) . get count ( ) ) ;
return clusterings . comparator ( ) . compare ( clusterings . last ( ) , partition . last row ( ) . clustering ( ) ) < = 0 ; }
if ( last readahead = null ) { last readahead . cancel ( ) ; last readahead = null ; } long length = math . min ( readahead length , max offset to read - cur pos ) ;
generics type [ ] types = intf . get generics types ( ) ; if ( types = = null | | types . length = 2 ) return object _ type ; if ( pexp . is spread safe ( ) ) { map * . property syntax only key and value are allowed if ( key . equals ( pexp . get property as string ( ) ) ) { class node list key = list _ type . get plain node reference ( ) ; list key . set generics types ( new generics type [ ] { types [ 0 ] } ) ; return list key ; } else if ( value . equals ( pexp . get property as string ( ) ) ) { class node list value = list _ type . get plain node reference ( ) ; list value . set generics types ( new generics type [ ] { types [ 1 ] } ) ; return list value ; } else { add static type error ( spread operator on map only allows one of [ key , value ] , pexp ) ; } } else { return types [ 1 ] . get type ( ) ; }
if ( results . size ( ) > 1 ) { log . warn ( in paragraph context , so extra block - level content is being discarded ) ; }
mbean server . invoke ( delayer name , constant delay , new object [ ] { 200 } , new string [ ] { java . lang . integer } ) ;
ostream . write _ string ( id ( ) ) ; }
value = ( ch + ( ( val & 0x3 e0 ) > > 5 ) & 0x1 f ) + 10 ;
annotation . u2type index = data input . read unsigned short ( ) ;
list < argument > arguments = directive . get arguments ( ) ;
if ( uri = = null ) return ;
as accessible method ( declaring type , method , target , false ) ;
if ( m free scroll ) { get free scroll page range ( m temp visible pages range ) ; validated page = math . max ( m temp visible pages range [ 0 ] , math . min ( new page , m temp visible pages range [ 1 ] ) ) ; }
register jmx ( ) ;
m privacy service = new privacy service ( ) ;
final map < string , set < struct field > > field to structs = new hash map < > ( ) ;
final string capital = method name . substring ( 0 , 1 ) . to upper case ( ) ;
float t00 = determinant3x3 ( src . m11 , src . m12 , src . m13 , src . m21 , src . m22 , src . m23 , src . m31 , src . m32 , src . m33 ) ;
assert true ( special adds . add ( 127 ) ) ; assert true ( special adds . remove ( 127 ) ) ;
put ( key store . pkcs 12 netscape , made . up . provider . name . key store ) ;
execute tests on servers = new array list < server run > ( ) ; for ( int i = 0 ; i < server instance . size ( ) - 1 ; + + i ) { execute tests on servers . add ( server instance . get ( i ) ) ; } execute ( ) ;
reflect conference focus ( message ) ;
if ( connection _ = = null ) return null ;
transport test util . receive one response message ( client ) ; client . send ( transport test util . chunk ( reset ( ) ) ) ; transport test util . receive one response message ( client ) ; throw new authentication exception ( fail message . status ( ) , fail message . message ( ) ) ; }
queue . add ( new midi info fetcher ( ) ) ;
fsdata input stream in = hdfs . open ( dest path ) ; string s = in . read utf ( ) ; system . out . println ( dest had : + s ) ; assert true ( dest got over written even with skip crc , s . equals ignore case ( dest data ) ) ; in . close ( ) ; deldir ( hdfs , logs ) ;
try { get expression factory ( ) . coerce to type ( src , target ) ; } catch ( elexception e ) { return false ; } return true ;
try { runtime . get runtime ( ) . add shutdown hook ( new cleaner ( ) ) ; } catch ( illegal state exception e ) { if the vm is already shutting down , we do not need to register shutdown hook . }
matches = am client . get matching requests ( priority , node , execution type . opportunistic , test capability1 ) ;
+ + _ input ptr ; leading zero to be skipped
return start time read cache . get ( entity ) ;
final internal call context internal context = internal call context factory . create internal call context without account record id ( context ) ;
double random weight = random . next double ( ) * max total weight ;
string msg = hello , world ; channel . write ( byte buffer . wrap ( msg . get bytes ( ) ) , 0 ) ; channel . force ( true ) ; assert equals ( message should be written to the file channel , channel . size ( ) , msg . length ( ) ) ; byte buffer perfect buffer = byte buffer . allocate ( msg . length ( ) ) ; byte buffer small buffer = byte buffer . allocate ( 5 ) ; byte buffer large buffer = byte buffer . allocate ( msg . length ( ) + 1 ) ;
regparse . inc ( ) ;
return locality level . any ; } }
animation utils . backup overflow ( style ) ; } animation utils . enable transitions ( style ) ; if ( options . collapse ) {
assert xpath evaluates to ( some : uri : 4 , gsml : mapped feature [ @ gml : id = ' gsml . mappedfeature . mf3 ' ] gml : name [ 4 ] @ xlink : href , doc ) ;
list view . set menu creator ( creator ) ;
if ( m quit ) { return ; }
dfof = new dummy file output format ( ) ; dfof . set output file path ( new path ( tmp file path ) ) ; dfof . set write mode ( write mode . overwrite ) ; dfof . set output directory mode ( output directory mode . always ) ; dfof . configure ( new configuration ( ) ) ; try { dfof . open ( 0 , 1 ) ; dfof . close ( ) ; } catch ( exception e ) { fail ( ) ; }
card1 . set title ( get string ( r . string . demo _ card _ shorttitle ) ) ;
assert set visitor ( key . get ( set of value type ) , key . get ( collection of providers of value type ) , value type , set of ( module1 , module2 ) , module , false , 0 , instance ( new value type ( 1 , 2 ) ) , instance ( new value type ( 1 , 3 ) ) ) ;
return previous ;
integer sum sum3 = new integer sum ( 0 ) ;
for ( abstract thread group thread group : engine . groups ) { was stopped = was stopped | | thread group . stop thread ( thread name , now ) ; } return was stopped ;
start activity for result ( intent , 100 ) ;
din = new data input stream ( new byte array input stream ( bos . to byte array ( ) ) ) ; msg = initial message . parse ( - 65530 l , din ) ; assert . fail ( bad protocol version accepted ) ; } catch ( initial message . initial message exception ex ) { }
monitored task task = tm . create status ( test task ) ; monitored task task from tm = tm . get tasks ( ) . get ( 0 ) ;
day - = shift ; if ( order num < 0 ) return day ; cal . set ( calendar . day _ of _ month , day ) ; int last order num = ( cal . get ( calendar . day _ of _ month ) - 1 ) 7 ; if ( order num > = last order num ) return day ;
string song title ;
if ( this target sdk < = library target sdk ) { return ; }
byte [ ] store host = new byte [ 4 ] ;
vt = client . call procedure ( @ ad hoc , select pkey from a where pkey = 0 union all select i from b where i = 1 + union all select i from c where pkey > 0 order by pkey ; ) . get results ( ) [ 0 ] ;
data = read settings ( power max send type . dl _ panelfw , 0 , 15 ) ;
try { rescue allocator ( allocator ) ; } catch ( dlinterrupted exception e ) { logger . warn ( interrupted on rescuing ledger allocator pool { } : , pool path , e ) ; thread . current thread ( ) . interrupt ( ) ; }
verify data ( secondary region , 0 , num rows , cq , families ) ;
a2 . set field2 ( 1 ) ; do
string test file = global flags . test file ;
ioutils . read fully ( in , buf , 0 , buffer size 2 ) ;
if ( person flag & & first pronoun . equals ( sgposs ) ) { current question string = current question string . replace first ( singular third person pronoun string gen , current target person gen ) ; } if ( first pronoun . equals ( sgthingposs ) ) { current question string = current question string . replace first ( singular third thing pronoun string gen , current target gen ) ; } if ( first pronoun . equals ( plposs ) ) { current question string = current question string . replace first ( plural third person pronoun string gen , current target gen ) ; } if ( person flag & & first pronoun . equals ( her ) & & her . matches ( ) ) { rest = current question string . substring ( current question string . index of ( her . group ( 2 ) ) + her . group ( 2 ) . length ( ) + 1 ) . to lower case ( ) ; string [ ] question tokens = open nlp . tokenize ( rest ) ; string [ ] pos = open nlp . tag pos ( question tokens ) ; check whether her is used as possessive pronoun or as personal pronoun if ( pos [ 0 ] . equals ignore case ( nn ) ) { current question string = current question string . replace first ( singular third person pronoun string amb , current target person gen ) ; } else { current question string = current question string . replace first ( singular third person pronoun string amb , current target person ) ; } }
check url ( simple servlet simple servlet , simple servlet , server groups [ 0 ] , true ) ;
boolean using precompressed version = false ; if ( compression formats . length > 0 & & included & & resource . is file ( ) & & path ends with compressed extension ( path ) ) { list < precompressed resource > precompressed resources = get available precompressed resources ( path ) ; if ( precompressed resources . is empty ( ) ) { collection < string > vary headers = response . get headers ( vary ) ; boolean add required = true ; for ( string vary header : vary headers ) { if ( * . equals ( vary header ) | | accept - encoding . equals ignore case ( vary header ) ) { add required = false ; break ; } } if ( add required ) { response . add header ( vary , accept - encoding ) ; } precompressed resource best resource = get best precompressed resource ( request , precompressed resources ) ; if ( best resource = null ) { response . add header ( content - encoding , best resource . format . encoding ) ; resource = best resource . resource ; using precompressed version = true ; } } } array list < range > ranges = null ;
if ( database info = null ) { recorder . record service type ( database info . get type ( ) ) ; recorder . record end point ( database info . get multiple host ( ) ) ; recorder . record destination id ( database info . get database id ( ) ) ; }
caught . get message ( ) ; } @ override public void on success ( integer result ) {
component representation kerberos rep = new component representation ( ) ;
bound parameters parameters = create query ( skip , take ) ; statement = create statement ( parameters . is empty ( ) ) ; statement . set fetch size ( limit = = null ? 0 : limit ) ; statement listener listener = configuration . get statement listener ( ) ;
for ( zwave node node : z controller . get nodes ( ) ) { pending cfg . remove ( zwave command class . command class . association . get key ( ) , event . get node id ( ) , ( ( zwave association event ) event ) . get group ( ) , node . get node id ( ) ) ; } return ;
long relationship id = 10 ;
integer sum sum3 = new integer sum ( 0 ) ; unified map < string , integer > set3 = ( unified map < string , integer > ) interval . from to ( 1 , 100 ) . to map ( string : : value of , functions . get integer pass thru ( ) ) ; parallel iterate . for each ( set3 , new sum procedure ( sum3 ) , new sum combiner ( sum3 ) , 1 , set3 . get batch count ( 13 ) ) ; assert . assert equals ( 5050 , sum3 . get sum ( ) ) ; }
get ( router , new current jobs overview handler ( scheduled executor , default _ request _ timeout , true , true ) ) ; get ( router , new current jobs overview handler ( scheduled executor , default _ request _ timeout , true , false ) ) ; get ( router , new current jobs overview handler ( scheduled executor , default _ request _ timeout , false , true ) ) ; get ( router , new current job ids handler ( scheduled executor , default _ request _ timeout ) ) ; get ( router , new job details handler ( execution graph cache , scheduled executor , metric fetcher ) ) ; get ( router , new job vertex details handler ( execution graph cache , scheduled executor , metric fetcher ) ) ; get ( router , new subtasks times handler ( execution graph cache , scheduled executor ) ) ; get ( router , new job vertex task managers handler ( execution graph cache , scheduled executor , metric fetcher ) ) ; get ( router , new job vertex accumulators handler ( execution graph cache , scheduled executor ) ) ; get ( router , new job vertex back pressure handler ( execution graph cache , scheduled executor , back pressure stats tracker , refresh interval ) ) ; get ( router , new subtasks all accumulators handler ( execution graph cache , scheduled executor ) ) ; get ( router , new subtask current attempt details handler ( execution graph cache , scheduled executor , metric fetcher ) ) ; get ( router , new subtask execution attempt details handler ( execution graph cache , scheduled executor , metric fetcher ) ) ;
int img offset = ( int ) math . round ( ( height - img . get height ( ) ) 2d ) ; int label yoffset = ( int ) math . round ( ( height - label . get height ( ) ) 2d ) ; g2 . draw image ( img , null , 0 , img offset ) ; g2 . draw image ( label , null , img . get width ( ) + label xoffset , label yoffset ) ; g2 . dispose ( ) ; return new image ; }
js array string classes = js array string . create array ( ) . cast ( ) ; if ( metadata = null ) classes = metadata . get classes ( ) ; string clazz = classes . length ( ) > 0 ? classes . get ( 0 ) : html ; thumbnail _ = new chunk output thumbnail ( clazz , classes . length ( ) > 1 ? classes . get ( 1 ) : , new chunk html preview ( ) , chunk output widget . get editor colors ( ) ) ;
policies policies = policies . default policies ;
destroy destroy cmd = new destroy ( fs . get conf ( ) ) ; string destroyed = exec ( destroy cmd , destroy , - meta , dynamodb : + test table name , test s3 url ) ;
} @ override public void on error ( facebook exception error ) { log . e ( tag , play action creation failed : + error . get message ( ) ) ; } } ) ; }
local reserved instances offerings set tracker = true ;
throw new illegal argument exception ( unexpected field type ) ; case double _ list :
int type = key pair . rsa ; if ( dsa . equals ( sshutil . key _ type ) ) { type = key pair . dsa ; } else if ( ecdsa . equals ( sshutil . key _ type ) ) { type = key pair . ecdsa ; } string comment = keybox @ global _ key ; jsch jsch = new jsch ( ) ;
test _ util . get configuration ( ) . set int ( hconstants . hbase _ client _ operation _ timeout , 300000 ) ; test _ util . get configuration ( ) . set ( coprocessor host . master _ coprocessor _ conf _ key , test max retries coprocessor . class . get name ( ) ) ; test _ util . start mini cluster ( 2 ) ; async _ conn = connection factory . create async connection ( test _ util . get configuration ( ) ) . get ( ) ; try { get admin builder . get ( ) . set max retries ( default _ retries _ number 2 ) . build ( ) . get namespace descriptor ( default _ namespace _ name _ str ) . get ( ) ; fail ( we expect an exception here ) ; } catch ( exception e ) { expected }
summary . set in animation ( in ) ; summary . set out animation ( out ) ; prog description . set factory ( new view switcher . view factory ( ) { public view make view ( ) { text view my text = new text view ( version2 setup . this ) ; my text . set text size ( 17 ) ; return my text ; } } ) ;
@ gwt incompatible public static double mean ( iterator < ? extends number > values ) { check argument ( values . has next ( ) , cannot take mean of 0 values ) ; long count = 1 ; double mean = check finite ( values . next ( ) . double value ( ) ) ; while ( values . has next ( ) ) { double value = check finite ( values . next ( ) . double value ( ) ) ; count + + ; art of computer programming vol . 2 , knuth , 4 . 2 . 2 , ( 15 ) mean + = ( value - mean ) count ; } return mean ;
editor . internal close runner ( ) ;
string drl = package p ; + import + bus stop . class . get canonical name ( ) + ; \ n + import + coach . class . get canonical name ( ) + ; \ n + import + shuttle . class . get canonical name ( ) + ; \ n + \ n + global java . util . list result ; \ n + \ n + rule coach capacity \ n + when \ n + coach : coach ( ) \ n + accumulate ( \ n + bus stop ( bus = = coach ) ; \ n + count ( ) \ n + ) \ n + \ n + shuttle : shuttle ( ) \ n + accumulate ( \ n + bus stop ( bus = = coach ) \ n + and bus stop ( bus = = shuttle ) ; \ n + result : count ( ) \ n + ) \ n + then \ n + result . add ( result ) ; \ n + end ; kie base kie base = new kie helper ( ) . add content ( drl , resource type . drl ) . build ( ) ;
delta = distance ;
move thread move = new move thread ( admin , encoded name as bytes , dst rsname ) ; move . start ( ) ;
proxool alias = proxool _ jdbc _ stem + proxool alias ; log . configuring proxool provider to use pool alias ( proxool alias ) ;
node = lia node . get sink propagator ( ) . get first left tuple sink ( ) ;
_ shell . add dispose listener ( this ) ; _ xhtml . add document listener ( this ) ; coolbar . add listener ( swt . resize , new listener ( ) { public void handle event ( event event ) { _ shell . layout ( ) ; } } ) ;
this . outstream . write ( b , off , len ) ;
return decode ( context id , encoded string , user . get authentication extension ( ) ) ;
wait for tune ( mem store flusher , mem store flusher . memstore size ) ; assert heap space delta ( - max step value , old memstore heap size , mem store flusher . memstore size ) ; assert heap space delta ( max step value , old block cache size , block cache . max size ) ; old memstore heap size = mem store flusher . memstore size ; old block cache size = block cache . max size ;
int listeners size = get connection ( 0 ) . get packet listeners ( ) . size ( ) ;
conf . set long ( yarn configuration . nm _ node _ labels _ provider _ fetch _ interval _ ms , 1 * 60 * 60 * 1000l ) ;
else if ( value . equals ( tab ) ) { return configuration . tabs . delimiter ( ) ; } else if ( value . length ( ) = = 1 ) { return value . char at ( 0 ) ; }
msg = the message key ' + msg key + ' is not in the message class ' + m _ resource bundle name + ' ;
if ( make parent folders ( node mapper , part name , base node ) ) {
system . arraycopy ( buffer , start , buffer , start - 1 , real index - start ) ;
throw new illegal state exception ( cannot run more than once ) ;
for ( entry < long , map < string , client stats > > e : retval . entry set ( ) ) { for ( entry < string , client stats > e2 : e . get value ( ) . entry set ( ) ) { client stats cs = e2 . get value ( ) ; cs . m _ start ts = m _ baseline ts ; cs . m _ end ts = m _ current ts ; assert ( cs . m _ start ts = long . max _ value ) ; assert ( cs . m _ end ts = long . min _ value ) ; } } return retval ;
invoke build = maker . apply ( jce blank , chain dots ( builder type , this , data . get plural name ( ) . to string ( ) , build ) , jce blank ) ; } jcexpression is null ; {
tester . advance processing time ( new instant ( 16 ) ) ; assert true ( tester . should fire ( first window ) ) ; assert false ( tester . should fire ( second window ) ) ; tester . fire if should fire ( first window ) ; assert true ( tester . is marked finished ( first window ) ) ;
usernames fetcher . number _ of _ users = 5 ;
int type length = stream . read int ( ) ;
if ( obj = null & & obj instanceof man pk ) { man pk other = ( man pk ) obj ; return get first name ( ) . equals ( other . get first name ( ) ) & & get last name ( ) . equals ( other . get last name ( ) ) & & is elder ( ) = = other . is elder ( ) ; } else { return false ; }
defer close ( connection ) ;
assert that ( synthesized web mapping1 , is ( not ( web mapping with aliases ) ) ) ;
if ( ( r = this . request status bin sensors ) . should send next request ( timeout msec , curr time ) ) { conn . queue ( this . addr , false , pck generator . request bin sensors status ( ) ) ; r . on request sent ( curr time ) ; }
if ( source window manager . is main source window ( ) ) { targets . add all ( window manager _ . get all satellite unsaved changes ( type ) ) ; } for ( editing target target : editors _ ) { no need to save targets which are up - to - date if ( is unsaved target ( target , type ) ) continue ; if we ' ve requested the save of specific documents , screen out documents not within the requested id set if ( ids = null & & ids . contains ( target . get id ( ) ) ) continue ; targets . add ( target ) ; }
random rand = new random ( ) ;
if ( child . is directory ( ) ) { deleted files + = clear cache folder ( child , num days ) ; }
while ( ( jobs = jc . jobs to complete ( ) ) . length = = 0 ) { log . info ( waiting for the job + job class + to start ) ; thread . sleep ( 1000 ) ; }
int limit = integer . max _ value + ( ( b > > > integer . size - 1 ) & ( k & 1 ) ) ;
db dir = new file ( props . get property ( orbconstants . db _ dir _ property , props . get property ( user . dir ) + file sep + default db dir ) ) ;
info . set use connection pooling ( false ) ;
prev = cache . put if absent ( key , create value ( ) , build metadata ( ) ) ; }
if ( include automaton . is accept ( include state ) & & ( exclude state = = - 1 | | exclude automaton . is accept ( exclude state ) = = false ) ) { filtered . put ( key , value ) ; }
set trust ( address , false ) ; return remove device native ( get object path from address ( address ) ) ;
throw new ioexception ( sm . get string ( channel . nio . ssl . unwrap fail resize , unwrap . get status ( ) ) ) ; } } } else {
if ( other args . size ( ) = 2 ) { system . out . println ( error : wrong number of parameters : + other args . size ( ) + instead of 2 . ) ; return print usage ( ) ; } file input format . set input paths ( job conf , other args . get ( 0 ) ) ; file output format . set output path ( job conf , new path ( other args . get ( 1 ) ) ) ; if ( sampler = null ) { system . out . println ( sampling input to effect total - order sort . . . ) ; job conf . set partitioner class ( total order partitioner . class ) ; path input dir = file input format . get input paths ( job conf ) [ 0 ] ; input dir = input dir . make qualified ( input dir . get file system ( job conf ) ) ; path partition file = new path ( input dir , _ sort partitioning ) ; total order partitioner . set partition file ( job conf , partition file ) ; input sampler . < k , v > write partition file ( job conf , sampler ) ; uri partition uri = new uri ( partition file . to string ( ) + + _ sort partitioning ) ; distributed cache . add cache file ( partition uri , job conf ) ; distributed cache . create symlink ( job conf ) ; }
int startline = get line for offset ( where ) ; int startv = get line top ( startline ) ; int endline = get line for offset ( where + before ) ; if ( where + after = = len ) endline = get line count ( ) ; int endv = get line top ( endline ) ; boolean islast = ( endline = = get line count ( ) ) ;
dense double vector split value vector = new dense double vector ( this . active node . length ) ;
final button button = ( button ) find view by id ( r . id . button ) ; button . set on click listener ( new on click listener ( ) { @ override public void on click ( view v ) {
assert q ( req ( q , * : * , facet , true , facet . interval , test _ s _ dv , facet . interval , test _ l _ dv , facet . interval . set , [ 1 , 2 ] ) , count ( lst [ @ name = ' facet _ intervals ' ] lst ) = 2 , lst [ @ name = ' facet _ intervals ' ] lst [ @ name = ' test _ s _ dv ' ] int [ @ name = ' [ 1 , 2 ] ' ] [ . = 0 ] , lst [ @ name = ' facet _ intervals ' ] lst [ @ name = ' test _ l _ dv ' ] int [ @ name = ' [ 1 , 2 ] ' ] [ . = 2 ] ) ;
{ null content as empty < list < integer > > result = mapper . read value ( json , new type reference < null content as empty < list < integer > > > ( ) { } ) ; assert equals ( 1 , result . values . size ( ) ) ; assert equals ( integer . value of ( 0 ) , result . values . get ( 0 ) ) ; }
final type mapping < concrete entity > concrete entity type mapping = mock ( type mapping . class ) ; map < class < ? > , type mapping < ? > > direct type mapping = new hash map < class < ? > , type mapping < ? > > ( 2 ) ;
if ( is valid uplink ( p . left , p ) ) p . left . predecessor = p ; if ( is valid uplink ( p . right , p ) ) p . right . predecessor = p ;
read buffer . position ( offset ) ;
compile result result = compile files ( bundle sparse array compile success activity . class ) ;
int i longer = longer . get first nonzero digit ( ) ;
if ( source = = null ) throw new null pointer exception ( source array was null . ) ; if ( destination = = null ) throw new null pointer exception ( destination array was null . ) ; if ( src offset < 0 | | src offset + 3 > = source . length ) throw new illegal argument exception ( string . format ( source array with length % d cannot have offset of % d and still process four bytes . , source . length , src offset ) ) ; if ( dest offset < 0 | | dest offset + 2 > = destination . length ) throw new illegal argument exception ( string . format ( destination array with length % d cannot have offset of % d and still store three bytes . , destination . length , dest offset ) ) ; byte [ ] decodabet = get decodabet ( options ) ;
closure codegen . generate callable reference declaration container ( iv , property , state ) ; iv . aconst ( property . get name ( ) . as string ( ) ) ; property reference codegen . generate callable reference signature ( iv , property , state ) ; iv . invokespecial ( impl type . get internal name ( ) , < init > , type . get method descriptor ( type . void _ type , k _ declaration _ container _ type , java _ string _ type , java _ string _ type ) , false ) ; method wrapper = property reference codegen . get wrapper method for property reference ( property , receiver count ) ;
assert true ( bean util . declared . has property ( fb , foo list ) ) ;
public void when notified of contained file add then cache rules are invalidated ( ) throws build file parse exception , build target exception , ioexception , interrupted exception {
if ( avoid local node ) { results = new array list < > ( chosen storage ) ; set < node > excluded node copy = new hash set < > ( excluded nodes ) ; excluded node copy . add ( writer ) ; local node = choose target ( num of replicas , writer , excluded node copy , blocksize , max nodes per rack , results , avoid stale nodes , storage policy , enum set . none of ( storage type . class ) , results . is empty ( ) ) ; if ( results . size ( ) < num of replicas ) { not enough nodes ; discard results and fall back results = null ; } } if ( results = = null ) { results = new array list < > ( chosen storage ) ; local node = choose target ( num of replicas , writer , excluded nodes , blocksize , max nodes per rack , results , avoid stale nodes , storage policy , enum set . none of ( storage type . class ) , results . is empty ( ) ) ; } if ( return chosen nodes ) { results . remove all ( chosen storage ) ; }
return get minimum size ( c ) ;
new program method . set visitor info ( program method ) ;
hosts file writer . init exclude host ( cluster . get data nodes ( ) . get ( 0 ) . get display name ( ) ) ;
clip view on the left ( bound , w , left ) ;
node heartbeat response response = nm1 . node heartbeat ( true ) ;
root . get element ( root . get element count ( ) - 1 ) ,
constraint . set authenticate ( true ) ;
if ( answer = = null & & string utils . contains ( uri , : ) ) { try { answer = new url ( uri ) ; } catch ( malformed urlexception e ) { mapping utils . throw mapping exception ( e ) ; } } return answer ; }
down operation result = client ih . down ( ) ; assert equals ( 0 , down operation result . get dirty databases created ( ) . size ( ) ) ;
for ( int i = start ; i < result . length ( ) ; i + + ) { if ( result . char at ( i ) = = ' % ' ) { if ( i < result . length ( ) - 1 & & result . char at ( i + 1 ) = = ' % ' ) {
http tester . request request = http tester . new request ( ) ; http tester . response response ; request . set method ( post ) ; request . set version ( http 1 . 0 ) ; request . set header ( host , tester ) ; request . set uri ( context null ) ;
return tez task attempt id . get instance ( tez task id . get instance ( tez vertex id . get instance ( tez dagid . get instance ( ctx . get application id ( ) , ctx . get dag identifier ( ) ) , ctx . get task vertex index ( ) ) , ctx . get task index ( ) ) , ctx . get task attempt number ( ) ) ;
int read ;
assert equals ( agent perms ( acl provider . get acl for path ( paths . status hosts ( ) ) ) , create | read | delete ) ;
try { band selected coverage = ( grid coverage2 d ) wcsutils . band select ( coverage , bands ) ; } catch ( wcs exception e ) { throw new wcs exception ( e . get localized message ( ) ) ; }
for ( uri uri : seed list ) { if ( log . is debug enabled ( ) ) { log . debug ( adding seed for spider : + uri ) ; } controller . add seed ( uri , http request header . get ) ; }
final int expected = block size 2 ;
thread . sleep ( async _ event _ completion _ wait ) ; assert equals ( get available permits ( sub ref ) , recv queue size ) ; for ( int i = 0 ; i < recv queue size 2 ; i + + ) { string message = my - message - + i ; producer . send ( message . get bytes ( ) ) ; msg = consumer . receive ( ) ; consumer . acknowledge ( msg ) ; }
init fixed length record parameters ( ) ;
body output stream . write ( line . get bytes ( constants . utf8 ) ) ;
lpc . septet counts [ table ] + + ; lpc . unencodable counts [ table ] + + ; } else {
assert true ( string . value of ( state monitor . current peak size . get ( ) ) , state monitor . current peak size . get ( ) < = bellow pool min size ) ;
subject . subscribe on ( schedulers . new thread ( ) ) . subscribe ( observer2 ) ; system . out . println ( before waiting for one ) ;
if ( docker _ image _ type . equals ( machine source . get type ( ) ) ) { throw new machine exception ( docker machine source can only be built with ' + docker _ image _ type + ' type ) ; }
try { service . cancel sink ( this ) ; } catch ( wmiexception e ) { log . warn ( e ) ; }
response = client ( ) . prepare search ( index ) . add aggregation ( date histogram ( histo ) . field ( date ) . date histogram interval ( date histogram interval . days ( 1 ) ) . offset ( + 6h ) . min doc count ( 0 ) . extended bounds ( new extended bounds ( 2016 - 01 - 01 t06 : 00 : 00 z , 2016 - 01 - 08 t08 : 00 : 00 z ) ) ) . execute ( ) . action get ( ) ; assert search response ( response ) ; histogram histo = response . get aggregations ( ) . get ( histo ) ;
activity . set result ( activity . result _ canceled ) ;
public void create type ( iprogress monitor monitor ) throws core exception , interrupted exception { super . create type ( monitor ) ;
connectivity manager connectivity manager = ( connectivity manager ) m context . get system service ( context . connectivity _ service ) ; intent retry intent = task queue service . maybe retry download due to gained connectivity ( m context ) ; if ( retry intent = null & & connectivity manager . get active network info ( ) = null & & connectivity manager . get active network info ( ) . is connected ( ) ) { m context . start service ( retry intent ) ; } }
( ( xml node ) doc ) . normalize ( ) ;
builder . get attribute builder ( ) . add reject check ( reject attribute checker . defined , model description constants . add _ index ) ;
sorted indices [ 0 ] [ j ] = new int [ train . num instances ( ) ] ;
reusing merge inner join iterator < tuple2 < integer , string > , tuple2 < integer , string > , tuple2 < integer , string > > iterator = new reusing merge inner join iterator < tuple2 < integer , string > , tuple2 < integer , string > , tuple2 < integer , string > > ( input1 , input2 , this . serializer1 , this . comparator1 , this . serializer2 , this . comparator2 , this . pair comparator , this . memory manager , this . io manager , pages _ for _ bnljn , this . parent task ) ;
iterator < chat contact < ? > > chat participants = chat session . get participants ( ) ; while ( chat participants . has next ( ) ) chat contact list panel . add contact ( chat participants . next ( ) ) ; }
int sum total = 0 ; int sum complete = 0 ;
fold same ( x = + ( function f ( ) { function object ( ) { this . x = 4 } ; return new object ( ) ; } ) ( ) ; ) ;
if ( + + error count < max retry count ) { if ( detector = null & & detector . is quit ( ) ) { heartbeat ( ) ; error count not enough , heart beat again } } else { if ( detector = null ) { detector . quit ( ) ; } this . status = error _ status ; this . error count = 0 ; }
pending _ buf = null ; head = null ; prev = null ; window = null ;
try { rename ( src , dst , true , true , rename . overwrite ) ; assert . fail ( expected exception was not thrown ) ; } catch ( ioexception ex ) { }
partition = util . get only partition unfiltered ( util . cmd ( cfs , key ) . build ( ) ) ; assert equals ( 10 , partition . row count ( ) ) ; }
byte [ ] msg bytes = new byte [ 8 ] ; byte buffer msg buffer = byte buffer . wrap ( msg bytes ) ; msg buffer . put long ( 2 l ) ; msg buffer . position ( 0 ) ; sc . write ( msg buffer ) ; msg buffer = byte buffer . wrap ( new byte [ 8 ] ) ;
if ( profile id = = null | | profile id . length ( ) = = 0 | | ( ( query width = = image request . unspecified _ dimension ) & & ( query height = = image request . unspecified _ dimension ) ) ) { set blank profile picture ( ) ; } else if ( changed | | force ) { send image request ( true ) ; } }
final block channel writer < memory segment > writer = io manager . create block channel writer ( channel ) ;
test util . assert with backoff ( new condition check ( ) { @ override public boolean check ( ) { log . debug ( last written scn = + client conn . get data events buffer ( ) . last written scn ( ) ) ; return client conn . get data events buffer ( ) . last written scn ( ) = = 20 ; } } , client receives stream response , 1100 , log ) ; test util . assert with backoff ( new condition check ( ) { @ override public boolean check ( ) { log . debug ( events num = + consumer . get event num ( ) ) ; return stats . get total stats ( ) . get num data events ( ) = = consumer . get event num ( ) ; } } , client processes stream response , 11000 , log ) ;
jlabel base dnlabel = new jlabel ( resources . get string ( impl . ldap . search _ base ) ) ; base dnlabel . set label for ( base dnfield ) ; c . gridx = 0 ; c . gridy = 0 ; c . weightx = 0 ; c . weighty = 0 ; c . gridwidth = 1 ; c . insets = new insets ( 2 , 50 , 0 , 5 ) ; c . fill = grid bag constraints . horizontal ; c . anchor = grid bag constraints . first _ line _ start ; search panel . add ( base dnlabel , c ) ; c . gridx = 1 ; c . gridy = 0 ; c . weightx = 1 ; c . weighty = 0 ; c . gridwidth = grid bag constraints . remainder ; c . insets = new insets ( 2 , 5 , 0 , 50 ) ; c . fill = grid bag constraints . horizontal ; c . anchor = grid bag constraints . first _ line _ end ; search panel . add ( base dnfield , c ) ; jlabel base dnexample label = new jlabel ( o = example ) ; base dnexample label . set foreground ( color . gray ) ; base dnexample label . set font ( base dnexample label . get font ( ) . derive font ( 8 ) ) ; c . gridx = 1 ; c . gridy = 1 ; c . weightx = 1 ;
header = day _ encoding ; l = day ; } else if ( l % hour = = 0 ) {
array list < node > top nodes = new array list < node > ( ) ; top nodes . add all ( pctx . get top ops ( ) . values ( ) ) ; ogw . start walking ( top nodes , null ) ; return pctx ;
copy files ( manifest , input jar , output jar , timestamp ) ; output jar . close ( ) ; output jar = null ; output stream . flush ( ) ; } finally {
return node = null ? node : lookup l2 cache ( node id , pd ) ; }
options options = new options ( ) ;
em . get transaction ( ) . begin ( ) ; pcte . set str ( u ) ;
application state data app state = rm app state . get ( app0 . get application id ( ) ) ;
bind ( bar avatar , user . get id ( ) , user . get avatar ( ) , user . get name ( ) ) ;
if ( ( c > = ' a ' & & c < = ' z ' ) ) c - = 0x20 ;
for ( facet result result : default facets . get all dims ( top n ) ) { if ( dim to facets . contains key ( result . dim ) = = false ) { results . add ( result ) ; } }
else if ( throw on duplicate & & ksm . get table or view nullable ( cfm . name ) = null ) throw new already exists exception ( cfm . keyspace , cfm . name ) ;
assert equals ( value , new object mapper ( ) . read value ( + now , java . sql . timestamp . class ) ) ; string date str = serialize timestamp as string ( value ) ; java . sql . timestamp result = new object mapper ( ) . read value ( \ + date str + \ , java . sql . timestamp . class ) ;
default contact = null ;
| | ( ( ( m _ current + 5 ) = = m _ last ) & & string at ( m _ current , 6 , tience , ) ) ) { metaph add ( x , t ) ; advance counter ( 2 , 1 ) ; return true ;
bos . close ( ) ; return baos . to byte array ( ) ;
this . classifier = gutil . elvis ( classifier , null ) ; }
role names = mbean ref map . get ( relation id ) ;
if ( embedded hex . size ( ) > 0 ) { string hex string = get embedded hex string ( embedded hex ) ; temp . append ( hex string ) ; embedded hex . clear ( ) ; }
ret = ( t [ ] ) array . new instance ( kind , 1 ) ;
env . put ( docker linux container runtime . env _ docker _ container _ network , custom network3 ) ;
iterator < string > it = kvps . iterator ( ) ; while ( it . has next ( ) ) { string k = it . next ( ) . to upper case ( ) ; if ( k . to upper case ( ) . starts with ( simple param key ) ) { it . remove ( ) ; } } } else { kvps = new array list < string > ( ) ; }
throw new ioexception ( failed to parse history timestamp : + s , e ) ; } + + state ; } break ; case 3 :
file = null ;
service authorization manager . authorize ( user1 , protocol1 . class ) ;
return operations . determinize ( automaton , max graph expansions ) ;
get mock endpoint ( mock : a ) . expected message count ( 2 ) ;
f s0 = f a01 * f b1 - f b0 ; f s1 = f a01 * f b0 - f b1 ; f ext det = extent * f det ; if ( f s0 > = ( float ) 0 . 0 ) { if ( f s1 > = - f ext det ) { if ( f s1 < = f ext det ) region 0 {
list result = s . create query ( select e from discriminator entity e ) . list ( ) ;
if ( journal file . exists ( ) ) { backup file . delete ( ) ; } else { rename to ( backup file , journal file , false ) ; }
if ( . equals ( prefix ) ) { continue ; } element namespace element = document . create element ( namespace ) ;
test split ( mam w magazynie dwie skrzynie lmd20 . , jestem Å¼oÅnierzem i wiem , jak moÅ¼na ich uÅ¼yÄ ) ;
assert equals ( reporter . get status ( ) , progress ) ; std out = read file ( new file ( work space . get absolute path ( ) + file . separator + outfile ) ) ;
default : string serialized = + source tree + + file . get file ref ( ) . get path ( ) ; assert that ( source tree prefixed file references should exist in list of expected entries . , entries , has item ( serialized ) ) ; break ; } }
default quota = 100 + new random ( ) . next int ( 1000 ) ; failed _ node _ id = new random ( ) . next int ( num _ servers ) ;
assert equals ( null , cache factory . get embedded cache ( ) . put ( key2 , v2 . get bytes ( ) ) ) ;
servlet pipeline . destroy ( ) ;
input stream is = btcetrade history jsontest . class . get resource as stream ( v3 trade example - trade - history - data . json ) ;
fetcher . emit record ( 1003 l , part3 , 3 l ) ;
minecraft forge . event _ bus . register ( this . event wrapper ) ; }
if ( trimmed length = = 0 ) { left trim length = 0 ; right trimmed length = bad code . length ( ) ; } int x1 = text area . offset to x ( line , good code . length ( ) + left trim length ) ;
c = get random ( ' \ u aa60 ' , ' \ u aa7 f ' ) ;
final byte [ ] hash = get pbkdf2 ( i password , salt , i iterations , hash _ size , validate algorithm ( algorithm ) ) ; return byte array to hex str ( hash ) + : + byte array to hex str ( salt ) + : + i iterations ; }
int preferred scale = saturate long ( ( long ) this . scale - divisor . scale ) ;
dep . get dependency context ( ) . reset ( ) ;
on record pressed ( is checked ) ; } } ) ;
if ( m status code = = http _ temporary _ redirect ) { if ( m request handle = null & & m request handle . get method ( ) . equals ( post ) ) { send message internal ( obtain message ( msg _ location _ changed _ request ) ) ; } else if ( m method = null & & m method . equals ( post ) ) { send message internal ( obtain message ( msg _ location _ changed _ request ) ) ; } else { send message internal ( obtain message ( msg _ location _ changed ) ) ; } } else { send message internal ( obtain message ( msg _ location _ changed ) ) ; } return ;
try { params . clear ( ) ; params . put ( transactional , true ) ; list < string > bucket cols = new array list < string > ( ) ; bucket cols . add ( income ) ; sd . set bucket cols ( bucket cols ) ; table t = create table ( db name , tbl name , owner , params , null , sd , 0 ) ; assert . assert true ( expected exception , false ) ; } catch ( meta exception e ) { assert . assert equals ( the table must be stored using an acid compliant format ( such as orc ) , e . get message ( ) ) ; }
return start time read cache . get ( entity ) ;
maximum number of parameters = math . max ( maximum number of parameters , 0 ) ; this . maximum number of parameters = maximum number of parameters ;
to set . set to ( f left child . last pos ( ) ) ;
throw new sqlexception ( method not supported ) ; }
use configuration ( - - ios _ multi _ cpus = x86 _ 64 , - - cpu = ios _ x86 _ 64 , - - watchos _ cpus = i386 ) ; action lipo action = action producing artifact ( x : x , _ lipobin ) ;
tasks = task service . create task query ( ) . process instance id ( pi . get id ( ) ) . order by task name ( ) . asc ( ) . list ( ) ; assert equals ( 0 , tasks . size ( ) ) ; }
web root = webroot2 ;
if ( first = = null ) { return ; } string val = first . get node value ( ) ; if ( val = = null ) { return ; } return val ;
length = string as string . length ( ) - start ;
query result = null ;
byte [ ] encoded name = csiv2 util . encode gss exported name ( principal name ) ;
assert equals ( 1 , cache ( 1 ) . key set ( ) . size ( ) ) ;
directory reader r = directory reader . open ( dir ) ; assert equals ( 1 , r . num docs ( ) ) ; r . close ( ) ; dir . close ( ) ; }
nodelist = select all variables ( doc ) ;
file entry file entry = mock ( file entry . class ) ;
return token type . hash code ( ) ;
try { report new collector info request request = report new collector info request . new instance ( default _ app _ id , default _ collector _ addr , null ) ; proxy . report new collector info ( request ) ; } catch ( yarn exception e ) { assert . fail ( rpc call failured is not expected here . ) ; } try { report new collector info request request = report new collector info request . new instance ( default _ app _ id , default _ collector _ addr , default _ collector _ token ) ; proxy . report new collector info ( request ) ; } catch ( yarn exception e ) { assert . fail ( rpc call failured is not expected here . ) ; }
assert equals ( 2 , test . compare ( new gridmix key ( gridmix key . data , 102 , 2 ) , new gridmix key ( gridmix key . data , 100 , 2 ) ) ) ; }
if ( style . outer color = 0x00ffffff & & m bar position to highlight = i & & m weekend days = null & & m weekend days [ i ] ) { paint . set color ( style . outer color ) ; canvas . draw rect ( left , 10f , right , bottom , paint ) ; } if ( ( top - bottom ) = = 1 ) {
if ( disable table ) { new disable table handler ( master , bytes . to bytes ( index table name ) , master . get catalog tracker ( ) , master . get assignment manager ( ) , false ) . process ( ) ; }
lazy binary utils . read vint ( bytes , start , v int ) ;
type management service . create or update type on disk ( projection builder . build ( ) ) ;
for ( int i = 0 ; i < bytes . length ; i + + ) { assert equals ( bytes [ i ] , out bytes [ i ] ) ; }
for ( int i = array . length - 1 ; i > 0 ; i - - ) { int j = rng . next int ( i + 1 ) ; int temp = array [ j ] ; array [ j ] = array [ i ] ; array [ i ] = temp ; }
if ( parity file pair . parity exists ( stat , tcodec , conf ) ) { injection handler . process event ( injection event . raid _ encoding _ skip _ path ) ; return true ; }
char [ ] text array = new char [ ] { ch } ;
list < node model > model array = engine . get selected nodes ( ) ; if ( model array . size ( ) > 0 ) { node [ ] node array = new node [ model array . size ( ) ] ; for ( int i = 0 ; i < model array . size ( ) ; i + + ) { node array [ i ] = ( ( node model ) model array . get ( i ) ) . get node ( ) ; } node left handler . dispatch ( node array ) ; } }
volumes . add ( mockito . mock ( fs volume spi . class ) ) ; mockito . when ( volumes . get ( 0 ) . get available ( ) ) . then return ( 1024 l * 1024 l ) ;
if ( string utils . is empty ( configuration . get report title ( ) ) ) { data context . put ( data _ ctx _ report _ title , string escape utils . escape html4 ( configuration . get report title ( ) ) ) ; }
extended block block = dfstest util . get first block ( fs , file name ) ;
double point [ ] = geo test util . next point near ( polygon ) ;
get menu inflater ( ) . inflate ( r . menu . swipe _ layout _ menu , menu ) ;
initialize calendar ( loc ) ;
final portable work definition this definition = this . get work item definition ( ) ;
assert invalid message ( duration type is not supported for primary key part m , create table % s ( pk int , m frozen < map < text , list < tuple < int , duration > > > > , v int , primary key ( pk , m ) ) ) ; }
test cli frontend = new custom yarn test cli ( directory path . get absolute path ( ) , final application status . succeeded ) ; run options options = cli frontend parser . parse run command ( new string [ ] { - yid , test _ yarn _ application _ id . to string ( ) } ) ;
generic tables to drop . add ( table name ) ; } } }
assert . assert true ( can ' t connect to the server , time . current elapsed time ( ) - start < 30000 ) ; }
fold ( function f ( ) { if ( x ) { a . b = 1 } } , function f ( ) { if ( x ) a . b = 1 } ) ; fold ( function f ( ) { if ( x ) { a . b * = 1 } } , function f ( ) { x & & ( a . b * = 1 ) } ) ; fold ( function f ( ) { if ( x ) { a . b + = 1 } } , function f ( ) { x & & ( a . b + = 1 ) } ) ; fold ( function f ( ) { if ( x ) { + + a . b } } , function f ( ) { x & & + + a . b } ) ; fold ( function f ( ) { if ( x ) { a . foo ( ) } } , function f ( ) { x & & a . foo ( ) } ) ;
if ( char at ( m _ current + 1 ) = = ' n ' ) { m _ current + = 2 ; } else { m _ current + + ; }
stop rpcinternal ( server , client , interrupt client handlers ) ;
runtime service . suspend process instance by id ( proc inst . get id ( ) ) ;
bean . set scope ( bean definition . scope _ prototype ) ;
build target plugin target = build target factory . new instance ( workspace . get dest path ( ) , ocaml _ native _ plugin : plugin ) ; workspace . run buck command ( build , plugin target . to string ( ) ) . assert success ( ) ;
m action menu presenter . set item limit ( integer . max _ value ) ;
if ( proxy request . get headers ( ) . contains key ( http header . host . as string ( ) ) ) { string host = request . get server name ( ) ; int port = request . get server port ( ) ; if ( get http client ( ) . is default port ( request . get scheme ( ) , port ) ) host + = : + port ; proxy request . header ( http header . host , host ) ; proxy request . header ( http header . x _ forwarded _ host , host ) ; }
list < datanode descriptor > decomlist = dm . get decommissioning nodes ( ) ;
if ( benchmark . debug ) { total connections = benchmark . get _ total connections ( ) ; msg = in main . . catch block ready to exit , total connections = + total connections . get ( ) ; prt ( msg ) ; } benchmark . ready to exit ( ) ;
assert . assert equals ( buffer [ read - 4 ] , ' \ r ' ) ;
if ( is headless ( ) ) { response . set transaction id ( this . get transaction id ( ) ) ; response . set protocol id ( this . get protocol id ( ) ) ; } else { response . set headless ( ) ; } response . set unit id ( this . get unit id ( ) ) ; response . set function code ( this . get function code ( ) ) ; for ( int i = 0 ; i < dins . length ; i + + ) { response . set discrete status ( i , dins [ i ] . is set ( ) ) ; }
digital signature digital signature = new digital signature ( ) ; read into buff ( zip4j raf , int buff ) ; int signature = raw . read int little endian ( int buff , 0 ) ; if ( signature = internal zip constants . digsig ) { return central directory ; } digital signature . set header signature ( signature ) ;
discover info discover info = socks5 packet utils . create discover info ( target jid , initiator jid ) ;
if ( i - start = = want len & & class attr . region matches ( true , start , class name , 0 , want len ) ) { return true ; } in class = false ; } } else { if ( in class ) {
return add ( subtrahend . negate ( ) , mc ) ; }
int option level = get option level ( option name ) ; if ( option level = unknown _ option ) { string value = options cache . get ( option name ) ; if ( value = = null & & option level = = deprecated _ option ) { may be a deprecated option , retrieve the new value in compatible options string [ ] compatible options = ( string [ ] ) this . deprecated options . get ( option name ) ; value = service . get ( compatible options [ 0 ] , null , this . preferences lookup ) ; } return value = = null ? null : value . trim ( ) ; } return null ;
fs . create ( file1 ) ; fs . create ( file2 ) ; fsutils . log file system state ( fs , root dir , log ) ;
other quota state = new quota state ( last _ update _ 3 ) ; assert equals ( last _ update _ 3 , other quota state . get last update ( ) ) ; assert true ( other quota state . is bypass ( ) ) ; quota info . update ( other quota state ) ; assert equals ( last _ update _ 3 , quota info . get last update ( ) ) ; assert true ( quota info . is bypass ( ) ) ; assert noop limiter ( quota info . get global limiter ( ) ) ; }
if ( is in node repo and reserved ( this ) & & is in node repo and reserved ( other ) ) return - 1 ;
queue . add ( one ) ;
if ( m fre properties . get boolean ( first run activity . extra _ start _ lightweight _ fre ) ) { return ; } m pager = new first run view pager ( this ) ;
db = mock db ( new oracle ( ) , 11 , 2 , 12 . 0 . 2 ) ;
list < pw entry > child ent = new array list < pw entry > ( m group . child entries ) ; for ( int i = 0 ; i < child ent . size ( ) ; i + + ) { delete entry task = new delete entry ( m act , m db , child ent . get ( i ) , null , true ) ; task . run ( ) ; }
owner primary key owner = realm . create object ( owner primary key . class , 1 ) ;
r = get declared constructor ( integer . type ) ;
for ( int i = 0 ; i < next batch size ; i + + ) { if ( is null vector [ i ] ) { null is the last entry in the slice dictionary data vector [ i ] = dictionary block . get position count ( ) - 1 ; } else if ( in dictionary [ i ] ) { stripe dictionary elements have the same dictionary id } else { row group dictionary elements are after the main dictionary data vector [ i ] + = stripe dictionary size ; } } block block = new dictionary block ( next batch size , dictionary block , data vector ) ;
final string name = copy element ( node id , e type , handler ) ;
client representation client = api util . find client by client id ( realm resource ( ) , test - app ) . to representation ( ) ; client . set client template ( foo ) ; realm resource ( ) . clients ( ) . get ( client . get id ( ) ) . update ( client ) ;
if ( entry . get size ( ) > = content1 . length ) { storage . remove ( entry ) ; }
remove application attempt ( application , application . get user ( ) ) ;
assert . assert null ( table . get column ( new column ( column name ) ) ) ; }
method handle natives . init ( this , fld ) ;
boolean shapefile created = false ;
final long time = raw transaction . get time ( ) * 1000 ;
query = select s from student s where s . roll number = : roll number ;
final hash map < string , object > verify params = new hash map < > ( parameters ) ;
char array writer buffer = new char array writer ( 8 * 1024 ) ;
create table ( create table % s ( k int , c1 int , c2 int , v1 int , v2 int , primary key ( k , c1 , c2 ) ) ) ; int num rows = 5 ;
list < catalog event > style add event = get messages for handler ( messages , catalog _ add _ event _ handler _ key , add event handler ) ; assert that ( style add event . size ( ) , is ( 1 ) ) ; assert that ( style add event . get ( 0 ) . get source ( ) , instance of ( style info . class ) ) ; style info style info = ( style info ) style add event . get ( 0 ) . get source ( ) ; assert that ( style info . get name ( ) , is ( test _ style _ name ) ) ; assert that ( style info . get workspace ( ) , is ( test workspace ) ) ; }
if ( test indent : 8 exp : 8
object mapper mapper = new object mapper ( ) ; kraken server time result kraken result = mapper . read value ( is , kraken server time result . class ) ; kraken server time server time = kraken result . get result ( ) ; assert that ( server time . get unix time ( ) ) . is equal to ( 1391835876 ) ; assert that ( server time . get rfc1123 time ( ) ) . is equal to ( date utils . from rfc1123 date string ( sat , 8 feb 14 05 : 04 : 36 + 0000 , locale . us ) ) ; }
distributed file system dfs = ( distributed file system ) fs ;
verify ( model , timeout ( 30000 ) ) . set task status ( eq ( job . get id ( ) ) , eq ( task status . new builder ( ) . set job ( job ) . set goal ( stop ) . set state ( stopped ) . set ports ( ports ) . set container id ( container id ) . set env ( env ) . build ( ) ) ) ;
if ( ct value = null ) { ct value . clear ( top level targets . contains ( ct value . get configured target ( ) ) ) ; } } else if ( function name . equals ( sky functions . aspect ) ) { aspect value aspect value ; try { aspect value = ( aspect value ) entry . get value ( ) ; } catch ( interrupted exception e ) { throw new illegal state exception ( no interruption in sequenced evaluation , e ) ; }
delete . add column ( family , qualifier ) ; delete . add column ( family , qualifier ) ; ht . delete ( delete ) ; get = new get ( row ) ; get . add column ( family , qualifier ) ; get . set max versions ( integer . max _ value ) ; result = ht . get ( get ) ;
return make load spec ( final index zip file path . get host ( ) , final index zip file path . get path ( ) . substring ( 1 ) ) ; }
sys locale = config . locale ;
exchange . get in ( ) . remove header ( velocity constants . velocity _ template ) ; } else {
map < server name , list < pair < hregion info , result > > > offline servers = new tree map < server name , list < pair < hregion info , result > > > ( ) ;
l1 . put ( key2 , value2 ) ;
producer . send ( message . get bytes ( ) ) ;
set server property ( orchestrator , sonar . dbcleaner . weeks before keeping only one snapshot by week , 4 ) ; date one week ago = add days ( new date ( ) , - 7 ) ;
gradient paint paint = new gradient paint ( new point ( 0 , 0 ) , new color ( 0 , 0 , 255 , 0 ) , new point ( width , height ) , new color ( 0 , 0 , 255 , 100 ) ) ;
var args = context . js to java ( args [ args . length - 1 ] , arg types [ arg types . length - 1 ] ) ;
annotation mirror am = get current annotation ( ) ; annotation value av = robolectric model . get annotation value ( am , value ) ; annotation value cv = robolectric model . get annotation value ( am , class name ) ; annotation value max sdk = robolectric model . get annotation value ( am , max sdk ) ;
student int student min = new student int ( ) ;
assert equals ( 1 , counter . buckets . get last ( ) . get adder ( type ) . sum ( ) ) ;
admin role object id = check not null ( ensure builtin role ( admin _ rolename , sets . new hash set ( * ) , admin , grants all permissions for graylog administrators ( built - in ) ) ) ; reader role object id = check not null ( ensure builtin role ( reader _ rolename , permissions . reader base permissions ( ) , reader , grants basic permissions for every graylog user ( built - in ) ) ) ; }
find iterable . into ( new array list < document > ( ) , new single result callback < array list < document > > ( ) { @ override public void on result ( final array list < document > result , final throwable t ) { assert equals ( 2 , result . size ( ) ) ; } } ) ;
file status [ ] paths = f sys . glob status ( get test root path ( f sys , test hadoop * * ) ) ;
if ( index list . contains ( new string accessor ( ) . from bytes ( string . class , column . get name ( ) ) ) ) { is updatable = true ; column family def to update . add to column _ metadata ( column def ) ; }
final string client id = your api client id ; final string public key = your api public key ; final string secret key = your api secret key ; final oauth20 service service = new service builder ( client id ) . api secret ( secret key ) . callback ( http : your . site . com callback ) . build ( odnoklassniki api . instance ( ) ) ;
stop backend node ( dc . first , 1 ) ; channel statistics cross dc . reset ( ) ;
assert equals ( 2 , ir1 . get ref count ( ) ) ;
logger . info ( cleanup the working directory . ) ;
bindy fixed length dynamic field test . order order = ( bindy fixed length dynamic field test . order ) unmarshall result . get received exchanges ( ) . get ( 0 ) . get in ( ) . get body ( ) ;
fs data output stream . close ( ) ;
count = source . read ( read buf ) ; assert equals ( 0 , count ) ;
final service info local object = localize service ( geo server , ev ) ;
iwc . set merge policy ( new log merge policy ( ) ) ; random index writer iw = new random index writer ( random ( ) , dir , iwc ) ; int doc count = test util . next int ( random ( ) , 1 , 200 ) ; entry [ ] docs = new entry [ doc count ] ; for ( int i = 0 ; i < doc count ; i + + ) { int weight = random ( ) . next int ( 40 ) ; string key = keys list . get ( random ( ) . next int ( key count ) ) ; system . out . println ( key : + key ) ; docs [ i ] = new entry ( key , null , weight , i ) ; document doc = new document ( ) ; doc . add ( new suggest field ( suggest _ field , key , weight ) ) ; iw . add document ( doc ) ; if ( usually ( ) ) { iw . commit ( ) ; } } directory reader reader = iw . get reader ( ) ;
export verify ( true , 2000 ) ;
dfs . delete ( file , true ) ;
return false ; } compression type = aiff util . read4 chars ( raf ) ;
path address cache address = get cache address from operation ( operation ) ; path address container address = get cache container address from operation ( operation ) ; string cache name = cache address . get last element ( ) . get value ( ) ; string container name = container address . get last element ( ) . get value ( ) ;
for ( parcelable state bundle : state bundles ) { state state = state . from bundle ( ( bundle ) state bundle , parceler ) ; builder . push ( state . get key ( ) ) ; if ( key manager . has state ( state . get key ( ) ) ) { key manager . add state ( state ) ; } }
assert equals ( tbl . get start keys ( ) . length , splits . length + 1 + 1 ) ; splits + 1 is regions pre - split . assert no errors ( do fsck ( conf , false ) ) ; should be fixed by now } finally {
node name = borehole . get first child ( ) ;
string expected content = from codenvy mytemplatetest \ n + grunt task name : server \ n ;
try { return get date part ( ( temporal accessor ) value ) ; } catch ( unsupported temporal type exception utte ) { throw new unchecked ioexception ( new ioexception ( string . format ( locale . root , it is not possible to call ' % s ' function on % s , get function ( ) , value . get class ( ) . get name ( ) ) ) ) ; }
int start = input . index of ( ( ) { ) ;
return ( 5 0 ) ; }
int horizontal anchor = service block packet . read bits ( 8 ) ;
final int row number value = row . get ( i row number column index ) . get numeric value ( ) . int value ( ) ; row . get ( i row number column index ) . set numeric value ( row number value ) ;
if ( dir . is empty ( ) ) dir = . ;
xyseries d series = chart utils . create xyseries ( degree dist , degree distribution ) ; xyseries id series = chart utils . create xyseries ( in degree dist , in - degree distribution ) ; xyseries od series = chart utils . create xyseries ( out degree dist , out - degree distribution ) ; xyseries collection dataset1 = new xyseries collection ( ) ;
files . create file ( file2 ) ;
delete cell = null ;
boolean is equal = ds1 . equals ( ds2 ) ; assert . assert equals ( is equal , false ) ; boolean is identical = ( ds1 = = ds2 ) ;
return 1 + 1 + 4 + buffer . remaining ( ) ; }
_ _ dynamically dispatchable dd = null ;
if ( ( r = this . request status bin sensors ) . should send next request ( timeout msec , curr time ) ) { conn . queue ( this . addr , false , pck generator . request bin sensors status ( ) ) ; r . on request sent ( curr time ) ; }
r = resource ( ) ; params = new multivalued map impl ( ) ; params . add ( app id , app1 . get application id ( ) . to string ( ) ) ; response = r . path ( ws ) . path ( v1 ) . path ( cluster ) . path ( scheduler app - activities ) . query params ( params ) . accept ( media type . application _ json ) . get ( client response . class ) ; assert equals ( media type . application _ json _ type + ; + jetty utils . utf _ 8 , response . get type ( ) . to string ( ) ) ; json = response . get entity ( jsonobject . class ) ; nm2 . node heartbeat ( true ) ;
throw new illegal state exception ( unsupported secret key length + get secret key length ( ) + for algorithm camellia ) ;
string current pipeline = d5 ; value stream map graph = new value stream map ( current pipeline , null ) ; graph . add upstream node ( new pipeline dependency node ( d2 , d2 ) , null , current pipeline ) ; graph . add upstream node ( new pipeline dependency node ( d1 , d1 ) , null , d2 ) ; graph . add upstream material node ( new scmdependency node ( g1 , g1 , git ) , null , d1 , new material revision ( null ) ) ; graph . add upstream material node ( new scmdependency node ( g1 , g1 , git ) , null , d2 , new material revision ( null ) ) ; graph . add upstream node ( new pipeline dependency node ( d4 , d4 ) , null , current pipeline ) ; graph . add upstream node ( new pipeline dependency node ( d3 , d3 ) , null , d4 ) ; graph . add upstream material node ( new scmdependency node ( g2 , g2 , git ) , null , d3 , new material revision ( null ) ) ; graph . add upstream material node ( new scmdependency node ( g2 , g2 , git ) , null , d4 , new material revision ( null ) ) ; node level map node level map = new level assignment ( ) . apply ( graph ) ;
return load zone data ( id ) ;
for ( file f : m _ config . list files ( new file ( tmp snapshotdir1 ) ) ) { assert true ( f . get name ( ) . starts with ( foo1 ) ) ; }
top docs hits = searcher . search ( query , 10 ) ;
gcevent event = ( gcevent ) abstract event ; update heap sizes ( event ) ;
current = null ; nothing found
shape drawable pressed = new shape drawable ( new border shape ( new rect f ( 0 , 0 , 0 , line active ) ) ) ;
if ( utils . get methods ( d ) . size ( ) > 0 & & d . get simple name ( ) . to string ( ) . starts with ( gles ) ) { writer . print ( \ t \ t glcontext . reset native stubs ( + utils . get simple class name ( d ) ) ; writer . println ( . class ) ; ) ; }
if ( proj policy = = projection policy . none ) { crs = resource . get native crs ( ) ; } else { crs = resource . get crs ( ) ; } if ( crs = null ) { envelope crs envelope = crs . get envelope ( crs ) ; if ( crs envelope = null ) { crs referenced envelope = new referenced envelope ( crs envelope ) ; } }
while ( ( l = instream . read ( tmp ) ) = - 1 & & thread . current thread ( ) . is interrupted ( ) ) { count + = l ; buffer . append ( tmp , 0 , l ) ; send progress message ( count , ( content length < = 0 ? 1 : content length ) ) ; }
obj = do type ( conf , new text ( ) , text . class ) ;
fast string writer buffer writer = new fast string writer ( ) ;
assert equals ( srv - crate1 . internal : 44300 , disco nodes . get ( 0 ) . get id ( ) ) ;
em = entity manager factory . create entity manager ( ) ;
if ( iterable instanceof sorted set ) { sorted set < e > sorted set = ( sorted set < e > ) iterable ; return optional . of ( sorted set . last ( ) ) ; } while ( true ) { e current = iterator . next ( ) ; if ( iterator . has next ( ) ) { return optional . of ( current ) ; } }
if ( issuer unique id = null ) { issuer unique id . encode ( tmp , der value . create tag ( der value . tag _ context , false , ( byte ) 1 ) ) ; }
object . put ( foo , qux ) ;
for ( vertex < integer > v : graph . get all vertex ( ) ) { min heap . add ( integer . max _ value , v ) ; }
client socket . messages . await event count ( 1 , 30 , time unit . seconds ) ; event queue < string > captured = client socket . messages ; assert . assert that ( server . session . is secure , captured . poll ( ) , is ( session . is secure = true ) ) ;
val . set exists ( false ) ; comp . set exists ( false ) ; func . get int ( ) ; assert false ( func . exists ( ) ) ; val . set exists ( false ) ; comp . set value ( - 234 ) . set exists ( true ) ; fill . set value ( 765 ) . set exists ( true ) ; func . get int ( ) ;
restore local workspace settings and services ( geoserver , source restore folder , source workspaces folder , dd ) ;
string proxy base url = geo server . get global ( ) . get proxy base url ( ) ; if ( proxy base url = null ) { try { uri proxy base uri = uri . create uri ( proxy base url ) ; if ( uri . host ( ) . equals ignore case ( proxy base uri . host ( ) ) ) { return true ; } } catch ( illegal argument exception e ) { logger . fine ( unable to parse proxy base url to a uri : + proxy base url ) ; } } if ( additional _ hostnames . contains ( uri . host ( ) ) ) { logger . log ( level . fine , hostname { 0 } is in known aliases for self , new object [ ] { uri . host ( ) } ) ; return true ; }
reservation definition rr1 = create reservation definition ( 10 * step , job arrival time 15 * step , job deadline new reservation request [ ] { reservation request . new instance ( resource . new instance ( 1024 , 1 ) , capability 20 , num containers 20 , concurrency 10 * step ) } , duration reservation request interpreter . r _ all , u1 ) ;
if ( source = = null ) throw new null pointer exception ( source array was null . ) ; if ( destination = = null ) throw new null pointer exception ( destination array was null . ) ; if ( src offset < 0 | | src offset + 3 > = source . length ) throw new illegal argument exception ( string . format ( source array with length % d cannot have offset of % d and still process four bytes . , source . length , src offset ) ) ; if ( dest offset < 0 | | dest offset + 2 > = destination . length ) throw new illegal argument exception ( string . format ( destination array with length % d cannot have offset of % d and still store three bytes . , destination . length , dest offset ) ) ; byte [ ] decodabet = get decodabet ( options ) ;
if ( render is cancelled ) { return ; }
map . put ( 1 , new person ( a , 15 ) ) ;
for ( string metadata name : all metadata names ) { final string builder buffer = metadata rows . get ( metadata name ) ; final map < string , string > metadata column = column metadata . get ( column . get key ( ) ) ; string metadata value = metadata column = null ? metadata column . get ( metadata name ) : null ; if ( metadata value = = null ) metadata value = ; if ( metadata value . length ( ) > column . get value ( ) ) metadata value = metadata value . substring ( 0 , column . get value ( ) ) ; buffer . append ( string . format ( % - + column . get value ( ) + s , format cell ( col name , column . get value ( ) , metadata value ) ) ) ; }
char [ ] buf = path . get buffer ( ) ; if ( context version . resources = null & & buf [ path end - 1 ] = ' ' ) { string path str = path . to string ( ) ; web resource file ;
string builder trace = new string builder ( ) ;
policy . set queues ( mock csqueues ( new string [ ] { a , b , c } , new int [ ] { 0 , 0 , 0 } , new float [ ] { 0 . 1f , 0 . 0f , 0 . 2f } , ) ) ; verify order ( policy , , new string [ ] { b , a , c } ) ;
log . error ( couldn ' t find the tenant? - should never happen , e ) ;
long b0 = udp . udp . i _ o . ordinal ( ) ; special flag to indicate io - record and not a rpc - record
object out = template . send body ( direct : noout , exchange pattern . in optional out , hi ) ;
stopping . set ( false ) ;
if ( is detailed ) { assert that ( task . get project ( ) , null value ( ) ) ; assert that ( task . get user ( ) , null value ( ) ) ; } else { assert that ( task . get project ( ) , not null value ( ) ) ; assert that ( task . get user ( ) , not null value ( ) ) ; }
assert equals ( attachment ; filename = primitive geo feature . xlsx , resp . get header ( content - disposition ) ) ; xssfworkbook wb = new xssfworkbook ( in ) ;
if ( metrics . close enough to previous ( last metrics _ ) ) { last metrics _ = metrics ; event bus _ . fire event ( new workbench metrics changed event ( metrics ) ) ; } session _ . persist client state ( ) ;
kerberos execution = find kerberos execution ( ) ;
jviewport view port = enclosing scroll pane . get viewport ( ) ;
job conf job conf = mr . create job conf ( ) ; job conf . set output committer ( committer with logs . class ) ; job conf . set task cleanup needed ( false ) ; running job r job = launch job ( job conf , in dir , out dir , input ) ; r job . wait for completion ( ) ; validate job ( r job , mr , false ) ; validate task error ( mr , 10 , 10 , 4 ) ; } } finally {
state = s ; magnet state = m state ; listener . connected ( this ) ; if ( _ log . should log ( log . debug ) ) _ log . debug ( start running the reader with + to string ( ) ) ;
socket wrapper base < long > wrapper = connections . remove ( long . value of ( socket ) ) ;
for ( int i = 0 ; i < psrcs . length ; i + + ) { srcs str [ i ] = get path name ( srcs [ i ] ) ; } dfs . concat ( get path name ( abs f ) , srcs str ) ; }
flush internal ( ) ; }
root node . add light ( pl ) ;
view view = v . find view by id ( r . id . image ) ;
super . configure expression ( camel context , expression ) ; }
map < string , integer > small _ profile = profile2 ; map < string , integer > large _ profile = profile1 ; if ( profile1 . size ( ) < profile2 . size ( ) ) { small _ profile = profile1 ; large _ profile = profile2 ; } double agg = 0 ;
assert invalid ( jdbc : presto : localhost : 8080? ssl = 0 , connection property ' ssl ' value is invalid : 0 ) ;
fix = suggested fix . builder ( ) . delete ( default case ) . postfix with ( case before default , initial comments + default source ) . build ( ) ; } } }
child realm . users ( ) . get ( exchanged user id ) . remove ( ) ;
final list < string > keys = populate store ( 5 , 0 , store , marshaller ) ;
key store . builder . new instance ( key store . get default type ( ) , default provider , fl . get parent file ( ) , prot pass ) ;
m shipping option section . set can add items ( false ) ; m payment method section . set can add items ( can add cards ) ;
assert equals ( payment3 . get purchased amount ( ) . compare to ( requested amount ) , 0 ) ; assert equals ( payment3 . get refunded amount ( ) . compare to ( big decimal . zero ) , 0 ) ; assert equals ( payment3 . get currency ( ) , currency ) ; assert equals ( payment3 . get transactions ( ) . size ( ) , 3 ) ; assert equals ( payment3 . get transactions ( ) . get ( 2 ) . get external key ( ) , chargeback transaction external key ) ; assert equals ( payment3 . get transactions ( ) . get ( 2 ) . get payment id ( ) , payment . get id ( ) ) ; assert null ( payment3 . get transactions ( ) . get ( 2 ) . get amount ( ) ) ; assert null ( payment3 . get transactions ( ) . get ( 2 ) . get currency ( ) ) ; assert equals ( payment3 . get transactions ( ) . get ( 2 ) . get processed amount ( ) . compare to ( big decimal . zero ) , 0 ) ; assert null ( payment3 . get transactions ( ) . get ( 2 ) . get processed currency ( ) ) ; assert equals ( payment3 . get transactions ( ) . get ( 2 ) . get transaction status ( ) , transaction status . payment _ failure ) ;
m root layout . set background color ( active element . get bg color ( ) ) ;
for ( string enabled module : args . get enabled modules ( ) ) { for ( string source : args . get sources ( enabled module ) ) { string short form = base home . to short form ( source ) ; modules . enable ( enabled module , short form ) ; } } args . set all modules ( modules ) ;
return stencil _ event _ boundary _ timer ;
integer async thread pool size = client properties . get value ( runtime properties , client properties . async _ threadpool _ size , integer . class ) ;
this . session . get remote ( ) . send string ( message , null ) ; }
if ( is json & & is xml ) { is xml = binding mode . equals ( auto ) | | binding mode . contains ( xml ) ; is json = binding mode . equals ( auto ) | | binding mode . contains ( json ) ; }
for ( int kk = 0 ; kk < n raws ; kk + = columncount ) { client . call procedure ( @ ad hoc , insert into number _ types values ( + cast ( ' + ( int ) raw data [ kk + 0 ] + ' as + num type names [ 0 ] + ) , + cast ( ' + ( int ) raw data [ kk + 1 ] + ' as + num type names [ 1 ] + ) , + cast ( ' + ( int ) raw data [ kk + 2 ] + ' as + num type names [ 2 ] + ) , + cast ( ' + ( int ) raw data [ kk + 3 ] + ' as + num type names [ 3 ] + ) , + cast ( ' + raw data [ kk + floatcolindex ] + ' as float ) , + cast ( ' + raw data [ kk + decimalcolindex ] + ' as decimal ) ) ; ) ; }
final list < channel buffer > components = new array list < channel buffer > ( arrays . length ) ; for ( byte [ ] a : arrays ) { if ( a = = null ) { break ; } if ( a . length > 0 ) { components . add ( wrapped buffer ( endianness , a ) ) ; } } return composite buffer ( endianness , components , false ) ;
object o1 = new input braces ( ) ;
process image procimg = modbus coupler . get reference ( ) . get process image ( ) ;
if ( find _ among _ b ( a _ 11 , 2 ) = = 0 ) { return false ; }
closeable reference < closeable image > closable reference = null ;
final jlabel label = new jlabel ( ) { private static final long serial version uid = 8139543899934835869 l ; @ override public string get text ( ) { final int index = pane . index of tab component ( button tab component . this ) ; if ( index = - 1 ) { return pane . get title at ( index ) ; } return null ; } } ; add ( label ) ;
assert equals ( bounded window . timestamp _ max _ value , residual tracker . get watermark ( ) ) ;
final inject i = ret val . get annotation ( inject . class ) ;
for ( int i = 0 ; i < 1000 ; i + + ) { writer . add row ( new decimal struct ( null ) ) ; } writer . add row ( new decimal struct ( new hive decimal writable ( 1 . 00 ) ) ) ; writer . add row ( new decimal struct ( new hive decimal writable ( 2 . 00 ) ) ) ;
string key store type = key store . get default type ( ) ;
controller . add test element ( new test sampler ( three ) ) ;
timestamp t = new timestamp ( d . get time ( ) 1000 ) ;
if ( size > 0 ) { for ( int i = 0 ; i < size ; i + + ) { indarray comp point = data . slice ( index [ i ] ) ; if ( point . get double ( 0 ) = = comp point . get double ( 0 ) & & point . get double ( 1 ) = = comp point . get double ( 1 ) ) return true ; } }
with terminal sized ( 4 , 2 ) ;
git material git = new git material ( git ) ; build cause p3build cause = create build cause ( as list ( p1 ) , new array list < > ( ) ) ; build cause p1build cause = create build cause ( new array list < > ( ) , as list ( git ) ) ; when ( pipeline service . build cause for ( p3 , 1 ) ) . then return ( p3build cause ) ; when ( pipeline service . build cause for ( p1 , 1 ) ) . then return ( p1build cause ) ;
set block span ( builder , new fancy quote span ( m block quote line width , m block quote line indent , m options . m block quote line color ) ) ; set block span ( builder , new foreground color span ( m options . m block quote text color ) ) ; set block span ( builder , new leading margin span . standard ( m block quote indent ) ) ; set block span ( builder , new style span ( typeface . italic ) ) ; break ; case strikethrough : set span ( builder , new strikethrough span ( ) ) ; break ; case hrule : set span ( builder , new horizontal line span ( m options . m hrule color , m hrule size , m hrule top bottom padding ) ) ; break ; case image :
byte [ ] family = new byte [ c . length - 1 ] ; system . arraycopy ( c , 0 , family , 0 , family . length ) ; return new byte [ ] [ ] { family } ; }
} , new dialog interface . on cancel listener ( ) { @ override public void on cancel ( dialog interface dialog ) { notify data model not deleted ( data ) ;
strs = params . get params ( common params . stream _ file ) ;
tree set < string > coprocess set = new tree set < > ( ) ;
resp = post as servlet response ( rest base controller . root _ path + imports , { } , application json ) ;
clip . width = 0 ;
comparator = ( comparator < ? super k > ) natural _ order ;
iinformation control creator information control creator = new iinformation control creator ( ) { @ override public iinformation control create information control ( shell shell ) { return new default information control ( shell , true ) ; } } ; f information presenter = new information presenter ( information control creator ) ;
snapshot protos . snapshot description snapshot desc = get snapshot ( ) ; procedure testing utility . set kill and toggle before store update ( proc exec , true ) ;
if ( enabled ) { add mouse listener ( pan handler ) ; add mouse motion listener ( pan motion handler ) ; add mouse wheel listener ( zoom handler ) ; } else { remove mouse listener ( pan handler ) ; remove mouse motion listener ( pan motion handler ) ; remove mouse wheel listener ( zoom handler ) ; }
remote addr valve rav = new remote addr valve ( ) ; rav . set deny ( . * ) ; rav . set deny status ( 404 ) ; examples . get pipeline ( ) . add valve ( rav ) ; tomcat . start ( ) ;
for ( int i run = 0 ; i run < m _ n runs ; i run + + ) { generate random nework generate random net ( bayes net , instances ) ; search super . search ( bayes net , instances ) ; calculate score f current score = calc score ( bayes net ) ; keep track of best network seen so far if ( f current score > f best score ) { f best score = f current score ; copy parent sets ( best bayes net , bayes net ) ; } }
m fragment manager . add on back stack changed listener ( new fragment manager . on back stack changed listener ( ) { public void on back stack changed ( ) { set layout ( ) ; } } ) ; }
if ( base column field diff impl . is equal or null ( this . get value ( ) , other . get value ( ) ) ) { result . add ( new base column field diff impl ( field _ value , extract default value ( this . get value ( ) ) , extract default value ( other . get value ( ) ) ) ) ; } return result ; }
from element . set text ( ) ; } else { from elements . add ( from element ) ; } } }
while ( reachables . is empty ( ) ) { node < t > node = reachables . poll ( ) ; for ( node < t > s : node . get successors ( ) ) { if ( s = = node ) { continue ; } ignore self - edge for ( node < t > p : node . get predecessors ( ) ) { if ( p = = node ) { continue ; } ignore self - edge add edge ( p , s ) ; } removes n - > s s . remove predecessor ( node ) ; if ( to remove . remove ( s ) ) { reachables . add ( s ) ; } } for ( node < t > p : node . get predecessors ( ) ) { if ( p = = node ) { continue ; } ignore self - edge p . remove successor ( node ) ; if ( to remove . remove ( p ) ) { reachables . add ( p ) ; } } after the node deletion , the graph is again well - formed and the original topological order is preserved . nodes . remove ( node . get label ( ) ) ; }
log . debug ( function task : create function { } is skipped as database { } + is newer than update , func name , db name ) ; return 0 ;
override cluster drop rate ( old state . get current override drop rate ( ) , tracker clients ) ;
for ( int i = 0 ; i < hash _ size ; i + + ) { if ( mac _ value [ i ] = data [ i + content . length ] ) { throw new alert exception ( alert protocol . bad _ record _ mac , new sslprotocol exception ( bad record mac ) ) ; } } system . arraycopy ( data , 0 , content , 0 , content . length ) ; inc sequence number ( read _ seq _ num ) ; return content ; }
title = nasty path setup ; env = new hash map < > ( ) ; env . put ( jetty . home , as target path ( title , app % 2 fnasty dist ) ) ; env . put ( jetty . base , as target path ( title , app % 2 fnasty base ) ) ; env . put ( war , as target path ( title , app % 2 fnasty base webapps foo ) ) ; data . add ( new object [ ] { arch , title , env } ) ;
schema = create table t0 ( id bigint not null , name varchar ( 32 ) not null , age integer , primary key ( id ) ) ; \ n + partition table t0 on column id ; \ n + create unique index user _ index3 on t0 ( id , age ) ; ; check valid unique and assume unique ( schema , null , msg pr ) ;
if ( use pre selected media type ( selected method , acceptable media types ) ) { return selected method . produces . combined type ; } final resource method resource method = selected method . method routing . method ;
get reachable config set ( input , s . configs , reach , t ) ;
int row with size length ;
string collection name = test solr cloud collection ; string config name = solr cloud collection config ; mini cluster . upload config set ( solr test case j4 . test _ path ( ) . resolve ( collection1 conf ) , config name ) ; collection admin request . create collection ( collection name , config name , 2 , 2 ) . process ( mini cluster . get solr client ( ) ) ; query request req = new query request ( ) ;
list < step > steps = traversal . get steps ( ) ;
final operation step handler write handler = new reload required write attribute handler ( value ) ; resource registration . register read write attribute ( value , null , write handler ) ; }
docking sensitivity = 0 ; floating = false ; floating x = floating y = 0 ; floating tool bar = null ; set orientation ( tool bar . get orientation ( ) ) ;
row = 999 ; for ( int c = 0 ; c < 4 ; c + + ) { assert true ( frame . vec ( c ) . is na ( row ) ) ; } } } finally {
output ( ent , outs ) ;
for ( int i = 0 ; i < this . values . length ; i + + ) { if ( is null ( this . values [ i ] ) ) count null + + ; else if ( is false ( this . values [ i ] ) ) count false + + ; } byte [ ] values = new byte [ this . values . length ] ;
timestamp token timestamp token info = new timestamp token ( encoded timestamp token info ) ;
fixture . cluster . set nodes ( nodes ) ; assert equals ( distributor : 6 storage : 6 . 4 . s : d . 5 . s : r , state after storage transition ( fixture , 5 , state . up ) ) ;
byte [ ] msg mac = new byte [ mac size ] ; system . arraycopy ( buf block , extra , msg mac , 0 , mac size ) ; if ( arrays . constant time are equal ( this . mac block , msg mac ) ) { throw new invalid cipher text exception ( mac check in gcm failed ) ; } }
res conf . set max capacity ( default xyz , 50 . 0f ) ; scheduler . set task tracker manager ( task tracker manager ) ;
return ( async model < ? extends model > ) this ; }
parser . set incoming frames handler ( extension stack ) ; io state . on opened ( ) ; log . debug ( outgoing = { } , outgoing ) ;
check locality of marked calls ( prefix + return { foo : 1 } . foo + suffix , f returns non local ) ;
if ( working = = null ) { working = new byte [ 28 * 28 ] ; }
selection event = new event ( ) ;
p . add child ( c1 ) ;
random access file raf = new random access file ( file . to uri ( ) . get path ( ) , rw ) ;
assert . assert equals ( 1 , id . get container id ( ) & 1 ) ; }
break ; default : throw new illegal argument exception ( unsupported authtype for aws request : + auth type ) ; } }
spinner spinner = ( spinner ) find view by id ( r . id . play movie file _ spinner ) ;
docx4 j . save ( word mlpackage , new java . io . file ( system . get property ( user . dir ) + out _ bookmarks deleter . docx ) ) ;
cs . get queue ( q2 ) . finish application attempt ( app , q2 ) ;
if ( tenants . contains key ( tenant name ) ) { executor . execute ( ( ) - > { create tenant ( tenant name ) ; } ) ; } }
identity token = new identity token ( ) ; identity token . anonymous ( true ) ; } } if ( ( sec mech . as _ context _ mech . target _ requires & establish trust in client . value ) = 0 ) {
ket = cursor ; switch ( among _ var ) { case 0 : break lab1 ; case 1 :
if ( ( f features & namespacedecls ) = 0 ) { f serializer . add attribute ( xmlns _ uri , attr prefix , xmlns _ prefix + : + attr prefix , cdata , attr ns ) ; f nsbinder . declare prefix ( attr prefix , attr ns ) ; f local nsbinder . declare prefix ( attr prefix , attr ns ) ; }
context . stop route ( tailable cursor consumer2 ) ;
if ( range start . equals ( range end ) ) { this . absolute values . add ( range start ) ; return ; } if ( range start > range end ) { in range x - y , if x is larger than y , the range is equivalent to x - max , min - y , where max is the largest value of the corresponding attribute and min is the smallest . for ( int i = range start ; i < = this . get max value ( ) ; i + + ) { this . absolute values . add ( i ) ; } for ( int i = this . get min value ( ) ; i < = range end ; i + + ) { this . absolute values . add ( i ) ; } } else { just keep adding from range start to range end ( both inclusive ) . for ( int i = range start ; i < = range end ; i + + ) { this . absolute values . add ( i ) ; } }
check for multiple differences ( ) ; template = ( object [ ] ) ( ( object [ ] ) m _ keys . element at ( 0 ) ) . clone ( ) ;
return null ; } } return response utils . build url ( base url , styles + target . get path ( ) , collections . < string , string > empty map ( ) , urltype . resource ) ; } else if ( ( http . equals ( graphic protocol ) | | https . equals ( graphic protocol ) ) ) { return null ; }
log . debug ( reading config property files for defaults \ n ) ; configuration default variables = new hash map < > ( ) ; arrays . stream ( configuration classes ) . map ( class : : get declared fields ) . map ( this : : extract default variables from configuration fields ) . filter ( objects : : non null ) . for each ( map - > configuration default variables . put all ( map ) ) ; log . debug ( \ n = = = = = \ n ) ;
boolean result = under test . wrap ( null , null ) ;
install ( new cache module ( config source ) ) ;
if ( new state = app state . running & & new state = app state . committing ) { state to ts map . put ( new state , context . get clock ( ) . get time ( ) ) ; }
while ( ( start index = sb . index of ( * , start index ) ) > = 0 ) { end index = sb . index of ( * , start index + 2 ) ; if ( end index < 0 ) { end index = totallen ; } token = sb . substring ( start index + 2 , end index ) ; comments . add ( token ) ; sb . replace ( start index + 2 , end index , _ _ _ yuicssmin _ preserve _ candidate _ comment _ + ( comments . size ( ) - 1 ) + _ _ _ ) ; start index + = 2 ; } css = sb . to string ( ) ;
hash map < string , string > params = new hash map < > ( ) ; params . put ( new name , new flow ) ; response response = auth mgmt resource . copy ( existing flow , params ) ; assert admin events . assert event ( realm _ name , operation type . create , encode . decode ( admin event paths . auth copy flow path ( existing flow ) ) , params , resource type . auth _ flow ) ; try { assert . assert equals ( copy flow , 201 , response . get status ( ) ) ; } finally { response . close ( ) ; } for ( authentication flow representation flow : auth mgmt resource . get flows ( ) ) { if ( flow . get alias ( ) . equals ignore case ( new flow ) ) { return flow ; } } return null ;
test round trip type ( map type , read values . stream ( ) . map ( value - > to hive map ( value , read null key value ) ) . collect ( to list ( ) ) ) ; if ( structural null tests enabled ) { values and nulls in simple map test round trip type ( map type , insert null every ( 5 , read values ) . stream ( ) . map ( value - > to hive map ( value , read null key value ) ) . collect ( to list ( ) ) ) ; all null values in simple map test round trip type ( map type , read values . stream ( ) . map ( value - > to hive map ( null , read null key value ) ) . collect ( to list ( ) ) ) ; }
term = bytes ref . deep copy of ( term ) ;
boolean right side = ( point x > m xcenter ) ; boolean top side = ( point y < m ycenter ) ; if ( right side & & top side ) { degrees = 90 - degrees ; } else if ( right side ) { degrees = 90 + degrees ; } else if ( top side ) { degrees = 270 - degrees ; } else { degrees = 270 + degrees ; } return degrees ;
all journals . add all ( map . key set ( ) ) ;
if ( peek token ( ) = token . colon ) kit . code bug ( ) ; consume token ( ) ;
image container container = new image container ( cached bitmap , request url , null , null ) ;
if ( name . length ( ) < = primitive _ class _ name _ max _ length ) { int index = arrays . binary search ( primitive _ names , name ) ; if ( index > = 0 ) { return primitive _ classes [ index ] ; } } return null ; }
electors [ 0 ] . quit election ( true ) ;
log . debug ( ignoring bucket option { } , key ) ; } else {
config = new local cluster ( sqlfeatures - hsql . jar , 1 , 1 , 0 , backend target . hsqldb _ backend ) ;
class < ? > super class = class type . get superclass ( ) ; if ( super class = = null ) throw e ; return method unwinder ( field name , super class ) ;
hb response = nm1 . node heartbeat ( true ) ; assert . assert true ( node action . resync = hb response . get node action ( ) ) ; hb response = nm2 . node heartbeat ( true ) ; assert . assert true ( node action . resync = hb response . get node action ( ) ) ;
string field name = jp . get current name ( ) ;
tuple incr dump = incremental load and verify ( db name , bootstrap dump . last repl id , repl db name ) ; verify run ( select a from + repl db name + . unptned order by a , unptn _ data , driver mirror ) ; }
m _ added classes = volt ddl tracker . m _ extra classses . to array ( new string [ 0 ] ) ; add extra classes ( jar output ) ; compile row limit delete stmts ( db , hsql , ddlcompiler . get limit delete stmt to xml entries ( ) ) ; }
expected rows = 3 ;
check cmd on driver ( driver . compile and respond ( merge into target using source + on target . p = source . p1 and target . a = source . a1 + when matched then update set b = 11 + when not matched then insert values ( a1 , b1 , p1 , q1 ) ) ) ; long txnid2 = txn mgr2 . get current txn id ( ) ; txn mgr2 . acquire locks ( driver . get plan ( ) , ctx , t2 , false ) ; locks = get locks ( txn mgr ) ; assert . assert equals ( unexpected lock count , 7 , locks . size ( ) ) ;
image = drawable to bitmap ( get drawable ( ) ) ; if ( canvas size > 0 ) update bitmap shader ( ) ; }
assert that ( response . has current ( ) ) . is true ( ) ;
jp . next token ( ) ; settable bean property prop = _ bean properties . find ( prop name ) ; if ( prop = null ) { normal case try { prop . deserialize and set ( jp , ctxt , bean ) ; } catch ( exception e ) { wrap and throw ( e , bean , prop name , ctxt ) ; } continue ; }
final account account json = create account with default payment method ( ) ;
int first group len = num digits % digits per int [ radix ] ;
array list < cell > queue = new array list < cell > ( ) ; queue . add ( start ) ; while ( queue . is empty ( ) & & queue . get ( 0 ) = end ) { cell home = queue . remove ( 0 ) ; cell [ ] neighbours = new cell [ 8 ] ; int x = home . x ; int z = home . z ; populate neighbours ( grid , neighbours , x , z , allow diags ) ; for ( int n = 0 ; n < 8 ; n + + ) { if ( neighbours [ n ] = null & & neighbours [ n ] . dist > home . dist + 1 ) { queue . add ( neighbours [ n ] ) ; neighbours [ n ] . dist = home . dist + 1 ; neighbours [ n ] . predecessor = home ; } } } int path length = end . dist + 1 ; + 1 for the start block .
thread . sleep ( scanner _ timeout + 100 ) ; } res = r . next ( ) ; } } catch ( scanner timeout exception e ) {
super . selection changed ( ) ; }
string value = a value ;
if ( from instanceof reader ) { optimize for common output types which are optimized to deal with char [ ] if ( to instanceof string builder ) { return copy reader to builder ( ( reader ) from , ( string builder ) to ) ; } else { return copy reader to writer ( ( reader ) from , as writer ( to ) ) ; } } else { check not null ( from ) ; check not null ( to ) ; long total = 0 ; char buffer buf = create buffer ( ) ; while ( from . read ( buf ) = - 1 ) { buf . flip ( ) ; to . append ( buf ) ; total + = buf . remaining ( ) ; buf . clear ( ) ; } return total ; }
return get server ( ) ; }
win def . hglrc h glrc = open gl32 . instance . wgl create context ( hdc ) ; open gl32 . instance . wgl make current ( hdc , h glrc ) ; win def . hglrc current context = open gl32 . instance . wgl get current context ( ) ; open gl32 . instance . wgl delete context ( h glrc ) ;
if ( obj instanceof asn1 object identifier ) { return new algorithm identifier ( ( asn1 object identifier ) obj ) ; }
throw e . get reason ( ) ; }
final list < path > paths = new linked list < path > ( ) ;
count = this . limit - start pos ;
execute ( select persons . name , offices . name from + employees as persons left join offices on office _ id = offices . id + order by offices . name nulls first ) ;
update j ( json add ( sdoc with children ( rdbq3 _ 1 , v1010 ) ) , params ( distrib _ update _ param , from _ leader ) ) ;
expr = new spel expression parser ( ) . parse raw ( new spr5899 class ( ) . to string ( ) ) ; assert equals ( instance , expr . get value ( e context ) ) ; expr = new spel expression parser ( ) . parse raw ( new spr5899 class ( null ) . to string ( ) ) ; assert equals ( instance , expr . get value ( e context ) ) ;
m filtered recycler view . update empty view ( empty view message type . no _ content ) ;
boolean found group = false ; for ( group representation ug : test realm resource ( ) . users ( ) . get ( thor . get id ( ) ) . groups ( ) ) { if ( ug . get id ( ) . equals ( gid ) ) { found group = true ; } } assert . assert true ( found group ) ;
type check ( line _ joiner . join ( function tag ( * * array < number > * strings ) { } , tag`template string` ; ) , new type inference . template _ argument _ mismatch ) ;
sc = ctxts . get ( orbversion service context . service _ context _ id ) ; if ( sc = null ) { orbversion service context ovsc = ( orbversion service context ) sc ; orbversion version = ovsc . get version ( ) ; orb . set orbversion ( version ) ; }
field web view core field = web view provider . get class ( ) . get declared field ( m web view core ) ; web view core field . set accessible ( true ) ; m web view core = web view core field . get ( web view provider ) ;
if ( ftp . client = = null ) { if ( ( ftp . follow talk ) & & ( ftp . log . is info enabled ( ) ) ) { ftp . log . info ( start client ) ; }
dfs cluster . restart name node ( 0 , true , - rolling upgrade , rollback ) ;
general empty < list < integer > > result2 = mapper . read value ( apos to quotes ( { ' value ' : null } ) , new type reference < general empty < list < integer > > > ( ) { } ) ; assert not null ( result2 . value ) ; assert equals ( 0 , result2 . value . size ( ) ) ; }
builder . get attribute builder ( ) . add rename ( ejb3 subsystem model . statistics _ enabled , ejb3 subsystem model . enable _ statistics ) ; transformation description . tools . register ( builder . build ( ) , subsystem registration , version ) ;
string url = cs . get url ( ) ;
if ( object helper . is empty ( configuration . get method ( ) ) ) { configuration . set method ( grpc utils . convert method2 camel case ( configuration . get method ( ) ) ) ; }
write message ( message ) ; }
return run state at least ( ctl . get ( ) , tidying ) ? 0 : workers . size ( ) ; } finally {
message retrieval listener < local message > retrieval listener = new message retrieval listener < local message > ( ) { @ override public void message started ( string message , int number , int of total ) { } @ override public void messages finished ( int number ) { } @ override public void message finished ( local message message , int number , int of total ) { if ( is message suppressed ( message ) ) { list < local message > messages = new array list < > ( ) ; messages . add ( message ) ; stats . unread message count + = ( message . is set ( flag . seen ) ) ? 1 : 0 ; stats . flagged message count + = ( message . is set ( flag . flagged ) ) ? 1 : 0 ; if ( listener = null ) { listener . list local messages add messages ( account , null , messages ) ; } } } } ;
assert equals ( 100 , rm out . get images ( ) . get ( 0 ) . get tx id ( ) ) ;
return deleted count ;
if ( ( eq _ s _ b ( 1 , u ) ) ) { cursor = limit - v _ 3 ; break lab0 ; }
if ( non standard delimiter = null ) { sql statement builder . set delimiter ( non standard delimiter ) ; }
confirmation menu . set confirm color ( get resources ( ) . get color ( r . color . white ) , color ) ; } else { confirmation menu . set confirm color ( get resources ( ) . get color ( r . color . white , get context ( ) . get theme ( ) ) , color ) ; } }
out . printin ( public final class ) ;
child = nodes . get ( 1 ) ;
if ( m tick radius > half right ) { for ( int i = 0 ; i < = m num segments ; i + + ) { float x = start right - i * m tick distance ; if ( x < = thumb x ) break ; canvas . draw circle ( x , thumb y , m tick radius , paint ) ; } }
assert that ( aop utils . is jdk dynamic proxy ( bean ) , is ( true ) ) ;
log . e ( tag , couldn ' t save mini thumbnail data for + id + ; disk full or mount read - only? + ex . get class ( ) ) ;
encoded string value previously sent by = parse encoded string value ( pdu data stream ) ; if ( null = previously sent by ) { try { headers . set encoded string value ( previously sent by , pdu headers . previously _ sent _ by ) ; } catch ( null pointer exception e ) { log ( null pointer error ) ; } catch ( runtime exception e ) { log ( header field + is not encoded - string - value header field ) ; return null ; } } break ;
paint border ( ctx , g , x , y , w , h , null ) ;
for ( cache dir dir : cache dirs ) { for ( file file : dir . obj files ) { file . delete ( ) ; if ( store . get usable space ( ) > min _ free _ space ) { return ; } } }
assert listener state ( l3 , service . state . inited ) ; assert listener event count ( l3 , 1 ) ;
assert xpath evaluates to ( 2 , count ( gmlcov : metadata gmlcov : extension wcsgs : time domain gml : time instant ) , dom ) ;
return parse ( content ) ; }
for ( int i = 0 ; i < sub clusters active . size ( ) ; i + + ) { try { future < nodes info > future = comp svc . take ( ) ; nodes info nodes response = future . get ( ) ; if ( nodes response = null ) { nodes . add all ( nodes response . get nodes ( ) ) ; } } catch ( throwable e ) { log . warn ( failed to get nodes report , e ) ; } }
queue manager . get leaf queue ( root . q . subqueue1 , true ) ; queue manager . get leaf queue ( root . q . subqueue2 , true ) ; web resource r = resource ( ) ;
int padding = ( page indicator land gutter right nav bar px + get hotseat height ( ) + hotseat land gutter px + m insets . left ) 2 ;
previous version = persister . get version ( object ) ;
descendant . parent to local coordinates ( local coords ) ; return local coords ;
intent intent = new intent ( this , back handling file picker activity . class ) ; intent . put extra ( file picker activity . extra _ allow _ multiple , m enable multiple selections ) ;
assert equals ( null , cache . get block ( single blocks [ 0 ] . cache key , true , false ) ) ; assert equals ( null , cache . get block ( single blocks [ 1 ] . cache key , true , false ) ) ; assert equals ( null , cache . get block ( multi blocks [ 0 ] . cache key , true , false ) ) ; assert equals ( null , cache . get block ( multi blocks [ 1 ] . cache key , true , false ) ) ;
create graph ( a to d , b to d , c to d , e to d , e to f , e to g ) ; path expander < void > expander = path expander builder . < void > empty ( ) . add ( to ) . build ( ) ; traversal description side = get graph db ( ) . traversal description ( ) . uniqueness ( node _ path ) . expand ( expander ) ; expect paths ( get graph db ( ) . bidirectional traversal description ( ) . mirrored sides ( side ) . traverse ( as list ( get node with name ( a ) , get node with name ( b ) , get node with name ( c ) ) , as list ( get node with name ( f ) , get node with name ( g ) ) ) , a , d , e , f , a , d , e , g , b , d , e , f , b , d , e , g , c , d , e , f , c , d , e , g ) ; }
int [ ] ints = new int [ el . properties types . length ] ; for ( int i = 0 ; i < ints . length ; i + + ) { ints [ i ] = ( integer ) el . properties . get ( i ) ; } return ints ; } else {
int temp = direction [ 1 ] ;
slice _ from ( ive ) ;
int timeout task delay = 4 ;
gl30 . gl delete framebuffers ( frame ) ;
model node rod op = util . create operation ( read _ operation _ description _ operation , jgroups tcp addr . append ( protocol , protocol name ) ) ; rod op . get ( name ) . set ( add ) ; model node result = domain test utils . execute for result ( rod op , master utils . get domain client ( ) ) ; assert . assert true ( result . get ( request _ properties ) . has defined ( add _ index ) ) ; } finally {
final byte [ ] padded = bytes . from long ( uid ) ;
row row = new row ( index ) ;
update input layout margins ( ) ; }
ioutils . skip fully ( input , 22 ) ;
string line number = utils . get my phone number ( m context ) ;
cache children ( col entry . get key ( ) , col entry . get value ( ) ) ; } } if ( for object = = null ) { if ( object map . is empty ( ) ) {
mutable object iterator < tuple2 < integer , string > > iterator = new merge iterator < > ( iterators , this . comparator ) ;
test listener . push expected event ( next event . phase ) ; final interval it = new interval ( clock . get utcnow ( ) , clock . get utcnow ( ) . plus days ( 31 ) ) ;
boolean is valid identifier strategy = false ; if ( identifier strategy = null ) { for ( identifier strategy identifier strategy type : identifier strategy . values ( ) ) { if ( identifier strategy type . name ( ) . equals ( identifier strategy . name ( ) ) ) { is valid identifier strategy = true ; break ; } } } if ( is valid identifier strategy ) { identifier strategy = identifier strategy . auto ; }
new path + = ? ;
new width = make length even ( width ) ;
cursor = start pos ;
root ( = [ 100 100 100 100 ] , x = [ 100 100 100 100 ] ) ; + root - a ( = [ 50 100 20 50 ] , x = [ 50 100 80 50 ] ) ; + a - b ( = [ 50 100 80 50 ] , x = [ 50 100 20 50 ] ) ; b string apps config =
provider manager . get instance ( ) . add iqprovider ( notification event iq . element _ name , notification event iq . namespace , new notification event iqprovider ( ) ) ;
collection < data field > c = data fields . values ( ) ;
computation graph configuration conf3 = new neural net configuration . builder ( ) . graph builder ( ) . add inputs ( in ) . set input types ( input type . convolutional ( 28 , 28 , 1 ) ) . add layer ( cnn , new convolution layer . builder ( ) . kernel size ( 2 , 2 ) . padding ( 0 , 0 ) . stride ( 2 , 2 ) . n out ( 3 ) . build ( ) , in ) ( 28 - 2 + 0 ) 2 + 1 = 14 . add layer ( pool , new subsampling layer . builder ( ) . kernel size ( 2 , 2 ) . padding ( 0 , 0 ) . stride ( 2 , 2 ) . build ( ) , cnn ) ( 14 - 2 + 0 ) 2 + 1 = 7 . add layer ( dense , new dense layer . builder ( ) . n out ( 10 ) . build ( ) , pool ) . add layer ( out , new output layer . builder ( ) . n in ( 10 ) . n out ( 5 ) . build ( ) , dense ) . set outputs ( out ) . build ( ) ;
build target foo lib target = build target factory . new instance ( foo : lib ) ; target node < ? , ? > foo lib node = apple library builder . create builder ( foo lib target ) . build ( ) ; build target foo genrule target = build target factory . new instance ( foo : genrule ) ; target node < ? , ? > foo genrule node = genrule builder . new genrule builder ( foo genrule target ) . set out ( foo ) . set cmd ( echo hi > out ) . set srcs ( immutable list . of ( default build target source path . of ( foo lib target ) ) ) . build ( ) ; build target bar lib target = build target factory . new instance ( bar : lib + cxx description enhancer . shared _ flavor ) ;
set < string > connector config updates copy = null ;
string [ ] includes = new string [ ] { updated , r * } ;
for ( map . entry < string , list < string > > entry : map of lists . entry set ( ) ) { rp . put ( entry . get key ( ) , entry . get value ( ) ) ; }
groovify fat java like groovy ast ( ast , groovy token names ) ;
long future auction = config . timestamp and inter event delay us for event ( current event number + num events for auctions ) . get key ( ) ;
frame size = id3 sync safe integer . buffer to value ( byte buffer ) ; if ( frame size < 0 ) { logger . warning ( get logging filename ( ) + : + invalid frame size : + identifier ) ; throw new invalid frame exception ( identifier + is invalid frame ) ; } else if ( frame size = = 0 ) { logger . warning ( get logging filename ( ) + : + empty frame : + identifier ) ; we dont process this frame or add to framemap becuase contains no useful information skip the two flag bytes so in correct position for subsequent frames byte buffer . get ( ) ; byte buffer . get ( ) ; throw new empty frame exception ( identifier + is empty frame ) ; } else if ( frame size > ( byte buffer . remaining ( ) - frame _ flags _ size ) ) { logger . warning ( get logging filename ( ) + : + invalid frame size larger than size before mp3 audio : + identifier ) ; throw new invalid frame exception ( identifier + is invalid frame ) ; }
try { rx java plugins . set init single scheduler handler ( init replace with immediate ) ; rx java plugins . init single scheduler ( unsafe default ) ; } finally { rx java plugins . reset ( ) ; }
html settings html settings = new html settings ( ) ; html settings . set image dir path ( inputfilepath + _ files ) ;
while ( i . has next ( ) ) { final map . entry < k , v > e = i . next ( ) ; s . write object ( e . get key ( ) ) ; s . write object ( e . get value ( ) ) ; }
write entry = this . mvcc . begin ( ) ;
search response = client ( ) . prepare search ( ) . set query ( multi match query ( the fast brown , field1 , field2 ) . cutoff frequency ( 3 ) . operator ( operator . and ) ) . get ( ) ;
big int = new big integer ( 178613588865784752580332404014434337809799306448796128931113691624 ) ;
reflection buffer . set depth buffer ( format . depth ) ;
if ( used variables cache . contains key ( variable name ) ) { return used variables cache . get ( variable name ) ; } if ( fetch all variables = = true ) { ensure variable instances initialized ( ) ; variable instance entity variable instance = variable instances . get ( variable name ) ; if ( variable instance = null ) { return variable instance ; } go up the hierarchy variable scope parent scope = get parent variable scope ( ) ; if ( parent scope = null ) { return parent scope . get variable instance ( variable name , true ) ; } return null ; } else { if ( variable instances = null & & variable instances . contains key ( variable name ) ) { return variable instances . get ( variable name ) ; } variable instance entity variable = get specific variable ( variable name ) ; if ( variable = null ) { used variables cache . put ( variable name , variable ) ; return variable ; } go up the hierarchy variable scope parent scope = get parent variable scope ( ) ; if ( parent scope = null ) { return parent scope . get variable instance ( variable name , false ) ; } return null ; }
http endpoint end2 = context . get endpoint ( http4 : www . google . com?http binding = other , http endpoint . class ) ;
timeseries query query = druids . new timeseries query builder ( ) . data source ( data _ schema . get data source ( ) ) . aggregators ( immutable list . < aggregator factory > of ( new long sum aggregator factory ( rows , rows ) ) ) . granularity ( granularities . all ) . intervals ( 0000 3000 ) . build ( ) ; array list < result < timeseries result value > > results = sequences . to list ( task . get query runner ( query ) . run ( query plus . wrap ( query ) , immutable map . < string , object > of ( ) ) , lists . < result < timeseries result value > > new array list ( ) ) ;
if ( i < tokens . length - 2 & & pos tag helper . has pos tag ( adj analyzed token readings , adj : p : . * ) & & arrays . as list ( Ð· , ÑÐ· , Ð·Ñ ) . contains ( tokens [ i + 1 ] . get token ( ) ) & & pos tag helper . has pos tag ( tokens [ i + 2 ] , ( noun | numr ) . * : v _ oru . * ) & & has overlap ignore gender ( master inflections , slave inflections ) ) { log exception ( ) ; return true ; }
throw ctxt . mapping exception ( _ value class , t ) ;
result . set coordinate system ( user query . get coordinate system ( ) ) ; result . set coordinate system reproject ( user query . get coordinate system reproject ( ) ) ; result . set start index ( user query . get start index ( ) ) ; result . set sort by ( user query . get sort by ( ) ) ; return result ;
break ; } } }
. skip ( query . get page offset ( ) ) . limit ( query . get page size ( ) ) . collect ( more collectors . to list ( ) ) ; }
parent = index column tester service . save ( parent ) ; parent saved parent = index column tester service . get parent by id ( parent . get id ( ) ) ;
buffer switcher max row count = new jtext field ( j edit . get property ( buffer switcher . max row count ) ) ; add component ( j edit . get property ( options . view . buffer switcher max rows count ) , buffer switcher max row count ) ; buffer switcher max row count . set editable ( show buffer switcher . is selected ( ) ) ;
class < ? extends catalog info > clazz = workspace info . class ;
x = x + ( int ) ( ( math . floor ( interval width * 0 . 1 f ) ) < 1 ? 1 : ( math . floor ( interval width * 0 . 1 f ) ) ) ;
return use _ predicate ;
thing thing = new thing ( ) ; thing . set name ( null ) ; s . save ( thing ) ; t . commit ( ) ; s . close ( ) ; s = open session ( ) ;
return query + + + in : + ( in path ? path : + file ) + + + repo : + login + + repo id ; }
while ( child index < parent . get child count ( ) ) { if ( filter leftover view ( parent , child index ) ) { child index + + ; } }
return ( t ) instantiate class ( class to create , constructor arg types , constructor args ) ;
files . delete ( cs folder . get ( ci . get name ( ) ) . dir ( ) ) ; resource ci folder = backup utils . dir ( cs folder , ci . get name ( ) ) ; do write ( ci , ci folder , coverage . xml ) ;
string will topic = payload . will topic ( ) ; byte [ ] will topic bytes = will topic = null ? encode string utf8 ( will topic ) : empty arrays . empty _ bytes ; byte [ ] will message = payload . will message in bytes ( ) ; byte [ ] will message bytes = will message = null ? will message : empty arrays . empty _ bytes ; if ( variable header . is will flag ( ) ) { payload buffer size + = 2 + will topic bytes . length ; payload buffer size + = 2 + will message bytes . length ; } string user name = payload . user name ( ) ;
string pairwise user id = access token . get subject ( ) ; assert . assert not equals ( pairwise user id , user . get id ( ) ) ;
assert true ( cache dfs used = dfs used ) ;
string path v = zkutil . join znode ( zkw . base znode , test multi failure v ) ;
if ( request dto = null ) {
discovery event evt = new discovery event ( this , url ) ; for ( discovery listener listener : listeners ) { listener . notify provisioning url ( evt ) ; }
list < bson > aggregate = arrays . as list ( match ( or ( eq ( scientist , darwin ) , eq ( scientist , einstein ) ) ) , group ( scientist , sum ( count , 1 ) ) ) ; object result = template . request body ( direct : aggregate , aggregate ) ; assert true ( result is not of type list , result instanceof list ) ; @ suppress warnings ( unchecked ) list < document > result list = ( list < document > ) result ;
task service . complete ( task . get id ( ) ) ; list < job > jobs = management service . create timer job query ( ) . list ( ) ;
double io rate = double . parse double ( values . next ( ) . to string ( ) ) ;
policy . set queues ( mock csqueues ( new string [ ] { a , b } , new int [ ] { 0 , 0 } , new float [ ] { 0 . 1f , 0 . 0f } , ) ) ; verify order ( policy , , new string [ ] { b , a } ) ;
establish connection ( ) ; content encoding = ; sampler . set content encoding ( content encoding ) ; post writer . set headers ( connection , sampler ) ; post writer . send post data ( connection , sampler ) ; check content type url encoded ( connection ) ;
for ( int i = 0 ; i < group by hash . get types ( ) . size ( ) ; i + + ) { assert equals ( page . get block ( i ) . get position count ( ) , 100 ) ; } assert equals ( page . get position count ( ) , 100 ) ; block assertions . assert block equals ( varchar , page . get block ( 0 ) , values block ) ; block assertions . assert block equals ( bigint , page . get block ( 1 ) , hash block ) ; }
test needed temps ( function foo ( a ) { var c ; c = 0 ; a ; } ; foo ( x ) ; , foo , empty _ string _ set ) ; }
start broker ( ) ;
skip . add ( method key ) ;
list < string > destinations = admin . persistent topics ( ) . get list ( prop - xyz use ns1 ) ; assert equals ( destinations . size ( ) , 0 ) ; assert equals ( admin . persistent topics ( ) . get partitioned topic metadata ( persistent : prop - xyz use ns1 ds2 ) . partitions , 0 ) ;
if ( fields1 . length = = 1 ) { return extract dslcontext ( records ) . select from ( table ) . where ( ( ( field < object > ) fields1 [ 0 ] ) . in ( extract values ( records , fields2 [ 0 ] ) ) ) . fetch ( ) ; }
s = new scanner ( 03 , 456 ) ; s . use locale ( locale . english ) ; try { s . next int ( 10 ) ; fail ( ) ; } catch ( input mismatch exception expected ) { } s = new scanner ( 03456 ) ;
if ( ctxt ser instanceof resolvable serializer ) { ( ( resolvable serializer ) ctxt ser ) . resolve ( this ) ; } ser = ctxt ser ;
string short command name = new file ( command name ) . get name ( ) ; return short command name + failed : + describe command error ( verbose , command line elements , env , cwd ) ;
input stream stream = new byte array input stream ( original model xml . get bytes ( utf - 8 ) ) ;
assert . assert equals ( kill bill client . get overdue state for account ( account json no tag . get account id ( ) , request options ) . get name ( ) , od1 ) ;
send message ( active window , new window event ( active window , window event . window _ deactivated , null ) ) ; if ( get global active window ( ) = null ) {
assert false ( edit log . select input streams ( edit streams , 0 , txid , 2 ) ) ;
map < string , string > dependencies = new hash map < > ( 128 ) ; for ( component metadata md : repo . component metadata map . values ( ) ) { if ( md . get dependencies ( ) = null ) dependencies . put all ( md . get dependencies ( ) ) ; } class loader cl = component metadata repo . class . get class loader ( ) ;
next tok ( ) ; if ( tok end - tok start > 1 ) { throw new atparse ex ( ) ; }
queue . add all ( 0 , children to build ) ;
inet socket address listen address = this . rpc server . get listener address ( ) ;
new pos = find sync position ( ) ; if ( new pos > = 0 ) {
assert equals ( cached content1 , index . get ( key1 ) ) ; assert equals ( cached content2 , index . get ( key2 ) ) ; assert null ( index . get ( key3 ) ) ;
assert false ( cb . allow request ( ) ) ; async result . to blocking ( ) . single ( ) ;
if ( aapt config options . use aapt cruncher = tri state . auto ) { return aapt config options . use aapt cruncher = = tri state . yes ; }
return fixed _ bytes ; }
assert . assert equals ( reader . error times ( ) , 0 ) ; assert . assert equals ( writer . aborted times ( ) , 1 ) ; assert . assert equals ( reader . get chunk count ( ) , observer . get chunk count ( ) ) ; assert . assert equals ( observer . error times ( ) , 1 ) ; assert . assert equals ( observer . get last event ( ) , on error ) ; }
mock thread thread2 = new mock thread ( ) ;
ddl = create table t ( id integer not null , num integer ) ; + alter table t add limit partition rows 6 ; + alter table t add limit partition rows 7 ; ;
map < string , string > aliases = get aliases ( parse plugins ) ;
segment s = head ;
audit query inner join audit query = audit reader factory . get ( entity manager ) . create query ( ) . for entities at revision ( customer . class , 1 ) . traverse relation ( address , join type . inner ) ;
assert xpath count ( 0 , ex : polymorphic feature [ @ gml : id = ' f3 ' ] ex : first value , doc ) ;
if ( m _ lookup type = = index lookup type . eq | | is reverse scan ( ) ) { m _ skip _ null _ predicate = null ; return ; } int search key size = m _ searchkey expressions . size ( ) ; int null expr index ;
class loader context class loader = thread . current thread ( ) . get context class loader ( ) ;
j + + ; } } else if ( j = = filter string as byte array . length - 1 ) { throw new illegal argument exception ( incorrect argument list ) ; } } } else {
final terms terms = request . get searcher ( ) . get slow atomic reader ( ) . terms ( field name ) ;
assert not live after x ( var a , b ; x : if ( b ) { b ( b ) } else { b ( b ) } ; , a ) ;
skip exit ( ) ;
if ( name . equals ( serializable . class . get name ( ) ) | | name . equals ( externalizable . class . get name ( ) ) | | name . starts with ( javax . ejb . ) | | name . starts with ( groovy . lang . ) ) { continue ; } names . add ( dot name ) ; }
throw e ; } catch ( exception e ) { log . warn ( e , metadata refresh failed , trying again soon . ) ; synchronized ( lock ) {
page . rewind ( ) ;
@ suppress warnings ( deprecation ) we ' re actually testing that the deprecated method still works byte buf client command = commands . new connect ( auth method . auth method none , ) ; channel . write inbound ( client command ) ; assert equals ( server cnx . get state ( ) , state . connected ) ; assert true ( get response ( ) instanceof command connected ) ; channel . finish ( ) ; }
task user task = task service . create task query ( ) . single result ( ) ; assert not null ( user task ) ; assert equals ( task , user task . get task definition key ( ) ) ;
if ( first instanceof modifier query node ) { modifier query node m = ( modifier query node ) first ; if ( m . get modifier ( ) = = modifier query node . modifier . mod _ not ) { { if ( true ) return new boolean query node ( arrays . < query node > as list ( m ) ) ; } } } { if ( true ) return first ; } }
bus handler . push expected events ( next event . invoice , next event . payment , next event . invoice _ payment ) ; clock . add days ( 27 ) ; assert listener status ( ) ; expected invoices . add ( new expected invoice item check ( new local date ( 2016 , 7 , 1 ) , new local date ( 2016 , 8 , 1 ) , invoice item type . recurring , new big decimal ( 249 . 95 ) ) ) ; invoices = invoice user api . get invoices by account ( account . get id ( ) , false , call context ) ; invoice checker . check invoice ( invoices . get ( 6 ) . get id ( ) , call context , expected invoices ) ; expected invoices . clear ( ) ;
. header ( rx - user , guava )
if ( shareable url = null & & post status . from post ( post ) = = post status . published ) { intent share intent = new intent ( m context , share and dismiss notification receiver . class ) ; share intent . put extra ( share and dismiss notification receiver . notification _ id _ key , notification id ) ; share intent . put extra ( intent . extra _ text , shareable url ) ; share intent . put extra ( intent . extra _ subject , post . get title ( ) ) ; pending intent pending intent = pending intent . get broadcast ( m context , 0 , share intent , pending intent . flag _ cancel _ current ) ; notification builder . add action ( r . drawable . ic _ share _ 24dp , m context . get string ( r . string . share _ action ) , pending intent ) ; }
return argb bitmap ; }
return ( mvccentry ) ctx . lookup entry ( key ) ;
recycler view . adapter root adapter = rv . get adapter ( ) ;
assert equals ( integer . max _ value , new object range ( 0 l , integer . max _ value ) . size ( ) ) ; assert equals ( integer . max _ value , new object range ( long . min _ value , long . max _ value ) . size ( ) ) ; assert equals ( integer . max _ value , new object range ( new big integer ( - 10 ) , new big integer ( long . to string ( ( long ) integer . max _ value ) + 1 l ) ) . size ( ) ) ; }
configure azure storage session ( ) ;
on view ( with id ( r . id . input validation error ) ) . check ( matches ( is displayed ( ) ) ) ; on view ( with id ( r . id . input validation success ) ) . check ( matches ( not ( is displayed ( ) ) ) ) ; }
list < persistence unit metadata holder > list puholders = new array list < persistence unit metadata holder > ( 1 ) ;
conf . set long ( dfs _ namenode _ checkpoint _ period _ key , long . max _ value ) ;
view . get scroll y ( ) + view . get padding top ( ) > view . get child at ( 0 ) . get top ( ) ; }
direct deps = new grouped list < sky key > ( ) ;
m bottom sheet behavior . set state ( bottom sheet behavior . state _ collapsed ) ; } } } ) ;
impl . scan ( bad duplicate frame socket . class ) ;
string contents = ( string ) node . get owner document ( ) . get user data ( content _ key ) ; if ( contents = = null | | contents . length ( ) < end offset ) { return null ; } boolean in attribute = false ;
if ( delete & & db datum . get status ( ) = = crawl datum . status _ db _ duplicate ) { reporter . incr counter ( indexer status , deleted ( duplicates ) , 1 ) ; output . collect ( key , delete _ action ) ; return ; }
if ( m drag object = null ) { item info drag info = m drag object . drag info ; if ( drag info instanceof shortcut info ) { component name cn = drag info . get target component ( ) ; if ( cn = null & & matcher . matches ( drag info , cn ) ) { cancel drag ( ) ; } } } }
try { listener . changed input ( ) ; } catch ( final exception exception ) { cutility functions . log exception ( exception ) ; }
assert equals ( 0 , metadata . time to next update ( now ) ) ;
string translation = translate source file ( public class test { private integer i = 1 ; private integer j = 2 ; + public int values [ ] = new int [ ] { i , j , 3 } ; } , test , test . m ) ; assert translation ( translation , [ iosint array new array with ints : ( jint [ ] ) { + [ self - > i _ int value ] , [ self - > j _ int value ] , 3 } count : 3 ] ) ; }
bldr . clear ( ) ; incoming packet . unpack message ( bldr ) ;
try { fs . cancel delegation token ( token ) ; assert . fail ( should have failed ) ; } catch ( invalid token it ) { } catch ( exception ex ) { assert . fail ( wrong exception : + ex ) ; }
if ( client connection manager = null ) { log . info ( shutting down client connection manager : + client connection manager ) ; client connection manager . shutdown ( ) ; client connection manager = null ; } super . do stop ( ) ;
long raw bucket id = ( md5 & 0x ffffffff00000000 l ) | location ;
while ( t . advance row ( ) ) { int partition = 0 ; partition = r . next int ( number of partitions ) ;
assert empty match ( Ð¿ÑÐ¾ÑÐ¸Ð»ÐµÐ¶Ð½Ð¸Ð¹ Ð¾ÑÑÐºÑÐ²Ð°Ð½Ð¾Ð¼Ñ ÑÐµÐ·ÑÐ»ÑÑÐ°Ñ ) ; assert empty match ( Ð°Ð»ÑÑÐµÑÐ½Ð°ÑÐ¸Ð²Ð½Ñ Ð¾Ð»ÑÐ³Ð°ÑÑÑÑÐ½ÑÐ¹ Ð¼Ð¾Ð´ÐµÐ»Ñ ) ; assert empty match ( Ð°Ð»ÑÑÐµÑÐ½Ð°ÑÐ¸Ð²Ð½Ñ Ð³Ð°Ð·Ð¾Ð²Ð¾Ð¼Ñ Ð²Ð°ÑÑÐ°Ð½ÑÐ¸ ) ; assert equals ( 1 , rule . match ( lang tool . get analyzed sentence ( Ð°Ð»ÑÑÐµÑÐ½Ð°ÑÐ¸Ð²Ð½Ñ Ð¾Ð»ÑÐ³Ð°ÑÑÑÑÐ½ÑÐ¹ Ð¿Ð¾ÑÑÐ´ÐºÑ ) ) . length ) ;
conf . set var ( hive conf . conf vars . hive _ metrics _ json _ file _ interval , 60000m ) ; metrics factory . init ( conf ) ;
final account account json no tag = create account no pmbundle and subscription ( ) ;
if ( type = = token . call & & ( context flags & ecf _ tail ) = 0 & & compiler env . is generate debug info ( ) & & its in try flag ) { type = icode _ tail _ call ; }
return new status ( error - + exception . get code ( ) , exception . get message ( ) ) ;
final float bottom break = min floor - bucket size ; if ( bottom break = first break & & ( sum ( first break ) - sum ( bottom break ) > cutoff ) ) { breaks . add ( bottom break ) ; } float left = first break ; boolean left set = false ;
for ( int i = 0 ; i < element index ; i + + ) consumer . accept ( cur chunk [ i ] ) ;
time zone tz1 = new support _ time zone ( - 5 * one _ hour , true ) ;
base file name mapping . put ( get base file name ( input path ) , bucket base file names ) ;
stop ( ) ;
feature types . add ( feature type name ) ;
enumeration < ? > enum from = node . children ( ) ; list < jmeter tree node > tmp = new array list < > ( ) ; while ( enum from . has more elements ( ) ) { jmeter tree node child = ( jmeter tree node ) enum from . next element ( ) ; tmp . add ( child ) ; } for ( jmeter tree node j meter tree node : tmp ) { copy . add ( j meter tree node ) ; }
final array list < detail column info > column infos = new array list < detail column info > ( ) ;
if ( to show . size ( ) > 0 ) { show options ( editor , to show , false ) ; } if ( editor = null & & error = null ) { vim plugin . show message ( message helper . message ( error , token ) ) ; vim plugin . indicate error ( ) ; }
gindex = mgmt . get graph index ( name ) ;
person um1 p1 = ( person um1 ) dao . find person ( person um1 . class , unimanytoone _ 1 ) ; assert . assert not null ( p1 ) ; p1 . set person name ( saurabh ) ; p1 . get address ( ) . set street ( brand new street ) ; dao . merge ( p1 ) ; person um1 p1 after merge = ( person um1 ) dao . find person ( person um1 . class , unimanytoone _ 1 ) ; assert . assert not null ( p1 after merge ) ; assert . assert equals ( saurabh , p1 after merge . get person name ( ) ) ; assert . assert equals ( brand new street , p1 after merge . get address ( ) . get street ( ) ) ;
out entry = new jar entry ( in entry ) ;
for ( object e : datatype . get enum constants ( ) ) if ( e . to string ( ) . equals ( value . to string ( ) ) ) return e ; throw new illegal argument exception ( no match for + value + in enum + datatype ) ; } else throw new illegal argument exception ( unexpected data type : + datatype ) ;
assert equals ( 2 , a . get abstract users manager ( ) . get num active users ( ) ) ;
namenode heartbeat service local heartbeat service = create local namenode hearbeat service ( ) ;
cmd list . add ( - - audio - track = + params . aid . get id ( ) ) ;
try { multimap . as map ( ) . remove ( sample key ) ; fail ( as map ( ) . remove ( ) succeeded on unmodifiable multimap ) ; } catch ( unsupported operation exception expected ) { } assert multimap remains unmodified ( multimap , original entries ) ; if ( multimap . is empty ( ) ) { k present key = multimap . key set ( ) . iterator ( ) . next ( ) ; try { multimap . as map ( ) . get ( present key ) . remove ( sample value ) ; fail ( as map ( ) . get ( ) . remove ( ) succeeded on unmodifiable multimap ) ; } catch ( unsupported operation exception expected ) { } assert multimap remains unmodified ( multimap , original entries ) ; try { multimap . as map ( ) . values ( ) . iterator ( ) . next ( ) . remove ( sample value ) ; fail ( as map ( ) . values ( ) . iterator ( ) . next ( ) . remove ( ) succeeded on unmodifiable multimap ) ; } catch ( unsupported operation exception expected ) { } try { ( ( collection < ? > ) multimap . as map ( ) . values ( ) . to array ( ) [ 0 ] ) . clear ( ) ; fail ( as map ( ) . values ( ) . to array ( ) [ 0 ] . clear ( ) succeeded on unmodifiable multimap ) ; } catch ( unsupported operation exception expected ) { } }
border path . add arc ( new rect f ( right - oval size , bottom - oval size , right , bottom ) , 0 , 90 ) ;
logger . error ( can ' t handle command { } on channel { } , command , channel uid ) ;
for ( odatabase document db : dbs ) { docs to compare . add ( load record ( db , server id , i , j + base count ) ) ; }
duplicated ldap user . set single attribute ( ldapconstants . email , user7 - changed @ email . org ) ; ldapstorage provider ldap fed provider = ldaptest utils . get ldap provider ( session , ldap model ) ; ldap fed provider . get ldap identity store ( ) . update ( duplicated ldap user ) ;
for ( solr document doc : docs ) { assert equals ( doc . to string ( ) , 2 , doc . size ( ) ) ; assert true ( doc . to string ( ) , doc . get field value ( val _ i ) instanceof integer ) ; assert true ( doc . to string ( ) , doc . get field value ( id ) instanceof string ) ; } for ( solr params p : arrays . as list ( params ( q , * : * , rows , 10 , fl , val _ * , subj * , id ) , params ( q , * : * , rows , 10 , fl , val _ * , fl , subj * , fl , id ) , params ( q , * : * , rows , 10 , fl , val _ * , fl , subj * , id ) ) ) { docs = assert search ( p ) ; assert equals ( p + = > + docs , 5 , docs . get num found ( ) ) ; shouldn ' t matter what doc we pick . . . for ( solr document doc : docs ) { string msg = p + = > + doc ; assert equals ( msg , 3 , doc . size ( ) ) ; assert true ( msg , doc . get field value ( val _ i ) instanceof integer ) ; assert true ( msg , doc . get field value ( subject ) instanceof string ) ; assert true ( msg , doc . get field value ( id ) instanceof string ) ; } }
value . rewind ( ) ; log invalid version ( value ) ; } } else {
provider [ ] secure random providers = security . get providers ( secure random . sha1 prng ) ;
set current page ( get current page ( ) - 1 ) ;
log . error ( error writing alert message to byte array output stream ) ;
document dom = get as dom ( oseo search?parent id = sentinel2 & cloud cover = 2 ] ) ;
vehicle builder . get wheel builder ( 3 ) . set radius ( 20 ) . set width ( 20 ) ;
stage ft stage = stage dao . most recent with builds ( pre condition . pipeline name , pre condition . ft stage ( ) ) ;
assert xpath evaluates to ( http : cf - pcmdi . llnl . gov documents cf - standard - names standard - name - table current cf - standard - name - table . html surface _ temperature , csml : point series feature [ @ gml : id = ' + id + ' ] csml : parameter @ xlink : href , doc ) ;
map < index , hive index query context > query contexts = new hash map < index , hive index query context > ( ) ;
try { template . send body ( direct : start , world ) ; fail ( should thrown an exception ) ; } catch ( exception e ) { ignore }
identity hash map map = new identity hash map ( ) ; object result ;
column names . add ( druid storage handler utils . default _ timestamp _ column ) ;
long curr scroll = view . get first visible position ( ) * child . get height ( ) - child . get bottom ( ) ;
boolean have children = get child count ( ) > 0 & & get count ( ) > 0 ; if ( have children & & m first position > 0 ) { ss . column count = m column count ; ss . column tops = m column tops ; ss . position data = m position data ; } else { ss . column count = m column count > = 0 ? m column count : 0 ; ss . column tops = new int [ ss . column count ] ; ss . position data = new sparse array < object > ( ) ; } return ss ;
long current count = os mbean . get open file descriptor count ( ) ; final string logmsg = open fds after test ( { } ) are not significantly higher than before ( { } ) ; if ( current count > initial fd count + 10 ) { consider as error log . error ( logmsg , long . value of ( current count ) , long . value of ( initial fd count ) ) ; } else { log . info ( logmsg , long . value of ( current count ) , long . value of ( initial fd count ) ) ; }
service info gwfs = new org . geoserver . wfs . wfsinfo impl ( ) ;
string host = 1 . 2 . 3 . 4 ;
if ( i = candidate & & knows ( candidate , i ) | | knows ( i , candidate ) ) { return - 1 ; }
system . out . println ( corba resource util . get text ( bootstrap . success , integer . to string ( initial port ) , file . get absolute path ( ) ) ) ; properties props = new properties ( ) ;
final fst . arc < chars ref > arc = new fst . arc < > ( ) ; int longest match ; chars ref longest output ; for ( int i = 0 ; i < sb . length ( ) ; i + + ) { arc . copy from ( first arc ) ; chars ref output = no _ output ; longest match = - 1 ; longest output = null ; for ( int j = i ; j < sb . length ( ) ; j + + ) { char ch = sb . char at ( j ) ; if ( fst . find target arc ( ch , arc , arc , bytes reader ) = = null ) { break ; } else { output = fst . outputs . add ( output , arc . output ) ; } if ( arc . is final ( ) ) { longest output = fst . outputs . add ( output , arc . next final output ) ; longest match = j ; } } if ( longest match > = 0 ) { sb . delete ( i , longest match + 1 ) ; sb . insert ( i , longest output ) ; i + = ( longest output . length - 1 ) ; } }
int pos = text . index of ( ' ' ) ;
if ( ( eq _ s _ b ( 1 , \ u00 fc ) ) ) { return false ; }
string file path = new string ( drive letter + \ \ testfile . txt ) ;
for ( int i = 0 ; i < char count ; i + + ) { current node = current node . transition ( str . char at ( i ) ) ; if ( current node = = null ) break ; }
set ( g , g , g ) ; cache . flush ( ) ; assert that ( cache . size ( ) ) . is equal to ( 10 ) ; assert absent ( a ) ; assert value ( b , b , b ) ; assert absent ( c ) ; assert value ( d , d , d ) ; assert value ( e , e , e ) ; assert value ( f , f , f ) ; }
int yyyyyy = ( ( high surrogate & 0x0003 ) < < 4 ) & 0x30 ;
string builder buffer = new string builder ( ) ;
preconditions . check not null ( tester . has id ( ) ) ;
try { get song tags ( song uris list ) ; } catch ( cannot read exception e ) {
touch up = delta > 0 ; if ( gravity = = gravity . top ) { delta = - delta ; }
this . announce = null ;
entity1 . add configs ( get config2 ( ) ) ;
itemx = doc . create element ( job name ) ;
assert true ( zktable read only . is enabled table ( zkw , table name ) ) ;
if ( test runner = null ) { test runner . cancel ( ) ; test runner = null ; } finish ( ) ;
byte [ ] expected key = { 0x04 , 0x01 , ( byte ) 0x ff , 0x00 , 0x02 , 0x04 } ; byte [ ] actual key = strategy . compute cache key ( query ) ; assert . assert array equals ( expected key , actual key ) ; segment analysis result = new segment analysis ( test segment , immutable list . of ( intervals . of ( 2011 - 01 - 12 t00 : 00 : 00 . 000 z 2011 - 04 - 15 t00 : 00 : 00 . 001 z ) ) , immutable map . of ( placement , new column analysis ( value type . string . to string ( ) , true , 10881 , 1 , preferred , preferred , null ) ) , 71982 , 100 , null , null , null , null ) ;
instant window timestamp = timestamp combiner . assign ( current window , windowing strategy . get window fn ( ) . get output time ( current input . get timestamp ( ) , current window ) ) ;
class2size . put if absent ( clazz , answer ) ;
sha1 impl . update hash ( seed , bytes , 0 , bytes . length - 1 ) ; seed length + = bytes . length ; }
get doc event bus ( ) . fire event ( event ) ;
if ( hive conf = null & & hive conf . get bool var ( conf vars . hive _ server2 _ support _ dynamic _ service _ discovery ) ) { try { remove server instance from zoo keeper ( ) ; } catch ( exception e ) { log . error ( error removing znode for this hive server2 instance from zoo keeper . , e ) ; } }
set1 = new line data set ( y vals1 , data set 1 ) ; set1 . set axis dependency ( axis dependency . left ) ;
snackbar manager . get instance ( ) . restore timeout if paused ( m manager callback ) ; break ; default :
if ( is right literal ) { return new literal restriction descr ( operator , expr descr . is negated ( ) , expr descr . get parameters ( ) , right value , literal restriction descr . type _ string ) ; }
height = measure spec . get size ( height measure spec ) ; } else {
p icon . y + = icons offset . x ; utils . draw image ( c , icon , ( int ) p icon . x , ( int ) p icon . y , icon . get intrinsic width ( ) , icon . get intrinsic height ( ) ) ;
string s = username + ( groupname = = null? : : + groupname ) ;
assert that ( count maximum backoff ( back off ) , equal to ( 0 l ) ) ; back off . reset ( ) ;
return line mesh ;
data map read only to validate = data map from string ( data string ) ;
flash info map . put ( slot , flash info ) ; }
final coverage store info store info = cat . get coverage store by name ( ir - rgb ) ; final coverage view coverage view = build rgb irview ( ) ; final catalog builder builder = new catalog builder ( cat ) ; builder . set store ( store info ) ; final coverage info coverage info = coverage view . create coverage info ( rgb _ ir _ view , store info , builder ) ; coverage info . get parameters ( ) . put ( use _ jai _ imageread , false ) ; coverage info . get dimensions ( ) . get ( 0 ) . set name ( red ) ; coverage info . get dimensions ( ) . get ( 1 ) . set name ( green ) ; coverage info . get dimensions ( ) . get ( 2 ) . set name ( blue ) ; coverage info . get dimensions ( ) . get ( 3 ) . set name ( infrared ) ; cat . add ( coverage info ) ;
for ( graph qlfield definition field definition : ( ( graph qlfields container ) root ) . get field definitions ( ) ) { for ( schema validation rule rule : rules ) { rule . check ( field definition , validation error collector ) ; } traverse ( field definition . get type ( ) , rules , validation error collector ) ; }
url = null ;
set < filter > destroyed so far = sets . new identity hash set ( ) ; for ( filter definition filter definition : filter definitions ) { filter definition . destroy ( destroyed so far ) ; } }
assert . assert equals ( get not allowed exception message ( hello for role ) , e . get cause ( ) . get message ( ) ) ;
path snapshottable path = new path ( tmp tmp - snap - test ) ;
larger = new big decimal ( temp bi , larger . scale + 1 ) ;
iblockhead server connection server conn = server . accept ( ) ;
assert . assert equals ( 0 , res . get memory size ( ) ) ;
verify ( listener , times ( 1 ) ) . notify kv state registered ( eq ( env . get job id ( ) ) , eq ( env . get job vertex id ( ) ) , eq ( expected key group range ) , eq ( banana ) , any ( kv state id . class ) ) ; keyed state handle snapshot = future util . run if not done and get ( backend . snapshot ( 682375462379 l , 4 , stream factory , checkpoint options . for checkpoint ( ) ) ) ;
thread handle . ping ( ) ; int bufsize = sock . get receive buffer size ( ) ; byte message [ ] = new byte [ bufsize ] ; datagram packet packet = new datagram packet ( message , bufsize ) ; sock . receive ( packet ) ;
get = new get ( keys [ 40 ] ) ; get . add column ( bytes _ family , qual2 ) ; result r = table . get ( get ) ; validate result ( r , qual2 , val2 ) ; table . close ( ) ;
if ( aapt config options . use aapt cruncher = tri state . auto ) { return aapt config options . use aapt cruncher = = tri state . yes ; }
if ( registered accounts . contains key ( account id ) ) return ; service registration registration = registered accounts . get ( account id ) ;
suite dispatcher dispatcher = new suite dispatcher ( m _ masterfile name ) ; suite runners = dispatcher . dispatch ( get configuration ( ) , m _ suites , get output directory ( ) , get test listeners ( ) ) ; }
return fetch value ( type , meta , is binary ) ;
string acceptance = acceptance ; string plugins = plugins ; string git plugins = git - plugins ; string cruise = cruise ; string git trunk = git - trunk ; string hg trunk = hg - trunk ; value stream map graph = new value stream map ( acceptance , null ) ;
if ( managed conn . is open ( ) ) { managed conn . open ( route , context , params ) ; } else { managed conn . set socket timeout ( http connection params . get so timeout ( params ) ) ; } try { establish route ( route , context ) ; } catch ( tunnel refused exception ex ) { if ( this . log . is debug enabled ( ) ) { this . log . debug ( ex . get message ( ) ) ; } response = ex . get response ( ) ; break ; }
if ( null pos [ 0 ] ) { output vector [ 0 ] = vector [ 0 ] > value ? 1 : 0 ; out nulls [ 0 ] = false ; } else { out nulls [ 0 ] = true ; }
final task executor gateway task executor gateway = mock ( task executor gateway . class ) ; when ( task executor gateway . request slot ( eq ( slot id ) , eq ( job id ) , eq ( allocation id ) , any string ( ) , eq ( resource manager id ) , any ( time . class ) ) ) . then return ( completable future . completed future ( acknowledge . get ( ) ) ) ; final task executor connection task executor connection = new task executor connection ( task executor gateway ) ;
hdfs . delete ( file in bar , true ) ;
mbean server . invoke ( on , enable debugger , null , null ) ; enabled = ( boolean ) mbean server . get attribute ( on , enabled ) ;
desc tag = component + . api . + type + . + element . get name ( ) ; }
if ( is native c40 ( c ) ) { char counts [ c40 _ encodation ] + = 2 . 0f 3 . 0f ; } else if ( is extended ascii ( c ) ) { char counts [ c40 _ encodation ] + = 8 . 0f 3 . 0f ; } else { char counts [ c40 _ encodation ] + = 4 . 0f 3 . 0f ; }
datanode info [ ] dinfo = namenode . get datanode report ( datanode report type . all ) ;
pair < integer , integer > pair = new pair < integer , integer > ( - 1 , - 1 ) ; return pair ; }
if ( word . trim ( ) . length ( ) = = 0 ) { white space buffer . append ( word ) ;
try { options parser . parse ( new string [ ] { - append , - update , - skipcrccheck , hdfs : localhost : 9820 source first , hdfs : localhost : 9820 target } ) ; fail ( append should fail if skip crc option is specified ) ; } catch ( illegal argument exception e ) { generic test utils . assert exception contains ( append is disallowed when skipping crc , e ) ; }
if ( tab placement = right | | selected index < 0 | | ( sel rect . x - 1 > w ) | | ( sel rect . y < y | | sel rect . y > y + h ) ) { g . draw line ( x + w - 2 , y + 1 , x + w - 2 , y + h - 3 ) ; g . set color ( dark shadow ) ; g . draw line ( x + w - 1 , y , x + w - 1 , y + h - 1 ) ; } else { break line to show visual connection to selected tab g . draw line ( x + w - 2 , y + 1 , x + w - 2 , sel rect . y - 1 ) ; g . set color ( dark shadow ) ; g . draw line ( x + w - 1 , y , x + w - 1 , sel rect . y - 1 ) ; if ( sel rect . y + sel rect . height < y + h - 2 ) { g . set color ( shadow ) ; g . draw line ( x + w - 2 , sel rect . y + sel rect . height , x + w - 2 , y + h - 2 ) ; g . set color ( dark shadow ) ; g . draw line ( x + w - 1 , sel rect . y + sel rect . height , x + w - 1 , y + h - 2 ) ; } }
m chart . highlight values ( null ) ; m chart . invalidate ( ) ;
line = line . replace all ( \ \ s + . * , ) ; line = line . replace all ( ^ . * , ) ;
web server buck event listener . build finished ( any object ( build event . finished . class ) ) ;
context runner ( ) . with property values ( spring . datasource . data : classpath : city . sql ) . run ( ( context ) - > { assert that ( context ) . has failed ( ) ; assert that ( context . get startup failure ( ) ) . is instance of ( bean creation exception . class ) ; } ) ; }
spdy session . put pending write ( stream id , new spdy session . pending write ( spdy data frame , promise ) ) ;
ks def . strategy _ options . put ( replication _ factor , 1 ) ; cassandra cli . client . system _ add _ keyspace ( ks def ) ; }
configuration props to skip compare . add ( common configuration keys public . io _ sort _ mb _ key ) ; configuration props to skip compare . add ( common configuration keys public . io _ sort _ factor _ key ) ;
string delete query = delete from person ;
final pair < string , string > shifter pair = addressing mode one generator . generate ( base offset , environment , instruction , instructions , shifter ) ; base offset = ( instruction . get address ( ) . to long ( ) * 0x100 ) + instructions . size ( ) ; final string shifter operand = shifter pair . first ( ) ;
tester . set ( node0 , new string value ( new ) ) ;
cr = client . call procedure ( trim proc , null , null , null , 2 ) ;
bolt metrics . execute tuple ( stream . get id ( ) , stream . get component name ( ) , execute latency . to nanos ( ) ) ;
local variable type info visitor . visit local variable type info ( clazz , method , code attribute , local variable type table [ index ] ) ;
if ( inprogress log segment . get first tx id ( ) = first tx id ) { future utils . set exception ( promise , new ioexception ( transaction id not as expected , + inprogress log segment . get first tx id ( ) + found , + first tx id + expected ) ) ; return ; }
set state ( call peer state . connected ) ; media handler . start ( ) ;
string file name2 = fsi . get output root ( ) + file . separator + entities + file . separator + cluster _ id + file . separator + user _ id + file . separator + flow _ name + file . separator + flow _ version + file . separator + 12345678 + file . separator + app _ id + file . separator + type2 + file . separator + id2 + file system timeline writer impl . timeline _ service _ storage _ extension ;
scheduler . shutdown ( ) ; event loop group . shutdown gracefully ( ) ; }
candidates . clear ( ) ;
store . set default ( preference constants . templates _ use _ codeformatter , true ) ;
input stream is = request . get input stream ( ) ;
view page indicator = launcher . find view by id ( r . id . page _ indicator ) ;
executor singleton . get executor ( ) . shutdown now ( ) ; }
ehandler . warning ( new transformer exception ( fmsg , ( saxsource locator ) xctxt . get saxlocator ( ) ) ) ; }
log . debugf ( map % s cluster address with % s server endpoint in address cache , cluster address , address ) ;
sink . add test ( import node02 . class ) ;
project explorer . open item by path ( project _ 1 + src main java commenttest + file for change + . java ) ;
if ( node . is decommissioned ( ) ) { continue ; }
get result = engine . get ( new get ( true , doc ) , searcher factory ) ;
char sep = get value separator ( ) ;
result [ j + + ] = ( byte ) ( ( c + shift ) & 0xff ) ;
set scrolling cache enabled ( false ) ;
if ( m property = null ) { set values ( property values holder . of object ( m property , ( type evaluator ) null , values ) ) ; } else { set values ( property values holder . of object ( m property name , ( type evaluator ) null , values ) ) ; } } else {
script = new script ( context ) { public object run ( ) { object args to pass = empty _ main _ args ; try { object args = get property ( args ) ; if ( args instanceof string [ ] ) { args to pass = args ; } } catch ( missing property exception e ) {
column projection utils . append read columns ( job clone , ts . get needed column ids ( ) , ts . get needed columns ( ) , ts . get needed nested column paths ( ) ) ;
call with expected result ( client , thea , @ ad hoc , select max ( uname ) from tu2 where points * 2 = 4 ) ; call with expected result ( client , false , @ explain , select max ( uname ) from tu2 where points * 2 = 4 ) ; call with expected result ( client , 2 , @ ad hoc , select min ( abs ( points ) ) from tu2 where abs ( points ) = 2 ) ; call with expected result ( client , true , @ explain , select min ( abs ( points ) ) from tu2 where abs ( points ) = 2 ) ;
assert . assert true ( case b . add ( 2 ) ) ;
job job = new job ( util . get configuration ( ) ) ;
c = get random ( ' \ u aa60 ' , ' \ u aa7 f ' ) ;
final string prefix = i direction = = direction . out ? connection _ out _ prefix : connection _ in _ prefix ; if ( i class name = = null | | i class name . is empty ( ) | | i class name . equals ( orient edge type . class _ name ) ) return prefix ; return prefix + i class name ;
cmd . add ( - dtests . nrtreplication . primary tcpport = + primary tcpport ) ; my primary gen = last primary gen ; } else {
checkpoint coordinator coord = new checkpoint coordinator ( jid , 600000 , 600000 , 0 , integer . max _ value , externalized checkpoint settings . none ( ) , new execution vertex [ ] { vertex1 , vertex2 } , new execution vertex [ ] { vertex1 , vertex2 } , new execution vertex [ ] { vertex1 , vertex2 } , new standalone checkpoint idcounter ( ) , new standalone completed checkpoint store ( 1 ) , null , executors . direct executor ( ) , shared state registry . default _ factory ) ; assert equals ( 0 , coord . get number of pending checkpoints ( ) ) ;
t = check assignment ( tt1 , attempt _ test _ 0002 _ m _ 000002 _ 0 on tt1 ) ;
file util . fully delete contents ( dfs dir ) ; dfs dir . delete ( ) ; backup dir . rename to ( dfs dir ) ;
no successor ( ) ; }
ball _ geo . add control ( ball _ phy ) ;
private static producer template producer template ( injection point ip , @ any instance < camel context > instance , cdi camel extension extension ) { return get qualifier by type ( ip , uri . class ) . map ( uri - > producer template from uri ( ip , instance , extension , uri ) ) . or else get ( ( ) - > default producer template ( ip , instance , extension ) ) ;
thread auditor . thread handle thread handle = sip stack . get thread auditor ( ) . add current thread ( ) ; while ( true ) { event wrapper event wrapper = null ; linked list events to deliver ; synchronized ( this . event mutex ) {
raw . write long little endian ( long byte , 0 , size of central dir ) ;
splits . add ( max val ) ; }
string folder name = gtask string utils . miui _ folder _ preffix ; if ( sql note . get id ( ) = = notes . id _ root _ folder ) folder name + = gtask string utils . folder _ default ; else if ( sql note . get id ( ) = = notes . id _ call _ record _ folder ) folder name + = gtask string utils . folder _ call _ note ; else folder name + = sql note . get snippet ( ) ; iterator < map . entry < string , task list > > iter = m gtask list hash map . entry set ( ) . iterator ( ) ;
combiner . add file ( build _ data _ filename , date , build info ) ; }
em1 . get transaction ( ) . begin ( ) ;
mini dfscluster . builder builder = new mini dfscluster . builder ( cfg ) ;
p . next token ( ) ; settable bean property prop = _ bean properties . find ( prop name ) ; if ( prop = null ) { normal case try { builder = prop . deserialize set and return ( p , ctxt , builder ) ; } catch ( exception e ) { wrap and throw ( e , builder , prop name , ctxt ) ; } continue ; }
boolean enable layer = left screen < = i & & i < = right screen & & should draw child ( layout ) ; layout . enable hardware layer ( enable layer ) ; } }
current thread . set context class loader ( host class loader ) ;
write filter ( true ) ; break ; } } catch ( ioexception e ) {
filter ( spi extensions , filters , result ) ;
for ( int i = 0 ; i < num buffers ; i + + ) { final buffer buffer = create buffer ( ) ; int size = get next multiple of ( get random number in range ( min buffer size , buffer _ size ) , 4 ) ; buffer . set size ( size ) ; current number = fill buffer with ascending numbers ( buffer , current number ) ; writer . write block ( buffer ) ; }
student big integer student min = new student big integer ( ) ;
if ( is transactional ) { annotation metadata builder transactional annotation = new annotation metadata builder ( spring java type . transactional ) ; method builder . add annotation ( transactional annotation ) ; }
if ( crypto aes enable & & crypto output stream = null ) { return crypto output stream ; }
store . store or update amrmtoken secret manager ( null , false ) ; assert equals ( rmstate store should have been in fenced state , true , store . is fenced state ( ) ) ; store . close ( ) ; }
return arrays . equals ( other , address ) ;
if ( ( item names . length = item descriptions . length ) | | ( item names . length = item types . length ) ) { throw new illegal argument exception ( array arguments item names [ ] , item descriptions [ ] and item types [ ] + should be of same length ( got + item names . length + , + item descriptions . length + and + item types . length + ) . ) ; }
versioned = ( file ) get servlet context ( ) . get attribute ( servlet context . tempdir ) ; config base = new file ( context . get catalina base ( ) , conf ) ;
modifier . rollback ( ) ;
for ( int i = 0 ; i < prev token maps . length ; i + + ) for ( int j = 0 ; j < tokens . length ; j + + ) prev token maps [ i ] . put ( tokens [ j ] , finder tags [ i ] [ j ] ) ; for ( int i = 0 ; i < finders . length ; i + + ) { int start = - 1 ; list < span > names = new array list < span > ( 5 ) ;
gl . gl disable ( gl10 . gl _ texture _ 2 d ) ;
geo server loader . set legacy ( use legacy data directory ( ) ) ;
verify ( session ) . rollback ( ) ; verify ( connection ) . set exception listener ( this . container ) ; verify ( connection ) . start ( ) ; }
for ( long l = 0 ; l < num to skip ; + + l ) { reader . advance ( ) ; }
int cy = oy - wcfg . sponge radius - 1 ;
fire lifecycle event ( after _ memberunregister _ event , member ) ;
power mockito . do throw ( new ioexception ( ) ) . when ( http json client ) . delete ( mockito . any int ( ) ) ;
test = db . person . total size ( ) ; count = em . create native query ( test , person mongo . class ) . get result list ( ) ; assert . assert not null ( count ) ; test = db . person . distinct ( \ person _ name \ ) ;
handle h = new handle ( nalle , s , null ) { @ override public void run handle ( ) { } @ override public boolean equals ( object o ) { return true ; } @ override public int hash code ( ) { return 0 ; } } ; assert equals ( 0 , s . purge ( ) ) ; s . register ( h ) ; s . remove ( nalle ) ; s . register ( h ) ; h . cancel ( ) ;
ast name node = get first child ( ) ; ast expr list node = name node . get next sibling ( ) ; initialize method node ( name node , in select ) ;
int dx = k + downsampled ix ;
stmt . execute ( drop generator survey _ gen _ id ; ) ; } catch ( sqlexception e ) {
for ( long i = 1 ; i < num entries ; + + i ) { log . info ( iteration : + i ) ; dbus event key key = new dbus event key ( rng utils . random long ( ) ) ; string value = rng utils . random string ( value length ) ; dbuf . start events ( ) ; long ts = time stamp + ( i * 1200 * 1000 ) ; assert true ( dbuf . append event ( key , p partition id , l partition id , ts , src id , schema id , value . get bytes ( charset . default charset ( ) ) , false ) ) ; test data map . put ( i , new key value ( key , value ) ) ; dbuf . end events ( current scn + i ) ; } min dbus event buffer scn = dbuf . get min scn ( ) ;
config = new facets config ( ) ; config . set index field name ( int , facets . int ) ; config . set multi valued ( int , true ) ; config . set index field name ( float , facets . float ) ; config . set multi valued ( float , true ) ; random index writer writer = new random index writer ( random ( ) , dir ) ;
_ context . stat manager ( ) . add rate data ( in net pool . duplicate , 1 ) ; _ context . message history ( ) . dropped other message ( message body , ( from router = null ? from router . calculate hash ( ) : from router hash ) ) ; _ context . message history ( ) . message processing error ( message body . get unique id ( ) , message body . get class ( ) . get simple name ( ) , duplicate expired ) ; return - 1 ; } else {
program class . u2interfaces count = data input . read unsigned short ( ) ; program class . u2interfaces = new int [ program class . u2interfaces count ] ; for ( int index = 0 ; index < program class . u2interfaces count ; index + + ) { program class . u2interfaces [ index ] = data input . read unsigned short ( ) ; }
assert equals ( option , config . get option ( accumulate function option . class , avg ) ) ;
f = ( field ) doc . get field value ( ssto ) ; assert not null ( should have ssto , f ) ; assert not null ( should have string value , f . string value ( ) ) ; assert null ( should not have token stream value , f . token stream value ( ) ) ; f = ( field ) doc . get field value ( sind ) ; assert not null ( should have sind , f ) ; assert null ( should not have string value : ' + f . string value ( ) + ' , f . string value ( ) ) ; assert not null ( should have token stream value , f . token stream value ( ) ) ; doc = process add ( chain , doc ( f ( id , 2 ) , f ( title , title [ 1 ] ) , f ( teststop , teststop [ 1 ] ) , f ( nonexistent , foobar ) , f ( ssto , teststop [ 1 ] ) , f ( sind , teststop [ 1 ] ) ) ) ; assert true ( title should be a field , doc . get field value ( title ) instanceof field ) ;
if ( i > 0 ) { out . write ( bytes , 0 , i ) ; }
byte [ ] expected = b string . get bytes ( charset util . iso _ 8859 _ 1 ) ; byte [ ] actual = new ascii string ( b string ) . to byte array ( ) ; assert array equals ( expected , actual ) ; }
run test ( generic test utils . get method name ( ) , false , 1 , 0 ) ;
if ( num fragments read from subscription < ( num messages to send 8 ) ) { num fragments read from subscription + = subscription . poll ( fragment handler subscription , 10 ) ; } else if ( is subscription closed ) { subscription . close ( ) ; is subscription closed = true ; } }
assert equals ( 0 , this . channel1 . send ( write buf , datagram socket1 address ) ) ;
try ( xcontent parser parser = get parser ( explanation ) ) { verify shard info ( parser , true , include disk info , shard routing state . unassigned ) ; parser . next token ( ) ; assert equals ( can _ allocate , parser . current name ( ) ) ; parser . next token ( ) ; assert equals ( allocation decision . no _ valid _ shard _ copy . to string ( ) , parser . text ( ) ) ; parser . next token ( ) ; assert equals ( allocate _ explanation , parser . current name ( ) ) ; parser . next token ( ) ; assert equals ( cannot allocate because all found copies of the shard are either stale or corrupt , parser . text ( ) ) ; verify stale shard copy node decisions ( parser , 2 , collections . singleton ( restarted node ) ) ; }
m scroll area offset = d . drag view . get drag region width ( ) 2 - d . x offset ; }
mock rm rm1 = new mock rm ( conf ) ; rm1 . start ( ) ; mock memory rmstate store mem store = ( mock memory rmstate store ) rm1 . get rmstate store ( ) ; mock nm nm1 = new mock nm ( 127 . 0 . 0 . 1 : 1234 , 15120 , rm1 . get resource tracker service ( ) ) ; nm1 . register node ( ) ;
return true ; } } ) ; fast item adapter . with event hook ( new radio button sample item . radio button click event ( ) ) ;
if ( ( get name ( ) = = null ) | | constants . bean _ key . equals ( get name ( ) ) ) { super . set label property ( nested property helper . get adjusted property ( request , original label property ) ) ; } else { super . set label property ( original label property ) ; }
string json = ( string ) mbean server . invoke ( on , explain endpoint json , new object [ ] { log : foo?group delay = 2000 & group size = 5 , true } , new string [ ] { java . lang . string , boolean } ) ;
result . multiply ( gen array [ i + 64 ] . a ) ;
file test war deep = new file ( new file ( war ) , os . separators ( deep ref ) ) ; uri uri = uri . create ( jar : + test war deep . to uri ( ) . to asciistring ( ) + other file ) ; assert expand uri ( jar : { war . uri } deep ref other file , uri ) ; }
url url = new url ( http : 127 . 0 . 0 . 1 : + shuffle handler . get config ( ) . get ( shuffle handler . shuffle _ port _ config _ key ) + map output?job = job _ 12345 _ 1 & reduce = 1 & map = attempt _ 12345 _ 1 _ m _ 1 _ 0 ) ; http urlconnection conn = ( http urlconnection ) url . open connection ( ) ; conn . set request property ( shuffle header . http _ header _ name , shuffle header . default _ http _ header _ name ) ; conn . set request property ( shuffle header . http _ header _ version , shuffle header . default _ http _ header _ version ) ; conn . connect ( ) ; data input stream input = new data input stream ( conn . get input stream ( ) ) ; assert . assert equals ( http urlconnection . http _ ok , conn . get response code ( ) ) ; assert . assert equals ( close , conn . get header field ( http header . connection . as string ( ) ) ) ; shuffle header header = new shuffle header ( ) ; header . read fields ( input ) ; input . close ( ) ; shuffle handler . stop ( ) ;
assert collection equals ( immutable set . of ( x ) , mgr . get labels on node ( node id ) ) ; rm . stop ( ) ;
assert null ( knxcore type mapper . to type ( ) should return null ( required data length too short ) , test to type ( dpt , new byte [ ] { } , decimal type . class ) ) ;
return original ; }
authentication run as = this . run as manager . build run as ( authenticated , object , attributes ) ; if ( run as = = null ) { if ( debug ) { logger . debug ( run as manager did not change authentication object ) ; } no further work post - invocation return new interceptor status token ( security context holder . get context ( ) , false , attributes , object ) ; } else { if ( debug ) { logger . debug ( switching to run as authentication : + run as ) ; } security context orig ctx = security context holder . get context ( ) ; security context holder . set context ( security context holder . create empty context ( ) ) ; security context holder . get context ( ) . set authentication ( run as ) ; need to revert to token . authenticated post - invocation return new interceptor status token ( orig ctx , true , attributes , object ) ; }
list < file info > nested files info = sqlparser . parse file statement ( file info , line ) ;
m app widget manager . start config activity ( app widget id , this , m app widget host , request _ create _ appwidget ) ;
app widget host . delete app widget id ( new widget ids [ i ] ) ;
case security _ commands _ supported _ report : handled by zwave security command class initialization
if ( m web chrome client = = null ) { return ; }
alphabetic index . immutable index en = create index ( locale . english ) ;
if ( shared context = null ) shared context . close ( ) ;
assert false ( target remote cache . contains key ( 5 ) ) ; assert false ( target remote cache . contains key ( 1 ) ) ; assert equals ( 4 , target remote cache . get ( 4 ) ) ; assert equals ( changed , target remote cache . get ( 8 ) ) ; assert equals ( new value , target remote cache . get ( new key ) ) ;
extension config config = extension config . parse ( econf ) ;
if ( prev cached block = null ) { cached block = prev cached block ; } else { cached blocks . put ( cached block ) ; log . trace ( added block { } to cached blocks , cached block ) ; }
pattern pattern = pattern . compile ( p . column _ name _ filter ) ; matcher m = pattern . matcher ( dummy ) ; for ( int column = 0 ; column < all _ col _ names . length ; column + + ) { m . reset ( all _ col _ names [ column ] ) ; if ( m . matches ( ) ) keep _ indexes . add ( column ) ; } } else {
advised . absquatulate ( ) ; assert equals ( 1 , nop . get count ( ) ) ; advised . set age ( new age ) ; assert equals ( new age , advised . get age ( ) ) ;
set indexed property type ( find indexed property type ( read indexed , write indexed ) ) ; }
response = new mock http servlet response ( ) ; services . logout ( request , response , null ) ;
return a . get declaring class ( ) . get package ( ) . equals ( b . get declaring class ( ) . get package ( ) ) ;
get activity ( ) . finish ( ) ; } } else {
{ m lbrac ( ) ; } break ;
final int k max ms = 1600 ;
if ( configuration . is sync configuration ( ) ) { throw new unsupported operation exception ( compacting is not supported yet on synced realms . see https : github . com realm realm - core issues 2345 ) ; } return base realm . compact realm ( configuration ) ; }
split configuration filter . from filename suffix ( abc - def _ ghi _ jkl ) ,
linear layout . layout params params = new linear layout . layout params ( linear layout . layout params . match _ parent , 1 ) ; params . set margins ( 0 , ( int ) ( 8 * density ) , 0 , ( int ) ( 8 * density ) ) ; sections . add view ( view , params ) ;
text [ ] elements = { new text ( zero ) , new text ( one ) , new text ( two ) } ;
set scale2 = set scale1 . set scale ( 1 , big decimal . round _ floor ) ;
assert that ( line . get sections ( 3 ) . get option list ( ) . get option count ( ) ) . is equal to ( 2 ) ;
application id app id2 = builder utils . new application id ( 100 , 2 ) ; application attempt id app attempt id2 = builder utils . new application attempt id ( app id2 , 1 ) ; rmapp attempt metrics attempt metric2 = new rmapp attempt metrics ( app attempt id2 , rm . get rmcontext ( ) ) ;
for ( int i = 0 ; i < num relevant fields ; i + + ) { if ( prod1 . get order ( i ) = prod2 . get order ( i ) ) { return false ; } } return true ;
assert equals ( 1 . 0 + ( char ) 1 , exec ( double x = 1 ; char y = 1 ; return x + y ; ) ) ;
test same ( var a = { x : 1 } ; f ( a . x ) ; ) ; }
short chain = new array list < x509 certificate > ( ) ; short chain . add ( root ) ; long chain = new array list < x509 certificate > ( ) ; long chain . add ( server ) ;
return m top view index ;
if ( first chunk offset > 0 ) { data file channel . position ( first chunk offset ) ; long checksum skip = ( first chunk offset bytes per checksum ) * checksum size ; note block in stream is seeked when created below if ( checksum skip > 0 ) { checksum file channel . position ( checksum skip + checksum file channel . position ( ) ) ; } } last chunk offset = first chunk offset ;
field [ ] fields = activity . get class ( ) . get declared fields ( ) ;
assert that ( measure1 . equals ( measure2 ) ) . is true ( ) ; assert that ( measure2 . equals ( measure1 ) ) . is true ( ) ; assert that ( measure1 . hash code ( ) ) . is equal to ( measure2 . hash code ( ) ) ;
incr id mapping . clear name maps ( ) ; integer rid = id + 10000 ; string smap str = gid + rid + + id ;
pending unencrypted writes . release and fail all ( ctx , sslengine _ closed ) ; return ;
m packages . clear ( ) ; } } }
if ( room . get user role ( ) . get role index ( ) > = 60 & & role index < 50 ) { this . add ( this . ban item ) ; } this . add separator ( ) ;
hoarder = ( hoarder ) s . merge ( hoarder ) ; assert equals ( 1 , hoarder . get items ( ) . size ( ) ) ; assert same ( hoarder . get favorite item ( ) , hoarder . get items ( ) . iterator ( ) . next ( ) ) ; assert equals ( item1 . get id ( ) , hoarder . get favorite item ( ) . get id ( ) ) ; assert equals ( item1 . get category ( ) , hoarder . get favorite item ( ) . get category ( ) ) ; s . get transaction ( ) . commit ( ) ; s . close ( ) ; s = open session ( ) ;
double tmp = x1 ; x1 = y1 - y0 ; y1 = tmp - y0 ; y0 = - y0 ; }
final comparator < cursor > comparator = sort _ comparators . get ( sort type ) ; if ( sort ascending ) { chain . add ( comparator ) ; } else { chain . add ( new reverse comparator < > ( comparator ) ) ; }
icq tester agent c list init tester agent = new icq tester agent ( system . get property ( tested _ impl _ user _ id _ prop _ name , null ) ) ;
holder . set class name ( servlet _ class ) ; context . get meta data ( ) . set origin ( name + . servlet . servlet - class , descriptor ) ; break ; } case web xml : case web defaults : case web override : {
s = new scanner ( 23 , 456 23 , 456 ) ;
if ( ( ch = in stream . read ( ) ) = = ' \ r ' ) {
queried relation t1 = ( ( queried relation ) mss . sources ( ) . get ( t3 . t1 ) ) ;
store1 . load ( ) ;
return rect arr2 [ 2 ] > rect arr1 [ 0 ] ; }
inflater output stream ios = new inflater output stream ( os ) ; ios . write ( compressed bytes , 0 , length ) ; string result = new string ( os . to byte array ( ) ) ;
notification service . register default notification for event ( dialing , new sound notification action ( sound properties . dialing , - 1 , false , true , false ) ) ;
bootstrap . bind ( new inet socket address ( port ) ) ;
if ( from group position = to group position ) { if ( from position < to position ) { mod to child position = to child position + 1 ; } else { mod to child position = to child position ; } }
test ( function foo ( ) { return 3 ; for ( ; y = = 4 ; ) { x + + ; return ; x = 4 } } , function foo ( ) { return 3 } ) ;
public static url get resource ( string resource name ) { class loader loader = more objects . first non null ( thread . current thread ( ) . get context class loader ( ) , resources . class . get class loader ( ) ) ; url url = loader . get resource ( resource name ) ; check argument ( url = null , resource % s not found . , resource name ) ; return url ;
if ( field name = null & & string utils . is blank ( field name ) ) { return entity label ; }
int expr pos = lambda form . arity ( ) ;
log . warn ( msg ) ;
throw wrapper . bad contact info list factory ( ex , param ) ; } } } ; return op ; }
if ( input . body ( ) = null & & input . body ( ) . content length ( ) = 0 ) { buffer body = new buffer ( ) ; input . body ( ) . write to ( body ) ; result . write utf8 ( body . sha256 ( ) . hex ( ) ) ; } else { result . write utf8 ( empty _ string _ hash ) ; } return result ;
test determination ( db2 my special platform , testing dialects . my special db2 dialect . class , resolvers ) ; try { test determination ( error database1 , void . type , resolvers ) ; fail ( ) ; } catch ( hibernate exception e ) { }
batch . draw ( tex , 0 , 0 ) ; batch . end ( ) ;
throw new ioexception ( npe ) ; }
min node . left . right = min node . right ;
if ( mapmsg . get msg type ( ) = = map message . msg _ init ) { mapmsg . set primary ( channel . get local member ( false ) ) ; return mapmsg ; }
host maps = new linked hash set < task in progress > ( ) ; running map cache . put ( node , host maps ) ; } host maps . add ( tip ) ; node = node . get parent ( ) ; } }
mat = new material ( asset manager , common mat defs misc unshaded . j3md ) ; mat . set texture ( color map , asset manager . load texture ( interface logo monkey . jpg ) ) ; geometry ground = new geometry ( ground , new quad ( 50 , 50 ) ) ; ground . set local rotation ( new quaternion ( ) . from angle axis ( - fast math . half _ pi , vector3f . unit _ x ) ) ; ground . set local translation ( - 25 , - 1 , 25 ) ; ground . set material ( mat ) ; root node . attach child ( ground ) ;
kbuilder . add ( resource factory . new class path resource ( test _ i18n person _ utf8 _ for test new class path resource . drl , get class ( ) ) , resource type . drl ) ; if ( kbuilder . has errors ( ) ) { fail ( kbuilder . get errors ( ) . to string ( ) ) ; } internal knowledge base kbase = knowledge base factory . new knowledge base ( ) ;
string tokenizer tokenizer = new string tokenizer ( extmap attr , ) ;
recipient address = p . get address ( ) ;
if ( write thread . exception = null ) { throw write thread . exception ; }
string [ ] pairs ; try { pairs = new string ( my buffer , charset ) . split ( \ \ & ) ; } catch ( unsupported encoding exception e ) { should not happen throw new runtime exception ( e ) ; } for ( int i = 0 ; i < pairs . length ; i + + ) { parse pair ( pairs [ i ] ) ; }
if ( ( bytes [ 1 ] & 0xc0 ) = 0x00 ) bits 6 + 7 throw new address format exception ( bits 0x40 and 0x80 must be cleared for ec - multiplied keys . ) ; ec multiply = true ; } else {
activities logger . app . record skipped app activity without allocation ( activities manager , node , application , application . get priority ( ) , activity diagnostic constant . skipped _ all _ priorities ) ;
string old col name = ast . get child ( 0 ) . get text ( ) ; string new col name = ast . get child ( 1 ) . get text ( ) ; string new type = get type string from ast ( ( astnode ) ast . get child ( 2 ) ) ; astnode constraint child = null ; int child count = ast . get child count ( ) ; for ( int i = 3 ; i < child count ; i + + ) { astnode child = ( astnode ) ast . get child ( i ) ; switch ( child . get token ( ) . get type ( ) ) { case hive parser . string literal : new comment = unescape sqlstring ( child . get text ( ) ) ; break ; case hive parser . tok _ altertable _ changecol _ after _ position : flag col = unescape identifier ( child . get child ( 0 ) . get text ( ) ) ; break ; case hive parser . kw _ first : first = true ; break ; case hive parser . tok _ cascade : is cascade = true ; break ; case hive parser . tok _ restrict : break ; default : constraint child = ( astnode ) child ; } } list < sqlprimary key > primary keys = null ; list < sqlforeign key > foreign keys = null ; list < sqlunique constraint > unique constraints = null ;
fact . start request ( ) ;
assert . assert true ( hashcodes . size ( ) > size * 0 . 95 ) ;
throw new azure exception ( file + f + has a parent directory + parent path + which is also a file . can ' t resolve . ) ;
if ( password = null & & password . equals ( ) ) { session . set password ( password ) ; } session . set config ( strict host key checking , no ) ; session . set config ( preferred authentications , publickey , keyboard - interactive , password ) ; session . set server alive interval ( server _ alive _ interval ) ; session . connect ( session _ timeout ) ; add pub key ( host system , session , app key . get public key ( ) ) ;
m iv top = top ; m iv right = right ; m iv bottom = bottom ; m iv left = left ; } }
headers . put ( camel google drive . comment id , comment2 . get comment id ( ) ) ; try { final com . google . api . services . drive . model . comment result4 = request body and headers ( direct : get , null , headers ) ; assert true ( should have thrown an exception . , false ) ; } catch ( exception e ) { e . print stack trace ( ) ; }
mock am am1 = rm . send amlaunched ( app1 . get current app attempt ( ) . get app attempt id ( ) ) ; am1 . register app attempt ( ) ; rm . wait for state ( app1 . get application id ( ) , rmapp state . killed ) ; assert . assert true ( application killed before lifetime value , ( system . current time millis ( ) - app1 . get submit time ( ) ) > 10000 ) ; map < application timeout type , string > update timeout = new hash map < application timeout type , string > ( ) ;
navi logger . info ( error : process manager could not get thread . exception % s , e ) ;
if ( filter type = null ) { abstract expression predicate = ( ( abstract scan plan node ) dml node ) . get predicate ( ) ; assert not null ( predicate ) ; assert equals ( filter type , predicate . get expression type ( ) ) ; assert true ( predicate . has any subexpression of class ( select subquery expression . class ) ) ; }
string error code = parse error code ( node ) ; if ( error code = = null | | error code . equals ( entity already exists ) ) return null ; entity already exists exception e = ( entity already exists exception ) super . unmarshall ( node ) ; return e ; }
if ( ( c = ' e ' ) & & ( c = ' e ' ) ) throw new number format exception ( ) ;
if ( w . get tragic exception ( ) = = null ) { w . close ( ) ; } w = null ;
assert equals ( big integer math . log10 ( value of ( x ) , mode ) , int math . log10 ( x , mode ) ) ; } }
task task = task service . create task query ( ) . task assignee like ( % \ \ % % ) . single result ( ) ;
folds . hide ( start , end ) ; if ( is line visible ( first line . get physical line ( ) ) ) { int first visible = get first visible line ( ) ; if ( first line . get physical line ( ) < first visible ) { first line . set physical line ( first visible ) ; first line . set scroll line ( 0 ) ; } else { first line . set physical line ( get prev visible line ( first line . get physical line ( ) ) ) ; first line . move scroll line ( - get screen line count ( first line . get physical line ( ) ) ) ; } }
test same ( * * @ constructor * function foo ( ) { } + var f = new foo ( ) ; + * * @ type { number } * f . bar = 4 ; ) ;
if ( log . should log ( log . debug ) ) log . debug ( msg . get unique id ( ) + : no records matching our hash was found ) ; return null ;
new tune direction = step direction . increase _ block _ cache _ size ;
string name to lookup = original bean name ( name ) ;
update base matrix ( image view . get drawable ( ) ) ;
map < string , declaration > inner declarations = a build stack instanceof group element ? ( ( group element ) a build stack ) . get inner declarations ( consequence name ) : a build stack . get inner declarations ( ) ;
handle bad request ( bad request , only rtmpt supported . , resp ) ;
return null ; } finally {
fragment f = m pager adapter . get item at ( i ) ; if ( f = = null ) { continue ; } view view = f . get view ( ) ;
map service map service = get node engine impl ( member ) . get service ( service _ name ) ; map < string , local map stats > stats = ( ( statistics aware service ) map service ) . get stats ( ) ; local map stats local map stats = stats . get ( map name ) ;
tomcat . add servlet ( ctx , tester servlet , new tester servlet ( ) ) ; ctx . add servlet mapping decoded ( foo , tester servlet ) ; tomcat . start ( ) ;
vertx . deploy verticle ( com . mycompany . my order processor verticle ) ;
return range ( ) ; }
if ( this . class name based index . is empty ( ) ) { transformer = find class based transformer ( class loader , class internal name , class metadata wrapper ) ; if ( transformer = null ) { return transformer ; } }
try { get song tags ( song uris list ) ; } catch ( cannot read exception e ) {
with terminal sized ( 3 , 2 ) . enter string ( \ 033 [ b ) . assert lines are ( , ) ;
int buf size = serv . get attribute ( domain socket . receive _ buffer _ size ) ;
if ( ( ancestor name . equals ( type . get name ( ) ) ) & & ( ancestor ns = null & & ancestor ns . equals ( type . get namespace ( ) ) ) | | ( ( type . get namespace ( ) = = null & & ancestor ns = = null ) ) ) { return true ; }
vector contacts = get targets ( msg ) ; if ( contacts . is empty ( ) ) { print log ( no target found , message discarded , log level . high ) ; if ( msg . is ack ( ) ) sip _ provider . send message ( message factory . create response ( msg , 404 , sip responses . reason of ( 404 ) , null ) ) ; return ; } print log ( message will be redirect to all user ' s contacts , log level . medium ) ;
catch ( java . io . ioexception e ) { catch it and then throw it immediately so that the finally { } block is called for cleanup . throw e ;
if ( exponent = i - 1 ) { break ; } else if ( i = = len - 1 ) { - - i ; break ; } continue ; case ' 0 ' : case ' 1 ' : case ' 2 ' : case ' 3 ' : case ' 4 ' :
for ( int i = 0 ; i < count ; i + + ) { s . record ( 100 ) ; sum + = 100 ; time . sleep ( cfg . time window ms ( ) ) ; assert equals ( sum , total metric . value ( ) , eps ) ; }
tuple index hash table . full fast iterator iter = new tuple index hash table . full fast iterator ( table ) ;
if ( shortcut satisfy outbound request ( bytes out ) ) return _ noop ;
final interesting properties in props = this . partial solution . get interesting properties ( ) . clone ( ) ;
default bindings _ . add ( new pair < key sequence , app command binding > ( keys , binding ) ) ;
extend with node ( target ) ; } else { type element npe element = elements . get type element ( java . lang . null pointer exception ) ; extend with node with exception ( target , npe element . as type ( ) ) ; }
list < string > return value = new linked list < string > ( ) ;
long filter = 0 ; int size = chars . cardinality ( ) ; boolean contains zero = chars . get ( 0 ) ;
record store . start loading ( ) ;
test advance past next sync marker at ( 1000 , 1000 ) ;
system . arraycopy ( _ data , length , _ data , 0 , _ pos - length ) ;
r2 . set pending ( resources . create resource ( 100 * gb ) ) ;
public void doco examples97 _ 10 ( ) throws exception { query ( ) . from ( log , payment ) . where ( log . item . instance of ( payment . class ) , log . item . id . eq ( payment . id ) ) . parse ( ) ;
final recording byte channel raw data = new recording byte channel ( ) ; final pack stream . packer packer = new pack stream . packer ( new buffered channel output ( raw data ) ) ; packer . pack struct header ( 2 , run . signature ( ) ) ;
final management resource registration root registration = subsystem . register subsystem model ( messaging subsystem root resource definition . instance ) ;
final health check health check = exec health check . of ( test , - e , file ) ;
log . trace ( adding { } as candidate at level { } , candidate , candidate diff ) ; candidates . put ( candidate diff , candidate ) ; } else {
for ( int i = 0 ; i < iterations ; i + + ) { callables . add ( new test compatibility singleton factory callable ( ) ) ; }
load features ( features - linear . json ) ;
toggle ( ) ;
float other val = axis values . get ( other hash ) ; if ( other val = null & & other val > effective dead zone ) { invoke actions ( other hash , false ) ; } invoke analogs and actions ( hash , value , effective dead zone , true ) ;
lr . add field ( click time , new log field ( log field . type _ string , lp . get next cb ( ) ) ) ;
write enum with null ( dest , sender status result ) ;
scope id = 0 ;
if ( privacy service . get client ( ) = = null ) return ; if ( replacing ) {
stream of ( text file , output ) . for each ( regex - > { pattern pat = pattern . compile ( regex ) ; matcher mat = pat . matcher ( output ) ; assert true ( output [ \ n + output + ] \ n contains regex match : + pat . pattern ( ) , mat . find ( ) ) ; } ) ;
master . balance switch ( false ) ; final zoo keeper watcher zkw = new zoo keeper watcher ( conf , table - creation , null ) ;
case swipeable . result _ canceled :
assert equals ( total list , p list ) ;
assert array equals ( text utils . split ( abc , def , , ) , new string [ ] { abc , def } ) ;
no successor ( ) ; }
if ( transparent = - 1 ) { trans index = find closest ( transparent ) ; } }
if ( is workset iteration ) { workset update output collector = ( workset update output collector < ot > ) output collector ; }
if ( this . item labels visible list = null ) { clone . item labels visible list = ( boolean list ) this . item labels visible list . clone ( ) ; }
tmp str = tmp str . replace all ( & , & amp ; ) . replace all ( \ , & quot ; ) . replace all ( ' , & apos ; ) . replace all ( < , & lt ; ) . replace all ( > , & gt ; ) ;
assert true ( temp dir . exists ( ) ) ; file dumb file = new file ( temp dir2 , dumb file ) ;
case so _ broadcast :
int hash = file . index of ( ) ;
if ( is supported ( return type ) ) { string fmsg = xslmessages . create xpathmessage ( xpatherror resources . er _ unsupported _ return _ type , new object [ ] { return type . to string ( ) } ) ; throw new illegal argument exception ( fmsg ) ; }
rest ( users ) . post ( lives ) . type ( user pojo . class ) . out type ( country pojo . class ) . route ( ) . bean ( new user service ( ) , lives where ) ; } } ; }
column selector factory . set row ( new map based row ( 0 , immutable map . < string , object > of ( value , 11 l ) ) ) ; for ( int i = 0 ; i < num rows ; i + + ) { assert . assert true ( string . value of ( i ) , grouper . aggregate ( i ) . is ok ( ) ) ; } assert . assert equals ( 1899 , grouper . get growth count ( ) ) ;
set paused ( false ) ;
if ( response code > = 200 & & response code < 300 ) return null ; else if ( response code = = http urlconnection . http _ conflict ) throw new obsolete version exception ( message ) ; else throw new unknown failure ( unknown failure occured in http operation : + response code + - + message ) ;
assert rows net ( execute net with paging ( select distinct a , count ( a ) from % s group by a limit 2 , page size ) , row ( 1 , 1 l ) , row ( 2 , 1 l ) ) ; assert rows net ( execute net with paging ( select distinct a , count ( a ) from % s limit 2 , page size ) , row ( 1 , 3 l ) ) ;
assert throws ( client ( ) . prepare search ( ) . set query ( more like this query ( new string [ ] { string _ value , int _ value } , new string [ ] { index } , null ) . fail on unsupported field ( true ) ) , search phase execution exception . class ) ;
global make env builder . put ( ar , get tool path fragment ( tool . ar ) . get path string ( ) ) ; global make env builder . put ( nm , get tool path fragment ( tool . nm ) . get path string ( ) ) ; global make env builder . put ( ld , get tool path fragment ( tool . ld ) . get path string ( ) ) ; path fragment objcopy tool = get tool path fragment ( tool . objcopy ) ; if ( objcopy tool = null ) { objcopy is optional in crosstool global make env builder . put ( objcopy , objcopy tool . get path string ( ) ) ; } global make env builder . put ( strip , get tool path fragment ( tool . strip ) . get path string ( ) ) ; path fragment gcovtool = get tool path fragment ( tool . gcovtool ) ;
jstype string _ value _ or _ object _ type = create union type ( string _ object _ type , string _ type ) ;
if ( _ factory config . has serializer modifiers ( ) ) { for ( bean serializer modifier mod : _ factory config . serializer modifiers ( ) ) { ser = mod . modify map serializer ( config , type , bean desc , ser ) ; } } return ser ; }
map < string , object > doc1 = new hash map < string , object > ( ) ;
x . print stack trace ( ) ; } } ; simple proxy . automatic flow automatic proxy flow = proxy . start automatic flow ( ) ; client . start handshake ( ) ; assert . assert true ( automatic proxy flow . stop ( 5 , time unit . seconds ) ) ; assert . assert true ( latch . await ( idle timeout * 2 , time unit . milliseconds ) ) ;
authentication flow representation test flow = get flow from rest ( test flow ) ;
model . get observable product ( ) . observe ( this , new observer < product entity > ( ) { @ override public void on changed ( @ nullable product entity product entity ) { model . set product ( product entity ) ; } } ) ;
default exports . initialize ( ) ;
list < string > list = new array list < > ( ) ;
throw new illegal argument exception ( null was passed as an object name ) ;
task attempt attempt = task1 . get attempts ( ) . values ( ) . iterator ( ) . next ( ) ;
snapshot sp handle = spego . get txn id ( ) ;
int new index = 0 ; for ( int index = 0 ; index < exception info count ; index + + ) { exception info exception info = exception infos [ index ] ; if ( exception info . u2start pc < exception info . u2end pc ) { exception infos [ new index + + ] = exception info ; } } return new index ;
set date and check for completion ( new date time ( 2012 , 2 , 28 , 23 , 59 , 59 , 0 , test time zone ) ) ; set date and check for completion ( new date time ( 2012 , 2 , 29 , 23 , 59 , 59 , 0 , test time zone ) ) ; set date and check for completion ( new date time ( 2012 , 3 , 1 , 23 , 59 , 59 , 0 , test time zone ) ) ; date time next date = clock . get utcnow ( ) . plus days ( 1 ) ;
ch = in . read ( ) ; if ( ch = = modbus bintransport . frame _ start _ token ) { return ch ; } else { in . reset ( ) ; return modbus bintransport . frame _ start ; } } else if ( ch = = modbus bintransport . frame _ end _ token ) {
content = new byte [ ( int ) entry . get size ( ) ] ;
s = open session ( ) ; tx = s . begin transaction ( ) ; item item1 _ 1 = ( item ) s . get ( item . class , item1 . get id ( ) ) ; tx . commit ( ) ; s . close ( ) ; s = open session ( ) ;
message bytes connection value mb = headers . get value ( constants . connection ) ; if ( connection value mb = null ) { byte chunk connection value bc = connection value mb . get byte chunk ( ) ; if ( find bytes ( connection value bc , constants . close _ bytes ) = - 1 ) { keep alive = false ; } else if ( find bytes ( connection value bc , constants . keepalive _ bytes ) = - 1 ) { keep alive = true ; } } if ( http11 ) { message bytes expect mb = headers . get value ( expect ) ; if ( expect mb = null ) { if ( expect mb . index of ignore case ( 100 - continue , 0 ) = - 1 ) { input buffer . set swallow input ( false ) ; request . set expectation ( true ) ; } else { response . set status ( http servlet response . sc _ expectation _ failed ) ; set error state ( error state . close _ clean , null ) ; } } }
job executable job = management service . move timer to executable job ( jobs . get ( 0 ) . get id ( ) ) ; management service . execute job ( executable job . get id ( ) ) ; assert equals ( 0 , management service . create job query ( ) . list ( ) . size ( ) ) ; jobs = management service . create timer job query ( ) . list ( ) ;
when ( vertex . get max parallelism ( ) ) . then return ( 222 ) ; when ( vertex . is max parallelism configured ( ) ) . then return ( true ) ; try { savepoint loader . load and validate savepoint ( job id , tasks , path , ucl , false ) ; fail ( did not throw expected exception ) ; } catch ( illegal state exception expected ) { assert true ( expected . get message ( ) . contains ( max parallelism mismatch ) ) ; }
m is selected item straightened in process = true ;
if ( effect _ tick = = 0 ) { set _ period ( period - ( ( portamento _ param & 0x0 f ) < < 2 ) ) ; } } else {
remove callbacks ( this ) ;
return ocommon const . empty _ bucket _ entry _ array ;
git material git = new git material ( git ) ; material config git config = git . config ( ) ; modification modification1 = checkin with comment ( rev1 , comment1 , new date ( ) ) ; modification modification2 = checkin with comment ( rev2 , comment2 , new date ( ) ) ; modification modification3 = checkin with comment ( rev3 , comment3 , new date ( ) ) ; build cause p1build cause = create build cause for revisions ( new array list < > ( ) , as list ( git ) , new modifications ( modification1 , modification2 ) ) ; build cause p2build cause = create build cause for revisions ( as list ( dependency material ( p1 , 1 ) ) , as list ( git ) , new modifications ( modification3 ) ) ; when ( pipeline service . build cause for ( p2 , 1 ) ) . then return ( p2build cause ) ;
string host _ 0 = host _ 0 ;
m _ inner fsb = new fast string buffer ( this ) ; }
return ( type literal < provider < t > > ) get ( types . provider of ( get type ( ) ) ) ;
get request header group ( ) . add header ( versupport . get version header ( ) ) ;
assert equals ( arrays . as list ( null , null ) , exec ( def b = new string builder ( ) ; def l = [ 1 , 2 ] ; l . stream ( ) . map ( i - > b . set length ( i ) ) . collect ( collectors . to list ( ) ) ) ) ;
return decimal ( + column . default _ precision + , + column . default _ scale + ) ; }
final socket channel socket channel = create client socket channel ( ) ;
set value into context ( ctx , ictx [ i ] ) ;
minuend . set values ( 4 l , 10023 l ) ;
if ( response code > = 200 & & response code < 300 ) return ; else throw map response code to error ( response code , message ) ;
properties p = new properties ( ) ; p . put ( property , id ) ;
assert . assert true ( nm token secret manager . is application attempt nmtoken present ( attempt . get app attempt id ( ) , nm1 . get node id ( ) ) ) ;
for ( int i = 0 ; i < = 255 ; i + + ) { input = ( ( float ) i ) 255 . 0f ; if ( input < = 0 . 04045f ) { output = input 12 . 92f ; } else { output = ( float ) math . pow ( ( input + 0 . 055f ) 1 . 055f , 2 . 4 ) ; } s8 tol8 [ i ] = ( byte ) math . round ( output * 255 . 0f ) ; } }
path = path as key ( path ) ; list < rest consumer context path matcher . consumer path > paths = new array list < rest consumer context path matcher . consumer path > ( ) ;
dec = hive decimal . create ( big int , 35 ) ; assert . assert equals ( 99999999 . 999999999999999999999999999999 , dec . to string ( ) ) ;
assert null ( table . get column ( byte buffer . wrap ( new byte [ ] { 5 } ) ) ) ;
exe = execute ( create roles - - config ' + config file . get name ( ) + ' - s name = testrole - s ' description = test role ' - o ) ; assert exit code and std err size ( exe , 0 , 0 ) ;
rsp = update ( params ( update . chain , tolerant - chain - max - errors - 10 , commit , true ) , doc ( f ( id , doc id1 ) , f ( foo _ i , 2001 ) ) , doc ( f ( id , doc id2 ) , f ( foo _ i , 1976 ) ) ) . process ( client ) ;
long cloud client docs = cloud client . query ( new solr query ( * : * ) ) . get results ( ) . get num found ( ) ;
file system fs = master . get master file system ( ) . get file system ( ) ;
byte buffer . get int ( ) ;
for ( metrics base m : registry . get metrics list ( ) ) { m . push metric ( metrics record ) ; }
if ( classification ) { vec resp vec = f . vec ( response idx ) ; f . replace ( response idx , resp vec . to categorical vec ( ) ) . remove ( ) ; dkv . put ( f . _ key , f ) ; } drfmodel model = null ; drfmodel model from checkpoint = null ;
holder . register problem ( placeholder , index value [ 0 ] is not allowed loc , problem highlight type . generic _ error _ or _ warning , range ) ; return false ;
super . channel open ( ctx , e ) ;
final list view lv = get list view ( ) ; lv . set clip to padding ( false ) ; final int vert padding = get resources ( ) . get dimension pixel size ( r . dimen . list _ vertical _ padding ) ; lv . set padding ( 0 , vert padding , 0 , vert padding ) ; views created = true ; if ( items loaded ) { on fragment loaded ( ) ; } }
rm1 . wait for state ( app . get application id ( ) , rmapp state . failed ) ; controlled clock clock = new controlled clock ( ) ;
map < string , optional < partition > > partitions = get metastore client ( table name . get schema name ( ) ) . get partitions by names ( table name . get schema name ( ) , table name . get table name ( ) , partition names ) ;
data node fault injector . set ( new data node fault injector ( ) { private int tries = 0 ; @ override public void fail mirror connection ( ) throws ioexception { if ( tries + + = = 0 ) { throw new ioexception ( failing mirror for space reservation ) ; } } } ) ;
byte [ ] content id = part . get content id ( ) ;
if ( p my tri info . assigned group [ 0 ] = = null & & p my tri info . assigned group [ 1 ] = = null & & p my tri info . assigned group [ 2 ] = = null ) { p my tri info . flag & = ( orient _ preserving ) ; p my tri info . flag | = ( p group . orient preservering ? orient _ preserving : 0 ) ; } }
default exchange holder . add property ( pe , exchange . aggregated _ size , exchange . get property ( exchange . aggregated _ size , integer . class ) ) ; default exchange holder . add property ( pe , exchange . aggregated _ timeout , exchange . get property ( exchange . aggregated _ timeout , long . class ) ) ;
get root ( ) . remove project ( project ) ; if ( project = = platform . get project manager ( ) . get active project ( ) ) { platform . get project manager ( ) . set active project ( null ) ; } } else { if ( child delta . get flags ( ) = = iresource delta . open ) { if ( project node . get project ( ) . is open ( ) ) { project node . open project ( ) ; } else {
my confrence members . add ( temp member ) ;
m layout end = integer . min _ value ;
stream . headers sent ( is informational ) ; } else { lifecycle manager . on error ( ctx , failure cause ) ; } return future ; } else {
plugin descriptor d = repo . get plugin descriptor ( args [ 0 ] ) ;
list < string > names = new array list < string > ( ) ; for ( method m : actual methods ) { filter out synthetic methods from , e . g . , coverage tools . if ( m . is synthetic ( ) ) { names . add ( m . to string ( ) . replace ( clazz . get name ( ) + ' . ' , ) ) ; } } return names ;
send window update ( 3 , ( 1 < < 31 ) - 1 ) ; parser . read frame ( true ) ; assert . assert equals ( 3 - rst - [ + http2 error . flow _ control _ error . get code ( ) + ] \ n , output . get trace ( ) ) ; }
} certificate verify = new certificate verify ( ds . sign ( ) ) ; send ( certificate verify ) ; }
assert false ( node util . is string result ( get node ( ( [ 1 , 2 ] ) ) ) ) ;
map red task new task = convert task to map join task ( curr task . get work ( ) , big table position ) ; new task . set task tag ( task . mapjoin _ only _ nobackup ) ;
if ( f may read chunks ) { count of bytes to get form buffer int read from buffer = ( len < bytes left ) ? len : bytes left ; system . arraycopy ( f data , f offset , b , off , read from buffer ) ; int read from stream = 0 ; if ( len > bytes left ) { read from stream = f input stream . read ( b , off + bytes left , len - bytes left ) ; } f offset + = read from buffer ; return read from buffer + ( ( - 1 = = read from stream ) ? 0 : read from stream ) ; } else { this will prevent returning more bytes than the remainder of the buffer array . if ( len > bytes left ) { len = bytes left ; } system . arraycopy ( f data , f offset , b , off , len ) ; f offset + = len ; return len ; }
assert true ( outputs . get ( 2 ) . contains ( data node uuid1 ) | | outputs . get ( 6 ) . contains ( data node uuid1 ) ) ;
add and expect illegal argument exception ( e , b ) ;
if ( uri path = null & & uri path . starts with ( ) ) { uri path = uri path . substring ( 2 ) ; }
list < node > top nodes = new array list < node > ( ) ;
dom . set style attribute ( tick , visibility , hidden ) ;
instructions . add ( reil helpers . create undef ( offset + 5 , operand size . byte , helpers . sign _ flag ) ) ; instructions . add ( reil helpers . create undef ( offset + 6 , operand size . byte , helpers . zero _ flag ) ) ; instructions . add ( reil helpers . create undef ( offset + 7 , operand size . byte , helpers . auxiliary _ flag ) ) ; instructions . add ( reil helpers . create undef ( offset + 8 , operand size . byte , helpers . parity _ flag ) ) ; instructions . add all ( helpers . write mul result ( environment , offset + 9 , result , size1 ) ) ; }
set state ( m src3 , not _ closed , finished , with _ result , mock ( object . class ) , not _ failed , null ) ; subscriber3 . on failure ( m src3 ) ; m in order . verify ( m src3 ) . close ( ) ; verify subscriber ( data source , m src3 , no _ interactions ) ; verify state ( data source , m src2 , not _ closed , finished , with _ result , val2 , failed , throwable ) ; test close ( data source , m src2 ) ; verify state ( data source , null , closed , finished , without _ result , null , failed , throwable ) ; }
sign in listener . on sign out ( status ) ;
cal . add ( gregorian calendar . day _ of _ month , 10 ) ;
client . test jq ( params ( p , q , * : * , json . facet , { x : { terms : { { terms } field : ' { multi _ ss } ' , all buckets : true } } } ) , facets = = { count : 6 , + x : { buckets : [ { val : a , count : 3 } , { val : b , count : 3 } ] , all buckets : { count : 6 } } } ) ;
sslfilter config ssl config = new sslfilter config ( ) ;
gwcinitializer gwc initializer = geo server extensions . bean ( gwcinitializer . class ) ;
write object internal ( object , unshared , true , true ) ;
text = env . read text file ( params . get ( input ) ) ; } else {
client resource app role application = api util . find client by client id ( test realm ( ) , app _ role _ application ) ; role resource app role1 = app role application . roles ( ) . get ( app _ role _ 1 ) ; role builder realm app composite role = role builder . create ( ) . name ( realm _ app _ composite _ role ) ; test realm ( ) . roles ( ) . create ( realm app composite role . build ( ) ) ; string id = test realm ( ) . roles ( ) . get ( realm _ app _ composite _ role ) . to representation ( ) . get id ( ) ; test realm ( ) . roles by id ( ) . add composites ( id , collections . singleton list ( app role1 . to representation ( ) ) ) ;
verify deterministic ( proto coder . of ( duration . class ) ) ;
if ( is control frame & & is final frame ) { throw new protocol exception ( control frames must be final . ) ; } boolean reserved flag1 = ( b0 & b0 _ flag _ rsv1 ) = 0 ;
offer ( sequence , 6 l , new long [ ] { 4 } ) ;
m ntp time = response time + m clock offset ; m ntp time reference = response ticks ; m round trip time = round trip time ; } catch ( exception e ) {
switch instruction . default offset = default offset ;
assert false ( result ) ; assert read test data ( ) ; assert no samples to read ( test _ format _ 2 ) ; }
presenter = create view id and create presenter ( ) ;
assert equals ( p find matching files test , problem marker info . file . to string ( ) ) ; assert equals ( new path ( test duplicate folder symbolic link . c ) , problem marker info . external path ) ; assert equals ( error , problem marker info . description ) ; }
try { original . close ( ) ; } catch ( sqlexception e ) { log . error ( can ' t close result set , e ) ; }
matcher = null ;
final object object = c . new instance ( planet model , input stream ) ;
string clean up temp folders = job parameters . get string ( backup . param _ cleanup _ temp ) ;
for ( i = mid - 1 ; i > = 0 ; i - - ) { long prev = time values . get long single value row ( i ) ; if ( time = prev ) { break ; } } return inclusive ? i + 1 : i ;
type information < tuple2 < integer , integer > > type info = type extractor . get for object ( new tuple2 < > ( 0 , 0 ) ) ; execution config config = new execution config ( ) ;
timer test task test task = new timer test task ( ) ; assert true ( unsheduled tasks should return false for cancel ( ) , test task . cancel ( ) ) ;
long ret = linux _ syscall ( arch . seccomp , bogus arg ) ;
try { line = reader . read line ( ) ; } catch ( ioexception exp ) { throw new data import handler exception ( data import handler exception . severe , problem reading from input , exp ) ; }
web app context wac = new web app context ( ) ;
if ( result = null ) { if ( response = null ) { async result . for message ( response ) . result = result ; response . send to target ( ) ; } return ; }
double load = 5 + 10 + 15 + 10 + 15 + 5 ;
ensure open ( false ) ; return config . get term index interval ( ) ;
assert true ( do an edit ( ) ) ; assert exit invocations ( 0 ) ;
final query query = new query ( ) . set current page num ( 1 ) . set filter ( new property filter ( article . article _ is _ published , filter operator . equal , true ) ) . add sort ( article . article _ create _ date , sort direction . descending ) ;
if ( ks def . strategy _ options = = null ) { ks def . strategy _ options = new linked hash map < string , string > ( ) ; }
opt _ len + = 3 * ( max _ blindex + 1 ) + 5 + 5 + 4 ; return max _ blindex ;
final list < string > parameter names deleted = new array list < string > ( this definition parameter names ) ;
instance inst ;
files . copy ( log file bak , log file ) ; corrupt byte in file ( log file , tx offset ) ; validation = edit log file input stream . scan edit log ( log file , long . max _ value , true ) ; long expected end tx id = ( tx id = = ( num _ txns + 1 ) ) ? num _ txns : ( num _ txns + 1 ) ; assert equals ( failed when corrupting txn opcode at + tx offset , expected end tx id , validation . get end tx id ( ) ) ; assert true ( validation . has corrupt header ( ) ) ; }
dir . create document ( name , to input stream ( ) ) ;
request = reservation list request . new instance ( reservation system test util . reservation q , , 1 , - 10 , true ) ; response = client . list reservations ( request ) ; assert . assert not null ( response ) ; assert . assert equals ( 1 , response . get reservation allocation state ( ) . size ( ) ) ; assert . assert equals ( response . get reservation allocation state ( ) . get ( 0 ) . get reservation id ( ) . get id ( ) , s request . get reservation id ( ) . get id ( ) ) ; } finally {
jar out . delete ( ) ;
datastore file vmx file = new datastore file ( file info . get vm path name ( ) ) ;
readable state < boolean > readable = value . contains ( a ) ;
mock zoo keeper zk = ( mock zoo keeper ) local zk connection service . get local zoo keeper ( ) ;
try { return new uri ( source ) ; } catch ( urisyntax exception e ) { if ( build config . debug ) log . d ( tag , source is not encoded , encoding now ) ; } try { url url = new url ( source ) ; return new uri ( url . get protocol ( ) , url . get user info ( ) , url . get host ( ) , url . get port ( ) , url . get path ( ) , url . get query ( ) , url . get ref ( ) ) ; } catch ( malformed urlexception | urisyntax exception e ) { throw new illegal argument exception ( e ) ; }
assert not equals ( proxied interface . class , proxy interface ) ;
context . set ( context . web context ( request , response , filter config ) ) ; final byte array output stream os = new byte array output stream ( ) ; final http servlet response wrapped response = new redirected stream servlet response wrapper ( os , response ) ; chain . do filter ( request , wrapped response ) ; final reader reader = new string reader ( new string ( os . to byte array ( ) , context . get ( ) . get config ( ) . get encoding ( ) ) ) ; final string writer writer = new string writer ( ) ; final string request uri = request . get request uri ( ) . replace first ( request . get context path ( ) , ) ; do process ( request uri , reader , writer ) ;
delete = new delete ( row1 ) ; delete . add column ( fam1 , qf1 ) ; delete . add column ( fam1 , qf1 ) ; res = region . check and mutate ( row1 , fam1 , qf1 , compare operator . equal , new binary comparator ( val2 ) , delete , true ) ; assert true ( res ) ; delete = new delete ( row1 ) ; res = region . check and mutate ( row1 , fam1 , qf1 , compare operator . equal , new binary comparator ( empty val ) , delete , true ) ; assert true ( res ) ;
assert true ( queue . remove ( 3 ) ) ; assert equals ( 4 , queue . size ( ) ) ;
amf null . write null to ( out ) ;
int delay in hundredths of asecond = read short ( ) ;
query . where ( survey . name . is not null ( ) ) ;
else { packet = page header . get packet list ( ) . get ( 1 ) ; list < ogg page header . packet start and length > packet list = page header . get packet list ( ) ; now at start of next packet , check this is the vorbis setup header b = new byte [ vorbis header . field _ packet _ type _ length + vorbis header . field _ capture _ pattern _ length ] ;
int [ ] ints = new int [ ] { 0 , 1 , 2 , 3 } ;
try { ( ( element ) * node * test addr ) . set attribute ns ( namespace uri , qualified name , new value ) ; fail ( throw _ namespace _ err ) ; } catch ( domexception ex ) { }
record param rp = model . get singleton ( ) . get db ( ) . get table param ( ) . insert ( param . get site ( ) , param . get type ( ) . name ( ) , param . get name ( ) , param . get times used ( ) , set to string ( param . get flags ( ) ) , set to string ( param . get values ( ) ) ) ; param . set id ( rp . get param id ( ) ) ; } else {
if ( ns elem = null ) body . append ( ns elem ) ;
return get caret shapes ( offset , get natural bounds ( ) , default _ caret _ policy ) ;
org . apache . hadoop . mapreduce . task attempt context task context = new org . apache . hadoop . mapreduce . task . task attempt context impl ( job , get task id ( ) , reporter ) ;
template model m3 = new template model ( ) ; m3 . add lhs item ( fp ) ; m3 . name = r3 ; m3 . add row ( new string [ ] { t1 , null } ) ; final string expected3 = rule \ r3 _ 0 \ \ n + dialect \ mvel \ \ n + when \ n + p1 : smurf ( field1 = = \ t1 \ , field1 = = \ value \ ) \ n + then \ n + end ;
close expired connections ( ) ; if ( unique pool entry . connection . is open ( ) ) { route tracker tracker = unique pool entry . tracker ; shutdown = ( tracker = = null | | can happen if method is aborted tracker . to route ( ) . equals ( route ) ) ; } else { if the connection is not open , create a new pool entry , as the connection may have been marked not reusable , due to aborts - - and the pool entry should not be reused either . there ' s no harm in recreating an entry if the connection is closed . recreate = true ; }
state . ul button pressed = 0 ; for ( int i = 0 ; i < 5 ; i + + ) { if ( state . r axis [ i ] = null ) { state . r axis [ i ] . x = 0 . 0f ; state . r axis [ i ] . y = 0 . 0f ; } }
verify contents ( server , job id , blob key , data ) ;
return m context . get package manager ( ) . get default activity icon ( ) ; }
info text pane . put client property ( jeditor pane . honor _ display _ properties , true ) ; info text pane . set opaque ( false ) ; info text pane . set editable ( false ) ; info text pane . set content type ( text html ) ; info text pane . set text ( to string ( certificates [ 0 ] ) ) ; final jscroll pane cert scroll = new jscroll pane ( info text pane ) ;
if ( default namespace = = null ) { return namespace ; }
if ( super wildcard = = null ) return false ;
assert false ( workspace dao . get by namespace ( user . get name ( ) ) . is empty ( ) ) ;
projection policy ppolicy = info . get projection policy ( ) ;
files . delete ( workspaces . get ( ws . get name ( ) ) . dir ( ) ) ; resource ws folder = backup utils . dir ( workspaces , ws . get name ( ) ) ; do write ( get catalog ( ) . get namespace by prefix ( ws . get name ( ) ) , ws folder , namespace . xml ) ; do write ( ws , ws folder , workspace . xml ) ;
boolean already group = ( part . m _ flags & ( sqlpattern factory . group | sqlpattern factory . capture ) ) = 0 ; sqlpattern part ret part = already group ? new sqlpattern part element ( part ) : part ; if ( capture ) { ret part . m _ flags | = sqlpattern factory . capture ; ret part . set capture label ( capture label ) ; } else { ret part . m _ flags | = sqlpattern factory . group ; } return ret part ;
run project analysis ( orchestrator , shared xoo - sample ) ; issue = issue client ( ) . find ( issue query . create ( ) ) . list ( ) . get ( 0 ) ; assert that ( issue . message ( ) ) . is equal to ( issue with custom message ) ; }
signal split data ( f start container , clone current , f start offset ) ;
object element ;
log . debug ( gms . view _ id is null , i ' m not the coordinator anymore ( leaving = % b ) ; + the new coordinator will handle the leave request , self _ leaving ) ; return ;
xml namespace . create from attr ( context . get runtime ( ) , attr ) ; } clear cached node ( attr ) ; nokogiri helpers . rename node ( attr , ns uri , node name ) ; } }
constructor . add parameter ( message source , spring java type . message _ source ) ;
conn . initialize ( client ) ;
assert true ( maaif . get aspect metadata ( ) . get per clause pointcut ( ) . get method matcher ( ) . matches ( test bean . class . get method ( get spouse ) , null ) ) ;
slice _ del ( ) ; break ; } } while ( false ) ; break ; case 7 :
raw key . reset ( ) ;
log . info ( using default application credentials ) ;
data output stream dos = fs block writer . start writing ( block type . meta ) ; meta data . get ( i ) . write ( dos ) ; fs block writer . write header and data ( output stream ) ;
wait for job executor to process all jobs ( 2000 , 100 ) ;
lock = null ; thread . sleep ( 1000 ) ; } } while ( lock = = null ) ; return new file with lock ( lock , file ) ; }
return model generator . to service model ( super model , zone , config server hosts , slobrok monitor manager ) ; }
if ( multimodule = = multimodule . standard ) { create module ( model , jar packaging provider , model ) ; create module ( repository , jar packaging provider , repository ) ; roo - 3762 : generate integration module by default create module ( integration , jar packaging provider , integration ) ; create module ( service - api , jar packaging provider , service . api ) ; create module ( service - impl , jar packaging provider , service . impl ) ; add dependencies between modules get project operations ( ) . add dependency ( repository , pom . get group id ( ) , model , { project . version } ) ; get project operations ( ) . add dependency ( integration , pom . get group id ( ) , model , { project . version } ) ; get project operations ( ) . add dependency ( service - api , pom . get group id ( ) , model , { project . version } ) ; get project operations ( ) . add dependency ( service - impl , pom . get group id ( ) , repository , { project . version } ) ; get project operations ( ) . add dependency ( service - impl , pom . get group id ( ) , service . api , { project . version } ) ; get project operations ( ) . add dependency ( service - impl , pom . get group id ( ) , model , { project . version } ) ; get project operations ( ) . add dependency ( service - impl , pom . get group id ( ) , integration , { project . version } ) ; }
try { parameter = new validation parameter ( ) ; validation service . save ( parameter ) ; assert . fail ( ) ; } catch ( rpc exception e ) { assert . assert true ( e . get message ( ) . contains ( constraint violation ) ) ; }
owhere clause jjtn000 = new owhere clause ( jjtwhereclause ) ; boolean jjtc000 = true ; jjtree . open node scope ( jjtn000 ) ; jjtn000 . jjt set first token ( get token ( 1 ) ) ; try { jjtn000 . base expression = or block ( ) ; jjtree . close node scope ( jjtn000 , true ) ; jjtc000 = false ; jjtn000 . jjt set last token ( get token ( 0 ) ) ; { if ( true ) return jjtn000 ; } } catch ( throwable jjte000 ) { if ( jjtc000 ) { jjtree . clear node scope ( jjtn000 ) ; jjtc000 = false ; } else { jjtree . pop node ( ) ; } if ( jjte000 instanceof runtime exception ) { { if ( true ) throw ( runtime exception ) jjte000 ; } } if ( jjte000 instanceof parse exception ) { { if ( true ) throw ( parse exception ) jjte000 ; } } { if ( true ) throw ( error ) jjte000 ; } } finally { if ( jjtc000 ) { jjtree . close node scope ( jjtn000 , true ) ; jjtn000 . jjt set last token ( get token ( 0 ) ) ; } }
tspan event span event = new span event ( ) ;
string s = this is not a palindrome ; string reverse = new string buffer ( s ) . reverse ( ) . to string ( ) ; assert equals ( should have overridden to reverse , not echo , reverse , oom . replace me ( s ) ) ; assert equals ( should have overridden no - arg overloaded replace me method to return fixed value , fixed method replacer . value , oom . replace me ( ) ) ;
if ( android res . is empty ( ) ) { error dialog . show error dialog ( project , no android _ res modules , could not find any android _ res modules in % s - can ' t move the new resource file , buck file ) ; return ; } if ( android res . size ( ) = = 1 ) { move to ( project , selection , new file , android res . get ( 0 ) ) ; } else { path resource file path = paths . get ( new file ) . get file name ( ) ; string resource file name = resource file path = = null ? null : resource file path . to string ( ) ; popup targets popup targets = new popup targets ( new file , project , selection , please choose an android resource module + ( resource file name = = null ? : for + resource file name ) , android res ) ; popup targets . on chosen ( ) will call this . move to ( ) jbpopup factory . get instance ( ) . create list popup ( popup targets ) . show in best position for ( editor ) ; }
double [ ] values = new double [ contained . size ( ) ] ; int [ ] indices = new int [ contained . size ( ) ] ; iterator it = contained . key set ( ) . iterator ( ) ; for ( int i = 0 ; it . has next ( ) ; i + + ) { integer index = ( integer ) it . next ( ) ; double value = ( double ) contained . get ( index ) ; values [ i ] = value . double value ( ) ; indices [ i ] = index . int value ( ) ; } instance inst = new sparse instance ( instance . weight ( ) , values , indices , output format peek ( ) . num attributes ( ) ) ;
aaa = new key value ( a , fam , qf , 1 , a ) ;
string [ ] freqs = get frequencies ( resolution , modes ) ;
agent2 . stop async ( ) . await terminated ( ) ;
mem store size memstore size = new mem store size ( ( long ) ( 3l * 1024l * 1024l * 1024l ) , ( long ) ( 1l * 1024l * 1024l * 1024l ) ) ; region server accounting . inc global mem store size ( memstore size ) ; assert equals ( flush type . above _ offheap _ higher _ mark , region server accounting . is above high water mark ( ) ) ; }
out . write char ( no _ entry _ value ) ;
rest request request = mux rest request ( immutable map . of ( 2 , r2 ) ) ; count down latch latch = new count down latch ( 1 ) ;
try { utx . rollback ( ) ; assert . fail ( should have gone to catch block1 ) ; } catch ( illegal state exception isex ) { assert . assert equals ( cannot locate a transaction for rollback . , isex . get message ( ) ) ; } utx . begin ( ) ;
end annotation ( ) ;
check position indexes ( off , off + len , b . length ) ;
if ( string utils . is not blank ( prefix ) ) { if ( prefix . starts with ( ) ) { sbuilder . append ( prefix . substring ( 1 ) ) ; } else { sbuilder . append ( prefix ) ; } include last if provided prefix doesn ' t include it if ( prefix . ends with ( ) ) { sbuilder . append ( ) ; } }
val = val % groups . get int ( count ) ;
parameters . clear ( ) ; return answer ;
m _ bayes net . add arc ( s parent , s child ) ; m _ j status bar . set text ( m _ bayes net . last action msg ( ) ) ; update status ( ) ; } catch ( exception e ) {
container . set background resource ( r . drawable . badge ) ; gradient drawable bg shape = ( gradient drawable ) container . get background ( ) ; bg shape . set color ( get resources ( ) . get color ( bg color ) ) ; iconic font drawable = new iconic font drawable ( this . context ) ; iconic font drawable . set icon ( icon ) ; iconic font drawable . set icon color ( get resources ( ) . get color ( icon color ) ) ; iconic font drawable . set icon padding ( ( int ) icon _ padding ) ; if ( sdk _ int < jelly _ bean ) { view _ icon . set background drawable ( iconic font drawable ) ; } else { view _ icon . set background ( iconic font drawable ) ; }
if ( unparseable extensions = = null ) { unparseable extensions = new tree map < string , extension > ( ) ; } unparseable extensions . put ( ext . get extension id ( ) . to string ( ) , new unparseable extension ( ext , e ) ) ; if ( debug = null ) { debug . println ( error parsing extension : + ext ) ; e . print stack trace ( ) ; hex dump encoder h = new hex dump encoder ( ) ; system . err . println ( h . encode buffer ( ext . get extension value ( ) ) ) ; } return ;
runtime . get runtime ( ) . halt ( - 1 ) ;
list < bytes reference > reference list = new ref list ( length ) ;
if ( get activity ( ) = null ) { find target fragment if ( ) ; } fragment fragment = m target fragment ;
throw new servlet exception ( sm . get string ( standard wrapper . init exception , get name ( ) ) , f ) ;
tasks = task service . create task query ( ) . order by due date nulls last ( ) . desc ( ) . list ( ) ; for ( int i = 0 ; i < 4 ; i + + ) { assert not null ( tasks . get ( i ) . get due date ( ) ) ; } assert equals ( task0 , tasks . get ( 0 ) . get name ( ) ) ; assert equals ( task2 , tasks . get ( 1 ) . get name ( ) ) ; assert equals ( task1 , tasks . get ( 2 ) . get name ( ) ) ; assert equals ( task3 , tasks . get ( 3 ) . get name ( ) ) ;
next char ( ) ; tabs = saved tabs ; return true ; }
final int node = arc . target ; fst . read first target arc ( arc , arc ) ; while ( true ) {
r = increment ( region , quota , mutation , cell scanner , nonce group , space quota enforcement ) ;
ringbuffer store store = get ringbuffer store ( store config , class loader ) ;
renderable world . queue visible chunks ( is first rendering stage for current frame ) ; if ( requested task list refresh ) { render task list generator . refresh ( ) ; requested task list refresh = false ; }
map < string , topology node > nodes = maps . new hash map ( ) ;
if ( creds = = null & & prov . retry ( ) ) { return null ; } } }
final map < string , string > user map = this . user map = null ? new hash map < > ( this . user map ) : collections . empty map ( ) ;
final string comment sharp url = comments . get comment sharp urlfor page ( page , comment id ) ; ret . put ( comment . comment _ name , comment name ) ; ret . put ( comment . comment _ content , comment content ) ; ret . put ( comment . comment _ url , comment url ) ; ret . put ( comment . comment _ sharp _ url , comment sharp url ) ; comment . put ( comment . comment _ sharp _ url , comment sharp url ) ; comment . put ( keys . object _ id , comment id ) ;
x . stack [ x . saved stack top ] = null ;
hive conf . set var ( conf vars . metastore _ cluster _ delegation _ token _ store _ cls , org . apache . hadoop . hive . thrift . dbtoken store ) ;
if ( data set . is highlight enabled ( ) ) continue ; list < highlight > highs = build highlights ( data set , j , x val , data set . rounding . closest ) ;
wsocket . wait for close ( 10 , time unit . seconds ) ;
m matrix . set rotate ( 90 , corner width 2 , corner height 2 ) ; m matrix . post translate ( canvas . get width ( ) - corner width , 0 ) ; canvas . draw bitmap ( m rounded corner bitmap , m matrix , m background clip paint ) ;
for ( int i = 0 ; i < 7 ; i + + ) { ocache entry cache entry = read buffer . load for read ( file id , i , false , write buffer , 1 , true ) ; read buffer . release from read ( cache entry , write buffer ) ; } assert . assert equals ( am . size ( ) , 0 ) ; assert . assert equals ( a1out . size ( ) , 2 ) ; pages 1 - 2 assert . assert equals ( a1in . size ( ) , 4 ) ; pages 3 - 6
out = transfer ;
int mtu = platform . get ( ) . get mtu ( socket ) ;
final groovy class loader loader = new groovy class loader ( ) ;
assert true ( rtd . get event type ( ) . equals ( event type . rs _ zk _ region _ split ) | | rtd . get event type ( ) . equals ( event type . rs _ zk _ region _ splitting ) ) ;
context . add step ( executor queue validation step handler . model _ validation _ instance , operation context . stage . model ) ;
source path resolver path resolver = default source path resolver . from ( new source path rule finder ( resolver ) ) ; string app id = adb helper . try to extract package name from manifest ( path resolver , has installable apk . get apk info ( ) ) ; return adb helper . uninstall app ( app id , uninstall options ( ) . should keep user data ( ) ) ? 0 : 1 ;
compile ( querydsl annotation processor . class , collections . singleton list ( path ) , overwrite ) ; assert equals ( modified , q type . last modified ( ) ) ;
assert equals ( 0000000000000001 , m _ provider . get element ( 0x120 ) ) ; m _ provider . set data layout ( stack data layout . bytes ) ;
assert true ( b array , arrays . equals ( my message , b array ) ) ;
string best match = null ; for ( iterator it = this . handler map . key set ( ) . iterator ( ) ; it . has next ( ) ; ) { string pattern = ( string ) it . next ( ) ; if ( match uri request pattern ( pattern , request uri ) ) {
i ref set . add all ( immutable bit set . range ( field count ) . as list ( ) ) ;
configured target cc rule a = get configured target ( foo : liba . so ) ;
if ( max primary = = 0 ) { double ratio = ( double ) max secondary ( double ) actual secondary ; return ( int ) ( actual primary * ratio ) ; } if ( max secondary = = 0 ) { return max primary ; }
border color = new color ( 245 , 153 , 15 ) ;
w1 = ourslot ;
final kie base kbase = load knowledge base from string ( str ) ; final kie session ksession = kbase . new kie session ( ) ; final org . kie . api . event . rule . agenda event listener ael = mock ( org . kie . api . event . rule . agenda event listener . class ) ;
if ( encoded [ 1 ] . is empty ( ) ) { contents = encoded [ 0 ] ; display contents = encoded [ 1 ] ; title = activity . get string ( r . string . contents _ contact ) ; }
byte [ ] data = message . get body ( byte [ ] . class ) ;
menu manager menu manager = new menu manager ( ) ;
port profile obj = new port profile vo ( port prof name , vsm id , low vlan id , high vlan id , p type , b type ) ; transaction txn = transaction . current txn ( ) ;
if ( m header view infos . size ( ) > 0 | | m footer view infos . size ( ) > 0 ) { m adapter = new header view list adapter ( m header view infos , m footer view infos , adapter ) ; } else { m adapter = adapter ; } m data changed = true ;
if ( get interested types ( ) = null ) { for ( class < ? > c : get interested types ( ) ) { if ( c . is annotation ( ) ) { find and add the classes that implement or extend the class . but not including the class itself add inherited types ( class map , ( set < string > ) class map . get ( c . get name ( ) ) ) ; } } }
compacting mem store . update lowest unflushed sequence id in wal ( true ) ; only if greater
assert same ( entry should be present in store test2 , 1 , store test20 . get ( entry . get key ( ) , null ) . size ( ) ) ; assert equals ( entry value should match , new string ( entry . get value ( ) ) , new string ( store test20 . get ( entry . get key ( ) , null ) . get ( 0 ) . get value ( ) ) ) ; }
favored node assignment helper helper = new favored node assignment helper ( servers , get conf ( ) ) ;
crossfade drawer layout . get small view ( ) . add view ( view , view group . layout params . match _ parent , view group . layout params . match _ parent ) ;
verify ( monitor , at least ( 2 ) ) . defragmenting node range ( any long ( ) , any long ( ) ) ; verify ( monitor , at most ( 10 ) ) . defragmenting node range ( any long ( ) , any long ( ) ) ; }
path out dir = get output path ( job ) ;
batch . set color ( 1f , 1f , 1f , 0 . 15f ) ;
byte [ ] rowkey = bytes . to bytes ( yyyyyyyyy ) ;
if ( value = = calendar . get maximum ( calendar . hour _ of _ day ) + 1 ) { value = 0 ; } calb . set ( calendar . hour _ of _ day , value ) ; return pos . index ; case pattern _ day _ of _ week : ' e ' { if ( use date format symbols ) {
best counts = new double [ 2 ] [ instances . num classes ( ) ] ;
if ( output strategy . is trigger specified ( ) & & ( output strategy . get trigger ( ) instanceof default trigger ) & & ( output strategy . get window fn ( ) instanceof global windows ) & & output strategy . is allowed lateness specified ( ) ) { throw new illegal argument exception ( except when using global windows , + calling . triggering ( ) to specify a trigger requires that the allowed lateness + be specified using . with allowed lateness ( ) to set the upper bound on how late + data can arrive before being dropped . see javadoc for more details . ) ; } if ( output strategy . is mode specified ( ) & & can produce multiple panes ( output strategy ) ) { throw new illegal argument exception ( calling . triggering ( ) to specify a trigger or calling . with allowed lateness ( ) to + specify an allowed lateness greater than zero requires that the accumulation + mode be specified using . discarding fired panes ( ) or . accumulating fired panes ( ) . + see javadoc for more details . ) ; }
collections . sort ( tools ui , new comparator ( ) { @ override public int compare ( object o1 , object o2 ) { integer p1 = ( ( tool ui ) o1 ) . get position ( ) ; integer p2 = ( ( tool ui ) o2 ) . get position ( ) ; return p1 . compare to ( p2 ) ; } } ) ;
assert false ( session . is dirty ( ) ) ; must be false to avoid unnecessary updates s . get transaction ( ) . rollback ( ) ;
int chunk index = ( i num _ of _ cell _ reps _ in _ chunk ) ; byte buffer block = chunks [ chunk index ] . get data ( ) ; get the byte buffer of the relevant chunk int j = i - chunk index * num _ of _ cell _ reps _ in _ chunk ; get the index of the cell - representation
test c ( files ) ; }
assert null ( plan . get reservation by id ( r4 ) ) ;
int [ ] new divisor = new int [ divisor length ] ; system . arraycopy ( divisor , 0 , new divisor , 0 , divisor length ) ; multiply multi precision ( new divisor , d1 ) ; divisor = new divisor ; multiply multi precision ( remainder , d1 ) ; }
send to consumers ( new exchange ) ;
verify initial data on loader ( cache3 ) ; }
assert multimap remains unmodified ( multimap , original entries ) ; if ( multimap . is empty ( ) ) { collection < v > values = multimap . as map ( ) . entry set ( ) . iterator ( ) . next ( ) . get value ( ) ; assert collection is unmodifiable ( values , sample value ) ; }
channel read ( go away frame ( 0 , 8 * cancel * , unpooled . copied buffer ( this is a test , utf _ 8 ) ) ) ;
for ( int i = 0 ; i < wheels . size ( ) ; i + + ) { vehicle builder . get wheel ( i ) ; }
thread . join ( ) ; } catch ( interrupted exception e ) { } next element = terminator ; }
if ( type = = element _ node ) node handle + + ;
parser . read frame ( true ) ; parser . read frame ( true ) ; parser . read frame ( true ) ; parser . read frame ( true ) ; parser . read frame ( true ) ; assert . assert equals ( 0 - settings - [ 3 ] - [ 1 ] \ n + 0 - settings - end \ n + 0 - settings - ack \ n + 0 - ping - [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 ] \ n + get simple response trace ( 1 ) , output . get trace ( ) ) ;
assert equals ( 2 , generator . get database structure ( ) . get times accessed ( ) ) ; assert equals ( increment + 1 , ( ( basic holder ) optimizer . get last source value ( ) ) . get actual long value ( ) ) ; assert equals ( i + 1 , ( ( basic holder ) optimizer . get last value ( ) ) . get actual long value ( ) ) ; assert equals ( increment + 1 , ( ( basic holder ) optimizer . get last source value ( ) ) . get actual long value ( ) ) ; }
sock = new socket ( host , port ) ;
component color model ccm = ( component color model ) cm ;
result = run checkstyle ( config ) ; }
{ error msg . type _ check _ err , ' ' { 0 } ' ' \ u d45 c \ u d604 \ u c2 dd \ u c758 \ u c720 \ u d615 \ u c744 \ u d655 \ u c778 \ u d558 \ u b294 \ u c911 \ u c624 \ u b958 \ u ac00 \ u bc1 c \ u c0 dd \ u d588 \ u c2 b5 \ u b2 c8 \ u b2 e4 . } ,
long acc time = accumulated times . get ( processor . get processor id ( ) ) ;
try { report new collector info request request = report new collector info request . new instance ( default _ app _ id , default _ collector _ addr , null ) ; proxy . report new collector info ( request ) ; } catch ( yarn exception e ) { assert . fail ( rpc call failured is not expected here . ) ; } try { report new collector info request request = report new collector info request . new instance ( default _ app _ id , default _ collector _ addr , default _ collector _ token ) ; proxy . report new collector info ( request ) ; } catch ( yarn exception e ) { assert . fail ( rpc call failured is not expected here . ) ; }
context activity context = m searchable . get activity context ( m context ) ; m provider context = m searchable . get provider context ( m context , activity context ) ; m outside drawables cache = outside drawables cache ;
while ( stmt . equals ( ) ) { stmt = reader . read line ( ) ; stmt = cleanup string ( stmt ) ; if ( stmt = = null ) return null ; }
return scope ; }
start server ( 1 , grpc util . default _ max _ header _ list _ size ) ; netty client transport transport = new transport ( new negotiator ( ) ) ;
element content = domutil . get first child element ( elm node ) ; xsannotation impl annotation = null ; if ( content = null & & domutil . get local name ( content ) . equals ( schema symbols . elt _ annotation ) ) { annotation = traverse annotation decl ( content , attr values , false , schema doc ) ; content = domutil . get next sibling element ( content ) ; } else { string text = domutil . get synthetic annotation ( elm node ) ; if ( text = null ) { annotation = traverse synthetic annotation ( elm node , text , attr values , false , schema doc ) ; } } xsobject list annotations ;
has side effects = true ; } else {
enum set < update task > tasks = ( enum set < update task > ) intent . get serializable extra ( arg _ update _ tasks ) ;
new fragment . show ( get fragment manager ( ) , date picker ) ; } } ) ; }
get mock endpoint ( mock : handled ) . expected message count ( 1 ) ;
xsltattribute def select attr def node = new xsltattribute def ( null , select , xsltattribute def . t _ expr , false , xsltattribute def . error , node ( ) ) ;
repository info current info = get ( info . get id ( ) ) ; handle repo rename ( current info , info ) ; }
try { new suppress specific constructor demo ( ) ; fail ( should have thrown illegal state exception ) ; } catch ( illegal state exception e ) { assert equals ( , e . get message ( ) ) ; } }
double [ ] col means = math . col means ( y ) ; for ( int i = 0 ; i < n ; i + + ) { double [ ] yi = y [ i ] ; for ( int j = 0 ; j < d ; j + + ) { yi [ j ] - = col means [ j ] ; } } }
multi layer configuration conf integrated = new neural net configuration . builder ( ) . optimization algo ( optimization algorithm . stochastic _ gradient _ descent ) . iterations ( 1 ) . seed ( 123 ) . list ( ) . layer ( 0 , new dense layer . builder ( ) . n in ( 28 * 28 * 1 ) . n out ( 10 ) . activation ( activation . relu ) . weight init ( weight init . xavier ) . build ( ) ) . layer ( 1 , new output layer . builder ( loss functions . loss function . mcxent ) . weight init ( weight init . xavier ) . activation ( activation . softmax ) . drop out ( 0 . 25 ) . n in ( 10 ) . n out ( 10 ) . build ( ) ) . backprop ( true ) . pretrain ( false ) . build ( ) ; multi layer network net integrated = new multi layer network ( conf integrated ) ;
rtype . load new version ( 000 , rtype . bytes initial ) ; assert equals ( hello andy , run unguarded ( rtype . get clazz ( ) , greet ) . stdout ) ;
config = camel context . get service call configuration ( configuration ref ) ;
current doc id = target doc id ;
if ( patt idx start > patt idx end ) { return pattern . ends with ( this . path separator ) ? path . ends with ( this . path separator ) : path . ends with ( this . path separator ) ; } if ( full match ) { return true ; } if ( patt idx start = = patt idx end & & patt dirs [ patt idx start ] . equals ( * ) & & path . ends with ( this . path separator ) ) { return true ; } for ( int i = patt idx start ; i < = patt idx end ; i + + ) { if ( patt dirs [ i ] . equals ( * * ) ) { return false ; } } return true ;
map < string , ioc value > newmap = new hash map < string , ioc value > ( ) ; for ( entry < string , object > en : map . entry set ( ) ) { ioc value v = object2value ( en . get value ( ) ) ; newmap . put ( en . get key ( ) , v ) ; } iv . set type ( ioc value . type _ normal ) ; iv . set value ( newmap ) ; return iv ;
objref = new corbaobject impl ( ) ; }
permission cm = new cumulative permission ( ) . set ( base permission . read ) . set ( base permission . administration ) ;
orient vertex . create link ( graph , v in record , doc , in field name ) ; v in record . save ( ) ; v out = null ;
drawer . set on drawer opening ( e - > { final transition animation = title burger . get animation ( ) ; animation . set rate ( 1 ) ; animation . play ( ) ; } ) ; drawer . set on drawer closing ( e - > { final transition animation = title burger . get animation ( ) ; animation . set rate ( - 1 ) ; animation . play ( ) ; } ) ; title burger container . set on mouse clicked ( e - > { if ( drawer . is hidden ( ) | | drawer . is hiding ( ) ) { drawer . open ( ) ; } else { drawer . close ( ) ; } } ) ; fxmlloader loader = new fxmlloader ( get class ( ) . get resource ( fxml ui popup main popup . fxml ) ) ;
repaint ( ) ;
thread . sleep ( 100 ) ; }
args . add ( more paths . relativize ( working directory , aidl file path ) . to string ( ) ) ; return args . build ( ) ; }
reader = new string reader ( comment \ nversion = 1 \ n comment2 \ nint a default = 0 ) ; assert that ( config utils . get def namespace ( reader ) , is ( ) ) ; try { config utils . get def namespace ( null ) ; fail ( ) ; } catch ( illegal argument exception e ) { }
duration cpu time = millis ( stats . get cpu time millis ( ) ) ; string cpu time summary = string . format ( cpu time : % . 1fs total , % 5s rows s , % 8s , % d % % active , cpu time . get value ( seconds ) , format count rate ( stats . get processed rows ( ) , cpu time , false ) , format data rate ( bytes ( stats . get processed bytes ( ) ) , cpu time , true ) , ( int ) percentage ( stats . get cpu time millis ( ) , stats . get wall time millis ( ) ) ) ; reprint line ( cpu time summary ) ; double parallelism = cpu time . get value ( milliseconds ) wall time . get value ( milliseconds ) ;
attempt authentication ( true ) ;
assert not blank ( test color map with cql , image , legend utils . default _ bg _ color ) ; } finally {
assert false ( hdfs . get file status ( p ) . get permission ( ) . get sticky bit ( ) ) ;
assert equals ( 2 , debug interceptor . get count ( ) ) ;
view pager . remove on page change listener ( listener ) ;
return new hdfs resource loader ( cfg ) ; }
app . handle ( new application finish event ( app id , application killed by resource manager ) ) ;
final int height = opt . out height ; final int width = opt . out width ; int in sample size = 1 ; if ( height > req height | | width > req width ) { final int half height = height 2 ; final int half width = width 2 ; calculate the largest in sample size value that is a power of 2 and keeps both height and width larger than the requested height and width . while ( ( half height in sample size ) > req height & & ( half width in sample size ) > req width ) { in sample size * = 2 ; } } return in sample size ;
scoped test bean bean2 = ( scoped test bean ) context . get bean ( singleton ) ;
service . load ( ) ; user = service . get user by username ( username ) ; string pw1 = user . get password ( ) ; assert not equals ( original pw , pw1 ) ; body = message format . format ( xml template , pw02 ) ; assert equals ( 200 , put as servlet response ( up _ uri , body , text xml ) . get status ( ) ) ;
edit . put int ( key , value ) ;
final int size = get child count ( ) ; for ( int i = 0 ; i < size ; + + i ) { final view child = get child at ( i ) ; if ( child . get visibility ( ) = gone ) { if ( debug ) log . v ( tag , measuring + i + + child + : + m child width measure spec ) ; child . measure ( m child width measure spec , m child height measure spec ) ; } } }
bits = chr - 71 ; } else if ( ( chr > = ' 0 ' ) & & ( chr < = ' 9 ' ) ) {
if ( c . is array ( ) ) { arrays - may recurse for multi - dimensional arrays class component class = c . get component type ( ) ; type code embedded type ; if ( component class . is primitive ( ) ) { embedded type = get primitive type code for class ( component class , tc orb ) ; } else { embedded type = create type code for class ( component class , tc orb ) ; } type code t = tc orb . create _ sequence _ tc ( 0 , embedded type ) ; string id = rep strs . create for java type ( c ) ; return tc orb . create _ value _ box _ tc ( id , sequence , t ) ; } else if ( c = = java . lang . string . class ) { strings type code t = tc orb . create _ string _ tc ( 0 ) ; string id = rep strs . create for java type ( c ) ; return tc orb . create _ value _ box _ tc ( id , string value , t ) ; }
if ( sub profile . is explicit ( ) | | sub profile instanceof overridable query profile ) { string entry prefix = prefix + entry . get key ( ) + . ; dereference compounded variants ( sub profile . get variants ( ) , entry prefix ) ; dereference compounded variants ( sub profile , entry prefix ) ; }
return path ; }
session . get transaction ( ) . begin ( ) ; object result = session . create query ( from jpa test entity where text = : text ) . set parameter ( text , unique _ text ) . get single result ( ) ; session . get transaction ( ) . commit ( ) ;
quota . set memory ( quota . get memory ( ) * arguments . rate multiplier ) ; starting stats . msg rate in = quota . get msg rate in ( ) ; starting stats . msg rate out = quota . get msg rate out ( ) ; starting stats . msg throughput in = quota . get bandwidth in ( ) ; starting stats . msg throughput out = quota . get bandwidth out ( ) ; final bundle data bundle data = new bundle data ( 10 , 1000 , starting stats ) ;
uac time = do update classes work ( client , null , null , del pattern , null ) ; res . update with row ( uac time ) ; }
object result = future . get ( 10 , time unit . seconds ) ; assert null ( result ) ; log . tracef ( put operation is done ) ;
find view by id ( r . id . ll _ about _ report _ bug ) . set on click listener ( new view . on click listener ( ) { @ override public void on click ( view v ) { cts . launch url ( https : github . com hora apps leaf pic issues ) ; } } ) ;
use configuration ( - - fdo _ optimize = does not exist ) ; reporter . remove handler ( fail fast handler ) ; try { update ( default flags ( ) . with ( flag . keep _ going ) ) ; fail ( ) ; } catch ( invalid configuration exception e ) { assert contains event ( no such package ' does not exist ' : build file not found on package path ) ; } }
cleanup operation result cleanup operation result = client a . cleanup ( options ) ; assert equals ( cleanup result code . ok , cleanup operation result . get result code ( ) ) ; assert equals ( 4 , cleanup operation result . get merged database files count ( ) ) ; assert equals ( 0 , cleanup operation result . get removed multi chunks count ( ) ) ; assert equals ( 0 , cleanup operation result . get removed old versions count ( ) ) ; test client client b = new test client ( b , test connection ) ;
assert equals ( 5 , data . length ) ; assert equals ( 65 , data [ 0 ] ) ; assert equals ( 66 , data [ 1 ] ) ; assert equals ( 67 , data [ 2 ] ) ; assert equals ( - 61 , data [ 3 ] ) ; assert equals ( - 90 , data [ 4 ] ) ; }
assert false ( m . is enabled ( mapper feature . sort _ properties _ alphabetically ) ) ;
if ( log . is debug enabled ( ) ) { log . debug ( getting broadcast variable ( + key + ) - first access , materializing . ) ; } array list < t > data = new array list < t > ( ) ;
if ( lc . captures > 0 ) { evt . event phase = event . capturing _ phase ; ancestors are scanned , root to target , for capturing listeners . for ( int j = pv . size ( ) - 1 ; j > = 0 ; - - j ) { if ( evt . stop propagation ) break ; someone set the flag . phase ends . handle all capturing listeners on this node node impl nn = ( node impl ) pv . get ( j ) ; evt . current target = nn ; list < lentry > node listeners = get event listeners ( nn ) ; if ( node listeners = null ) { list < lentry > nl = ( list ) ( ( array list ) node listeners ) . clone ( ) ; call listeners in the order in which they got registered int nlsize = nl . size ( ) ; for ( int i = 0 ; i < nlsize ; i + + ) { lentry le = nl . get ( i ) ; if ( le . use capture & & le . type . equals ( evt . type ) & & node listeners . contains ( le ) ) { try { le . listener . handle event ( evt ) ; } catch ( exception e ) { all exceptions are ignored . } } } } } }
sstable reader r0 = mock schema . sstable ( 0 , cfs ) , r1 = mock schema . sstable ( 1 , cfs ) , r2 = mock schema . sstable ( 2 , cfs ) ;
process inode ( in , v , skip blocks , , false ) ; num inodes - - ; while ( num inodes > 0 ) { num inodes - = process directory ( in , v , skip blocks ) ; } }
collection < double > long string = get double list ( median , field facets , string _ sd , double , long ) ;
string [ ] projection = new string [ ] { modules columns . _ id , modules columns . repo _ id , modules columns . pkgname , modules columns . title , modules columns . summary , modules columns . description , modules columns . description _ is _ html , modules columns . author , modules columns . support , modules columns . created , modules columns . updated , } ; string where = modules columns . preferred + = 1 and + modules columns . pkgname + = ? ;
merge region dirs ( target , hi ) ;
animator background = object animator . of argb ( fab expand , view utils . background _ color , context compat . get color ( this , r . color . designer _ news ) , context compat . get color ( this , r . color . background _ light ) ) . set duration ( fab expand duration ) ;
if ( count contact groups ( ) = = 0 ) return true ; else return false ; }
return rect arr2 [ 0 ] < rect arr1 [ 2 ] ;
write raw bytes ( null byte , 0 , 1 ) ;
process instance = runtime service . start process instance by key ( one task process , 456 , collection util . singleton map ( var , value ) ) ;
replica group strategy config strategy config = table config to compare . get validation config ( ) . get replica group strategy config ( ) ;
down _ prot . down ( new event ( event . stable , stable _ digest ) ) ;
final map < integer , collection < tuple match > > expected matches map = left outer join tuples ( collect tuple data ( input1 ) , collect tuple data ( input2 ) ) ; final flat join function matcher = new tuple match removing join ( expected matches map ) ;
java type key type , content type ; if ( param types . size ( ) > 0 ) { key type = param types . get ( 0 ) ; content type = ( param types . size ( ) > = 2 ) ? param types . get ( 1 ) : _ unknown type ( ) ; return map type . construct ( clz , key type , content type ) ; } return _ map type ( clz ) ;
remote cache < object , object > remote = cache factory1 . get hot rod cache ( ) ; assert null ( remote . with flags ( flag . force _ return _ value ) . put ( key , value1 ) ) ;
for ( int i = 0 ; i < enumeration . length ; i + + ) { f notation enum vals . put ( enumeration [ i ] , attribute name ) ; } if ( f table of notationattribute names . contains key ( element name ) = = false ) { f table of notationattribute names . put ( element name , attribute name ) ; } else {
buffer . position ( buffer . position ( ) + 1 ) ; continue ;
if ( target = = null ) { append to start of list end . next = this . start ; this . start = start ; } else { next = target . next ; target . next = start ; start . prev = target ; end . next = next ; if ( next = null ) next . prev = end ; }
assert equals ( should truncate bytes at nul terminator , unicode . length ( ) , native . to string ( unicodez . get bytes ( native . default _ encoding ) ) . length ( ) ) ; }
assert query ( select count ( cast ( orderkey as varchar ) | | try ( to _ base ( 100 , cast ( round ( totalprice 100 ) as bigint ) ) ) ) from orders , select sum ( case when cast ( round ( totalprice 100 ) as bigint ) between 2 and 36 then 1 else 0 end ) from orders ) ;
int idx start = drop sql . index of ( table str , 0 ) + 5 ; int idx end = drop sql . last index of ( drop str ) ; table name = drop sql . substring ( idx start , idx end ) . trim ( ) ; if ( table name . starts with ( \ ) & & table name . ends with ( \ ) ) { table name = table name . substring ( 1 , table name . length ( ) - 1 ) ; }
g . set time range ( 0 l , ts - 1 ) ;
int [ ] [ ] source blocks = get block addresses ( session , lob id , 0 , integer . max _ value ) ; int [ ] [ ] target blocks = get block addresses ( session , new lob id , 0 , integer . max _ value ) ; try { copy block set ( source blocks , target blocks ) ; } catch ( hsql exception e ) { return result . new error result ( e ) ; }
final float alpha = sprite . get alpha ( ) ; final float color abgrpacked int = color utils . convert rgbato abgrpacked float ( sprite . get red ( ) * alpha , sprite . get green ( ) * alpha , sprite . get blue ( ) * alpha , alpha ) ; this . m sprite batch . draw without checks ( sprite , color abgrpacked int ) ;
map operator base < in , out , map function < in , out > > po = new map operator base < in , out , map function < in , out > > ( function , new unary operator information < in , out > ( get input type ( ) , get result type ( ) ) , name ) ;
count = select once ( select type . timeout , 100 ) ;
sample slsb no interface view = ( sample slsb ) ctx . lookup ( java _ global _ namespace _ prefix + app _ name + + module _ name + + ejb name + + sample slsb . class . get name ( ) ) ;
async timeout node = head . next ;
oos . write object ( connection provider instanceof serializable ? connection provider : null ) ;
region locator locator = test _ util . get connection ( ) . get region locator ( table name ) ; string region name = locator . get all region locations ( ) . get ( 0 ) . get region info ( ) . get encoded name ( ) ; hregion region = ( hregion ) test _ util . get rsfor first region in table ( table name ) . get region ( region name ) ; hstore store = region . get stores ( ) . iterator ( ) . next ( ) ; cache config cache conf = store . get cache config ( ) ; cache conf . set cache data on write ( true ) ; cache conf . set evict on close ( true ) ; block cache cache = cache conf . get block cache ( ) ; insert data ( table ) ;
if ( text = null & & message content view . get text ( ) . length ( ) = = 0 ) { message content view . set characters ( text ) ; } string type = intent . get type ( ) ;
response response = interceptor . submit application ( null , null ) ; assert . assert equals ( bad _ request , response . get status ( ) ) ;
long folder id = intent . get long extra ( notes . intent _ extra _ folder _ id , 0 ) ; int widget id = intent . get int extra ( notes . intent _ extra _ widget _ id , app widget manager . invalid _ appwidget _ id ) ; int widget type = intent . get int extra ( notes . intent _ extra _ widget _ type , notes . type _ widget _ invalide ) ; int bg res id = intent . get int extra ( notes . intent _ extra _ background _ id , resource parser . get default bg id ( this ) ) ;
put routable factory ( message _ batchdocumentupdate , new routable factories50 . batch document update message factory ( ) , from50 ) ;
process result second build result = workspace . run buck build ( : a ) ; second build result . assert success ( successful build should exit with 0 . ) ; build target b = build target factory . new instance ( : b ) ;
assert u ( del i ( 0 ) ) ;
path test dir = get data test dir ( dfscluster _ + uuid . random uuid ( ) . to string ( ) ) ;
try { get connection ( connection idx ) . get account manager ( ) . create account ( username , password ) ; created user idx . add ( connection idx ) ; } catch ( xmppexception e ) { e . print stack trace ( ) ; fail ( e . get message ( ) ) ; }
return new rpc ( keys . get ( 0 ) . home _ node ( ) , rpc ) . add completer ( this ) . call ( ) ;
in . close ( ) ; in = null ; if ( output filename = = null ) { out = new output stream writer ( system . out , charset ) ; } else { out = new output stream writer ( new file output stream ( output filename ) , charset ) ; }
this . connection map . remove ( connection . get connection id ( ) , endpoint ) ;
for ( int count = 0 ; count < dimension count ; count + + ) { external type + = java constants . type _ array ; } return external type ;
in order . verify ( o ) . on next ( 2 ) ; in order . verify ( o , never ( ) ) . on next ( 0 ) ; in order . verify ( o , never ( ) ) . on next ( 1 ) ; in order . verify ( o , never ( ) ) . on next ( any int ( ) ) ; in order . verify ( o ) . on error ( any ( test exception . class ) ) ; verify ( o , never ( ) ) . on complete ( ) ; }
filtering = false ;
_ must parse . put ( unit , true ) ;
try ( jdbcprepared statement stmt sel = session . prepare statement ( sel _ db2 lk ) ) { stmt sel . set int ( 1 , token ) ; try ( jdbcresult set db result = stmt sel . execute query ( ) ) { clob ddl stmt ; long ddl length ; long ddl start = 1 l ; while ( db result . next ( ) ) { ddl stmt = db result . get clob ( 1 ) ; try { ddl length = ddl stmt . length ( ) + 1 l ; sb . append ( ddl stmt . get sub string ( ddl start , ddl length . int value ( ) ) ) ; sb . append ( line _ sep ) ; } finally { try { ddl stmt . free ( ) ; } catch ( throwable e ) { log . debug ( error freeing clob : + e . get message ( ) ) ; } } } } } monitor . worked ( 2 ) ;
if ( pm . check permission ( target . activity info . permission , src package ) = package manager . permission _ granted ) { return false ; } if ( utilities . atleast _ marshmallow ) { these checks are sufficient for below m devices . return true ; }
if ( at . get id ( ) . equals ( ta id ) ) { return at . get finish time ( ) ; }
jsonarray objects = new jsonarray ( ) ; for ( url info url info : urls ) { jsonobject obj = new jsonobject ( ) ; obj . put ( url , url info . get url ( ) ) ; objects . put ( obj ) ; }
soapmessage response = read soap response ( soap action header , sm tr064 request , _ url + tr064service . get control url ( ) ) ; logger . trace ( raw soap response from fritz box : { } , soap to string ( response ) ) ; if ( response = = null ) { logger . warn ( error retrieving soap response from fritz box ) ; continue ; } values . put all ( item map . get soap value parser ( ) . parse values from soap message ( response , item map , item configuration ) ) ;
node new callee = node util . new qname ( compiler , base class node . get qualified name ( ) + . call , callee , goog . base ) ; n . replace child ( callee , new callee ) ; compiler . report change to enclosing scope ( new callee ) ; } else {
string auth = basic c2 nvd hq6c2 vjcm v0 ; string out = template . request body and header ( netty - http : http : localhost : { { port } } foo , hello world , authorization , auth , string . class ) ; assert equals ( bye world , out ) ; assert mock endpoints satisfied ( ) ;
init map processor init processor = new init map processor ( ) ; for ( int i = 0 ; i < entries ; + + i ) { map . execute on key ( i , init processor ) ; } assert equals ( entries , map . size ( ) ) ;
slot = slot . ordered next ; }
position - = set bits in current word ; }
test same ( var foo = [ ' foo ' , [ ' bar ' ] ] ; ) ;
get gc resource ( ) . get model ( ) . print detailed information ( ) ; }
int current index = start index ; while ( true ) { final geo point current point = input . get ( current index ) ; no identical points . add ( current point ) ; while ( true ) { current index = get legal index ( current index + 1 , input . size ( ) ) ; if ( current index = = start index ) { break ; } final geo point next non identical point = input . get ( current index ) ; if ( next non identical point . is numerically identical ( current point ) ) { break ; } } if ( current index = = start index ) { break ; } } if ( no identical points . size ( ) < 3 ) { return null ; }
if ( value instanceof list ) { value = ( ( list ) value ) . to array ( ) ; } list < object > list of arguments = new array list < object > ( ) ; list of arguments . add ( key ) ;
if ( this . read buffer configured for write = read buffer con figured for write ) { if ( read buffer con figured for write ) { switching to write int remaining = read buffer . remaining ( ) ; if ( remaining = = 0 ) { read buffer . clear ( ) ; } else { read buffer . compact ( ) ; } } else { switching to read read buffer . flip ( ) ; } this . read buffer configured for write = read buffer con figured for write ; }
assert equals ( true , thread context . get header ( header . 1 ) ) ;
assert not null ( ext . get field names ( ) . get ( html ) ) ;
return get contributor data for dimension ( metric id , current start , current end , baseline start , baseline end , dimension , filters , granularity ) ;
fsutils . validate root path ( new path ( hdfs : a : 9000 hbase ) ) ;
check non existent file ( ) ;
return new native boolean ( b ) ;
meta data meta data = new meta data ( ) ;
if ( width = = 0 ) return ; if ( m num answer lines = = 1 ) { m contents view . measure ( measure spec . make measure spec ( width - refine width , measure spec . exactly ) , measure spec . make measure spec ( height , measure spec . exactly ) ) ; } m contents view . get layout params ( ) . width = m contents view . get measured width ( ) ; m contents view . get layout params ( ) . height = m contents view . get measured height ( ) ; m refine view . measure ( measure spec . make measure spec ( m refine width , measure spec . exactly ) , measure spec . make measure spec ( height , measure spec . exactly ) ) ; m refine view . get layout params ( ) . width = m refine view . get measured width ( ) ; m refine view . get layout params ( ) . height = m refine view . get measured height ( ) ;
try { int argcnt = global . size ( ) ; exec2 ex = new exec2 ( str , global ) ; ast ast = ex . parse ( ) ; env . push ( global . size ( ) - argcnt ) ; push space for temps ast . exec ( env ) ; env . post write ( ) ; } catch ( runtime exception t ) { env . remove _ and _ unlock ( ) ; throw t ; } return env ;
assert match no docs query ( 3 , null ) ; assert query equals ( term 1 . 0 1 2 , null , term ) ; assert query equals ( term term1 term2 , null , term term term ) ; analyzer a = new mock analyzer ( random ( ) , mock tokenizer . whitespace , false ) ;
boolean s1 mv = sources . get ( 0 ) instanceof multi value source ; boolean s2 mv = sources . get ( 1 ) instanceof multi value source ; if ( s1 mv & & s2 mv ) { mvr . mv1 = ( multi value source ) sources . get ( 0 ) ; mvr . mv2 = ( multi value source ) sources . get ( 1 ) ; } else if ( s1 mv | | s2 mv ) {
if ( new selection = null ) { for ( string selection arg : selection args ) { new selection = new selection . replace first ( \ \ ? , selection arg ) ; } } this . condition string = new selection ; }
stream . write ( new byte [ buffer _ size ] ) ; assert . assert equals ( buffer _ size , stream . get buf ( ) . length ) ;
if ( get child count ( ) < = 0 ) return super . on intercept touch event ( ev ) ;
logger . error ( unexpected error writing on + connection id , cause ) ;
int visible column count = prefs . get int ( id + - visible - column - count , column . length ) ;
return utf8 string . from bytes ( new byte [ 0 ] ) ;
assert equals ( 2 l , task service . create task query ( ) . count ( ) ) ;
jtable helper . set optimal header width ( this , i ) ;
wait for stable quota size ( conn , tn , null ) ;
if ( raw base . is assignable from ( subtype . get type ( ) ) ) { yes annotated class curr = annotated class . construct without super types ( subtype . get type ( ) , ai , config ) ; _ collect and resolve ( curr , subtype , config , ai , subtypes ) ; }
share a = 10 * byte utils . bytes _ per _ mb ; share b = total cache - share a ; def a = test utils . make store definition ( store a , share a ( 1024 * 1024 ) ) ; bdb storage . update ( def a ) ; cache size a = long . min _ value ;
headers . put ( camel box . types , null ) ; @ suppress warnings ( rawtypes ) final java . util . list result = request body and headers ( direct : getenterpriseevents , null , headers ) ;
a [ left ] = a [ less - 1 ] ; a [ less - 1 ] = pivot1 ;
method builder method = c . get method ( builder ) ;
do run ( ) ;
return instance _ size + buffer . get retained size ( ) ;
oval path . reset ( ) ; oval path . move to ( get width ( ) 2 , bottom ) ; middle of the bottom side of the circle oval path . arc to ( oval rect , 90 , 90 , false ) ; draw arc to the left oval path . line to ( left , bottom ) ; move to bottom - left corner oval path . line to ( get width ( ) 2 , bottom ) ; move back to origin oval path . close ( ) ; canvas . draw path ( oval path , viewport paint ) ;
assert equals ( three , last value for subscriber1 . get ( ) ) ; system . out . println ( about to send on complete ) ;
hide serialization progress ( ) ;
output = strip first line ( output ) ; return new byte array input stream ( output ) ;
return autowire ; }
boolean descending next = false ; for ( sort term term : sort term ) { if ( term . equals ( sort term . reverse ) ) { if ( descending next ) { throw new illegal argument exception ( double reverse in sort term is not allowed ) ; } descending next = true ; } else { sort terms with descending . add ( new sort term with descending ( term , descending next ) ) ; descending next = false ; } } return sort terms with descending ;
if ( text utils . is empty ( data . get center text1 ( ) ) ) { final int text1 height = math . abs ( center circle text1 font metrics . ascent ) ; if ( text utils . is empty ( data . get center text2 ( ) ) ) { draw text 2 only if text 1 is not empty . final int text2 height = math . abs ( center circle text2 font metrics . ascent ) ; canvas . draw text ( data . get center text1 ( ) , center x , center y - text1 height * 0 . 2f , center circle text1 paint ) ; canvas . draw text ( data . get center text2 ( ) , center x , center y + text2 height , center circle text2 paint ) ; } else { canvas . draw text ( data . get center text1 ( ) , center x , center y + text1 height 4 , center circle text1 paint ) ; } }
tmp = get array dimensions ( ( element ) children . get ( 0 ) ) ;
hcolumn descriptor hcd = new hcolumn descriptor ( family ) ;
not member view . set background color ( style . get main background color ( ) ) ; ( ( text view ) not member view . find view by id ( r . id . not _ member _ text ) ) . set text color ( style . get text primary color ( ) ) ;
backup . incremental ( backup path . get path ( ) ) ;
return r ; }
result = ssl host configs . get ( sni host name ) ;
try { int value = integer . parse int ( unicode . to string ( ) , 16 ) ; out . write ( ( char ) value ) ; unicode . set length ( 0 ) ; in unicode = false ; had slash = false ; } catch ( number format exception nfe ) { throw new runtime exception ( unable to parse unicode value : + unicode , nfe ) ; }
font . default font ( ) . draw ( screen , ( current page + 1 ) + + total pages , menu _ width 2 , 261 , font . align . centered ) ;
throw new illegal state exception ( unexpected exception thrown : + e . get message ( ) , e ) ;
assert not null ( response . class . get method ( success , object . class , cache . entry . class ) ) ; assert not null ( response . class . get method ( error , volley error . class ) ) ; assert not null ( response . class . get method ( is success ) ) ; assert not null ( response . listener . class . get declared method ( on response , object . class ) ) ;
remote iterator < zone reencryption status > it = dfs admin . list reencryption status ( ) ; assert true ( it . has next ( ) ) ; zone reencryption status zs = it . next ( ) ; assert equals ( zone . to string ( ) , zs . get zone name ( ) ) ; assert equals ( zone reencryption status . state . completed , zs . get state ( ) ) ; assert true ( zs . get completion time ( ) > 0 ) ; assert true ( zs . get completion time ( ) > zs . get submission time ( ) ) ; assert equals ( 10 , zs . get files reencrypted ( ) ) ; }
list verified mock = verify ( mock2 ) ;
database = null ; }
set storage info ( last node , null ) ; return true ;
seen . remove ( a ) ;
class loader old = thread . current thread ( ) . get context class loader ( ) ; try { thread . current thread ( ) . set context class loader ( cl ) ; deployment md = activator . do deploy ( ) ; } finally { thread . current thread ( ) . set context class loader ( old ) ; } string ra name = deployment md . get deployment name ( ) ; service name ra service name = connector services . get resource adapter service name ( ra name ) ; value = new resource adapter deployment ( deployment md , ra name , ra service name ) ; registry . get value ( ) . register resource adapter deployment ( value ) ; management repository . get value ( ) . get connectors ( ) . add ( value . get deployment ( ) . get connector ( ) ) ; context . get child target ( ) . add service ( ra service name , new resource adapter service ( ra service name , value . get deployment ( ) . get resource adapter ( ) ) ) . set initial mode ( mode . active ) . install ( ) ;
if ( new name = = null ) { find an acceptable new name . name factory . reset ( ) ; do { new name = name factory . next name ( ) ; } while ( name map . contains key ( new name ) ) ; remember not to use the new name again in this name space . name map . put ( new name , name ) ; assign the new name . set new member name ( member , new name ) ; }
mockito . verify ( machine source adapter ) . serialize ( eq ( machine source ) , eq ( machine source . class ) , any ( json serialization context . class ) ) ;
clob . get contents ( session . get progress monitor ( ) ) ;
throw new schema exception ( hive metastore database is not initialized . please use + schematool ( e . g . . schematool - init schema - db type . . . ) to create the schema . if + needed , don ' t forget to include the option to auto - create the underlying database + in your jdbc connection string ( e . g . ?create database if not exist = true for mysql ) ) ;
if ( service instanceof protocol provider service ) { int service event type = service event . get type ( ) ; if ( service event type = = service event . registered ) handle provider added ( ( protocol provider service ) service ) ; else if ( service event type = = service event . unregistering ) handle provider removed ( ( protocol provider service ) service ) ; }
revoked partitions = assigned partitions ; rebalance listener . on partitions revoked ( revoked partitions ) ; assigned partitions = collections . empty list ( ) ; rebalance listener . on partitions assigned ( assigned partitions ) ; thread . run once ( - 1 ) ; assert true ( thread . tasks ( ) . is empty ( ) ) ;
entity manager bind ( ee module description , service target , pu , pu service name , transaction manager , transaction synchronization registry ) ;
string october id = xpath . evaluate ( gf : watertemp [ gf : location = ' ncom _ wattemp _ 000 _ 20081031 t0000000 _ 12 . tiff ' ] @ fid , dom ) ; string november id = xpath . evaluate ( gf : watertemp [ gf : location = ' ncom _ wattemp _ 000 _ 20081101 t0000000 _ 12 . tiff ' ] @ fid , dom ) ; dom = get as dom ( rest base controller . root _ path + workspaces wcs coveragestores watertemp coverages watertemp index granules + october id + . xml ) ;
log . error ( e . get message ( ) , e ) ;
delegate = application context . get bean ( dimension default value selection strategy factory impl . class ) ;
assert that ( db . get db client ( ) . quality profile dao ( ) . select ordered by organization uuid ( db session , org ) ) . is empty ( ) ;
cache = new lru taxonomy writer cache ( ncats 10 ) ;
assert true ( am client . remote requests table . is empty ( ) ) ; am client . unregister application master ( final application status . succeeded , null , null ) ;
res = worker resource . do enable ( ) ;
if ( values . length < other . values . length ) { create a new one . values = new value [ other . values . length ] ; }
response get resp = client . target ( jaxrs _ app _ url ) . request ( ) . header ( http headers . authorization , auth header ) . get ( ) ;
store . dec ref ( ) ;
list < object > keys = offer and drain ( queue , key ) ;
invocation matcher invocation matcher = matchers binder . bind matchers ( mocking progress ( ) . get argument matcher storage ( ) , invocation ) ; invocation container . set method for stubbing ( invocation matcher ) ; return null ;
server name random server = servers . get ( random . next int ( servers . size ( ) ) ) ; assignments . get ( random server ) . add ( region ) ; num random assignments + + ; if ( old server name = null ) old hosts no longer present . add ( old server name . get hostname ( ) ) ; } else if ( local servers . size ( ) = = 1 ) {
jar _ = package job jar ( ) ; if ( jar _ = null ) { job conf _ . set jar ( jar _ ) ; } if ( ( cache archives = null ) | | ( cache files = null ) ) { get uris ( cache archives , cache files ) ; boolean b = distributed cache . check uris ( file uris , archive uris ) ; if ( b ) fail ( link _ uri ) ; }
int p2 = get precedence ( ops . add , ops . sub ) ;
if ( unused children = null ) { element . set attribute ( iproject type . unused _ children , unused children ) ; }
weka offscreen chart renderer rcr = new weka offscreen chart renderer ( ) ; string tip text = rcr . options tip text html ( ) ; tip text = tip text . replace ( < html > , < html > comma separated list of options : < br > ) ; opts lab . set tool tip text ( tip text ) ; } else {
set request auth ( cite , cite ) ;
menu item check clipboard item = new menu item ( tools . get label ( messages . get string ( gui menu check clipboard ) ) ) ;
model node connector = new server . get ( constants . http _ listener , http ) ;
assert false ( threshold . reached ( file , 2 l , info ) ) ; assert true ( threshold . reached ( file , 1 l , info ) ) ; }
assert equals ( command test async , command . get thread pool key ( ) . name ( ) ) ;
test constraint passed ( get constraint ( test string , avail ) , ) ; test constraint default message ( get constraint ( test string , avail ) , a , property [ { 0 } ] of class [ { 1 } ] with value [ { 2 } ] is not contained within the list [ { 3 } ] ) ;
start database ( copy dir , record formats name ) . shutdown ( ) ;
second . get lifecycle service ( ) . terminate ( ) ; wait all for safe state ( first ) ; assert true eventually ( new all tasks running within num of nodes ( scheduler , 1 ) ) ; }
midnight before transition = time ( 1990 - 03 - 31 t00 : 00 : 00 . 000 - 04 : 00 ) ;
to one relation metadata generator . add one to one primary key join column ( property auditing data , value , current mapper , entity name , insertable ) ; } }
byte string name = byte string . encode utf8 ( headers . name ( i ) . to lower case ( locale . us ) ) ;
for ( inode inode : caching temp queue ) { cache name internal ( inode ) ; } this . caching temp queue = null ; this . image loaded = true ; }
expected elements . add all ( get non aggregated entities ( ) ) ; expected elements . add all ( get non aggregated entities ( ) ) ; expected elements . add all ( get non aggregated edges ( ) ) ; expected elements . add all ( get non aggregated edges ( ) ) ; result elements . sort ( get json sort ( ) ) ;
if ( this . get object name ( ) = null ) { notification notification = new notification ( j2ee . state . stopped , this . get object name ( ) , sequence number + + ) ; broadcaster . send notification ( notification ) ; }
iv . set image drawable ( drawer image loader . get instance ( ) . get image loader ( ) . placeholder ( iv . get context ( ) , drawer image loader . tags . profile . name ( ) ) ) ;
if ( ( child instanceof index scan plan node ) = = false ) return plan ; index scan plan node isp = ( index scan plan node ) child ;
if ( response type . get name ( ) . equals ( thymeleaf ) ) { create link factory class ( cid builder . get name ( ) ) ; } } else {
if ( this itr . prepare next ( ) | * do not use | | * other itr . prepare next ( 1 ) ) { return false ; }
app . get context ( ) . get event handler ( ) . handle ( new task attempt event ( task1 attempt1 . get id ( ) , task attempt event type . ta _ done ) ) ; app . wait for state ( task1 attempt1 , task attempt state . succeeded ) ;
assert equals ( a & quot ; b & lt ; c & gt ; d & amp ; , html escaper ( ) . escape ( a \ b < c > d & ) ) ;
init on completions ( abstracts , upper , on completions ) ;
assert true ( region . is row locked ( locked row ) ) ;
dfs = new mini dfscluster ( conf , slaves , true , null ) ;
read only file channel . close ( ) ;
input stream is = btceticker jsontest . class . get resource as stream ( v3 marketdata example - ticker - data . json ) ;
assert equals ( hello world , out . get in ( ) . get body ( string . class ) ) ;
m recycler view . set item animator ( new default item animator ( ) ) ;
gles20 . gl uniform matrix4fv ( mvp matrix handle , 1 , false , m mvpmatrix , 0 ) ; gles20 . gl enable ( gles20 . gl _ blend ) ;
assert equals ( expected matching job ids , job id , client . get job ( job id ) . get job status ( ) . get job id ( ) ) ; assert equals ( expected matching start times , rj . get job status ( ) . get start time ( ) , client . get job ( job id ) . get job status ( ) . get start time ( ) ) ; } finally {
debug servlet . clear ( ) ; req = new update request ( ) ; client . set query params ( set of ( server only , both ) ) ; req . set query params ( set of ( request only , both ) ) ; set req params of ( req , server only , request only , both , neither ) ; try { client . request ( req ) ; } catch ( parse exception ignored ) { } verify servlet state ( client , req ) ;
log . warn ( + unmatched rows + unmatched rows are found : + row text ) ;
toast . make text ( this , r . string . google _ play _ services _ failed , toast . length _ long ) . show ( ) ;
try { reservation id reservation id = reservation system test util . get new reservation id ( ) ; agent right . create reservation ( reservation id , u1 , plan , rr1 ) ; fail ( ) ; } catch ( planning exception e ) { expected failure }
a searcher . get index reader ( ) . dec ref ( ) ; }
set < symbol > try arguments = parent . get assignments ( ) . get expressions ( ) . stream ( ) . flat map ( expression - > extract try arguments ( expression ) . stream ( ) ) . collect ( to set ( ) ) ; set < symbol > singletons = dependencies . entry set ( ) . stream ( ) . filter ( entry - > entry . get value ( ) = = 1 ) reference appears just once across all expressions in parent project node . filter ( entry - > try arguments . contains ( entry . get key ( ) ) ) they are not inputs to try . otherwise , inlining might change semantics . filter ( entry - > child . get assignments ( ) . is identity ( entry . get key ( ) ) ) skip identities , otherwise , this rule will keep firing forever . map ( map . entry : : get key ) . collect ( to set ( ) ) ;
if ( expression instanceof expression ) { this . expression = expression node helper . to expression definition ( ( expression ) expression ) ; } else if ( expression instanceof predicate ) { this . expression = expression node helper . to expression definition ( ( predicate ) expression ) ; } else { this . expression = expression ; }
list . set selected indices ( indices ) ; }
annotated java type param type = parameter types . get ( i ) ;
canvas . save ( ) ;
cached store . stop cache update service ( 1 ) ;
buffer . duplicate ( ) . get ( data ) ; return data ;
dispatch executor . interrupt dispatch ( store . host id ) ;
response = client ( ) . admin ( ) . indices ( ) . prepare get mappings ( * ) . set types ( * ) . execute ( ) . action get ( ) ;
buffer . clear ( ) ; buffer . limit ( 3 ) ; channel . read ( buffer , 5 ) ; buffer . flip ( ) ; assert equals ( ( byte ) 0 , buffer . get ( ) ) ; assert equals ( ( byte ) 2 , buffer . get ( ) ) ; assert equals ( ( byte ) 0 , buffer . get ( ) ) ;
assert query ( select b from nation n , ( values ( 0 , null ) , ( 0 , cast ( - 1 as bigint ) ) , ( 0 , cast ( 0 as bigint ) ) ) t ( a , b ) where n . regionkey + 100 > t . b and n . nationkey = t . a , values - 1 , 0 ) ;
while ( cache iterator . has next ( ) & & is deleted cache entry ( cache iterator . peek next ( ) ) ) { if ( store iterator . has next ( ) ) { final ks next store key = store iterator . peek next key ( ) ; advance the store iterator if the key is the same as the deleted cache key if ( compare ( cache iterator . peek next key ( ) , next store key ) = = 0 ) { store iterator . next ( ) ; } } cache iterator . next ( ) ; } return cache iterator . has next ( ) | | store iterator . has next ( ) ;
final hash set < string > ports = new hash set < > ( ) ;
database . run command ( new document ( build info , 1 ) , new single result callback < document > ( ) { @ override public void on result ( final document build info , final throwable t ) { system . out . println ( build info ) ; } } ) ;
builder . delete char at ( builder . length ( ) - 1 ) ; }
if ( ce . get status ( ) . get code ( ) = = iresource status . operation _ failed ) { throwable e = ce . get status ( ) . get exception ( ) ; if ( e instanceof java model exception ) { throw ( java model exception ) e ; } }
logger . trace ( execute ( ) method is called ) ; try { open port ( ) ; get all configured keys from providers , get needed read commands for them , send those read commands for ( davis binding provider provider : providers ) { collection < davis command > commands = davis value type . get read commands by keys ( provider . get configured keys ( ) ) ; for ( davis command command : commands ) { if ( wakeup ( ) ) { send command ( command ) ; } else { logger . warn ( wakeup failed , trying reset sequence ) ; reset after error ( ) ; } } } close port ( ) ; } catch ( initialization exception e ) { logger . error ( e . get message ( ) ) ; }
int pos = xml string . index of ( : sld layout ) ; 11 int close tag pos = xml string . index of ( > , pos ) ; if ( xml string . substring ( pos , close tag pos ) . contains ( vml _ decl ) ) { nothing to do ; vml namespace is already declared } else { xml string = xml string . substring ( 0 , pos + 11 ) + vml _ decl + + xml string . substring ( pos + 11 ) ; } try { ioutils . write ( xml string , os , utf - 8 ) ; be sure to write utf - 8 irrespective of default encoding * fix confirmed by running a presentation containing eg mÃ¶g * through round trip test , * with run configuration setting - dfile . encoding = iso - 8859 - 1 , * verified powerpoint ( 2010 ) can open it . * } catch ( ioexception e ) { throw new jaxbexception ( e . get message ( ) , e ) ; }
background color span [ ] color spans = m input . get text ( ) . get spans ( 0 , m input . length ( ) , background color span . class ) ; for ( background color span color span : color spans ) { m input . get text ( ) . remove span ( color span ) ; } }
if ( layout = null ) { try { layout . paint ( graphic , paint area , map content ) ; } catch ( exception e ) { throw new service exception ( problem occurred while trying to watermark data , e ) ; } } timeout . stop ( ) ;
for ( key value scanner scanner : snapshot . get scanners ( ) ) { scanner . close ( ) ; } memstore . clear snapshot ( snapshot . get id ( ) ) ; assert true ( chunk creator . get pool size ( ) > 0 ) ; }
cs . kill all apps in queue ( a ) ; rm1 . wait for state ( app1 . get application id ( ) , rmapp state . killed ) ; rm1 . wait for app removed from scheduler ( app1 . get application id ( ) ) ; assert equals ( 0 * gb , leaf queue . get metrics ( ) . get used amresource mb ( ) ) ; assert equals ( 0 , leaf queue . get metrics ( ) . get used amresource vcores ( ) ) ; rm1 . close ( ) ; }
dummy = dummy entry . create ( key , hash , entry ) ;
date = obj . to string ( ) ;
if ( cl id > - 1 & & check cluster access ( db , db . get cluster name by id ( cl id ) ) ) clusters . add ( db . get cluster name by id ( cl id ) . to lower case ( locale . english ) ) ;
state store . store container paused ( container id ) ;
int offset = 0 ; int segment count = 0 ; for ( segment s = buffer . head ; offset < byte count ; s = s . next ) { if ( s . limit = = s . pos ) { throw new assertion error ( s . limit = = s . pos ) ; empty segment . this should not happen } offset + = s . limit - s . pos ; segment count + + ; }
neo stores store = neo stores rule . builder ( ) . build ( ) ;
instance = null ;
assert true ( is type ( node , type ) ) ;
final float width = get bounds ( ) . width ( ) - m shadow paint . get stroke width ( ) ; final float height = get bounds ( ) . height ( ) - m shadow paint . get stroke width ( ) ; final float left = get bounds ( ) . left + ( m shadow paint . get stroke width ( ) 2 ) ; final float top = get bounds ( ) . top + ( m shadow paint . get stroke width ( ) 2 ) ;
app trust manager = get trust manager ( app key store ) ;
if ( spdy syn stream frame . is truncated ( ) ) { spdy rst stream frame spdy rst stream frame = new default spdy rst stream frame ( stream id , spdy stream status . internal _ error ) ; ctx . write and flush ( spdy rst stream frame ) ; return ; }
submit topology ( name , topo conf , topology , opts , new storm submitter . progress listener ( ) { @ override public void on start ( string src file , string target file , long total bytes ) { system . out . printf ( start uploading file ' % s ' to ' % s ' ( % d bytes ) \ n , src file , target file , total bytes ) ; } @ override public void on progress ( string src file , string target file , long bytes uploaded , long total bytes ) { int length = 50 ; int p = ( int ) ( ( length * bytes uploaded ) total bytes ) ; string progress = string utils . repeat ( = , p ) ; string todo = string utils . repeat ( , length - p ) ; system . out . printf ( \ r [ % s % s ] % d % d , progress , todo , bytes uploaded , total bytes ) ; } @ override public void on completed ( string src file , string target file , long total bytes ) { system . out . printf ( \ n file ' % s ' uploaded to ' % s ' ( % d bytes ) \ n , src file , target file , total bytes ) ; } } ) ;
log . debug ( running hfile cleaners ) ;
return is undirected & & check direction ( flag ) ;
context ( ) . get conductor ( ) . get conductor ( ) . on phone book imported ( ) ; return ;
if ( selected nodes . is empty ( ) ) { final list < node type > all nodes = graph helpers . get nodes ( m _ graph ) ; all nodes . remove all ( selected nodes ) ; show nodes ( selected nodes , all nodes ) ; }
new policy qualifier info ( null ) ;
if ( roles = null ) { cluster status . add ( roles , roles ) ; }
service . shutdown now ( ) ;
cleanup = cleanup phase . after _ method ;
student oracle no sqlsql date student max = new student oracle no sqlsql date ( ) ; student max . set age ( ( short ) get max value ( short . class ) ) ; student max . set id ( ( date ) get max value ( date . class ) ) ; student max . set name ( ( string ) get max value ( string . class ) ) ; em . persist ( student max ) ;
catch ( volt overflow exception e ) { m _ buffer . position ( pos ) ; throw e ; }
m callbacks = s dummy callbacks ; }
if ( project operations . is multimodule project ( ) ) { project operations . add dependency ( impl type . get module ( ) , new dependency ( org . springframework , spring - tx , , dependency type . jar , dependency scope . compile ) ) ; } }
service = new default directory service ( ) ;
app widget host . delete app widget id ( new widget ids [ i ] ) ; } } finally { cursor . close ( ) ; } } }
cas . add castor ( date2 string . class ) ;
ios = image io . create image output stream ( get file ( ) ) ; writer . set output ( ios ) ;
solutions . add ( current _ index , new _ solution ) ;
t = jp . next token ( ) ;
case is _ push _ supported : { atomic boolean result = ( atomic boolean ) param ; result . set ( is push supported ( ) ) ; break ; } case push _ request : { do push ( ( request ) param ) ; break ; }
int idx = buffer . reader index ( ) ; buffer . reader index ( 0 ) ;
if ( is subscription from ruim ) { new ss . set roaming ( is roaming between operators ( m cdma roaming , new ss ) ) ; } else { new ss . set roaming ( m cdma roaming ) ; }
file store root = new mock file store ( dir . to string ( ) + ( dev nvme0n1p1 ) , btrfs , dev nvme0n1p1 ) ;
collection < range > row id ranges = get ranges from domain ( row id domain , serializer ) ;
int tmp off x = offset x + size ; int tmp off y = offset x + size ; int tmp w = w + offset x + size + size ; int tmp h = h + offset x + size ;
return result ; } else if ( ch = = - 1 ) {
assert . assert equals ( { \ id \ : 123 , \ name \ : \ \ \ \ \ \ } , text ) ;
mutable object iterator < int pair > build input = new uniform int pair generator ( num _ keys , build _ vals _ per _ key , false ) ;
logger . fine ( not limiting styles ) ;
if ( combinable & & function instanceof combine function < ? , ? > ) { this . function = function instanceof rich group reduce function < ? , ? > ? new rich combine to group combine wrapper ( ( rich group reduce function < ? , ? > ) function ) : new combine to group combine wrapper ( ( combine function < ? , ? > ) function ) ; }
if ( down stream buffer size > 0 ) { codec . get conf ( ) . set int ( io _ file _ buffer _ size _ key , down stream buffer size ) ; } compression input stream cis = codec . create input stream ( down stream , decompressor ) ; buffered input stream bis2 = new buffered input stream ( cis , data _ ibuf _ size ) ; return bis2 ;
file status status = fs . get file status ( file path ) ; block location [ ] locations = fs . get file block locations ( status , 0 , 10 ) ; }
assert equals ( expected , f . with zone ( tokyo ) . parse local date time ( 2007 - 03 - 04 12 : 30 + 07 : 00 asia tokyo ) ) ;
payment transaction . get id ( ) . equals ( payment . get transactions ( ) . get ( payment . get transactions ( ) . size ( ) - 1 ) . get id ( ) ) ) { if ( payment smhelper . is success state ( current payment state name ) ) { last success payment state = current payment state name ; } else { for ( int i = payment . get transactions ( ) . size ( ) - 2 ; i > = 0 ; i - - ) { final payment transaction transaction = payment . get transactions ( ) . get ( i ) ; if ( transaction status . success . equals ( transaction . get transaction status ( ) ) ) { last success payment state = payment smhelper . get successful state for transaction ( transaction . get transaction type ( ) ) ; break ; } } } }
copy current event ( jp ) ; } }
log . info ( job { } was successfully submitted to the job manager { } . , ( ( job manager messages . job submit success ) message ) . job id ( ) , get sender ( ) . path ( ) ) ;
if ( init _ with _ ip ) { cname = addresses [ 0 ] . get host name ( false ) . to lower case ( ) ; } else { cname = inet address . get by name ( addresses [ 0 ] . get host address ( ) ) . get host name ( false ) . to lower case ( ) ; }
assert equals ( 1 , conn listener . action count by data store . size ( ) ) ; assert true ( conn listener . action count by data store . contains key ( source data store ) ) ; assert equals ( 1 , conn listener . action count by data store . get ( source data store ) . borrow count ) ; assert equals ( 1 , conn listener . action count by data store . get ( source data store ) . release count ) ; }
clip view on the right ( cur page bound , cur page width , right ) ;
vector expression ve = new vector udfadaptor ( expr , output column num , result type name , arg descs ) ;
validate response headers ( response headers , 200 , 200 ) ;
run and verify ( 2014 - 01 - 01 10 : 30 : 45 , 2014 - 01 - 31 , udf ) ; run and verify ( 2014 - 01 - 14 10 : 30 : 45 , 2014 - 01 - 31 , udf ) ; run and verify ( 2014 - 01 - 31 10 : 30 : 45 . 1 , 2014 - 01 - 31 , udf ) ; run and verify ( 2014 - 02 - 02 10 : 30 : 45 . 100 , 2014 - 02 - 28 , udf ) ; run and verify ( 2014 - 02 - 28 10 : 30 : 45 . 001 , 2014 - 02 - 28 , udf ) ; run and verify ( 2016 - 02 - 03 10 : 30 : 45 . 000000001 , 2016 - 02 - 29 , udf ) ; run and verify ( 2016 - 02 - 28 10 : 30 : 45 , 2016 - 02 - 29 , udf ) ; run and verify ( 2016 - 02 - 29 10 : 30 : 45 , 2016 - 02 - 29 , udf ) ;
http method method = request . get method ( ) ; if ( content type = = null & & method = null & & supported _ methods . contains ( method ) ) { flux < data buffer > body = request . get body ( ) . do on next ( o - > { body not empty , back to 415 . . throw new unsupported media type status exception ( media type , this . supported media types ) ; } ) ; if ( is body required | | ( adapter = null & & adapter . supports empty ( ) ) ) { body = body . switch if empty ( mono . error ( handle missing body ( body parameter ) ) ) ; } return ( adapter = null ? mono . just ( adapter . from publisher ( body ) ) : mono . from ( body ) ) ; } return mono . error ( new unsupported media type status exception ( media type , this . supported media types ) ) ;
final image view chevron = ( image view ) group view . find view by id ( r . id . stats _ list _ cell _ chevron ) ;
assert same ( s2 , ( ( abstract persistent collection ) p . children ) . get session ( ) ) ; s2 . get transaction ( ) . rollback ( ) ; s2 . close ( ) ; s1 . get transaction ( ) . rollback ( ) ; s1 . close ( ) ; }
cluster . set lease period ( lease period , lease period ) ;
em . get lock mode ( null ) ; assert . fail ( should have gone to catch block ) ; }
throw new too long frame exception ( ) ;
slice _ del ( ) ; break ; } limit _ backward = v _ 2 ; return true ; }
int fragment readable bytes = min ( header block . readable bytes ( ) , max fragment length ) ;
assert same ( lia mem . get segment memory ( ) , b mem . get segment memory ( ) ) ;
info . intent = new intent ( ) ;
try { reservation id reservation id = reservation system test util . get new reservation id ( ) ; agent right . create reservation ( reservation id , u1 , plan , rr1 ) ; fail ( ) ; } catch ( planning exception e ) { expected failure }
collection < completable future < execution > > allocation futures = ejv . allocate resources for all ( slot provider , queued , location preference constraint . all ) ; all allocation futures . add all ( allocation futures ) ;
string operation key = create operation key ( info . get name ( ) , signature . length ) ; operations . put ( operation key , signature ) ; }
file system fs = util . get hbase cluster ( ) . get master ( ) . get master file system ( ) . get file system ( ) ;
long create time = system . current time millis ( ) ;
out . put ( ( byte ) 0 ) ;
subscriptions . add ( client id ) ; core . client queues . put ( client id , new concurrent linked queue < namespace notification > ( ) ) ; core . subscriptions . put ( new namespace event key ( event ) , subscriptions ) ; core . subscriptions . put ( new namespace event key ( event2 ) , subscriptions ) ; core . clients . put ( client id , new client data ( client id , handler , host , 3000 ) ) ; dispatcher . assign client ( client id ) ; dispatcher . loop sleep time = 20 ; handler . fail chance = 0 . 8f ;
if ( is in node repo and reserved ( this ) & & is in node repo and reserved ( other ) ) return - 1 ;
mention . start index = sentence . size ( ) ;
for ( int cl id : cls . get polymorphic cluster ids ( ) ) get database ( ) . get metadata ( ) . get command cache ( ) . invalidate results of cluster ( get database ( ) . get cluster name by id ( cl id ) ) ; if ( value = null & & attribute = = attributes . superclass ) { check class exists ( database , class name , decode class name ( value ) ) ; }
render . init ( constants . get encoding ( ) , constants . get dev mode ( ) ) ;
percent = percent . divide ( pointfive ) . set scale ( 0 , rounding mode . up ) . multiply ( pointfive ) . min ( connection . get main volume max ( ) ) . max ( big decimal . zero ) ; string db string = string . value of ( percent . int value ( ) ) ;
out = parent . parent . parent . dir . create output ( index file names . segment file name ( parent . parent . parent . segment , index file names . prox _ extension ) ) ;
int max resolution = 0 ;
string message = parameters . get ( message ) ; if ( message = null & & message . length ( ) > 250 ) { message = message . substring ( 0 , 250 ) + . . . ; } server tools . print ( log message from client : + message + - user - agent : + http exchange . get request headers ( ) . get first ( user - agent ) ) ; string response = ok ; http exchange . send response headers ( http urlconnection . http _ ok , response . get bytes ( encoding ) . length ) ; http exchange . get response body ( ) . write ( response . get bytes ( encoding ) ) ; } else {
if ( params . has ( output ) ) { tweets . write as text ( params . get ( output ) ) ; } else { system . out . println ( printing result to stdout . use - - output to specify output path . ) ; tweets . print ( ) ; }
boolean disconnect required ; static host provider [ ] host provider array = new static host provider [ num clients ] ; inet socket address [ ] cur host for each client = new inet socket address [ num clients ] ; int [ ] num clients per host = new int [ 9 ] ;
return is fast stats same ( old part , new part ) ;
crosstool release crosstool = crosstool stl pair . crosstool release ; string builder toolchain map = new string builder ( ) ;
return state1 . compare to ( state2 ) ;
first set bit in word + = sequence length ;
alter . clear value updates ( ) ; alter . add value update ( value update . create add ( new string field value ( banana ) ) ) ; alter = serialized copy ( alter , test doc . get data type ( ) ) ; alter . apply to ( test doc ) ; assert equals ( 1 , ( int ) ( ( weighted set ) test doc . get field value ( strws ) ) . get ( new string field value ( banana ) ) ) ; alter . clear value updates ( ) ; alter . add value update ( value update . create decrement ( new string field value ( banana ) , 1 ) ) ; alter = serialized copy ( alter , test doc . get data type ( ) ) ; alter . apply to ( test doc ) ; assert equals ( 0 , ( int ) ( ( weighted set ) test doc . get field value ( strws ) ) . get ( new string field value ( banana ) ) ) ; alter2 . clear value updates ( ) ; alter2 . add value update ( value update . create add ( new string field value ( banana ) ) ) ; alter2 = serialized copy ( alter2 , test doc . get data type ( ) ) ; alter2 . apply to ( test doc ) ; assert equals ( 1 , ( int ) ( ( weighted set ) test doc . get field value ( strws2 ) ) . get ( new string field value ( banana ) ) ) ; alter2 . clear value updates ( ) ;
assert . assert null ( fsdir . get inode ( file id ) ) ;
return named components . get ( name ) ;
class < ? extends web driver > clazz = class . for name ( class name ) . as subclass ( web driver . class ) ; all sessions . register driver ( desired capabilities , clazz ) ;
assert that ( romeo @ montague . com , equals char sequence ( fwd . get forwarded stanza ( ) . get from ( ) ) ) ;
touch up = delta > 0 ; if ( gravity = = gravity . top ) { delta = - delta ; }
image loader configuration config = new image loader configuration . builder ( this ) . memory cache ( new lru memory cache ( 2 * 1024 * 1024 ) ) . memory cache size ( 2 * 1024 * 1024 ) . memory cache size percentage ( 13 ) . build ( ) ; default
int w3 = content length ; int ourslot = append node ( w0 , w1 , w2 , w3 ) ; previous sibling = ourslot ; }
for ( plugin plugin : plugins ) { plugins element . append child ( plugin . as xmlelement ( ) ) ; }
measurer . set position ( text . get begin index ( ) ) ;
kc reg exec exe = execute ( nonexistent ) ; assert exit code and stream sizes ( exe , 1 , 0 , 1 ) ; assert . assert equals ( stderr first line , unknown command : nonexistent , exe . stderr lines ( ) . get ( 0 ) ) ; }
if ( spdy headers frame . is last ( ) ) { half close stream ( stream id , false , e . get future ( ) ) ; } } else if ( msg instanceof spdy window update frame ) {
body builder . append formal line ( filter reg bean . set filter ( new % s ( ) ) ; , get name of java type ( new java type ( org . springframework . orm . jpa . support . open entity manager in view filter ) ) ) ;
fsdata output stream out = dfs . create ( file ) ; out . hflush ( ) ; out . hsync ( ) ;
verify ( mocked listener , times ( 1 ) ) . on navigation item selected ( m bottom navigation . get menu ( ) . find item ( r . id . destination _ profile ) ) ;
int response id = 10 ;
default async http client config . builder builder = new default async http client config . builder ( ) ;
stream request stream request no entity = client generated stream request ( action request . class , resource method . action , new data map ( ) , content type , accept types , accept content type per client , stream attachments ? generate request attachments ( ) : null , accept response attachments ) ;
exchange . set out ( null ) ;
target machine . get options ( ) . set position independent executable ( true ) ;
int row to select = table model . get row count ( ) - 1 ;
final timeseries query query1 = druids . new timeseries query builder ( ) . data source ( appenderator tester . datasource ) . intervals ( immutable list . of ( intervals . of ( 2000 2001 ) ) ) . aggregators ( arrays . < aggregator factory > as list ( new long sum aggregator factory ( count , count ) , new long sum aggregator factory ( met , met ) ) ) . granularity ( granularities . day ) . build ( ) ; final list < result < timeseries result value > > results1 = lists . new array list ( ) ;
int bytes in color map = ( c map depth * c map length ) > > 3 ; int bits per color = math . min ( c map depth 3 , 8 ) ; byte [ ] c map data = new byte [ bytes in color map ] ; dis . read ( c map data ) ;
conf1 = utility1 . get configuration ( ) ;
spy model . deliver user action ( user action , null , m mock user action callback ) ;
if ( uri = = null ) { throw new null pointer exception ( uri is null ) ; } list < http cookie > cookies = new array list < http cookie > ( ) ;
assert false ( ri . contains range ( bytes . to bytes ( z ) , bytes . to bytes ( z ) ) ) ;
node break target = n ; for ( ; control flow analysis . is break target ( break target , null * no label * ) ; break target = break target . get parent ( ) ) { if ( break target . is function ( ) | | break target . is script ( ) ) { no break target . return n ; } } node follow = control flow analysis . compute follow node ( break target ) ;
for ( string word : valid key set ) { assert equals ( word . length ( ) , ( int ) mdag map . get ( word ) ) ; } }
for ( int i = 0 , j = counters . length - 1 ; i < j ; i + + , j - - ) { int temp = counters [ i ] ; counters [ i ] = counters [ j ] ; counters [ j ] = temp ; } } counters [ ] has the pixels of the module
if ( descriptor = null ) return descriptor ; string descriptor ;
if ( requires crosstool ( base command ) ) { if cc is used , silently throw in the crosstool filegroup as a dependency . inputs . add transitive ( cpp helper . get toolchain using default cc toolchain attribute ( rule context ) . get crosstool middleman ( ) ) ; } if ( requires jdk ( base command ) ) { if javac is used , silently throw in the jdk filegroup as a dependency . note we expand java - related variables with the * host * configuration . inputs . add transitive ( java helper . get host javabase inputs ( rule context ) ) ; } for ( nested set < artifact > extra inputs : get extra input artifacts ( rule context , base command ) ) { inputs . add transitive ( extra inputs ) ; }
system . out . println ( = = = = closing the connection on a ) ;
set execution timeout ( - 1 ) ;
m security button . set image resource ( id ) ;
get = new get ( keys [ 40 ] ) ; get . add column ( bytes _ family , qual2 ) ; result r = table . get ( get ) ; validate result ( r , qual2 , val2 ) ; table . close ( ) ;
if ( is found ) { return false ; } if ( image detail . class . is assignable from ( detail . get class ( ) ) & & is image detail supported ( ) ) { delete image detail ( ) ; account image = null ; } if ( fire change events ) fire server stored details change event ( provider , server stored details change event . detail _ removed , detail , null ) ;
m _ null array length = ( ( m _ column types . size ( ) + 7 ) & - 8 ) > > 3 ;
entity manager factory ( ) . get cache ( ) . evict all ( ) ; stats . clear ( ) ;
camera . set display orientation ( 90 ) ;
test impl ( new long [ ] { 51000 , 50101 , 50123 , 49999 } , 51000 , 50101 , 50123 , 49999 new int [ ] { 0 , 0 , 0 , 0 } , c2 schunk . class , false ) ;
stack size - = 1 ; evaluate instruction block ( clazz , method , code attribute , offset + branch instruction . length ( offset ) ) ; }
byte [ ] current id = zkutil . get data and watch ( watcher , leader znode ) ; if ( current id = null & & bytes . equals ( current id , node id ) ) {
for ( int i = 0 ; i < 12 ; i + + ) { int part start = i * 100 ; int part end = ( i + 1 ) * 100 - 1 ; @ language ( sql ) string insert partitions = + insert into + table name + + select ' bar ' foo , part + from unnest ( sequence ( + part start + , + part end + ) ) as tmp ( part ) ; assert update ( session , insert partitions , 100 ) ; }
show cursor ( ) ; }
final service builder < weld bootstrap service > weld bootstrap service builder = service target . add service ( weld bootstrap service name , weld bootstrap service ) ;
view . translate ( new vector2f ( display . get width ( ) 2 , display . get height ( ) 2 ) ) ;
for ( file system provider provider : sl ) { string scheme = provider . get scheme ( ) ; add to list if the provider is not file and isn ' t a duplicate if ( scheme . equals ignore case ( file ) ) { boolean found = false ; for ( file system provider p : list ) { if ( p . get scheme ( ) . equals ignore case ( scheme ) ) { found = true ; break ; } } if ( found ) { list . add ( provider ) ; } } } return list ;
sql = select p _ d0 from p where p . p _ d3 10 > 0 order by p _ d3 10 , p _ d2 ;
int effective batch size = input . size ( 0 ) * input . size ( 2 ) * input . size ( 3 ) ;
string expected = ( current buffer tail token + next buffer head token ) . replace ( delimiter , ) ;
product = ( product multipler _ intword _ decimal ) ; if ( product > full _ max _ highword _ decimal ) { return false ; } result [ 0 ] = ( long ) z1 * multipler _ intword _ decimal + ( long ) z0 ;
return app component ;
orient . instance ( ) . get profiler ( ) . stop chrono ( db . + odatabase record thread local . instance ( ) . get ( ) . get name ( ) + . command . + i command . to string ( ) , command executed against the database , begin time , db . * . command . * , null , user string ) ; } } } } catch ( runtime exception ee ) {
if ( params . get route ( ) = = null | | params . get route ( ) . has hops ( ) ) { params . set route ( get cluster route ( routing table ) ) ; log . log ( log level . debug , no route specified ; resolved implicit + storage cluster : + params . get route ( ) . to string ( ) ) ; } }
if ( get temp feed link ( ) = null & & get temp category ( ) = null & & get temp category ( ) . ends with ( user ) & & get url ( ) . to lower case ( ) . index of ( picasaweb . google ) > - 1 ) { add child ( new images feed ( get temp feed link ( ) . replace ( data entry base , data feed base ) ) ) ; } else { super . manage item ( ) ; } }
final double pi = bellard . compute pi ( b , sums ) ; util . print bit skipped ( b ) ; util . out . println ( util . pi2string ( pi , bellard . bit2terms ( b ) ) ) ; return 0 ; }
auth config json = { } ;
template . send body and header ( get ftp url ( ) + & move existing = { file : parent } renamed - { file : onlyname } & eager delete target file = true , bye world , exchange . file _ name , hello . txt ) ;
short unexisting value = ( short ) 9999 ;
if ( i txt _ keyword . size ( ) > 0 ) { iiometadata node i txt _ parent = new iiometadata node ( i txt ) ; for ( int i = 0 ; i < i txt _ keyword . size ( ) ; i + + ) { iiometadata node i txt _ node = new iiometadata node ( i txt entry ) ; i txt _ node . set attribute ( keyword , i txt _ keyword . get ( i ) ) ; i txt _ node . set attribute ( compression flag , i txt _ compression flag . get ( i ) ? true : false ) ; i txt _ node . set attribute ( compression method , i txt _ compression method . get ( i ) . to string ( ) ) ; i txt _ node . set attribute ( language tag , i txt _ language tag . get ( i ) ) ; i txt _ node . set attribute ( translated keyword , i txt _ translated keyword . get ( i ) ) ; i txt _ node . set attribute ( text , i txt _ text . get ( i ) ) ; i txt _ parent . append child ( i txt _ node ) ; } root . append child ( i txt _ parent ) ; }
throw new illegal argument exception ( unknown argument + key ) ;
assert pretty print ( label : alert ( ) ; , label : alert ( ) ; \ n ) ;
assert false ( path to lucene index . exists ( ) ) ;
assert true ( dfs . mkdirs ( new path ( parent , nqdir0 nqdir30 nqdir33 ) ) ) ;
if ( f lexical handler = null ) { f lexical handler . start entity ( name ) ; }
string desc tag = element . get description tag ( ) ; if ( desc tag = = null ) {
if ( _ ignorable props = null & & _ ignorable props . contains ( prop name ) ) { jp . skip children ( ) ; } else if ( _ any setter = null ) { _ any setter . deserialize and set ( jp , ctxt , bean , prop name ) ; continue ; } else {
assert . assert equals ( 1 , allocate response . get updated containers ( ) . size ( ) ) ; updated container uc = allocate response . get updated containers ( ) . get ( 0 ) ; assert . assert equals ( container . get id ( ) , uc . get container ( ) . get id ( ) ) ; assert . assert equals ( execution type . guaranteed , uc . get container ( ) . get execution type ( ) ) ;
file ppsf = psf . get parent file ( ) ; if ( ppsf = = null | | is file system ( ppsf ) ) {
int wait count = 0 ; while ( nm . get service state ( ) = state . started & & wait count + + = 20 ) { log . info ( waiting for nm to stop . . ) ; thread . sleep ( 1000 ) ; } assert . assert true ( nm . get service state ( ) = = state . started ) ; nm . stop ( ) ; }
set draggable ( true ) ;
for ( int i = 0 ; i < iterations ; i + + ) { boolean remove md5 = rand . next boolean ( ) ; create image ( txid + i , true , remove md5 ) ; this should be in the manifest later expected images . add ( new remote image ( txid + i , remove md5 ? null : digests . get ( txid + i ) ) ) ; } for ( int i = 0 ; i < iterations ; i + + ) { create image ( txid + i + iterations , false , false ) ; this should not be in the manifest later }
student oracle no sqlsql date student = new student oracle no sqlsql date ( ) ;
doc = node arg . get owner document ( ) ;
container id c id = create container id ( 0 ) ;
can wrap any = utilities . get use vectorized input file format ( conf , this ) ;
string mime = kmz ? kmzmap output format . mime _ type : kmlmap output format . mime _ type ;
p = new slow log parsed document printer ( index , pd , 10 , true , 3 ) ;
for ( int expected key : expected keys ) { driver . process ( topic2 , expected key , yy + expected key ) ; } driver . flush state ( ) ; proc . check and clear process result ( 0 : ( x0 + yy0 < - x0 + y0 ) , 1 : ( x1 + yy1 < - x1 + y1 ) , 2 : ( x2 + yy2 < - x2 + null ) , 3 : ( x3 + yy3 < - x3 + null ) ) ;
system . out . println ( xml utils . marshalto string ( wml package . get main document part ( ) . get jaxb element ( ) , true , true ) ) ;
assert not null ( access token . get refresh token ( ) ) ; oauth2 access token new access token = refresh access token ( access token . get refresh token ( ) . get value ( ) ) ; assert false ( new access token . get value ( ) . equals ( access token . get value ( ) ) ) ; verify access tokens ( access token , new access token ) ; cancel token ( access token . get value ( ) ) ; cancel token ( new access token . get value ( ) ) ; }
manifest manifest = new manifest ( ) ; attributes attributes = manifest . get main attributes ( ) ; attributes . put ( new attributes . name ( manifest - version ) , 1 . 0 ) ; byte array output stream man out = new byte array output stream ( ) ; manifest . write ( man out ) ; byte [ ] man bytes = man out . to byte array ( ) ; file file = file . create temp file ( support _ platform file . get new platform file ( hyts _ manifest1 , ) , . jar ) ; jar output stream jar out = new jar output stream ( new file output stream ( file . get absolute path ( ) ) ) ; zip entry entry = new zip entry ( meta - inf ) ; entry . set size ( 0 ) ; jar out . put next entry ( entry ) ; entry = new zip entry ( jar file . manifest _ name ) ; entry . set size ( man bytes . length ) ; jar out . put next entry ( entry ) ; jar out . write ( man bytes ) ; entry = new zip entry ( myfile ) ;
if ( diff = 0 ) { ( ( pushback input stream ) in ) . unread ( buf , len - diff , diff ) ; } try { read and verify data descriptor ( in b , out ) ; } catch ( exception e ) { if ( failure = = null ) { otherwise we ' re already going to throw failure = e ; } }
job metrics . set metrics ( immutable list . of ( make counter metric update ( counter name , counter namespace , s2 , 1233 l , false ) , make counter metric update ( counter name , counter namespace , s2 , 1233 l , true ) , make counter metric update ( other counter [ min ] , other namespace , s2 , 0 l , false ) , make counter metric update ( other counter [ min ] , other namespace , s2 , 0 l , true ) ) ) ; dataflow metrics dataflow metrics = new dataflow metrics ( job , dataflow client ) ;
process jar file lists ( ) ;
string super class name = ( ( type element ) super class element ) . get qualified name ( ) . to string ( ) ; if ( context . contains meta entity ( super class name ) | | context . contains meta embeddable ( super class name ) ) { return true ; }
msg = muc2 . next message ( 1000 ) ; assert not null ( first message is null , msg ) ; delay information delay = delay information manager . get delay information ( msg ) ; assert not null ( message contains no delay information , delay ) ; simple date format utc _ format = new simple date format ( yyyy mmdd ' t ' hh : mm : ss ) ; utc _ format . set time zone ( time zone . get default ( ) ) ; system . out . println ( utc _ format . format ( delay . get stamp ( ) ) ) ; assert equals ( body of first message is incorrect , message 3 , msg . get body ( ) ) ;
match = ( path index = = matching context . path length ) ;
category panel . this . hide ( ) ; } } ) ; }
args = new string [ ] { - remove from cluster node labels } ; assert true ( 0 = rm admin cli . run ( args ) ) ;
fake timer . advance ( key update interval + 1 ) ;
response . set resource ( capability ) ;
assert jq ( req ( fq , filt , q , { func } + f2 , group , true , group . field , f , fl , id , rows , 2 ) , grouped = = { ' + f + ' : { ' matches ' : 10 , ' groups ' : [ + { ' group value ' : 1 , ' doclist ' : { ' num found ' : 3 , ' start ' : 0 , ' docs ' : [ { ' id ' : ' 8 ' } ] } } , + { ' group value ' : 3 , ' doclist ' : { ' num found ' : 2 , ' start ' : 0 , ' docs ' : [ { ' id ' : ' 3 ' } ] } } + ] } } ) ;
collection . pre insert ( this ) ;
int action = ev . get action ( ) & motion event . action _ mask ; switch ( ev . get action ( ) & motion event . action _ mask ) { case motion event . action _ cancel : if ( m drag state = = dragging ) { cancel drag ( ) ; } do action up or cancel ( ) ; break ; case motion event . action _ up :
matches metadata = false ;
resource res = base . add path ( foo ) ;
cluster = get nine node cluster ( ) ; s1 = get store ( cluster , 1 , 9 , 9 , 9 , 0 , routing strategy type . to _ all _ strategy , new voldemort exception ( ) ) ;
assert equals ( < hey > ho < hey > , cache factory . get hot rod cache ( ) . get ( key ) ) ;
listenable future < buffer result > future = buffer . get ( first , ( long ) 0 , size of pages ( 1 ) ) ;
map . put ( 1 , new person ( a , 75 ) ) ; map . put ( 2 , new person ( a , 95 ) ) ; map . remove ( 1 ) ;
for ( map < string , string > action : actions ) { if ( action . contains key ( name ) | | action . contains key ( class ) ) { op . add error ( no ' name ' or ' class ' specified for action : + action ) ; return current config ; } string klass = action . get ( class ) ; try { container . get resource loader ( ) . find class ( klass , trigger action . class ) ; } catch ( exception e ) { log . warn ( could not load class : , e ) ; op . add error ( action not found : + klass + + e . get message ( ) ) ; return current config ; } }
try ( thread context context = new thread context ( settings . empty ) ) { context . read headers ( in ) ; }
assert true ( entry cache . get size ( ) = 0 ) ;
byte [ ] encap buffer = new byte [ encap length ] ;
bs putint ( m _ block crc ) ;
if ( thread . current thread ( ) . is interrupted ( ) ) send success message ( status . get status code ( ) , response . get all headers ( ) , null ) ;
out . write float ( _ load factor ) ;
stmt . execute ( drop table if exists time _ test ) ; stmt . execute ( drop table if exists date _ test ) ; stmt . execute ( create _ table _ timetest ) ; stmt . execute ( create _ table _ datetest ) ;
if ( options [ i ] . char at ( 1 ) = = ' - ' ) return - 1 ;
array list it . clear ( ) ;
if ( hash index . element count = = 0 ) { return false ; }
right side = new jtabbed pane ( ) ;
this . curr request time stamp = curr time ;
move pivot to start of slice ( array , from , to ) ; double pivot = array [ from ] ;
if ( conn . is transparent ( ) ) { check for ' proxy - connection ' directive connection header = response headers . get first header ( proxy - connection ) ; }
if ( fm . find fragment by id ( android . r . id . content ) = = null ) { app list fragment list = new app list fragment ( ) ; fm . begin transaction ( ) . add ( android . r . id . content , list ) . commit ( ) ; }
try { this . channel1 . connect ( datagram socket1 address ) ; fail ( should throw illegal state exception . ) ; non - nls - 1 } catch ( illegal state exception e ) { ok . } }
format result = format factory . get associated extension ( null ) ;
filter config filter config = mockito . mock ( filter config . class ) ;
new exchange . set property ( exchange . aggregated _ size , size ) ;
class name imported type = imported types . get ( simple name ) ; if ( imported type = null ) return imported type ;
block index = get block index for position ( blocks , position , half split , block index ) ;
rule . get activity ( ) . recycler view . set adapter ( adapter ) ; }
prepare listeners ( flush , new default flush event listener ( ) , listener array ) ;
m _ first free + = available ; }
try { detector . enable input filter ( filter ) ; if ( data . length > min _ length ) { detector . set text ( data ) ; matches = detector . detect all ( ) ; } } catch ( exception e ) { log . debug ( exception from icu4 j ( ignoring ) : , e ) ; } if ( matches = null ) { for ( charset match match : matches ) { add clue ( match . get name ( ) , detect , match . get confidence ( ) ) ; } }
string separator = ( args . length < 1 | | args [ 0 ] = = undefined . instance ) ? , : script runtime . to string ( args [ 0 ] ) ; if ( this obj instanceof native array ) { native array na = ( native array ) this obj ; if ( na . dense only ) { string buffer sb = new string buffer ( ) ; for ( int i = 0 ; i < length ; i + + ) { if ( i = 0 ) { sb . append ( separator ) ; } if ( i < na . dense . length ) { object temp = na . dense [ i ] ; if ( temp = null & & temp = undefined . instance & & temp = scriptable . not _ found ) { sb . append ( script runtime . to string ( temp ) ) ; } } } return sb . to string ( ) ; } } if ( length = = 0 ) { return ; }
string builder xml = new string builder ( ) ; xml . append ( < root > ) ; for ( int i = 0 ; i < 4000 ; + + i ) { xml . append ( < tag + i + > ) ; xml . append ( < tag + i + > ) ; } xml . append ( < root > ) ; parse ( xml . to string ( ) , new default handler ( ) ) ; }
byte buffer with info bbwi = cdr output object . get byte buffer with info ( ) ;
if ( random = null ) { param = new parameters with random ( param , random ) ; } digest . reset ( ) ;
final rule descr rule descr = new rule descr ( my rule ) ;
stats obj . set col name ( col name ) ; stats obj . set col type ( column type ) ; column statistics data stats data = new column statistics data ( ) ;
g2 . draw line ( start x , start y , start x , end y ) ;
path qualified = obtained . make qualified ( new path ( wanted uri ) ) ;
assert equals ( 0 , hbck . get overlap groups ( table ) . size ( ) ) ;
conf . set ( angel conf . angel _ action _ type , mlconf . angel _ ml _ inc _ train ( ) ) ; lrrunner runner = new lrrunner ( ) ; runner . inc train ( conf ) ; }
for ( query node child : children ) { child . remove from parent ( ) ; } array list < query node > existing children = new array list < > ( get children ( ) ) ;
if ( ret val = null & & ( ( map ) ret val ) . is empty ( ) & & is element collection map ) { if ( is byte buffer ) { set field value ( entity , member , cassandra data translator . marshal map ( map generic classes , key class , value class , ( map ) ret val ) ) ; } else { iterator keys = ( ( map ) ret val ) . key set ( ) . iterator ( ) ; while ( keys . has next ( ) ) { object key value = keys . next ( ) ; result map . put ( key value , ( ( map ) ret val ) . get ( key value ) ) ; } set field value ( entity , member , result map ) ; } } else if ( ret val = null & & ( ( map ) ret val ) . is empty ( ) ) { set field value ( entity , member , result map ) ; } break ;
if ( queued > executed ) { executed + = execute sql ( ) ; } return executed ;
string var str = jdbc uri . get fragment ( ) ;
if ( duration . get standard hours ( ) < 12 ) { return searches . date histogram interval . minute ; } else if ( duration . get standard days ( ) < 3 ) { return searches . date histogram interval . hour ; } else if ( duration . get standard days ( ) < 30 ) { return searches . date histogram interval . day ; } else if ( duration . get standard days ( ) < ( 30 * 2 ) ) { return searches . date histogram interval . week ; } else if ( duration . get standard days ( ) < ( 30 * 18 ) ) { return searches . date histogram interval . month ; } else if ( duration . get standard days ( ) < ( 365 * 3 ) ) { return searches . date histogram interval . quarter ; } else { return searches . date histogram interval . year ; }
request layout ( ) ;
assert false ( store . is data available ( keyspace2 , factory . from string ( 50 ) ) ) ;
expected = test _ string ;
try { if ( zkassign . transition node opened ( this . server . get zoo keeper ( ) , hri , this . server . get server name ( ) , this . version ) = = - 1 ) { log . warn ( completed the open of region + name + but when transitioning from + opening to opened got a version mismatch , someone else clashed + so now unassigning - - closing region on server : + this . server . get server name ( ) ) ; } else { log . debug ( region transitioned to opened in zookeeper : + r . get region info ( ) + , server : + this . server . get server name ( ) ) ; result = true ; } } catch ( keeper exception e ) { log . error ( failed transitioning node + name + from opening to opened - - closing region , e ) ; } return result ;
assert . assert equals ( sc05 , chosen . get id ( ) ) ; }
cb . set store ( store ) ;
stor iosqlite . get ( ) . object ( tweet . class ) . with query ( query . builder ( ) . table ( tweets table . table ) . where ( tweets table . column _ id + = ? ) . where args ( tweet id ) . build ( ) ) . prepare ( ) . as rx single ( )
int int length = ( ( byte length - keep ) + 3 ) > > > 2 ; int [ ] result = new int [ int length ] ; int b = byte length - 1 ; for ( int i = int length - 1 ; i > = 0 ; i - - ) { result [ i ] = a [ b - - ] & 0xff ; int bytes remaining = b - keep + 1 ; int bytes to transfer = math . min ( 3 , bytes remaining ) ; for ( int j = 8 ; j < = ( bytes to transfer < < 3 ) ; j + = 8 ) result [ i ] | = ( ( a [ b - - ] & 0xff ) < < j ) ; } return result ;
exchanger . exchange ( callback ) ; } else if ( data frames = = 3 | | data frames = = 4 | | data frames = = 5 ) {
list < remote access vpn vo > remote access vpns = _ remote access vpn dao . find by account ( account id ) ; list < vpn user vo > vpn users = _ vpn user . list by account ( account id ) ; for ( vpn user vo vpn user : vpn users ) { _ remote access vpn mgr . remove vpn user ( account id , vpn user . get username ( ) ) ; } try { for ( remote access vpn vo vpn : remote access vpns ) { _ remote access vpn mgr . destroy remote access vpn ( vpn . get server address id ( ) ) ; } } catch ( resource unavailable exception ex ) { s _ logger . warn ( failed to cleanup remote access vpn resources as a part of account id = + account id + cleanup due to exception : , ex ) ; account cleanup needed = true ; }
int chunk buf len = 16 ;
assert . assert equals ( worker2 , worker for other task ) ;
for ( int i = 0 ; i < prev token maps . length ; i + + ) for ( int j = 0 ; j < tokens . length ; j + + ) prev token maps [ i ] . put ( tokens [ j ] , finder tags [ i ] [ j ] ) ; for ( int i = 0 ; i < finders . length ; i + + ) { int start = - 1 ; list < span > names = new array list < span > ( 5 ) ;
array template array1 = template class . new instance ( ) ;
rmapp metrics app metrics = app . get rmapp metrics ( ) ; num amcontainer preempted = app metrics . get num amcontainers preempted ( ) ; preempted resource mb = app metrics . get resource preempted ( ) . get memory size ( ) ; num non amcontainer preempted = app metrics . get num non amcontainers preempted ( ) ; preempted resource vcores = app metrics . get resource preempted ( ) . get virtual cores ( ) ; memory seconds = app metrics . get memory seconds ( ) ; vcore seconds = app metrics . get vcore seconds ( ) ; resource seconds map = app metrics . get resource seconds map ( ) ; preempted memory seconds = app metrics . get preempted memory seconds ( ) ; preempted vcore seconds = app metrics . get preempted vcore seconds ( ) ; preempted resource seconds map = app metrics . get preempted resource seconds map ( ) ; application submission context app submission context = app . get application submission context ( ) ; unmanaged application = app submission context . get unmanaged am ( ) ; app node label expression = app . get application submission context ( ) . get node label expression ( ) ;
if ( racks = = null ) { system . out . println ( generating rack names for tasktrackers ) ; racks = new string [ num task trackers ] ; for ( int i = 0 ; i < racks . length ; + + i ) { racks [ i ] = network topology . default _ rack ; } }
assert false ( out . contains ( hi world ) ) ; out = ( string ) mbean server . invoke ( name , browse message body , new object [ ] { 0 } , new string [ ] { java . lang . integer } ) ; assert not null ( out ) ;
view helper . set alpha ( view , 0 ) ;
collection < double > long string = get double list ( sum , field facets , string _ sd , double , long ) ; array list < double > long string test = calculate number stat ( long string test start , sum ) ; assert equals ( get raw response ( ) , long string , long string test ) ;
query request request = new query request ( params ( file , managed - schema ) ) ; request . set path ( admin file ) ; final atomic boolean read file = new atomic boolean ( ) ; request . set response parser ( new response parser ( ) { @ override public string get writer type ( ) { return mock ; unfortunately this gets put onto params wt = mock but it apparently has no effect } @ override public named list < object > process response ( input stream body , string encoding ) { try { if ( body . read ( ) > = 0 ) read file . set ( true ) ; } catch ( ioexception e ) { throw new runtime exception ( e ) ; } return null ; } @ override public named list < object > process response ( reader reader ) { throw new unsupported operation exception ( todo unimplemented ) ; todo } } ) ; client . request ( request ) ; runs request
verify ( future ) . cancel ( true ) ; }
jpa entity type . get builder ( ) . apply id attribute ( attribute factory . build id attribute ( jpa entity type , ( property ) component . get property iterator ( ) . next ( ) ) ) ; } } }
throw new illegal argument exception ( invalid downsampling specifier ' + specification + ' : must provide at least interval and function ) ; } else if ( parts . length > 3 ) {
url rollover = ( url = null ) & & ( mouse x > left _ margin & & mouse x < right edge ) ;
get customer ( ids . customer id , remote factory ) ;
int blocks = get sequence count ( w ) + 1 ;
int sum = 0 ; for ( int i = 0 ; i < 3 ; i + + ) sum = sum + i ; if ( sum > 4 ) sum + = 2 ; else if ( sum < 2 ) sum + = 1 ; else sum + = 100 ;
logger . log ( level . warning , error message . general _ write _ problem _ closing _ file _ handle . get msg ( af . get file ( ) . get absolute path ( ) , ioe . get message ( ) ) , ioe ) ;
int save read offset = read offset ;
string default account = e . get attribute ( default account ) ; user config user = new user config ( ) ;
assert equals ( appbar on screen xy [ 1 ] + appbar height , toolbar on screen xy [ 1 ] + toolbar height + toolbar lp . bottom margin , 1 ) ;
continue ; } if ( realtime segment zkmetadata . get status ( ) = = status . in _ progress ) { instances to assign realtime segment . remove all ( state . get instance set ( partition ) ) ; } } }
for ( int i = 0 , count = _ velocity samples . size ( ) ; i < count - 2 ; i + + ) { if ( current time - _ velocity samples . get ( i ) . time > 1000 ) { _ velocity samples . remove ( 0 ) ; i - - ; count - - ; } else { break ; } }
default stor iocontent resolver . builder ( ) . content resolver ( mock ( content resolver . class ) ) . build ( ) ;
class < ? > outer clazz = clazz . get declaring class ( ) ;
if ( first chunk offset > 0 ) { long offset in file = block inline checksum reader . get pos from block offset ( first chunk offset , bytes per checksum , checksum size ) ; data file channel . position ( offset in file ) ; } last chunk offset = first chunk offset ;
close quietly ( raw socket ) ;
return null ; } } ) . filter ( e - > e = null ) . collect ( collectors . to list ( ) ) ; json file manager . write to disc ( utilities . object to json ( offer for json list ) , offers _ statistics ) ; }
p . set current value ( bean ) ;
output . add ( post merge windowed value ) ;
m icon shift . x = m icon last touch pos . x - sv . get icon center ( ) . x ; m icon shift . y = m icon last touch pos . y - m launcher . get device profile ( ) . icon size px ; drag view dv = m launcher . get workspace ( ) . begin drag shared ( sv . get icon view ( ) , ( popup container with arrow ) get parent ( ) , sv . get final info ( ) , new shortcut drag preview provider ( sv . get icon view ( ) , m icon shift ) , new drag options ( ) ) ;
view target = activity . find view by id ( m configuration . m target view id ) ; if ( target = null ) { mask view . set target rect ( common . get view abs rect ( target , parent x , parent y ) ) ; } }
assert equals ( ttml style . style _ bold _ italic , resolved . get style ( ) ) ; }
. put ( direct _ write _ view _ urn , new view evaluator factory ( ctxt ) ) . put ( direct _ stateful _ par _ do _ urn , new stateful par do evaluator factory < > ( ctxt ) ) . put ( direct _ gbko _ urn , new group by key only evaluator factory ( ctxt ) ) . put ( direct _ gabw _ urn , new group also by window evaluator factory ( ctxt ) ) . put ( direct _ test _ stream _ urn , new test stream evaluator factory ( ctxt ) ) . put ( direct _ merge _ accumulators _ extract _ output _ urn , new multi step combine . merge and extract accumulator output evaluator factory ( ctxt ) )
m _ sum of weights = sum of weights ;
final pom mock pom1 = set up mock pom ( path to pom 1 , new java type ( com . example . domain . choice ) , new java type ( com . example . domain . vote ) ) ; final pom mock pom2 = set up mock pom ( path to pom 2 , new java type ( com . example . web . choice controller ) , new java type ( com . example . web . vote controller ) ) ; when ( mock project operations . get poms ( ) ) . then return ( arrays . as list ( mock pom1 , mock pom2 ) ) ;
output = null ;
if ( refresh = = true | | initialise done = = false ) { result . add ( this . get supported message ( ) ) ; } return result ; }
final uri art _ content _ uri = uri . parse ( content : media external audio albumart ) ; int album id col index = m app . get service ( ) . get cursor ( ) . get column index ( media store . audio . media . album _ id ) ; long album id = m app . get service ( ) . get cursor ( ) . get long ( album id col index ) ; return content uris . with appended id ( art _ content _ uri , album id ) . to string ( ) ; } }
m rotation = angle 90 ; camera . set display orientation ( display angle ) ;
expected entries = math . max ( expected entries , 2 ) ; int table size = integer . highest one bit ( expected entries ) ;
model node auth = model . get ( subsystem , security , security - domain , other , authentication , classic ) ;
continue ; } break ; }
set scale2 = set scale1 . set scale ( 1 , big decimal . round _ floor ) ;
node root = node util . get root of qualified name ( q name node ) ;
verify snapshot request ( true ) ;
if ( idx last separator > = 0 ) { idx last separator + = classpath uri locator . prefix . length ( ) - 1 ; }
path app dir empty = new path ( done app home dir , app dir name + 3 ) ;
file status [ ] archived store files = fs . list status ( store archive ) ; assert archive equal to original ( store files , archived store files , fs ) ;
if ( previous selection = null ) { on update ( get binding index ( previous selection ) , previous selection , false ) ; previous selection . set shortcut ( previous selection . get shortcut ( ) ) ; reverts text }
if ( set closed ( ) ) { return ; } default local channel paired channel = this . paired channel ;
default properties = m _ xml _ properties ;
compare results ( result , previous result ) ; } } return result ; }
final int element [ ] data = new int element [ 5 ] ; for ( int i = 0 ; i < data . length ; i + + ) { data [ i ] = new int element ( i , i ) ; } for ( int v = 1 ; v < data . length - 1 ; v + + ) { {
collection < future < third eye response > > futures = submit requests ( thirdeye requests ) ; int i = 0 ;
return fetch value ( type , meta , is binary ) ;
m track index = m muxer . add track ( new format ) ;
select item ( item ) ;
for ( int i = 0 ; i < file count ; i + + ) { path a = new path ( dir , a + i ) ; fs . delete ( a , false ) ; }
for ( int i = 0 ; i < list . size ( ) ; i + + ) { jnlp resource respath = ( jnlp resource ) list . get ( i ) ; version id vid = new version id ( respath . get version id ( ) ) ; int sts = match entry ( name , vs , dreq , respath , vid ) ; if ( sts = = download response . sts _ 00 _ ok ) { if ( result [ 0 ] = = null | | vid . is greater than ( best version id ) ) { result [ 0 ] = respath ; best version id = vid ; } } else { error = math . max ( error , sts ) ; } } return ( result [ 0 ] = null ) ? download response . sts _ 00 _ ok : error ;
from ( direct : start ) . recipient list ( ) . xquery ( concat ( ' mock : foo . ' , person @ city ) , string . class ) ;
js . execute script ( javascript : ( + function ( ) { + var d = array ( ' com ' , ' co . jp ' , ' ca ' , ' fr ' , ' de ' , ' co . uk ' , ' it ' , ' cn ' , ' es ' , ' com . br ' ) ; + for ( var i = 0 ; i < d . length ; i + + ) { + document . cookie = ' + ckey + = ; path = ; domain = . google . ' + d [ i ] + ' ; ' ; + } + } ) ( ) ; ) ;
execute net ( protocol version , alter table % s add foo text ) ; table metadata metadata = schema . instance . get table metadata ( keyspace ( ) , mv1 _ test ) ; assert . assert not null ( metadata . get column ( byte buffer util . bytes ( foo ) ) ) ; update view ( insert into % s ( k , asciival , bigintval , foo ) values ( ? , ? , ? , ? ) , 0 , foo , 1 l , bar ) ;
t element = extended vocabulary . get ( word ) ; if ( element = null ) return ( int ) element . get element frequency ( ) ; return 0 ; }
assert equals ( 1 , listener latch . get count ( ) ) ; assert false ( task . is done ( ) ) ; assert false ( task . is cancelled ( ) ) ;
whitebox . set internal state ( tsdb , metrics , metrics ) ;
ensure governor has field ( get entity factory field ( ) ) ;
if ( params . get ( common params . q ) = = null ) { params . add ( common params . q , * : * ) ; }
else if ( event instanceof task event ) { task event handler . publish ( ( task event ) event ) ; return false ; } else { throw new illegal state exception ( received unexpected event of type + event type + at reader . ) ; } }
throw new illegal argument exception ( out of range : + value ) ; }
int [ ] partition keys = split props . get split partition keys ( ) ;
content . set overlay ( predicate , 0 , new color drawable ( get resources ( ) . get color ( r . color . media _ shade _ overlay _ color ) ) , gravity . fill ) ; drawable progress drawable = get resources ( ) . get drawable ( android . r . drawable . progress _ horizontal ) ;
iterator it = class source . key set ( ) . iterator ( ) ; while ( it . has next ( ) ) unq name table . add ( ( string ) it . next ( ) ) ; return unq name table ;
assert rows net ( execute net with paging ( select a , b , s , count ( b ) , count ( s ) from % s group by a , page size ) , row ( 1 , null , 1 , 0 l , 1 l ) , row ( 2 , null , 2 , 0 l , 1 l ) , row ( 4 , null , 3 , 0 l , 1 l ) ) ; assert rows net ( execute net with paging ( select a , b , s , count ( b ) , count ( s ) from % s group by a , b , page size ) , row ( 1 , null , 1 , 0 l , 1 l ) , row ( 2 , null , 2 , 0 l , 1 l ) , row ( 4 , null , 3 , 0 l , 1 l ) ) ;
java type id type = prov . get type factory ( ) . find type parameters ( type , object id generator . class ) [ 0 ] ; gen = prov . object id generator instance ( bean desc . get class info ( ) , object id info ) ; return object id writer . construct ( id type , object id info . get property name ( ) , gen , object id info . get always as id ( ) ) ; }
return ( new size + 1 ) & 0x7ffffffe ;
log . d ( tag , on complete ) ; } @ override public void on error ( throwable e ) {
m app executors . disk io ( ) . execute ( new runnable ( ) { @ override public void run ( ) { final user user = m user data source . get user ( ) ;
assert jq ( req ( q , { join from = title _ s to = title _ s + whatever score ( ) + } name _ s : dave , fl , id ) , response = = { ' num found ' : 2 , ' start ' : 0 , ' docs ' : [ { ' id ' : ' 3 ' } , { ' id ' : ' 4 ' } ] } ) ; assert jq ( req ( q , { join from = dept _ ss to = dept _ id _ s + whatever score ( ) + } title _ s : mts , fl , id , debug query , true ) , response = = { ' num found ' : 3 , ' start ' : 0 , ' docs ' : [ { ' id ' : ' 10 ' } , { ' id ' : ' 12 ' } , { ' id ' : ' 13 ' } ] } ) ;
class . for name ( org . apache . cassandra . service . storage proxy ) ;
public void test cookbook ( rest client rest client , root builder wrapper < long , greeting > builders ) throws exception {
if ( wevtapi . instance . evt update bookmark ( h bookmark , event array [ i ] ) ) { throw new win32 exception ( kernel32 . instance . get last error ( ) ) ; }
selenium . select frame ( frame2 ) ;
deselect all ( ) ;
r = run unguarded ( clazz , run m ) ;
if ( common string map . contains key ( version ) ) { atomic integer index = new atomic integer ( 1 < < 31 ) ; string resource path = resources strings + version + . x . txt ; try ( buffered reader br = resource reader ( resource path ) ) { common string map . put ( version , br . lines ( ) . collect ( collectors . to map ( value - > index . get and add ( value . length ( ) + 1 ) , value - > value ) ) ) ; } catch ( null pointer exception ex ) { throw new runtime exception ( no common strings file found for version + version ) ; } } return hash bi map . create ( common string map . get ( version ) ) ;
offset children top and bottom ( bottom offset ) ;
header = second _ encoding ;
if ( temp rss > mb target ) { return ; } } try { thread . sleep ( 2000 ) ; } catch ( exception e ) { } } } } ;
session . get transaction ( ) . begin ( ) ; session . delete ( troop ) ; session . get transaction ( ) . commit ( ) ; session . close ( ) ;
ssdpsearch socket list ssdp search sock list = get ssdpsearch socket list ( ) ; if ( ssdp search sock list . open ( ) = = false ) return false ; ssdp search sock list . add search listener ( this ) ; ssdp search sock list . start ( ) ;
net broadcast tuple tuple = new net broadcast tuple ( network . get network ( ) . get layer wise configurations ( ) , network . get network ( ) . params ( ) , network . get network ( ) . get updater ( ) . get state view array ( ) ) ; shared training configuration configuration = shared training configuration . builder ( ) . threshold ( threshold ) . min threshold ( min threshold ) . shake frequency ( shake frequency ) . threshold step ( threshold step ) . step trigger ( step trigger ) . step delay ( step delay ) . void configuration ( void configuration ) . debug longer iterations ( debug longer iterations ) . number of workers per node ( num workers per node ) . build ( ) ;
if ( r _ mark _ s un uz ( ) ) { break lab30 ; } break lab29 ; } while ( false ) ; cursor = limit - v _ 10 ; lab31 : do {
string host = req . get header ( host _ header ) ;
assert equals ( size of pending reconstructions , 0 , pending reconstructions . size ( ) ) ;
string deps = deps = [ ; for ( int i = 1 ; i < count ; + + i ) { deps + = \ : deps + i + \ , ; } deps + = ] , ; cpp code generator helper . create main class and build file with deps ( target _ parallel _ deps , deps , project path ) ; } catch ( ioexception e ) {
metrics . end stored scope ( scope name ) ; assert equals ( long . value of ( 1 ) , foo scope . get num counter ( ) ) ; final long t1 = foo scope . get time counter ( ) . long value ( ) ; assert true ( t1 > period ms ) ; assert same ( foo scope , metrics . get stored scope ( scope name ) ) ;
try { rescue allocator ( allocator ) ; } catch ( dlinterrupted exception e ) { logger . warn ( interrupted on rescuing ledger allocator pool { } : , pool path , e ) ; thread . current thread ( ) . interrupt ( ) ; }
random rand = new random ( ) ;
arrays . sort ( id and pos ) ; final boolean from user = false ;
execute ( update % s using timestamp 2 set v1 = 2 , v2 = 2 where k = 0 and c = 0 ) ;
assert . assert true ( processed requests . peek ( ) = = pre session committed req ) ; processor . run ( ) ;
if ( len = = 0 ) { return new byte [ 0 ] ; } else if ( len < 4 ) { throw new illegal argument exception ( base64 - encoded string must have at least four characters , but length specified was + len ) ;
return tree node . get value ( ) + ;
return section . first app item . view type = = all apps grid adapter . view _ type _ icon ;
super ( null , new audio format ( mp3 elementary input stream . mp3 , 44100 , 16 , 2 , 626 , 44100f 1152f , true ) , - 1 ) ;
if ( system . get property ( property _ prefix ) = null ) { pc . set property prefix ( system . get property ( property _ prefix ) + . ) ; } if ( props = null ) { properties initial props = new properties ( ) ; initial props . put all ( props ) ; log . debug ( string . format ( added % d initial properties , props . size ( ) ) ) ; pc . set initial properties ( initial props ) ; }
try ( directory dir = new directory ( ) ) { executor service exec = random executor service or null ( ) ; sort info info = check sort ( dir , new offline sorter ( dir , foo , offline sorter . default _ comparator , buffer size . megabytes ( 1 ) , 2 , - 1 , exec , test util . next int ( random ( ) , 1 , 4 ) ) , generate random ( ( int ) offline sorter . mb * 20 ) ) ; if ( exec = null ) { exec . shutdown now ( ) ; } assert true ( info . merge rounds > 10 ) ; }
closeable closeable = new foo ( ) . must be closed annotated method ( ) ;
boolean false value = parser . parse expression ( ' xyz ' instanceof t ( int ) ) . get value ( boolean . class ) ; assert false ( false value ) ;
web driver d = get driver ( ) ;
iterator < ? > it = all _ api _ commands . key set ( ) . iterator ( ) ;
if ( ( out _ grouping ( g _ v , 97 , 252 ) ) ) { break lab4 ; }
return myself ;
writer osw = new output stream writer ( output ) ; set writer internal ( osw , false ) ; }
final float s ;
stop sleeper . skip sleep cycle ( ) ;
try ( double serializer double serializer = new double serializer ( ) ) { double serializer . build ( null , partition key ( 42 l ) , arrays . as list ( cn ( 42 ) , cn ( 43 ) , cn ( 44 ) , cn ( 45 ) , cn ( 46 ) , cn ( 47 ) , cn ( 48 ) , cn ( 49 ) , cn ( 50 ) , cn ( 51 ) ) , 0 l ) ; assert equals ( double serializer . rie old serialized , double serializer . rie new serialized ) ; }
} plugin manager . post message ( on received error , data ) ; } } ;
if ( map . contains key ( token ) ) { string val = map . get ( token ) ; output . write ( val . get bytes ( ) ) ; system . out . print ( val ) ; continue ; } if ( keywords = null & & is keyword ( keywords , token ) ) { map . put ( token , get ( ) ) ; string val = map . get ( token ) ; output . write ( val . get bytes ( ) ) ; system . out . print ( val ) ; increment ( ) ; } else { output . write ( token . get bytes ( ) ) ; system . out . print ( new string ( token . get bytes ( ) ) ) ; }
segment head = din . read unsigned byte ( ) ;
reset ( visitor ) ; do return ( true ) . when ( visitor ) . process row ( ( result ) any object ( ) ) ; meta scanner . meta scan ( conf , visitor , tablename , hconstants . empty _ byte _ array , 1000 ) ; verify ( visitor , times ( 3 ) ) . process row ( ( result ) any object ( ) ) ;
throw new null pointer exception ( ) ;
all journals . add all ( map . key set ( ) ) ;
for ( int i = 0 ; i < num _ lists ; i + + ) { table . insert or replace record ( overwrite lists [ i ] ) ; } field list = whitebox . get field ( compacting hash table . class , partitions ) ;
if ( for class . is interface ( ) ) { throw new illegal argument exception ( can not add specific mapping for an interface ( + for class . get name ( ) + ) ) ; } if ( modifier . is abstract ( for class . get modifiers ( ) ) ) { throw new illegal argument exception ( can not add specific mapping for an abstract class ( + for class . get name ( ) + ) ) ; } if ( _ direct class mappings = = null ) { _ direct class mappings = new hash map < class key , json serializer < ? > > ( ) ; }
int group end = find group end ( pre grouped page , pre grouped partition hash strategy , 0 ) ;
boolean result = task executor service . update fragment ( fragment id , true ) ; assert true ( result ) ; task wrapper2 = task executor service . preemption queue . peek ( ) ; assert null ( task wrapper2 ) ; assert false ( task wrapper . is in preemption queue ( ) ) ; r1 . complete ( ) ; r1 . await end ( ) ;
throwable cause = exchange . get exception ( ) ; if ( cause = null ) { throwable found = find suitable exception ( cause , method ) ; if ( found = null ) { if ( found instanceof exception ) { throw ( exception ) found ; } else { wrap as exception throw new camel exchange exception ( error processing exchange , exchange , cause ) ; } } special for runtime camel exceptions as they can be nested if ( cause instanceof runtime camel exception ) { if the inner cause is a runtime exception we can throw it directly if ( cause . get cause ( ) instanceof runtime exception ) { throw ( runtime exception ) ( ( runtime camel exception ) cause ) . get cause ( ) ; } throw ( runtime camel exception ) cause ; } okay just throw the exception as is if ( cause instanceof exception ) { throw ( exception ) cause ; } else { wrap as exception throw new camel exchange exception ( error processing exchange , exchange , cause ) ; } } class < ? > to = is future ? get generic type ( exchange . get context ( ) , method . get generic return type ( ) ) : method . get return type ( ) ;
collection < string > classes = conf . get trimmed string collection ( kmsconfiguration . kms _ audit _ logger _ key ) ; if ( classes . is empty ( ) ) { log . info ( no audit logger configured , using default . ) ; result . add ( simple kmsaudit logger . class ) ; return result ; } for ( string c : classes ) { try { class < ? > cls = conf . get class by name ( c ) ; result . add ( cls . as subclass ( kmsaudit logger . class ) ) ; } catch ( class not found exception cnfe ) { throw new runtime exception ( failed to load + c + , please check + configuration + kmsconfiguration . kms _ audit _ logger _ key , cnfe ) ; } }
return parse ( content ) ; }
sb . append ( s . to lower case ( ) ) ; everything goes lowercase
builder . add server config ( config , false ) ;
list < abstract expression > indexed exprs = null ; try { indexed exprs = abstract expression . from jsonarray string ( exprsjson , from table scan ) ; } catch ( jsonexception e ) { e . print stack trace ( ) ;
if ( end pos . get row ( ) = = chunk end . get row ( ) & & end pos . get column ( ) > 0 ) { end pos . set column ( 0 ) ; }
return expression builder . constant expression ( ) ; } else if ( expressions . size ( ) = = 1 ) {
immutable list . builder < session id > builder = new immutable list . builder < > ( ) ; builder . add all ( per session driver entries . key set ( ) ) ; return builder . build ( ) ; }
final boolean has next1 = itr . has next ( ) ;
this . writers = new array list < > ( ) ; final set < message body writer > custom mbws = providers . get custom providers ( injection manager , message body writer . class ) ; final set < message body writer > mbws = providers . get providers ( injection manager , message body writer . class ) ; add writers ( writers , custom mbws , true ) ; mbws . remove all ( custom mbws ) ;
assert u ( adoc ( id , 5 , str , c ) ) ;
if ( settings = null ) { for ( int i = 0 ; i < settings . length ; i + + ) { settings [ i ] . apply setting ( factory ) ; } } }
base url = base url . substring ( 0 , colon index ) ; } }
hash map < class < ? > , class < ? > > mixins = new hash map < class < ? > , class < ? > > ( ) ; mixins . put ( sub class . class , mix in . class ) ; mixins . put ( base class . class , mix in2 . class ) ; mapper . set mix ins ( mixins ) ; map < string , object > result ; result = write and map ( mapper , new sub class ( 1 , 2 ) ) ; assert equals ( 1 , result . size ( ) ) ;
rec = new record ( ) ; update pos = rnd . next int ( values . length + 1 ) ; for ( int i = 0 ; i < values . length ; i + + ) { if ( i = = update pos ) { rec . write ( writer ) ; rec = new record ( ) ; rec . read ( reader ) ; } final int pos = permutation1 [ i ] ; rec . set field ( pos , values [ pos ] ) ; } rec . write ( writer ) ;
client response response = current client . call procedure with client timeout ( batch timeout override type . no _ timeout , procedure , timeout , unit , parameters ) ; return response ;
set state ( m src2 , not _ closed , not _ finished , without _ result , null , not _ failed , null ) ;
string uploader class = context . uploader class ( config ) ;
if ( get option type ( option ) = ioption . libraries ) { p info = from build to proj ( p info ) ; } else { the ioption . libraries type is morphed to = > icsetting entyr library _ file it * isn ' t * a workspace path flags & = icsetting entry . value _ workspace _ path ; p info = new path info ( option value . get value ( ) , false , subst ) ; }
string num = ; num + = c ; num + = iterator . next ( ) ; num + = iterator . next ( ) ; return ( char ) integer . parse int ( num , 8 ) ; } else {
crc verifier . record crc ( hash , crc ) ;
buffer . append ( en ) ;
start database ( copy dir , record formats name ) . shutdown ( ) ;
if ( file . delete ( ) ) { response . set status ( http servlet response . sc _ no _ content ) ; response . flush buffer ( ) ; } else response . send error ( http servlet response . sc _ forbidden ) ;
f = test field . class . get declared field ( int sfield ) ;
assert equals ( c1 . read entries ( 10 ) , collections . empty list ( ) ) ;
array list < string > arr = new array list < string > ( ) ;
assert null ( task service . create task query ( ) . single result ( ) ) ; wait for job executor to process all jobs ( 5000 l , 250 l ) ;
this . add ( m _ color attrib ) ; validate ( ) ;
return fast copy hash map . class ;
assert folder contents ( processed coredumps . get ( 0 ) , coredump path . get file name ( ) . to string ( ) ) ;
object key ; if ( object instanceof activation ) { key = class object type . match _ object type . get class type ( ) ; } else if ( object instanceof fact ) { key = ( ( fact ) object ) . get fact template ( ) . get name ( ) ; } else { key = object . get class ( ) ; } object type conf object type conf = this . type conf map . get ( key ) ;
final krb5 login configuration krb5configuration = new krb5 login configuration ( null , null , true , utils . get login configuration ( ) ) ;
if ( out = null ) { try { out . close ( ) ; } catch ( exception e ) {
bob . set likes ( brie ) ;
raw erasure coder benchmark . perform bench ( encode , raw erasure coder benchmark . coder . rs _ coder , 3 , 200 , 200 ) ; raw erasure coder benchmark . perform bench ( decode , raw erasure coder benchmark . coder . rs _ coder , 4 , 135 , 20 ) ; }
this http connection manager = new multi threaded http connection manager ( ) ;
student double primitive student min = new student double primitive ( ) ;
ioexception io exception = new ioexception ( ) ;
if ( time to live > = 0 & & is disable time to live ( ) ) { template . set time to live ( time to live ) ; } template . set session transacted ( transacted ) ;
if ( ffmpeg ) { log . warn ( ffmpeg validation failed ) ; }
archiving client . disable hfile backup ( ) ;
if ( http log . logv ) { http log . v ( blowing away the referer on an https - > http redirect ) ; } m headers . remove ( referer ) ;
map < string , string > pre = new hash map < string , string > ( ) ; decode header ( hin , pre , this . parms , this . headers ) ; if ( null = this . remote ip ) { this . headers . put ( remote - addr , this . remote ip ) ; this . headers . put ( http - client - ip , this . remote ip ) ; }
delete thread . join ( ) ; hook . unblock ( ) ; arrive and await ( get phaser , 2000 ) ;
context . stop ( ) ;
registry reg = locate registry . get registry ( null , registry port , client socket factory ) ; test registry ( reg ) ; return reg ; } catch ( remote exception ex ) { logger . debug ( rmi registry access threw exception , ex ) ; logger . info ( could not detect rmi registry - creating new one ) ;
byte [ ] buffer = new byte [ 1024 ] ; int len ; input stream in stream = zip file . get input stream ( entry ) ;
result = col . get style ( ) . as length ( c , cssname . width ) ; }
assert true ( hdfs . exists ( ss path ) ) ; file status status after rename = hdfs . get file status ( ss path ) ;
bf . log . warn ( previous levels is non - null + prev levels [ i ] . key ( ) + but not pointing to us at level + i ) ;
for ( aggregation info aggregation info : aggregations info ) { string aggregation function name = aggregation info . get aggregation type ( ) . to lower case ( ) ; if ( allowed _ aggregation _ functions . contains ( aggregation function name ) ) { return false ; } }
int actual size = 0 ; for ( ; actual size < all . length ; actual size + + ) { if ( all [ actual size ] = = null ) { break ; } } return arrays . copy of range ( all , 0 , actual size ) ;
configuration prefix to skip compare . add ( yarn configuration . yarn _ client _ app _ submission _ poll _ interval _ ms ) ;
pipeline pipeline = pipeline . create ( options ) ;
boolean caught no node = false ;
expression e = javascript compiler . compile ( sqrt ( _ score ) + ln ( popularity ) ) ; simple bindings bindings = new simple bindings ( ) ; bindings . add ( new sort field ( popularity , sort field . type . int ) ) ; bindings . add ( new sort field ( _ score , sort field . type . score ) ) ; rescorer rescorer = e . get rescorer ( bindings ) ; hits = rescorer . rescore ( searcher , hits , 10 ) ;
statistics controller controller = lookup . get default ( ) . lookup ( statistics controller . class ) ;
if ( batch . mode ( ) = = message batch . mode . oob ) { message batch oob _ batch = new message batch ( local _ addr , batch . sender ( ) , batch . cluster name ( ) , batch . multicast ( ) , message batch . mode . oob , len ) ; for ( long tuple < message > tuple : list ) { long seq = tuple . get val1 ( ) ; message msg = win . get ( seq ) ; we * have * to get the message , because loopback means we didn ' t add it to win if ( msg = null & & msg . is flag set ( message . flag . oob ) & & msg . set transient flag if absent ( message . transient flag . oob _ delivered ) ) oob _ batch . add ( msg ) ; } deliver batch ( oob _ batch ) ; }
draw layout ( canvas , is text cue ) ; return ; }
if ( the corba loc object . get rirflag ( ) ) { result = bootstrap resolver . resolve ( the corba loc object . get key string ( ) ) ; } else { result = get iorusing corbaloc ( the corba loc object ) ; } return result ; }
loc path iterator lpi = ( loc path iterator ) eo . get expression ( ) ;
if ( this . get material def ( ) = other . get material def ( ) ) { return false ; }
load properties ( props , configuration . class . get resource as stream ( web - inf + twitter4 j _ properties ) ) ;
return is cphs voice message indicator address ( ) & & ( orig bytes [ offset _ address _ value ] & 0xff ) = = 0x10 ;
assert equals ( druid writable . get value ( ) . size ( ) , writable . get value ( ) . size ( ) ) ;
sort and combine ( ) ; break ;
char [ ] p = program . get instructions ( ) ;
set < list < integer > > degenerate = sets . cartesian product ( ) ; check hash code ( degenerate ) ; check hash code ( sets . cartesian product ( set ( 1 , 2 ) ) ) ; int num = integer . max _ value 3 * 2 ; tickle overflow - related problems
if ( written = = 0 ) { break ; }
if ( change type = = state change type . always | | old state . equals ( state ) ) { publish state ( state ) ; } }
matcher spaced decimal matcher = decimal _ space _ pattern . matcher ( text ) ;
rpc multiplexer q0mux = mock ( rpc multiplexer . class ) ; when ( q0mux . get and advance current index ( ) ) . then return ( 0 ) ; fcq . set multiplexer ( q0mux ) ;
try { post submit ( ) ; } catch ( exception e ) { system . err . println ( e . get message ( ) ) ; e . print stack trace ( ) ; assert . fail ( post - submit work caused an error : + e . get message ( ) ) ; }
this . scheduler = ordered scheduler . new builder ( ) . core pool size ( num threads ) . name ( distributed log service - executor ) . trace task execution ( true ) . stats logger ( stats logger . scope ( scheduler ) ) . build ( ) ;
set view layout ( view layout . timers _ at _ top ) ;
if ( sig [ 0 ] = = 0x30 & & in asn . 1 ( ( sig [ 1 ] + 2 = = sig . length ) | | ( ( sig [ 1 ] & 0x80 ) = 0 & & ( sig [ 2 ] & 0xff ) + 3 = = sig . length ) ) ) { 2bytes for len int index = 3 ; if ( ( sig [ 1 ] & 0x80 ) = 0 & & ( sig [ 2 ] & 0xff ) + 3 = = sig . length ) index = 4 ; byte [ ] r = new byte [ sig [ index ] ] ; byte [ ] s = new byte [ sig [ index + 2 + sig [ index ] ] ] ; system . arraycopy ( sig , index + 1 , r , 0 , r . length ) ; system . arraycopy ( sig , index + 3 + sig [ index ] , s , 0 , s . length ) ; r = chop0 ( r ) ; s = chop0 ( s ) ; buffer buf = new buffer ( ) ; buf . put mpint ( r ) ; buf . put mpint ( s ) ; sig = new byte [ buf . get length ( ) ] ; buf . set off set ( 0 ) ; buf . get byte ( sig ) ; } return sig ;
path foo = dir . resolve ( foo ' bar ) ;
final string consequence name = rule . get consequence ( ) . get class ( ) . get name ( ) ;
set no cache ( i response ) ;
pivot x = width 2 ;
val . set value ( 20 f ) . set exists ( true ) ;
stop ( true ) ; }
annotation attributes attributes = get annotation attributes ( web controller . class , component ) ;
append entry to view ( text to add . to string ( ) , timestamp ) ;
check capacity ( ) ;
final input mongo input = input service . find ( input . get persist id ( ) ) ; final extractor extractor = input service . get extractor ( mongo input , extractor id ) ; input service . remove extractor ( mongo input , extractor . get id ( ) ) ; final string msg = deleted extractor < + extractor id + > of type [ + extractor . get type ( ) + ] + from input < + input id + > . ;
a . remove ( b ) ; assert equals ( 4 , tree . num annotations ( ) ) ;
int this thread id = 2 ;
kill bill client . create invoice ( account json . get account id ( ) , future date , created by , reason , comment ) ;
annotation . element values accept ( clazz , this ) ;
reset advice monitor ( ) ;
sslserver socket ss = ( sslserver socket ) get context ( ) . get server socket factory ( ) . create server socket ( 0 ) ;
invisibly processed = out . length = = 0 & & input len > 0 ;
m manifest . write ( print ) ;
final channel selection channel selection = raster symbolizer . get channel selection ( ) ;
excepted = false ;
int last word = ret val . get ( ret val . length ( ) - 1 ) ; if ( concise set utils . is literal ( last word ) ) { last word = concise set utils . clear bits after in last word ( last word , concise set utils . max literal length modulus ( end index ) ) ; } ret val . set ( ret val . length ( ) - 1 , last word ) ; trim zeros ( ret val ) ;
condition fragment by id = new condition fragment ( ) . set table alias ( alias ) . set condition ( column names , ? ) ; string builder where string = new string builder ( ) ;
throw new xpath exception ( xpath exception . type _ err , e . get message ( ) ) ; } }
b tree node . remove key at ( cursor , key pos , key count ) ;
list < article > article list = new array list < > ( ) ; for ( int i = 0 ; i < 3 ; i + + ) { article article = new article ( m titles [ i ] , m contents [ i ] ) ; article list . add ( article ) ; } m adapter . set datas ( article list ) ; }
return new middle component data ( new middle map element not key component mapper ( main generator . get ver ent cfg ( ) , prefix ) , 0 ) ;
running mode mode = psagent context . get ( ) . get running mode ( ) ;
this . ulhc = new geo point ( planet model , sin top lat , sin left lon , cos top lat , cos left lon , top lat , left lon ) ; this . urhc = new geo point ( planet model , sin top lat , sin right lon , cos top lat , cos right lon , top lat , right lon ) ; final double middle lat = ( top lat - math . pi * 0 . 5 ) * 0 . 5 ; final double sin middle lat = math . sin ( middle lat ) ; this . cos middle lat = math . cos ( middle lat ) ;
result = this . invoker . invoke ( invocation ) ;
bogus = true ;
do return ( nn ctime + 1 ) . when ( mock storage info ) . get ctime ( ) ;
string [ ] temps = infos [ 0 ] . split ( : ) ;
headers . put ( camel google calendar . event id , event id ) ;
control buffer binary . clear ( ) ;
my uid = authentication backup data . get authenticated uid ( ) ;
htu . verify numeric rows ( table , f , start row , end row , replica id ) ; }
b . bind ( port ) . sync ( ) . channel ( ) . close future ( ) . sync ( ) ; } finally {
load balancer = load balancer type . create load balancer ( route context ) ;
final float angle offset = ( slice angle - slice space middle angle 2 . f ) 2 . f ; angle = angle + angle offset ;
future < object > put future = fork ( ( ) - > { try { return cache ( 1 ) . put ( my key , new value ) ; } catch ( exception e ) { log . errorf ( e , put failed : % s , e . get message ( ) ) ; throw e ; } } ) ;
assert false ( hrl3 . equals ( hrl4 ) ) ;
label label = get label control ( inner ) ;
int count = maximum _ view _ hierarchy _ level ; while ( parent = null & & count - - > 0 ) { if ( parent instanceof log container provider ) { return ( log container provider ) parent ; } else { parent = parent . get parent ( ) ; } } return null ; }
string [ ] include paths = scan info . get include paths ( ) ;
int row to select = model . get row count ( ) - 1 ;
sql = select * from ( select distinct ( a ) from r1 offset ? ) t1 ;
} else if ( anchor index = = - 1 ) {
register email activity . on new user ( new user . builder ( email auth provider . provider _ id , test constants . email ) . set name ( test constants . name ) . set photo uri ( test constants . photo _ uri ) . build ( ) ) ; edit text email = register email activity . find view by id ( r . id . email ) ;
template . request body ( server uri + & message = % 2 bworld , null , object . class ) ; assert mock endpoints satisfied ( ) ; }
base adapter = new sticky grid headers list adapter wrapper ( adapter ) ;
if ( is an extra column ( table , to remove ) ) continue ; column drops - - ; columns . remove ( index to remove ) ; if ( ( partition col index > = 0 ) & & ( partition col index > index to remove ) ) { partition col index - - ; }
generic udfto unix time stamp udf2 = new generic udfto unix time stamp ( ) ; object inspector [ ] args2 = { value oi , value oi } ; udf2 . initialize ( args2 ) ; val = 2001 - 01 - 01 ;
test nodes [ 0 ] = data nodes [ 1 ] ; test nodes [ 1 ] = data nodes [ 2 ] ; test nodes [ 2 ] = data nodes [ 0 ] ; cluster . set random seed ( 0x deadbeef ) ; cluster . sort by distance ( data nodes [ 0 ] , test nodes , test nodes . length ) ; assert true ( test nodes [ 0 ] = = data nodes [ 0 ] ) ; assert true ( test nodes [ 1 ] = = data nodes [ 1 ] ) ; assert true ( test nodes [ 2 ] = = data nodes [ 2 ] ) ;
filtered set < e > filtered = ( filtered set < e > ) unfiltered ;
_ ewyk . produce ( ) ;
assert true ( cursor . next ( ) ) ;
alpha list . clear ( ) ;
naming enumeration benum = init ctx . list ( java : comp env ) ; assert equals ( 4 , results . size ( ) ) ;
system . set property ( zkclient config . enable _ client _ sasl _ key , false ) ; try { zk = create client ( ) ; try { zk . get data ( abc , null , null ) ; assert . fail ( should not be able to read data when not authenticated ) ; } catch ( keeper exception . no auth exception e ) { success } zk . close ( ) ; } finally { enable client sasl system . set property ( zkclient config . enable _ client _ sasl _ key , true ) ; }
enqueue notification ( entry key , hash , value reference , removal cause . expired ) ; } else { record locked read ( e , now ) ;
set . add all ( list ) ;
term term = new term ( path , good ) ;
final path subdir = new path ( zone , dirsub ) ; fs wrapper . mkdir ( subdir , fs permission . get dir default ( ) , true ) ; for ( int i = 10 ; i < 15 ; + + i ) { dfstest util . create file ( fs , new path ( subdir , file + i ) , len , ( short ) 1 , 0x feed ) ; }
messaging controller . get instance ( get application ( ) ) . get account stats ( this , m account , m adapter . m listener ) ; on refresh ( refresh _ remote ) ;
if ( azk props . contains key ( constants . configuration keys . azkaban _ server _ external _ analyzer _ topic ) ) { return ; }
} else if ( m _ encoding info . is in encoding ( c ) ) { writer . write ( c ) ;
add authorization header ( headers , per request authorization header ( request , realm ) ) ;
lazy skip prox count + = freq ;
assert true ( return ts > input ts ) ;
string locations3 = dir0 , dir1 ;
assert . assert true ( has lease ( cluster , path2 ) ) ; out1 . close ( ) ;
list < string > expected selection columns = arrays . as list ( double , double _ array , float , float _ array , int , int _ array , long , long _ array , string , string _ array ) ; assert . assert equals ( selection columns , expected selection columns ) ; }
if ( ( is offline ( ) & & get detail ( ) = = thing status detail . none ) | | is offline ( ) ) { logger . debug ( changing status of { } from { } ( { } ) to offline ( { } ) , thing . get uid ( ) , get status ( ) , get detail ( ) , status detail ) ; update status ( thing status . offline , status detail , status message ) ; return ; }
aaa = new key value ( a , fam , qf , 1 , a ) ;
layer = gwc . get ( ) . get tile layer by name ( cite : lakes ) ; assert equals ( yourblobstore , layer . get blob store id ( ) ) ; gwc . get ( ) . remove blob stores ( collections . singleton ( yourblobstore ) ) ;
check file not exists ( configuration , path1 ) ;
for ( int i = length - 1 ; i > = 0 & & b = = 0 ; i - - ) { switch ( bit operator ) { case and : b = ( this . value [ i ] & value [ i + offset ] ) & 0xff ; break ; case or : b = ( this . value [ i ] | value [ i + offset ] ) & 0xff ; break ; case xor : b = ( this . value [ i ] ^ value [ i + offset ] ) & 0xff ; break ; } } return b = = 0 ? 1 : 0 ;
return new string response ( deactivated + path ( tenant resource . api _ path , tenant name , application resource . api _ path , application name , environment resource . api _ path , environment , region , region , instance , instance name ) ) ;
byte [ ] column qualifier = separator . qualifiers . join ( column prefix bytes , bytes . to bytes ( qualifier ) ) ; return column qualifier ; }
assert that ( state wrapper . state , null value ( ) ) ; assert that ( state wrapper . exit , null value ( ) ) ; assert that ( state wrapper . count . get ( ) , is ( 0 ) ) ; test bean14 test bean14 = context . get bean ( test bean14 . class ) ;
final custom field new custom field = new string custom field ( cf name , cf value , object type . account , account id , call context . get created date ( ) ) ; events listener . push expected event ( next event . custom _ field ) ;
assert true ( inet addresses . for string ( 1 . 2 . 3 . 4 ) = inet addresses . get coerced ipv4 address ( inet addresses . for string ( : : 1 . 2 . 3 . 4 ) ) ) ;
application id application2 = make application id ( t2 , a2 ) ;
edit . put int ( key , value ) ;
l + = xoff + yoff * width ; break ; default :
return l ; }
if ( test char = = ' ? ' ) { index + + ; start = index ; while ( index < end ) { test char = p _ uri spec . char at ( index ) ; if ( test char = = ' ' ) { break ; } if ( test char = = ' % ' ) { if ( index + 2 > = end | | is hex ( p _ uri spec . char at ( index + 1 ) ) | | is hex ( p _ uri spec . char at ( index + 2 ) ) ) { throw new malformed uriexception ( query string contains invalid escape sequence ) ; } } else if ( is reserved character ( test char ) & & is unreserved character ( test char ) ) { throw new malformed uriexception ( query string contains invalid character : + test char ) ; } index + + ; } m _ query string = p _ uri spec . substring ( start , index ) ; }
ocommand executor sqlhasync cluster . replace cluster ( this , server instance , database name , cl ) ; }
for ( validation demo entity : validation demos ) { validation result result = validator . validate output ( entity ) ; check ( result . is valid ( ) ) ; } return validation demos ;
drawable drawable = get activity icon ( component ) ;
cmd = new string builder ( script line ) ; } else {
index reader r = w . get reader ( ) ; bit set missing = new fixed bit set ( r . max doc ( ) ) ; for ( int doc id = 0 ; doc id < r . max doc ( ) ; doc id + + ) { document doc = r . document ( doc id ) ; if ( missing set . contains ( doc . get field ( id ) . numeric value ( ) ) ) { missing . set ( doc id ) ; } } for ( int iter = 0 ; iter < 100 ; iter + + ) { doc id set iterator values = field creator . iterator ( r ) ; assert equals ( - 1 , values . doc id ( ) ) ; while ( true ) { int doc id ; if ( random ( ) . next boolean ( ) ) { doc id = values . next doc ( ) ; } else { int range ; if ( random ( ) . next int ( 10 ) = = 7 ) { big jump range = r . max doc ( ) - values . doc id ( ) ; } else { small jump range = 25 ; } int inc = test util . next int ( random ( ) , 1 , range ) ; doc id = values . advance ( values . doc id ( ) + inc ) ; } if ( doc id = = no _ more _ docs ) { break ; } assert false ( missing . get ( doc id ) ) ; } }
create file ( fs , new path ( tmp test delete ) ) ;
int term count = this . get count sum ( term counters ) ; int term count log = ( ( term count > 100 ) ? ( ( int ) math . log10 ( term count ) ) : 2 ) ; system . out . println ( web term importance filter : term count log is + term count log ) ;
set property ( xinclude _ handler , f xinclude handler ) ;
check flow activity table ( cluster , user , flow , flow version , runid , c1 , app created time ) ;
argument captor < rest li service exception > wrapped ex capture = argument captor . for class ( rest li service exception . class ) ; verify ( _ response handler ) . build exception response data ( eq ( _ rest request ) , eq ( _ routing result ) , wrapped ex capture . capture ( ) , any map ( ) , any list ( ) ) ; verify ( _ response handler ) . build partial response ( _ routing result , response app data ) ; verify ( _ response handler ) . build rest exception ( wrapped ex capture . capture ( ) , eq ( partial response ) ) ; verify ( _ callback ) . on error ( rest exception , execution report , _ request attachment reader , response attachments ) ; verify ( _ rest request ) . get headers ( ) ; verify zero interactions ( _ routing result ) ; verify no more interactions ( _ rest request , _ response handler , _ callback ) ; assert not null ( response app data ) ; assert equals ( http status . s _ 500 _ internal _ server _ error , response app data . get response envelope ( ) . get status ( ) ) ; assert null ( response app data . get response envelope ( ) . get record ( ) ) ; final rest li service exception restli ex1 = wrapped ex capture . get all values ( ) . get ( 0 ) ;
setup vector dimension ( resource info . time , time , dimension presentation . list , null , null , null ) ; mock http servlet response response = get as servlet response ( wms?service = wms & version = 1 . 1 . 1 & request = get map + & bbox = - 180 , - 90 , 180 , 90 & styles = & width = 80 & height = 40 & srs = epsg : 4326 + & layers = + get layer id ( v _ time _ elevation ) + & time = 2011 - 05 - 02 , 2011 - 05 - 04 , 2011 - 05 - 10 & format = + gifmap response . image _ gif _ subtype _ animated + & transparent = false & bgcolor = 0xff0000 ) ;
string [ ] comps = util . components ( child . substring ( from ) , separator ) ; return comps = null & & comps . length < = 1 ; }
log . debug ( start deleting released cache because + [ size , allowed size , number sub dir , allowed number sub dir ] = + [ + size + , + allowed size + , + number sub dir + , + allowed number sub dir + ] ) ;
add item to database ( context , item , container , screen id , cell x , cell y ) ;
session factory session factory = emf . create entity manager ( ) . unwrap ( session . class ) . get session factory ( ) ;
string content location = part . get header ( content - location ) [ 0 ] ;
for ( byte [ ] field : afields ) { field [ 3 ] = inc package id ( ) ; buf = sc . write to buffer ( field , buf ) ; }
mock1 . reset ( ) ; mock1 . expected minimum message count ( 2 ) ; camel1 . resume ( ) ; mock1 . assert is satisfied ( ) ; }
linked list < value matcher > temp = new linked list < > ( ) ;
connector . set connect timeout millis ( timeout ) ;
exchange . set property ( exchange . unit _ of _ work _ process _ sync , boolean . true ) ;
dut . create all participating fragment work ( plan . remote work ) ;
if ( this . end description index = = - 1 ) { this . end description index = this . comment builder . last index of ( * . concat ( ioutils . line _ separator ) ) + 2 + ioutils . line _ separator . length ( ) ; } }
create trail files ( dir . get absolute path ( ) , trail _ filename _ prefix , 150 * num txns , 24 lines and 2 scns each * , 1250 * num lines per file * , 1 * num lines per newline * , \ n , 0 , 399 * corrupt last scn in 3rd file * , quux , false , ) ; trail file position setter pos setter = new trail file position setter ( dir . get absolute path ( ) , trail _ filename _ prefix ) ;
return obj + 0 . 5 * sample . length * lambda * wnorm ;
test action ( new task ( create closed exception stream ( data , failed ) ) { @ override public void run ( ) throws ioexception { bbis ( ) . try read ( new byte [ 10 ] ) ; } } , failed , true ) ;
if ( log . is debug enabled ( ) ) log . debug ( rejecting { } , request ) ; if ( is insert headers ( ) ) response . add header ( do sfilter , unavailable ) ; response . send error ( get too many code ( ) ) ; } }
if ( get content length ( start , - 1 l ) < = max content length ) { return continue . retained duplicate ( ) ; } pipeline . fire user event triggered ( http expectation failed event . instance ) ; return too _ large . retained duplicate ( ) ; }
class definition def = null ; if ( success ) { def = class definition factory . generate declared bean ( type descr , type , pkg registry , unresolved types , unprocesseable descrs ) ;
final string exclude minus dot star = exclude . substring ( 0 , exclude . length ( ) - 2 ) ; if ( class or static member . starts with ( exclude minus dot star ) & & class or static member . equals ( exclude minus dot star ) ) { final string member = class or static member . substring ( exclude minus dot star . length ( ) + 1 ) ;
preconditions . check argument ( large threads > 0 & & small threads > 0 ) ; final string n = thread . current thread ( ) . get name ( ) ;
put in all cache ( key , value ) ; assert value in all caches in primary site ( key , value ) ; all caches should have the value in primary site assert value in cache ( 1 , null , key , value ) ; assert value in cache ( 1 , cache type . backup _ to _ site _ 1 _ and _ 2 . name ( ) , key , value ) ; assert cache empty ( 2 , 0 , cache type . backup _ to _ site _ 1 _ and _ 2 . name ( ) ) ;
consumer = consumer cache . acquire polling consumer ( endpoint ) ;
buffer . reset reader index ( ) ;
string action = data . get ( action ) ; string extra data = data . get ( extra _ data ) ; logd ( tag , got fcm message , + action + = + action + , + extra _ data + = + extra data ) ; if ( action = = null ) { loge ( tag , message received without command action ) ; return ; }
try { timestamp interval = integer . parse int ( p . get property ( timestamp _ interval _ property , timestamp _ interval _ property _ default ) ) ; } catch ( number format exception nfe ) { throw new workload exception ( unable to parse the + timestamp _ interval _ property , nfe ) ; } try { time units = time unit . value of ( p . get property ( timestamp _ units _ property , timestamp _ units _ property _ default ) . to upper case ( ) ) ; } catch ( illegal argument exception e ) { throw new workload exception ( unknown time unit type , e ) ; }
acceptor . set backlog ( backlog ) ;
map < ? , ? > result = reader _ with _ arrays . for type ( map . class ) . read value ( empty _ array ) ;
while ( queue . poll ( ) = null ) ; mod count + + ; arrays . fill ( table , null ) ; size = 0 ;
try { try ( prepared statement ps = create preapred statement using set xxx ( sql ) ; result set res = ps . execute query ( ) ) { assert prepared statement result as expected ( res ) ; } try ( prepared statement ps = create preapred statement using set object ( sql ) ; result set res = ps . execute query ( ) ) { assert prepared statement result as expected ( res ) ; } } catch ( exception e ) { e . print stack trace ( ) ; fail ( e . to string ( ) ) ; }
for ( transaction t : transactions ) { tree . add ( t . get hash ( ) . get bytes ( ) ) ; }
val . set values ( 1810 - 12 - 02 t10 : 30 : 15 z , 1931 - 03 - 16 t18 : 15 : 45 z , 2023 - 11 - 01 t20 : 30 : 15 z , 1931 - 03 - 16 t18 : 15 : 45 z ) ; comp . set value ( 1931 - 03 - 16 t18 : 15 : 45 z ) . set exists ( true ) ; fill . set exists ( false ) ; iterator < long > values2 = arrays . as list ( date1 . get time ( ) , date3 . get time ( ) ) . iterator ( ) ; func . stream longs ( value - > { assert true ( values2 . has next ( ) ) ; assert equals ( values2 . next ( ) . long value ( ) , value ) ; } ) ; assert false ( values2 . has next ( ) ) ; val . set values ( 1810 - 12 - 02 t10 : 30 : 15 z , 1931 - 03 - 16 t18 : 15 : 45 z , 2023 - 11 - 01 t20 : 30 : 15 z , 1931 - 03 - 16 t18 : 15 : 45 z ) ;
assert true ( s . has next long ( 10 ) ) ; assert equals ( 23456 , s . next long ( 10 ) ) ; assert true ( s . has next long ( 10 ) ) ; assert equals ( 23456 , s . next long ( 10 ) ) ;
literal . append ( c ) ;
ui [ 0 ] = new user info ( user1 , drowssap , new string [ ] { group2 } ) ; string updated = compile with groups ( false , null , gi2 , ui , base , baseprocs ) ; catalog cat updated = catalog for jar ( updated ) ; verify diff ( cat original , cat updated , false ) ; }
logd ( tag , http _ not _ modified : data has not changed since + ref timestamp ) ; return null ; } else {
log . w ( package manager , failure retrieving icon 0x + integer . to hex string ( resid ) + in package + package name , e ) ;
type = ( base dmntype impl ) resolve type ref ( dmn model , node , item def , item def , item def . get type ref ( ) ) ;
set < integer > matrix ids = matrix meta manager . get matrix ids ( ) ;
raw rank profile raw child = new raw rank profile ( rank profile registry . get rank profile ( search , child ) , attribute fields ) ;
allocator . recycle byte blocks ( buffers , 1 , 1 + buffer upto ) ;
serialized = 0 ; unserialized = 0 ; list < custom type > custom types list = new array list < custom type > ( ) ;
string taskpath = current task ; if ( taskpath = null & & taskpath . equals ( path ) ) { log . info ( retrying data watch on + path ) ; tot _ wkr _ get _ data _ retry . increment and get ( ) ; get data set watch async ( ) ; } else {
local access control list tracker = true ; } else {
gbm = new gbm ( parms ) . train model ( ) . get ( ) ;
this . timer = timer ;
view share container = res . find view by id ( r . id . share history container ) ;
int size = providers . size ( ) ;
set record stats ( false ) ; }
size minimum size = new size ( 300 , 300 ) ; size size = dom metrics . adjusted element size ( preferred size _ , minimum size , 0 , pad 100 ) ; client margin frame _ . set size ( size . width + px , size . height + px ) ; if ( desktop . is desktop ( ) ) desktop . get frame ( ) . set shiny dialog url ( url _ ) ;
pre auth ( ) ;
model node config = resource . tools . read model ( context . read resource ( path address . empty _ address ) ) ;
connector page sink sink = page sink provider . create page sink ( transaction . get transaction handle ( ) , session , output handle ) ; sink . append page ( data . to page ( ) ) ; collection < slice > fragments = get future value ( sink . finish ( ) ) ;
config . set advertised address ( advertised address ) ; } else if ( is blank ( config . get advertised address ( ) ) ) {
final translation result source result = helpers . translate operand ( environment , offset , source operand , true ) ; instructions . add all ( source result . get instructions ( ) ) ;
key < deep learning model > key = key . make ( ae _ model ) ;
return work . accept ( new work executor < t > ( ) , connection ) ;
try { if ( position = = m app . get service ( ) . get current song index ( ) ) { holder . song title text . set text color ( m colors [ 0 ] ) ; holder . artist text . set text color ( m colors [ 0 ] ) ; } else if ( m app . get current theme ( ) = = common . light _ theme ) { holder . song title text . set text color ( uielements helper . get theme based text color ( m context ) ) ; holder . artist text . set text color ( uielements helper . get small text color ( m context ) ) ; } else if ( m app . get current theme ( ) = = common . dark _ theme ) { holder . song title text . set text color ( uielements helper . get theme based text color ( m context ) ) ; holder . artist text . set text color ( uielements helper . get small text color ( m context ) ) ; } } catch ( exception e ) { e . print stack trace ( ) ; } return convert view ;
while ( metrics queue . is empty ( ) ) { handle metric publisher publish message ( executor id , metrics queue . poll ( ) ) ; } } } ; looper . add tasks on wakeup ( metrics executors tasks ) ; }
if ( read cpu info file ) { return ; }
instance = geo server extensions . bean ( cswrecord descriptor . class ) ;
service unit zk utils . acquire name space ( zk cache . get zoo keeper ( ) , service unit zk utils . path ( test bundle ) , new namespace ephemeral data ( pulsar : otherhost : 8881 , pulsar : otherhost : 8884 , http : otherhost : 8080 , https : otherhost : 4443 , false ) ) ; assert true ( cache . get owned bundles ( ) . is empty ( ) ) ; thread . sleep ( 500 ) ;
if ( string utils . is not blank ( identifier column ) ) { final annotation metadata builder column builder = new annotation metadata builder ( column ) ; column builder . add string attribute ( name , identifier column ) ; annotations . add ( column builder ) ; }
main . run ( ) ;
note restore completed ( ) ; compiler log . info ( no more @ update application catalog calls when using ddl mode ) ; }
increment counter ( startup progress , loading _ edits , loading edits file , 1000 l ) ; startup progress . end step ( loading _ edits , loading edits file ) ; startup progress . end phase ( loading _ edits ) ; assert equals ( 5000 l , view . get count ( loading _ edits , loading edits file ) ) ; view = startup progress . create view ( ) ; assert not null ( view ) ; assert equals ( 6000 l , view . get count ( loading _ edits , loading edits file ) ) ; }
org . crsh . cli . impl . invocation . command invoker < instance < t > , ? > invoker = match . get invoker ( ) ;
field f = metrics source . class . get declared field ( global source source ) ;
jpopup menu menu = new jpopup menu ( ) ;
artwork file . delete ( ) ; file . delete ( ) ; } }
assert true ( ( ( string ) result . return value ) . starts with ( virtual . callee one @ ) ) ; }
} else if ( look ahead1 char = = ' ' ) { normalized . append ( ) ; i + = 2 ; i = read line ( sql , normalized , i ) ; break ; } else {
assert equals ( i , second . get from ( ) ) ; assert equals ( ( i + 1 ) % 10 , second . get to ( ) ) ; } else {
update left right buttons ( ) ; update date ( ) ;
create files ( length , num files , random , job ) ; task attempt context context = map reduce test util . create dummy map task attempt context ( job . get configuration ( ) ) ;
if ( resource require explicit registration ) { try { mbean utils . destroy mbean ( resource link ) ; } catch ( exception e ) { log . warn ( sm . get string ( naming resources . mbean destroy fail , resource link . get name ( ) ) , e ) ; } } resource link . set naming resources ( null ) ; }
zero index = normalize index ( 0 , inactive counts . get normalizing index offset ( ) , inactive counts . length ( ) ) ;
assert equals ( 800 , rd . granted . size ( ) ) ; log . info ( stopping test cmfailure transient ) ; } catch ( throwable t ) {
for ( final annotation metadata annotation : field . get annotations ( ) ) { append indent ( ) ; output annotation ( annotation ) ; this . new line ( false ) ; }
publish progress ( ) ;
m security button . set image resource ( id ) ;
m arrow . set scale x ( 0 ) ; m arrow . set scale y ( 0 ) ; animator arrow scale = create arrow scale anim ( 1 ) . set duration ( ( long ) get resources ( ) . get integer ( r . integer . config _ popup arrow open duration ) ) ; m open close animator = shortcut anims ; shortcut anims . play sequentially ( reveal anim , arrow scale ) ; shortcut anims . start ( ) ; }
reporting interaction mode = config . reporting interaction mode ( ) ;
final date time result = ( ( default subscription internal api ) subscription internal api ) . get effective date for new bcd ( new bcd , effective date , internal call context ) ; assert . assert equals ( result , internal call context . to utcdate time ( new local date ( 2012 - 06 - 03 ) ) ) ;
headers . put ( camel box . folder id , camel _ test _ root _ folder _ id ) ;
if ( null = = bitmap ) { bitmap = bitmap . create bitmap ( canvas . get width ( ) , canvas . get height ( ) , bitmap . config . argb _ 8888 ) ; } canvas temp = new canvas ( bitmap ) ; paint paint = new paint ( ) ; paint . set anti alias ( true ) ; paint . set color ( ( event check ) ? background color : color . parse color ( b0 b0 b0 ) ) ; paint . set stroke width ( utils . dp to px ( 2 , get resources ( ) ) ) ; temp . draw line ( get height ( ) 2 , get height ( ) 2 , get width ( ) - get height ( ) 2 , get height ( ) 2 , paint ) ; paint transparent paint = new paint ( ) ; transparent paint . set anti alias ( true ) ; transparent paint . set color ( get resources ( ) . get color ( android . r . color . transparent ) ) ; transparent paint . set xfermode ( new porter duff xfermode ( porter duff . mode . clear ) ) ; temp . draw circle ( view helper . get x ( ball ) + ball . get width ( ) 2 , view helper . get y ( ball ) + ball . get height ( ) 2 , ball . get width ( ) 2 , transparent paint ) ; canvas . draw bitmap ( bitmap , 0 , 0 , new paint ( ) ) ;
target . add all ( this . partition pages ) ;
string inputfilepath = home dev workspace docx4j sample - docs pkg . pkg ; java . io . file input stream fin = new java . io . file input stream ( inputfilepath ) ;
p . add child ( c1 ) ;
random access data stream a = new random access data ( ) ;
assert equals ( ( char ) 1 + ( short ) 1 , exec ( return ( char ) 1 + ( short ) 1 ; ) ) ;
create scheduling request ( 2048 , queue1 , user1 , 1 ) ;
str = ( string ) args . get ( time decay ) ; time decay = ( str = = null ) ? true : boolean . parse boolean ( str ) ; description = concurrent lfu cache ( max size = + limit + , initial size = + initial size + , min size = + min limit + , acceptable size = + acceptable size + , cleanup thread = + new thread + , time decay = + boolean . to string ( time decay ) ;
test ( line _ joiner . join ( ( ) = > { var x = 1 ; var y = 2 ; y ; } ; , ( ) = > { var z = 3 ; var w = 4 ; w ; } ; ) , line _ joiner . join ( ( ) = > { var x = 1 ; x = 2 ; x ; } ; , ( ) = > { var z = 3 ; z = 4 ; z ; } ; ) ) ; }
synchronized ( service mutex ) { check active ( ) ; return register instance ( create hazelcast instance ( config ) ) ; }
list < server name > servers = new array list < > ( ) ;
pending mo . cause = connection . disconnect cause . invalid _ number ;
logger . info ( wrap data statics feature error ) ; } @ override public void on success ( data feature feature ) {
for ( int i = 0 ; i < 5 ; i + + ) { mocked request handler req = grid helper . create new session handler ( registry , firefox ( ) ) ; req . process ( ) ; test session session = req . get session ( ) ; assert not null ( session ) ; assert equals ( session . get slot ( ) . get proxy ( ) . get total used ( ) , 2 ) ; }
unregister beans ( ) ; throw ex ; }
zoo keeper zk = qb . create client ( watcher , host ports [ test peer idx ] , connection _ timeout ) ; watcher . wait for connected ( connection _ timeout ) ; long local session id2 = zk . get session id ( ) ;
show empty view ( r . string . notifications _ account _ required , 0 , r . string . sign _ in ) ;
cursor = skip _ white _ space ( array , cursor , end ) ;
final text message text message = ( text message ) reply ;
try { file utils . delete directory ( temp dir ) ; } catch ( ioexception e ) { logger . warning ( could not delete temp directory : + temp dir . get absolute path ( ) + due to : + e . get message ( ) ) ; } }
a = rotate left ( a + f ( b , c , d ) + x [ 0 ] + 0xd76aa478 , s11 ) + b ;
dfs . concat ( trg path , files , restricted ) ;
project explorer . select item ( project _ name2 + src ) ;
if ( port exists ( port name ) ) { logger . warn ( port { } does not exists according to the system , we will still try to open it , port name ) ; }
new file = check file permissions ( file , read only ) ;
string left = sql . substring ( last ndx , ndx ) ;
replace task with conditional task ( curr task , cnd tsk ) ;
ch . close future ( ) . remove listener ( close listener ) ;
tomcat . add servlet ( ctx , servlet , new echo query string servlet ( ) ) ; ctx . add servlet mapping decoded ( , servlet ) ; tomcat . start ( ) ;
allow third . count down ( ) ; try { wait for 3rd to complete o3 . wait for thread done ( ) ; } catch ( throwable e ) { throw new runtime exception ( failed waiting on threads , e ) ; }
if ( command line . has option ( ' m ' ) ) { subscription group config . set consume from min enable ( boolean . parse boolean ( command line . get option value ( ' m ' ) . trim ( ) ) ) ; }
iterator joins = secondary tables . values ( ) . iterator ( ) ; iterator join columns = secondary table joins . values ( ) . iterator ( ) ; while ( joins . has next ( ) ) { object uncasted column = join columns . next ( ) ; join join = ( join ) joins . next ( ) ; create primary columns to secondary table ( uncasted column , property holder , join ) ; } }
overlap distance = - m settings . get view width px ( ) 4 ;
final int buf size = 10 ;
double tmp = summands [ 0 ] + summands [ 1 ] ;
parent = index column tester service . save ( parent ) ; parent saved parent = index column tester service . get parent by id ( parent . get id ( ) ) ;
bind many to many second pass ( this . collection , persistent classes , key columns , inverse columns , element columns , is embedded , coll type , ignore not found , unique , cascade delete enabled , association table binder , property , property holder , building context ) ; return false ;
this . jms messaging util . send text message ( request scoped cdibean . say hello ( ) , message . get jmsreply to ( ) , null ) ;
final int idx term offset = indexed term . offset ; final int prior term offset = prior term . offset ; final int limit = math . min ( prior term . length , indexed term . length ) ; for ( int byte idx = 0 ; byte idx < limit ; byte idx + + ) { if ( prior term . bytes [ prior term offset + byte idx ] = indexed term . bytes [ idx term offset + byte idx ] ) { return byte idx + 1 ; } } return math . min ( 1 + prior term . length , indexed term . length ) ; }
byte [ ] best mac addr = empty _ bytes ;
final list < link > copy = new array list < > ( links . size ( ) ) ;
return partitions + ( ( remainder > 0 ) ? 1 : 0 ) ;
sheet . get cells ( ) . get item ( 1 , 1 ) . set value ( first name ) ;
client . set timeline collector info ( collector info . new instance ( collector manager . get rest server bind address ( ) ) ) ; client . init ( conf ) ; client . start ( ) ; cluster entity cluster = new cluster entity ( ) ; cluster . set id ( yarn configuration . default _ rm _ cluster _ id ) ; flow run entity flow = new flow run entity ( ) ; flow . set user ( user group information . get current user ( ) . get short user name ( ) ) ; flow . set name ( test _ flow _ name ) ; flow . set version ( test _ flow _ version ) ; flow . set run id ( 1 l ) ; flow . set parent ( cluster . get type ( ) , cluster . get id ( ) ) ; application entity app = new application entity ( ) ; app . set id ( app id . to string ( ) ) ; flow . add child ( app . get type ( ) , app . get id ( ) ) ; application attempt id attempt id = application attempt id . new instance ( app id , 1 ) ; application attempt entity app attempt = new application attempt entity ( ) ; app attempt . set id ( attempt id . to string ( ) ) ; container id container id = container id . new container id ( attempt id , 1 ) ;
if ( var . equals ( var . get scope ( ) . get arguments var ( ) ) ) { return false ; }
k base = ( internal knowledge base ) knowledge base factory . new knowledge base ( kconf ) ; ksession = ( stateful knowledge session impl ) k base . new kie session ( ) ; create segment memory ( n5 , ksession ) ; bm = ( beta memory ) ksession . get node memory ( n1 ) ;
on replicate visitor visitor = new on replicate visitor ( source , id , entity , false ) ;
approve access token grant ( http : anywhere , true ) ;
assert . assert equals ( timeseries result ( immutable map . of ( a , 31 l , b , 31 . 0 , c , 31 l , d , 31 . 0 ) ) , run query ( query , factory , immutable list . of ( index2 ) ) ) ;
port . write bytes ( src ) ;
this . whitelist file = conf . get ( dfs . namenode . whitelist . file ) ;
read3 response response2 = nfsd . read ( xdr _ req . as read only wrap ( ) , security handler , new inet socket address ( localhost , 1234 ) ) ;
if ( obs [ i ] instanceof iccontainer | | obs [ i ] instanceof itranslation unit ) { res = ( ( icelement ) obs [ i ] ) . get resource ( ) ; } else if ( obs [ i ] instanceof iresource ) {
field [ ] fields = util . get all declared fields with annotations ( protocol . get class ( ) , local address . class ) ;
consistency checker checker = new consistency checker ( ) ; m _ table state map = new hash map < string , table save file state > ( ) ;
if ( has curlies ( ) | | are on same line ( get left curly ( ) , get right curly ( ) ) ) { check children ( list child , get checked children ( ) , get children expected indent ( ) , true , can children be nested ( ) ) ; }
throw new parse exception ( e . get message ( ) , - 1 ) ; }
byte buffer utils . copy from stream to buffer ( out , source , key value . row _ length _ size - common length ) ; row with size length = out . get short ( out . position ( ) - key value . row _ length _ size ) + key value . row _ length _ size ;
layer name = catalog configuration . remove workspace prefix ( layer name , catalog ) ;
oracle net connection descriptor parser parser = new oracle net connection descriptor parser ( rac ) ;
long time us = convert granule to time ( current granule ) ;
date time test = new date time ( 2015 , 1 , 10 , 2 , 55 , 0 , 0 , iso _ paris ) ;
cut start = true ;
final timer task < ? > timer task = timer . get timer task ( ) ;
_ context . router ( ) . rebuild router info ( true ) ;
last batch . projection size = output projection column map . length ; last batch . projected columns = output projection column map ; vec ptfoperator . forward ( last batch , null ) ;
buffered reader input = new buffered reader ( new input stream reader ( new data input stream ( input stream ) ) ) ;
if ( clazz . get name ( ) . matches ( ^ javax * \ \ . . * ) ) { return false ; } type ifaces [ ] = clazz . get generic interfaces ( ) ;
title = extract ( doc buf , ti , ti _ end , h2 , null ) ;
return get name + get name ( ) + - - get c - - + get c ( ) + - - get d - - + get d ( ) + - - get i - - + get i ( ) + - - get f - - + get f ( ) + - - get b - - + get b ( ) + - - get date - - + get date ( ) + auto + get auto ( ) ; }
http response decoder decoder = new http response decoder ( 4096 , 4096 , 5 ) ;
try { field one . get maximum value ( 1000000 l ) ; assert true ( false ) ; } catch ( unsupported operation exception e ) { assert true ( true ) ; }
set progress bar indeterminate visibility ( false ) ; } } ) ; } } ) . start ( ) ; } else {
try { cloud storage dropbox . load as string ( saved cloud entry dropbox . get persist data ( ) ) ; } catch ( parse exception e ) { e . print stack trace ( ) ;
defer close ( connection ) ;
if ( ary = = null ) return put int ( - 1 ) ; put int ( ary . length ) ; return put a1 ( ary , ary . length ) ; }
check state ( key crypter = null , not encrypted ) ; linked list < eckey > encrypted keys = lists . new linked list ( ) ; for ( eckey key : keys ) { if ( key . is encrypted ( ) ) throw new illegal argument exception ( cannot provide already encrypted keys ) ; encrypted keys . add ( key . encrypt ( key crypter , aes key ) ) ; } return import keys ( encrypted keys ) ;
xbf . get bean ( resource1 , resource test bean . class ) ;
oname = new object name ( catalina : type = store config ) ;
float inverse circle samples = 1 . 0f circle samples ;
this . do fn = get do fn ( ) ; do fn invoker = do fn invokers . invoker for ( do fn ) ; do fn invoker . invoke setup ( ) ;
fault = new swift invalid response exception ( error message , req . get method ( ) , uri , resp ) ;
for ( int i = 0 ; i < alist . length ; i + + ) { assert true ( alist [ i ] . equals ( blist [ i ] ) ) ; assert true ( blist [ i ] . equals ( clist [ i ] ) ) ; assert true ( clist [ i ] . equals ( i = = 0? far future : i = = 1? future : bare ) ) ; } }
session information session = session registry . get session information ( session id2 ) ;
border path . add arc ( new rect f ( left , bottom - oval size , left + oval size , bottom ) , 90 , 90 ) ;
throw new unknown error ( e . get message ( ) ) ;
verify ( response observer , timeout ( 100 ) ) . on next ( route summary captor . capture ( ) ) ;
resource pool . put ( note id , paragraph id , message . return resource name , ret ) ;
offset = styled document . get paragraph element ( offset ) . get end offset ( ) ; mlr = get multi line run ( offset ) ; offset = mlr = = null ? offset : mlr . end ( ) ; }
if ( inst . is missing ( structure . class index ( ) ) ) { out w . write ( ? ) ; } else { out w . write ( structure . attribute ( structure . class index ( ) ) . value ( ( int ) inst . value ( structure . class index ( ) ) ) ) ; }
for ( string date : dates that should not work ) { for ( format date time formatter date time formatter : root object mapper . defaults . dynamic _ date _ time _ formatters ) { try { date time formatter . parser ( ) . parse millis ( date ) ; fail ( string . format ( locale . root , expected exception when parsing date % s in root mapper , date ) ) ; } catch ( exception e ) { } } }
properties . put all ( parent . properties ) ; config _ location = parent . config _ location ; set defaults ( ) ; }
index index = this . manager . get index for update ( this . container path , true , * reuse index file * true * create if none * ) ; if ( index = null ) this . manager . save index ( index ) ; return true ;
set include root ( false ) ;
fis = sqlparser . parse file statement ( file myddl . sql ' quote sample . sql ' ' space file . sql ' ) ;
file system . set faulty rename ( new file ( cache dir , disk lru cache . journal _ file _ backup ) , false ) ; executor . jobs . remove first ( ) . run ( ) ; assert journal equals ( clean b 1 1 ) ;
alg id . encode ( tmp ) ;
record route list record route list = last response . get record route headers ( ) ; if ( record route list = = null ) { if the record route list is null then we can construct the ack from the specified contact header . note the 3xx check here because 3xx is a redirect . the contact header for the 3xx is the redirected location so we cannot use that to construct the request uri . if ( last response . get contact headers ( ) = null & & last response . get status code ( ) 100 = 3 ) { contact contact = ( contact ) last response . get contact headers ( ) . get first ( ) ; javax . sip . address . uri uri = ( javax . sip . address . uri ) contact . get address ( ) . get uri ( ) . clone ( ) ; ack request . set request uri ( uri ) ; } return ack request ; } ack request . remove header ( route header . name ) ;
verify run ( select * from + db name + _ dupe . virtual _ view2 , empty , driver mirror ) ;
try { persister . refresh inserted rows ( ) ; } catch ( throwable e ) { log . error ( error refreshing rows after update , e ) ; } } } } ; return persister . apply changes ( monitor , false , apply listener ) ; } catch ( dbexception e ) {
run hot swap pass ( null , null , ensure default pass config ( ) . garbage collect checks ) ; this . get type registry ( ) . clear named types ( ) ; this . remove synthetic vars input ( ) ; run hot swap ( original root , js , this . ensure default pass config ( ) ) ; }
long long key length = get key data structure size ( rlength , flength , qlength ) ; if ( long key length > integer . max _ value ) { throw new illegal argument exception ( keylength + long key length + > + integer . max _ value ) ; }
linked list node node = new linked list node ( key , value ) ; insert at front of linked list ( node ) ; map . put ( key , node ) ; }
list < byte [ ] > previous qualifiers = rows to qualifier . get ( row id ) ;
return component years . get ( base index ) + ( component years . get ( base index + 1 ) - component years . get ( base index ) ) 2 ;
for ( file status f : list paths ) { path found = new path ( f . get path ( ) . to uri ( ) . get path ( ) ) ; if ( parity file path . equals ( found ) ) { parity file pair ppair = parity file pair . get parity file ( codec . get codec ( code _ used ) , file stat , conf ) ; if ( ppair = null ) { log . info ( raid file found : + ppair . get path ( ) ) ; raided = true ; } break ; } }
assert true ( new store defs from metadat repo . get ( 1 ) . get required reads ( ) = = 2 ) ;
tab . load url ( params ) ;
context . set context data ( new hash map < string , object > ( ) ) ;
reset state ( ) ;
width = math . min ( m video width , width spec size ) ;
explicit intent . set component ( component ) ; return explicit intent ;
node record node = iterables . single ( record change set . get node records ( ) . changes ( ) ) . for reading data ( ) ; dynamic label record id . set ( iterables . single ( node . get dynamic label records ( ) ) . get id ( ) ) ; return record state ;
return new iterator < string > ( ) { int index = - 1 ; public void remove ( ) { string list . this . remove ( index ) ; index - - ; } public string next ( ) { return data [ + + index ] ; } public boolean has next ( ) { return index + 1 < count ; } } ;
command name = command name . replace all ( + , ) ; found = names . contains ( command name ) ; } else { break ; } }
integer rows = template . request body ( mybatis : count?statement type = select one , null , integer . class ) ; assert equals ( there should be 1 rows , 1 , rows . int value ( ) ) ; template . send body ( direct : start , 123 ) ;
requests in progress . put ( connection , new request record ( http request , completion handler ) ) ;
if ( d1 = d2 ) { return double . na n ; }
string auth header = exchange . get request headers ( ) . get first ( authorization ) ; if ( auth header = = null ) { return challenge result . not _ sent ; } }
if ( smile processor . contains emoji ( spannable string ) ) { spannable string = emoji ( ) . process emoji compat mutable ( spannable string , smile processor . configuration _ bubbles ) ; has spannable = true ; }
try { jabber provider factory . install account ( null , jabber account1 properties ) ; fail ( installing an account with a null account id must result + in a null pointer exception ) ; } catch ( null pointer exception exc ) { that ' s what had to happen }
assert . assert true ( failed to undeploy : + result , operations . is successful outcome ( result ) ) ;
mob file writer . append metadata ( fd . max seq id , major , mob cells ) ; mob file writer . close ( ) ; mob store . commit file ( mob file writer . get path ( ) , path ) ; } else {
d2 config . configure ( ) ; system . out . println ( finished populating zookeeper with d2 configuration ) ; }
int doc freq = terms enum . doc freq ( ) ; if ( doc freq > = freqmin & & doc freq < = freqmax ) {
string message = expected . get message ( ) ;
conf . set float ( capacity scheduler configuration . intraqueue _ preemption _ max _ allowable _ limit , ( float ) 0 . 5 ) ; string labels config = = 100 , true ; ;
m folded label = ( m folded label = = null ) ? no text : m folded label ;
string visualized = encode high level ( a1 b2 c3 d4 e5 f6 g7 h8 i9 j0 k1 l2 ) ;
client message request = cache put all codec . encode request ( name with prefix , entries , expiry policy data , completion id ) ;
if ( 23505 . equals ( ex . get sqlstate ( ) ) ) { return true ; }
map < rule , node processor > expr rules = new linked hash map < rule , node processor > ( ) ; expr rules . put ( new rule reg exp ( r1 , expr node dynamic value desc . class . get name ( ) + % ) , new dynamic value predicate proc ( ) ) ; dispatcher disp = new default rule dispatcher ( null , expr rules , ctx ) ; graph walker egw = new default graph walker ( disp ) ; list < node > start nodes = new array list < node > ( ) ; start nodes . add ( pred ) ; egw . start walking ( start nodes , null ) ; }
files . create file ( dir . resolve ( txt . foo ) ) ; files . create file ( dir . resolve ( b foo . xml ) ) ; path watcher path watcher = new path watcher ( ) ; path watcher . set update quiet time ( quiet _ time , time unit . milliseconds ) ;
assert that ( values mov avg , null value ( ) ) ;
assert that ( c , component ) . has ( sub component with ( c , test footer component . matcher ( c ) . text ( raw text ) . build ( ) ) ) ;
thread stop writer thread = new thread ( new runnable ( ) { @ override public void run ( ) { try { log . debug ( initiating + tswr . op name ( ) ) ; tswr . run ( recovering block ) ; log . debug ( finished + tswr . op name ( ) ) ; } catch ( throwable t ) { log . error ( stop writer thread got unexpected exception for + tswr . op name ( ) , t ) ; failure . compare and set ( null , stop writer thread got unexpected + exception for + tswr . op name ( ) + : + t . get message ( ) ) ; } } } ) ; stop writer thread . start ( ) ; while ( terminate slow writer . got interruption . get ( ) ) { wait until stop writer thread attempts to stop our slow writer by sending it an interrupted exception . thread . sleep ( 1 ) ; }
if ( length > 0 ) { byte [ ] value = new byte [ length ] ; always synchronized on data synchronized ( data ) { buffer . reset ( ) ; if ( buffer . read ( value ) = length ) { throw new ioexception ( short der value read ( encode ) ) ; } out . write ( value ) ; } }
clip offset = m layout . get width ( ) + stack tab . s stacked tab visible size ;
if ( is platform ( windows ) ) { return ; } byte array output stream os = new byte array output stream ( ) ;
store element array ( a writer , indent + 2 , interceptor . get members ( ) ) ; }
message = clicked : child + group position + - + child position ;
log . debug ( no master found and cluster is stopped ; bailing out ) ; return null ;
core map sentence as map = input . sentence . as core map ( sentence : : ner tags ) ; list < core label > tokens = sentence as map . get ( core annotations . tokens annotation . class ) ;
prediction = classifier prediction ;
( ( client thread ) seq to thread . get ( 0 ) ) . close ( ) ; wait for leader ( threads , 1 ) ;
service . process ( put1 ) ;
{ assert . assert equals ( bs puller . get servers ( ) , exp _ serverinfo _ 2 , server set ) ; do execute and change state ( bs puller , create set server message ( false , bs puller ) ) ; assert . assert equals ( bs puller . get current server idx ( ) = = - 1 , true , current server index undefined ) ; assert . assert equals ( bs puller . get curent server ( ) = = null , true , current server null ) ; assert . assert equals ( bs puller . get servers ( ) , exp _ serverinfo _ 3 , server set ) ; assert . assert equals ( bs puller . to tear conn after handling response ( ) , false , tear conn after handling response ) ; assert . assert equals ( conn state . get state id ( ) , state id . pick _ server , server set change while request _ start _ scn ) ; assert . assert equals ( bs puller . get queue list string ( ) , relay puller queue : [ pick _ server ] , queue : server set change while request _ start _ scn ) ; }
management service . move timer to executable job ( timer job . get id ( ) ) ; management service . execute job ( timer job . get id ( ) ) ; }
throw new state invariant error ( ble . get message ( ) ) ;
type info util type info util = type lib util . get type info util ( index ) ; typeattr type attr = type info util . get type attr ( ) ; this . create java doc header ( type attr . guid . to guid string ( ) , doc string ) ; int c funcs = type attr . c funcs . int value ( ) ;
type = test to type ( dpt , new byte [ ] { ( byte ) 0x c0 } , decimal type . class ) ;
resource = resource . copy with new id value ( string . format ( { % s } , joiner . on ( rdot txt entry . int _ array _ separator ) . join ( styleable resources map . values ( ) ) ) ) ;
string title = get title ( index ) ;
this . model _ id = new key v3 . model key v3 ( m . model _ id ) ; this . find _ compatible _ frames = m . find _ compatible _ frames ; if ( null = m . models ) { this . models = new model schema base v3 [ m . models . length ] ; int i = 0 ; for ( model model : m . models ) { this . models [ i + + ] = ( model schema v3 ) schema server . schema ( this . get schema version ( ) , model ) . fill from impl ( model ) ; } }
details . select ( matches . get ( sorted . get ( 0 ) ) ) ; }
dynamic realm dynamic realm = dynamic realm . get instance ( configuration ) ;
buffer . put ( state . family name with size ) ; key rest length = key length - row length - state . family name with size . length - ( key value . row _ length _ size + key value . timestamp _ type _ size ) ;
assert . assert false ( list bucketing pruner utils . not bool operand ( boolean . true ) ) ;
hive conf . set long var ( conf , conf vars . mapredminsplitsize , long . max _ value ) ;
if ( w < 8 | | w > 15 ) { inflate end ( z ) ; return jzlib . z _ stream _ error ; } wbits = w ; z . istate . blocks = new inf blocks ( z , z . istate . wrapper type = = wrapper type . none? null : this , 1 < < w ) ;
if ( output fs . exists ( snapshot tmp dir ) ) { system . err . println ( a snapshot with the same name ' + snapshot name + ' may be in - progress ) ; system . err . println ( please check + snapshot tmp dir + . if the snapshot has completed , ) ; system . err . println ( consider removing + snapshot tmp dir + before retrying export ) ; return 1 ; }
v value = data . get ( key ) ; if ( value = null ) { if ( record stats ) { stats counter . record hits ( 1 ) ; } return value ; } boolean [ ] missed = new boolean [ 1 ] ; value = data . compute if absent ( key , k - > {
ignore ( ignore ) ;
child injector = null ; weak key set utils . await clear ( weak ref ) ;
get data ( parent path + + our child , false , inspector , null ) ; } } else {
if ( next run ns < system . nano time ( ) ) { notify job consumer ( ) ; return 0 l ; } long diff = ( long ) math . ceil ( ( double ) ( next run ns - system . nano time ( ) ) ns _ per _ ms ) ; ensure consumer on time ( diff ) ; return diff ;
primitive data = this . input ;
integer decimals val = extract val ( decimals ) ; if ( decimals val = null ) { power = inline ( ten . pow ( decimals val , math context . decimal128 ) ) ; } else { power = dsl . power ( inline ( ten ) , decimals ) ; } return dsl . decode ( ) . when ( field . sign ( ) . greater or equal ( zero ( ) ) , field . mul ( power ) . floor ( ) . div ( power ) ) . otherwise ( field . mul ( power ) . ceil ( ) . div ( power ) ) ;
if ( rlen < len ) { xselement decl [ ] ret1 = new xselement decl [ rlen ] ; system . arraycopy ( ret , 0 , ret1 , 0 , rlen ) ; ret = ret1 ; }
int milliseconds = end . get ( calendar . millisecond ) - start . get ( calendar . millisecond ) ;
type deserializer type deser = type . get type handler ( ) ;
is = cl . get resource as stream ( www + uri ) ; return is ;
lost task tracker ( tracker ) ;
find viewables and attachments ( message , output viewable parts , output non viewable parts ) ;
execution job vertex = create execution job vertex ( 4 , 1 < < 15 ) ;
if ( proc method = null ) { string msg = procedure : + short name + has multiple public run ( . . . ) methods . ; msg + = only a single run ( . . . ) method is supported . ; throw compiler . new volt compiler exception ( msg ) ; }
ch = catfile . read ( ) ; while ( ch < = ' ' ) { all ctrls are whitespace ch = catfile . read ( ) ; if ( ch < 0 ) { return null ; } }
zkassign . delete all nodes ( this . watcher ) ; }
byte [ ] nonce = web socket util . random bytes ( 16 ) ; string key = web socket util . base64 ( channel buffers . wrapped buffer ( nonce ) ) ; string accept seed = key + magic _ guid ;
long xy = get za ( ) ;
client . create stream ( stream name , 1 ) ;
i _ p1 = cursor ; break lab0 ; } while ( false ) ; cursor = v _ 1 ;
int vertex number = 0 ; for ( string name : network input names ) { graph vertex gv = new input vertex ( this , name , vertex number , null ) ; output vertices : set later all names reverse . put ( name , vertex number ) ; vertices [ vertex number + + ] = gv ; }
fc view . remove acl ( mount on nn1 ) ; assert equals ( 0 , fc view . get acl status ( mount on nn1 ) . get entries ( ) . size ( ) ) ; assert equals ( 0 , fc . get acl status ( target test root ) . get entries ( ) . size ( ) ) ;
chart . set drag enabled ( true ) ;
return new gssapiendpoint authorizing callback handler ( ) ; } else if ( plain . equals ( mechanism name ) ) {
column names . add ( druid storage handler utils . default _ timestamp _ column ) ;
note returned note = notebook repo . set note revision ( note . get id ( ) , revision1 . id , null ) ; assert that ( returned note ) . is not null ( ) ; assert that ( returned note . get paragraphs ( ) . size ( ) ) . is equal to ( paragraph count _ 1 ) ;
entry . entry . remove extra field ( zip64 extended information extra field . header _ id ) ;
location = e . get approval uri ( ) ;
if ( tag _ style . equals ( element . get tag name ( ) ) ) { if ( curr = = null | | curr . get node type ( ) = = node . element _ node | | ( curr . get node type ( ) = = node . text _ node & & curr . get node value ( ) . trim ( ) . is empty ( ) & & ( curr . get previous sibling ( ) = = null | | curr . get previous sibling ( ) . get node type ( ) = = node . element _ node ) ) ) { return true ; } }
authentication simple http invoker request executor executor = new authentication simple http invoker request executor ( ) ; http urlconnection conn = new mock http urlconnection ( new url ( http : localhost ) ) ; executor . prepare connection ( conn , 10 ) ;
this . seq = seq ; this . state = state ; unstash all ( ) ; stop invalidation timer ( ) ; }
byte buf result = unpooled . buffer ( ) ;
type = _ type ( identity ) ;
if ( ( orig val & new value mask ) = = 0 & & shifted position of1 = 0 ) { num non zero regs + + ; } storage buffer . put ( position , ( byte ) ( unsigned bytes . max ( ( byte ) ( orig val & new value mask ) , shifted position of1 ) | ( orig val & original value mask ) ) ) ;
node . remove ( ) ;
optional < immutable map < string , debug section > > results = debug _ section _ finder . find ( buffer ) ;
( ( orecord id ) document . get identity ( ) ) . from string ( database . get storage ( ) . get configuration ( ) . index mgr record id ) ;
availability guard . fulfill ( requirement _ 2 ) ;
set < object name > set = mbean server . query names ( new object name ( * : type = errorhandlers , * ) , null ) ; assert equals ( 2 , set . size ( ) ) ;
deliver subchannel state ( subchannel2 , connectivity state info . for non error ( ready ) ) ; in order . verify ( helper ) . update balancing state ( eq ( ready ) , picker captor . capture ( ) ) ; round robin picker picker1 = ( round robin picker ) picker captor . get value ( ) ; assert that ( picker1 . drop list ) . contains exactly ( null , null ) ;
fs . delete ( file name , false ) ;
assert true ( input . read fully ( target , 0 , test _ data . length , false ) ) ;
field to date resolution = new hash map < string , date tools . resolution > ( ) ;
string text = exchange . get context ( ) . get properties ( ) . get ( jackson xmlconstants . enable _ xml _ type _ converter ) ; enabled = true . equals ignore case ( text ) ; init = true ; }
list < string > brokers = new array list < string > ( ) ; brokers . add ( broker1 ) ; brokers . add ( broker2 ) ; assert . assert equals ( connection . get broker list ( ) , brokers ) ; }
sql session sql session = sql session factory . open session ( ) ;
decoder = ( ( channel handler factory ) decoder ) . new channel handler ( ) ; } add to pipeline ( decoder - + x , channel pipeline , decoder ) ; }
for ( int i = 0 ; i < 3 ; i + + ) { task service . complete ( b tasks . get ( i ) . get id ( ) ) ; } list < task > c tasks = task service . create task query ( ) . task name ( c ) . list ( ) ;
test . set attribute ( suite , result . test class name ) ;
q . add ( new term query ( new term ( b , should ) ) , occur . should ) ; solr plugin utils . set min should match ( q , 100 % , true ) ; assert equals ( 3 , q . build ( ) . get minimum number should match ( ) ) ; }
verify ( new volume , never ( ) ) . check dirs ( ) ;
server = http server . create ( new inet socket address ( 0 ) , 10 ) ;
panel _ = new vertical panel ( ) ; panel _ . add style name ( style _ . list panel ( ) ) ;
record bytes . clear ( ) ;
for ( task task : task service . create task query ( ) . list ( ) ) { authentication . set authenticated user id ( task . get assignee ( ) ) ; map < string , object > var map = new hash map < string , object > ( ) ; var map . put ( test , test ) ; task service . complete ( task . get id ( ) , var map ) ; authentication . set authenticated user id ( null ) ; }
conf . unset ( dfs _ namenode _ rpc _ bind _ host _ key ) ;
if ( last resource kind = = uri resource kind . count | | last resource kind = = uri resource kind . value ) { resource content type = text _ plain _ with _ cs _ utf _ 8 ; } else { resource content type = content type ; } break ; default :
if ( rhs . defaulting & & defaulting ) { if ( rhs . aa hint value = = aa hint value & & rhs . fm hint value = = fm hint value ) { return tx = = null ? rhs . tx = = null : tx . equals ( rhs . tx ) ; } return false ; } else { return rhs . get anti aliasing hint ( ) = = get anti aliasing hint ( ) & & rhs . get fractional metrics hint ( ) = = get fractional metrics hint ( ) & & rhs . get transform ( ) . equals ( get transform ( ) ) ; }
this . verticals = new string [ math . max ( 10 , max container nesting level ) + 1 ] ; this . verticals [ 0 ] = ; no frame this . verticals [ 1 ] = ; synthetic root level this . verticals [ 2 ] = ; engine level for ( int i = 3 ; i < verticals . length ; i + + ) { verticals [ i ] = verticals [ i - 1 ] + theme . vertical ( ) ; }
sig agg . include exclude ( new include exclude ( o . d , null ) ) ;
double dbl value = ( double ) s . create criteria ( human . class ) . set projection ( projections . max ( height inches ) ) . unique result ( ) ;
testins . read ( readbuf ) ;
if ( first . get key ( first . number of keys ( ) - 1 ) . compare to ( node . get key ( 0 ) ) > 0 ) return false ; node < t > last = node . get child ( node . number of children ( ) - 1 ) ;
pair < integer , long > period position ; try { period position = seek timeline . get period position ( window , period , seek position . window index , seek position . window position us ) ; } catch ( index out of bounds exception e ) { the window index of the seek position was outside the bounds of the timeline . throw new illegal seek position exception ( timeline , seek position . window index , seek position . window position us ) ; } if ( timeline = = seek timeline ) { our internal timeline is the seek timeline , so the mapped position is correct . return period position ; }
site = extension . get http sessions site ( api utils . get authority ( params . get string ( action _ param _ site ) ) , false ) ; if ( site = = null ) { throw new api exception ( api exception . type . illegal _ parameter , action _ param _ site ) ; } api response list response = new api response list ( name ) ;
builder . set optional import enum ( unittest import . import enum . import _ bar ) ; builder . add repeated import enum ( unittest import . import enum . import _ bar ) ;
types . remove if ( c - > c . is annotation present ( api . class ) ) ; types . sort ( comparator . comparing ( type - > type . get name ( ) ) ) ; logger . debug ( ( ) - > { string builder builder = new string builder ( listing of all + types . size ( ) + annotated types : ) ; builder . append ( eol ) ; types . for each ( e - > builder . append ( e ) . append ( eol ) ) ; return builder . to string ( ) ; } ) ;
jpanel tp panel = new jpanel ( ) ; jlabel tp label = new jlabel ( throughput _ label ) ; tp panel . add ( tp label ) ;
off screen plot . set xindex ( x ax ) ; off screen plot . set yindex ( y ax ) ; off screen plot . set cindex ( master instances . num attributes ( ) - 1 ) ; string color att = get option ( optional args , - color ) ; int temp c = get index of attribute ( master instances , color att ) ; if ( temp c > = 0 ) { off screen plot . set cindex ( temp c ) ; } string has errors = get option ( optional args , - has errors ) ;
if ( allows file transfer ( ) ) return null ; if ( file utils . is image ( file . get name ( ) ) ) { create a thumbnailed file if possible . operation set thumbnailed file factory tf op set = contact . get protocol provider ( ) . get operation set ( operation set thumbnailed file factory . class ) ; if ( tf op set = null ) { byte [ ] thumbnail = get file thumbnail ( file ) ; if ( thumbnail = null & & thumbnail . length > 0 ) { file = tf op set . create file with thumbnail ( file , thumbnail _ width , thumbnail _ height , image png , thumbnail ) ; } } }
n = ( q < = write? write : end ) - q ; if ( n > z . avail _ out ) { n = z . avail _ out ; } if ( n = 0 & & r = = jzlib . z _ buf _ error ) { r = jzlib . z _ ok ; }
log . debug ( creating keyspace { } . . . , keyspace name ) ; ks def ksdef = new ks def ( ) . set name ( keyspace name ) . set cf _ defs ( new linked list < cf def > ( ) ) cannot be null but can be empty . set strategy _ class ( storage config . get ( replication _ strategy ) ) . set strategy _ options ( strategy options ) ;
if ( jchar2 < 0x dc00 ) { in . position ( x ) ; out . position ( out pos ) ; return coder result . malformed for length ( 1 ) ; }
string orig module dir = ant project name . replace ( analyzers - , analysis ) ;
print statement ( create function foo . bar ( \ f \ int , s object ) + returns object + language javascript as ' function ( f , s ) { return { \ a \ : 1 } } ' ) ; print statement ( create function foo . bar ( location geo _ point , geo _ shape ) + returns boolean + language javascript as ' function ( location , b ) { return true ; } ' ) ; }
final file file = new file ( file identifier ) ; string existing = null ; try { existing = file utils . read file to string ( file ) ; } catch ( final ioexception ignored ) { } if ( new contents . equals ( existing ) ) { mutable file = update file ( file identifier ) ; }
if ( ( type cache = null ) & & ( type cache instanceof big decimal ) ) { big decimal val = ( big decimal ) type cache ; use type cache ( ) ; return val ; } set radix ( 10 ) ; clear caches ( ) ;
byte buffer fbuf = f . get channel ( ) . map ( file channel . map mode . read _ only , 0 , f . length ( ) ) ; f . seek ( 0 ) ;
verify ( listener ) . on data read ( eq ( ctx ) , any int ( ) , any ( byte buf . class ) , any int ( ) , any boolean ( ) ) ; } finally {
try { bytes consumed + = write plaintext data ( src ) ; } catch ( exception e ) { throw new sslexception ( e ) ; }
client response bad response = perform call ( rm _ web _ service _ path + format ( apps _ appid _ priority , app id ) , null , null , null , post ) ; assert equals ( sc _ internal _ server _ error , bad response . get status ( ) ) ;
assert equals ( 8 , xpath . evaluate ( count ( + envelope base + [ gml : begin position = ' 2008 - 10 - 31 t00 : 00 : 00 . 000 z ' and gml : end position = ' 2008 - 10 - 31 t00 : 00 : 00 . 000 z ' ] ) , dom ) ) ; assert equals ( 8 , xpath . evaluate ( count ( + envelope base + [ gml : begin position = ' 2008 - 11 - 01 t00 : 00 : 00 . 000 z ' and gml : end position = ' 2008 - 11 - 01 t00 : 00 : 00 . 000 z ' ] ) , dom ) ) ; }
client . delete by query ( * : * ) ; delete everything client . commit ( ) ; assert num found ( * : * , 0 ) ; make sure it got in string f = val _ i ;
try { repository service . change deployment tenant id ( process definition id with tenant , ) ; fail ( ) ; } catch ( exception e ) { }
m30 = 0 ;
return useless _ email ; } catch ( exception e ) {
is running cluster ( ) ;
project workspace . process result build result = workspace . run buck command ( build , com example modern : xcompile ) ; build result . assert failure ( ) ; }
application utils . remove query param ( k edit published ) ; server _ . get edit published docs ( edit published , new simple request callback < js array string > ( ) { @ override public void on response received ( js array string docs ) { new source files opener ( docs ) . run ( ) ; } } ) ;
transfer plugin plugin = plugins . get ( config to . get transfer settings ( ) . get type ( ) , transfer plugin . class ) ; transfer settings transfer settings = config to . get transfer settings ( ) ;
default mqproducer producer = new default mqproducer ( please _ rename _ unique _ group _ name ) ;
type = tf . construct raw map like type ( string . class ) ;
fc1 . mkdir ( test path , fs permission . get default ( ) , true ) ;
if ( reg _ magic > = magic _ on & & at _ start & & ( prev _ at _ start & & prevchr = = magic . hat ) ) { curchr = magic . star ; }
for ( int i = 0 ; i < 3 ; i + + ) { mocked request handler req = grid helper . create new session handler ( registry2 , firefox ( ) ) ; req . process ( ) ; test session session = req . get session ( ) ; assert not null ( session ) ; sessions . add ( session ) ; } assert equals ( 50 , proxy1 . get resource usage in percent ( ) , 0f ) ;
long hash = ( ( ( ( ( ( long ) year - 1970 ) * 12 ) + ( month - 1 ) ) * 30 ) + day of month ) * 24 ; hash = ( ( ( ( ( ( hash + hours ) * 60 ) + minutes ) * 60 ) + seconds ) * 1000 ) + millis ; hash - = zone offset ; int normalized = is normalized ( ) ? 1 : 0 ; int era = 0 ; era e = get era ( ) ; if ( e = null ) { era = e . hash code ( ) ; } int zone = zoneinfo = null ? zoneinfo . hash code ( ) : 0 ; return ( int ) hash * ( int ) ( hash > > 32 ) ^ era ^ normalized ^ zone ;
string param base = results base + param : parameter ; assert that ( dom , has xpath ( param base + [ @ name = ' search terms ' and @ value = ' { search terms } ' and @ minimum = ' 0 ' ] ) ) ; assert that ( dom , has xpath ( param base + [ @ name = ' count ' and @ value = ' { count } ' and @ minimum = ' 0 ' and @ min inclusive = ' 0 ' and @ max inclusive = ' 100 ' ] ) ) ;
check failure ( index , 200 ) ; check success ( index , 400 , ofs1 _ 450 , 400 ) ; check success ( index , 500 , ofs1 _ 1100 , 500 ) ; check success ( index , 300 , 2100 , 300 ) ; long ofs1 _ 1010 = index . get position parser ( ) . set gen id ( 1010 , 1 ) ; assert equals ( 2100 , index . get larger offset ( ofs1 _ 1010 ) ) ; assert equals ( head check , 32 , index . get head ( ) ) ; assert equals ( tail check , 32 , index . get tail ( ) ) ;
query translator impl translator = create new query translator ( select count ( * ) from human h ) ; assert equals ( incorrect return type count , 1 , translator . get return types ( ) . length ) ; assert equals ( incorrect return type , long type . instance , translator . get return types ( ) [ 0 ] ) ; translator = create new query translator ( select count ( h . height inches ) from human h ) ; assert equals ( incorrect return type count , 1 , translator . get return types ( ) . length ) ; assert equals ( incorrect return type , long type . instance , translator . get return types ( ) [ 0 ] ) ;
dom = get as dom ( wcs?request = describe eocoverage set & version = 2 . 0 . 1 & service = wcs & eoid = sf _ _ spatio - temporal _ dss + & subset = phenomenon time ( \ 2008 - 11 - 01 t00 : 00 : 00 . 000 z \ , \ 2008 - 11 - 01 t01 : 00 : 00 . 000 z \ ) ) ; assert equals ( 8 , xpath . evaluate ( count ( wcs : coverage descriptions wcs : coverage description ) , dom ) ) ; assert equals ( 1 , xpath . evaluate ( count ( wcseo : dataset series descriptions ) , dom ) ) ;
( file type _ . can source on save ( ) & & doc update sentinel _ . source on save ( ) ) ; }
vis . create new ( ) ;
string result = template . request body and header ( ignite - compute : abc?execution type = apply , test ignite compute resources . test _ closure , ignite constants . ignite _ compute _ params , camel , string . class ) ; assert _ ( ) . that ( result ) . is equal to ( hello camel ) ;
package scan class resolver package resolver = get bean for type ( package scan class resolver . class ) ;
try { spec . add index column ( new hcolumn descriptor ( col ) , ql1 , new separator partition ( - - , 0 ) , value type . string , 10 ) ; assert . fail ( the testcase should fail if the value position with the separator is passed as zero . ) ; } catch ( illegal argument exception e ) { }
if ( need suggested ) { set suggested email ( sign id edit text ) ; }
this . series list . add ( series ) ; series . add change listener ( this ) ; fire dataset changed ( ) ; }
matcher statement matcher = sqlparser . match drop procedure ( ddl statement . statement ) ;
port = create server socket ( 0 ) ; }
scanner . seek to ( ) ; read all records ( scanner ) ; scanner . seek to ( get some key ( 50 ) ) ; assert true ( location lookup failed , scanner . seek to ( get some key ( 50 ) ) = = 0 ) ;
if ( converter instanceof multi level up down command converter ) { logger . debug ( multilevel switch multi level up down command converter ) ; if ( true . equals ignore case ( arguments . get ( invert _ state ) ) ) { logger . trace ( multilevel switch multi level up down command converter - invert ) ; if ( command = = up down type . up ) { command = up down type . down ; } else { command = up down type . up ; } logger . trace ( multilevel switch multi level up down command converter - inverted : { } , command ) ; } }
int cnt7 = 0 ;
assert equals ( null , simple language . simple ( { header . bar } , date . class ) . evaluate ( exchange , object . class ) ) ; assert equals ( null , simple language . simple ( { header . unknown } , string . class ) . evaluate ( exchange , object . class ) ) ; }
seg info stat . live doc status = test live docs ( reader , info stream , fail fast ) ;
source path resolver path resolver = default source path resolver . from ( new source path rule finder ( resolver ) ) ; string app id = adb helper . try to extract package name from manifest ( path resolver , has installable apk . get apk info ( ) ) ; return adb helper . uninstall app ( app id , uninstall options ( ) . should keep user data ( ) ) ? 0 : 1 ;
list . add ( node info ( i , per node , node state . unhealthy ) ) ;
check one term ( a , ÏÎ±ÏÏÎ¿ÏÏ , ÏÎ±ÏÏ ) ; check one term ( a , ÏÎ±ÏÏÎ¿Ï , ÏÎ±ÏÏ ) ; check one term ( a , ÏÎ±ÏÏÎ¿ÏÎ´ÎµÏ , ÏÎ±ÏÏ ) ; check one term ( a , ÏÎ±ÏÏÎ¿ÏÎ´ÏÎ½ , ÏÎ±ÏÏ ) ;
assert equals ( 0 , apn1 . get child count ( ) ) ; assert equals ( 0 , apn2 . get child count ( ) ) ; }
for ( struct field field : output oi . get all struct field refs ( ) ) { col aliases . add ( field . get field name ( ) ) ; }
} else if ( result . get pattern ( ) . is out capable ( ) & & result . has out ( ) & & result . is failed ( ) ) {
for ( charset provider charset provider : service loader . load ( charset provider . class ) ) { cs = charset provider . charset for name ( charset name ) ; if ( cs = null ) { return cache charset ( charset name , cs ) ; } } throw new unsupported charset exception ( charset name ) ;
dbsdata type data type = data source . get local data type ( type name ) ;
public void test application id pbimpl ( ) throws exception { validate pbimpl record ( application id pbimpl . class , application id proto . class ) ;
tester . advance input watermark ( new instant ( 11 ) ) ;
check one term ( a , ÏÎ±ÏÏÎ¿ÏÏ , ÏÎ±ÏÏ ) ; check one term ( a , ÏÎ±ÏÏÎ¿Ï , ÏÎ±ÏÏ ) ; check one term ( a , ÏÎ±ÏÏÎ¿ÏÎ´ÎµÏ , ÏÎ±ÏÏ ) ; check one term ( a , ÏÎ±ÏÏÎ¿ÏÎ´ÏÎ½ , ÏÎ±ÏÏ ) ;
final list < expected invoice item check > expected draft invoices = new array list < expected invoice item check > ( ) ; expected draft invoices . add ( new expected invoice item check ( invoice item type . external _ charge , big decimal . ten ) ) ;
avatar stack manager . store image ( image , this . next image index ) ;
super . test group by7 ( ) ;
class loader loader = wild fly security manager . get current context class loader privileged ( ) ;
object o = xpath . evaluate ( xstr , doc , type ) ;
assert equals ( number of sent and received xhtmp bodies does not match , bodies sent , bodies received ) ; }
attr . is specified ( true ) ; attr . is id attribute ( false ) ;
throw new illegal state exception ( couldn ' t get secure random number , e ) ;
if ( group interval = null ) { object helper . not null ( camel context , camel context , this ) ; log scheduler service = camel context . get executor service manager ( ) . new single thread scheduled executor ( this , throughput logger ) ; runnable scheduled log task = new scheduled log task ( ) ; log . info ( scheduling throughput log to run every + group interval + millis . ) ; must use fixed rate to have it trigger at every x interval log scheduler service . schedule at fixed rate ( scheduled log task , group delay , group interval , time unit . milliseconds ) ; }
string buffer buf = new string buffer ( ) ;
bis . mark ( 10 ) ; importer imp = importer factory . create importer ( bis , debug ) ;
for ( resource info ri : catalog . get resources ( resource info . class ) ) { if ( cite . equals ( ri . get store ( ) . get workspace ( ) . get name ( ) ) ) { tam . put limits ( cite , ri , new data access limits ( catalog mode . hide , filter . exclude ) ) ; } }
for ( int i = 0 ; i < integer . max _ value data size ; i + + ) ab . put a1 ( data ) ;
data entry writer war writer = new jar writer ( writer ) ; if ( output is war ) {
if ( menu . find item ( r . id . delete repo ) = null ) menu . find item ( r . id . delete repo ) . set visible ( false ) ; removing delete permission .
final string xml = < rule > + < name > bug report rule < name > + < model version > 1 . 0 < model version > + < attributes > + < metadata list > + < lhs > + < dsl sentence > + < definition > if process instance < definition > + < values > + < dsl sentence > + < lhs > + < rhs > + < dsl sentence > + < definition > my log { myout } < definition > + < values > + < org . drools . workbench . models . datamodel . rule . dslvariable value > + < value > 5 - 4 sample out < value > + < org . drools . workbench . models . datamodel . rule . dslvariable value > + < org . drools . workbench . models . datamodel . rule . dslvariable value > + < value > myout < value > + < org . drools . workbench . models . datamodel . rule . dslvariable value > + < values > + < dsl sentence > + < rhs > + < is negated > false < is negated > + < rule > ; rule model rm = rule template model xmlpersistence impl . get instance ( ) . unmarshal ( xml ) ;
if ( emulate csv ) { return invalid date time string ; }
preference . set fragment ( null ) ;
variant = 2 ; }
for ( int i = f namespace size ; i > 0 ; i - = 2 ) { if ( f namespace [ i - 1 ] = = uri ) { if ( f namespace [ i - 1 ] . equals ( uri ) ) { if ( get uri ( f namespace [ i - 2 ] ) = = uri ) if ( get uri ( f namespace [ i - 2 ] ) . equals ( uri ) ) return f namespace [ i - 2 ] ; } }
grid fsupload options options = new grid fsupload options ( ) . chunk size bytes ( 1024 ) . metadata ( new document ( type , presentation ) ) ; object id file id = grid fsbucket . upload from stream ( mongodb - tutorial , stream to upload from , options ) ;
zoo keeper watcher new connection zk = connection . get zoo keeper watcher ( ) ;
_ peers to ack . add all ( not yet ) ;
flink kafka consumer base < row > kafka consumer = get kafka consumer ( topic , properties , deserialization schema ) ;
test entity entity read = cursor . get ( key ) ;
filter . syn _ filt ( apond1 , apond1s , apond2 , apond2s , h , 0 , ld8 kconstants . long _ h _ st , mem _ zero , 0 , 0 ) ;
threw = false ; try { m _ client . call procedure ( @ ad hoc , drop table dropme if exists ; ) ; } catch ( proc call exception pce ) { threw = true ; } assert false ( dropping bad table with if exists should not have failed , threw ) ;
load complete . set ( true ) ;
float [ ] [ ] splits = tyahoo sketch split . get split value ( this . train data store , this . param . num split ) ;
test no nulls ( client , t _ pb , new string [ ] {
state = content ( ) ; state . do cdata = false ; super . characters ( chars , start , length ) ; } catch ( ioexception except ) {
string pairwise user id = access token . get subject ( ) ; assert . assert not equals ( pairwise user id , user . get id ( ) ) ;
headers = new headers . builder ( ) . add ( www - authenticate , dig es t r ea lm = \ myrealm \ , nonce = \ fjalskdflwejrlaskdfjlaskdjflaks + jdflkasdf \ , qop = \ auth \ , stale = \ false \ ) . build ( ) ;
den0 = tab _ den0 [ ptr _ den0 + + ] ;
int post version = cat . post version ; em . get transaction ( ) . begin ( ) ; kitten kitty = new kitten ( ) ; kitty . set name ( kitty ) ; list kittens = new array list < kitten > ( ) ; kittens . add ( kitty ) ; cat . set kittens ( kittens ) ; em . get transaction ( ) . commit ( ) ; assert equals ( post version should have been incremented . , post version + 1 , cat . post version ) ;
main one time = long . value of ( check url ( main - one , false ) ) ; main two time = long . value of ( check url ( main - two , false ) ) ; main three time = long . value of ( check url ( main - three , false ) ) ; other two time = long . value of ( check url ( other - two , false ) ) ; test one time = long . value of ( check url ( test - one , false ) ) ; assert . assert true ( main one time > other two time ) ;
assert that ( account info . get success ( ) ) . is equal to ( 1 ) ;
new key value ( rows _ two [ 0 ] , families [ 0 ] , qualifiers _ two [ 0 ] , values [ 1 ] ) , new key value ( rows _ two [ 0 ] , families [ 0 ] , qualifiers _ two [ 2 ] , values [ 1 ] ) , new key value ( rows _ two [ 0 ] , families [ 0 ] , qualifiers _ two [ 3 ] , values [ 1 ] ) , new key value ( rows _ two [ 0 ] , families [ 1 ] , qualifiers _ two [ 0 ] , values [ 1 ] ) , new key value ( rows _ two [ 0 ] , families [ 1 ] , qualifiers _ two [ 2 ] , values [ 1 ] ) , new key value ( rows _ two [ 0 ] , families [ 1 ] , qualifiers _ two [ 3 ] , values [ 1 ] ) ,
protocol provider service source provider = currently installed providers . get ( account id ) ; operation set persistent presence presence op set = source provider . get operation set ( operation set persistent presence . class ) ; contact group new proto group = presence op set . create unresolved contact group ( contact group uid , persistent data , ( parent proto group = = null ) ? presence op set . get server stored contact list root ( ) : parent proto group ) ;
take node outof service ( 0 , node outof service . get datanode uuid ( ) , time . now ( ) + expiration _ in _ ms , null , admin states . normal ) ;
if ( u = null ) {
new registry directory ( null , no meaning url ) ; fail ( ) ; } catch ( illegal argument exception e ) {
finish schema ( ) ;
int c = read ( ) ;
simple data file chunk set iterator iterator = new simple data file chunk set iterator ( generate data file chunk set ( num chunks , 10 , num entries per chunks , num entries per collision ) , true ) ; if ( num chunks * num entries per chunks = = 0 ) { assert equals ( iterator . has next ( ) , false ) ; } else { int entry = 0 ; for ( int collided entry = 0 ; collided entry < num chunks * ( num entries per chunks num entries per collision ) ; collided entry + + ) { assert equals ( iterator . has next ( ) , true ) ; byte buffer buffer = iterator . next ( ) ; read number assert equals ( buffer . array ( ) . length , num entries per collision * byte utils . size _ of _ int ) ; for ( int index = 0 ; index < num entries per collision ; index + + ) { assert equals ( buffer . get int ( byte utils . size _ of _ int * index ) , entry + + ) ; } } }
add training instance ( new instance ) ; }
stat manager stat mgr = _ context . stat manager ( ) ;
rebalance utils . executor shut down ( service , long . max _ value ) ;
fs . set replication ( file , ( short ) ( repl factor + 1 ) ) ;
field f = reflection utils . find named field ( o , path ) ; if ( null = = f ) throw new h2 oillegal argument exception ( _ exclude _ fields , filter fields , path ) ; try { f . set ( o , null ) ; } catch ( illegal access exception e ) { throw new h2 oillegal argument exception ( _ exclude _ fields , filter fields , path ) ; }
zkutil . set data ( zkw , node name , master address tracker . to byte array ( sn , 0 ) ) ;
array list < expr node desc > gb keys = expr node desc utils . gen expr node desc ( rs , 0 , gb info . gb keys . size ( ) - 1 , false , false ) ; for ( int i = 0 ; i < gb info . gb keys . size ( ) ; i + + ) { ci = rs col info lst . get ( i ) ; col output name = gb info . output col names . get ( i ) ; output col names . add ( col output name ) ; col info lst . add ( new column info ( col output name , ci . get type ( ) , , false ) ) ; col expr map . put ( col output name , gb keys . get ( i ) ) ; }
if ( params . contains key ( database ) | | params . contains key ( database name ) | | params . contains key ( host name ) | | params . contains key ( password ) | | params . contains key ( user name ) ) { return false ; } return true ;
deferred object [ ] null args = { new deferred java object ( null ) } ; output = ( date writable ) udf . evaluate ( null args ) ; assert null ( to _ date ( ) with null string , output ) ; }
collection < address > coords = util . determine actual merge coords ( views ) ; if ( coords . is empty ( ) ) coords = util . determine merge coords ( views ) ; https : issues . jboss . org browse jgrp - 2092 if ( coords . is empty ( ) ) { log . error ( % s : unable to determine merge leader from % s ; not starting a merge , gms . local _ addr , views ) ; return null ; } return new membership ( coords ) . sort ( ) . element at ( 0 ) ; establish a deterministic order , so that coords can elect leader }
map < string , long > updated counters = new hash map < > ( ) ;
cursor . close ( ) ;
reside menu = new reside menu ( this ) ; reside menu . set background ( r . drawable . resize _ menu _ menu _ background ) ; reside menu . attach to activity ( this ) ; reside menu . set menu listener ( menu listener ) ;
total cost + = source term deletion costs [ source term index ] ; } else {
assert that ( count maximum backoff ( back off ) , equal to ( 0 l ) ) ; back off . reset ( ) ;
domain type get domain post constraints = owsf . create domain type ( ) ;
sort ( fields ) ;
add import = false ;
fade in . set animation listener ( transition listener ) ;
int consumers = channel factory . consumers ( ) ;
return get concurrent update solr client ( url . to string ( ) + + collection , http client , 6 , 1 ) ; case 1 : return get http solr client ( url . to string ( ) + + collection , http client ) ; case 2 : cloud solr client client = get cloud solr client ( cluster . get zk server ( ) . get zk address ( ) , random ( ) . next boolean ( ) , http client , 30000 , 60000 ) ; client . set default collection ( collection ) ; return client ; } throw new runtime exception ( impossible ) ; }
evict entry ( ce . key ) ;
final long r time = last return time ; final long b time = last borrow time ; if ( r time > b time ) { return r time - b time ; }
select desc select = new select desc ( key exprs , output names ) ; select operator select op = ( select operator ) operator factory . get and make child ( select , parent of rs ) ;
if ( prev type = = null ) { field to type . put ( name , new field type ( type ) ) ; } else { type = merge term vector options ( type , prev type ) ; } return create field ( name , value , type ) ;
store [ 0 ] . acquire lock ( k , k , null , tx [ 0 ] [ 0 ] ) ;
stop self ( ) ;
graph . add layer ( name layer ( block name , activation1 , 0 ) , new activation layer . builder ( ) . activation ( activation . tanh ) . build ( ) , input ) ;
final segment < k , v > [ ] segments = this . segments ; int size ; boolean overflow ; true if size overflows 32 bits long sum ; sum of mod counts long last = 0 l ; previous sum int retries = - 1 ; first iteration isn ' t retry try { for ( ; ; ) { if ( retries + + = = retries _ before _ lock ) { for ( int j = 0 ; j < segments . length ; + + j ) { ensure segment ( j ) . lock ( ) ; force creation } } sum = 0 l ; size = 0 ; overflow = false ; for ( int j = 0 ; j < segments . length ; + + j ) { segment < k , v > seg = segment at ( segments , j ) ; if ( seg = null ) { sum + = seg . mod count ; int c = seg . count ; if ( c < 0 | | ( size + = c ) < 0 ) { overflow = true ; } } } if ( sum = = last ) { break ; } last = sum ; } } finally { if ( retries > retries _ before _ lock ) { for ( int j = 0 ; j < segments . length ; + + j ) { segment at ( segments , j ) . unlock ( ) ; } } } return overflow ? integer . max _ value : size ;
while ( q . size ( ) > 1 ) { root = q . peek first ( ) ; while ( root = null ) { root = q . poll first ( ) ; system . out . print ( root . data + ) ; if ( root . left = null ) { q . offer last ( root . left ) ; } if ( root . right = null ) { q . offer last ( root . right ) ; } root = q . peek first ( ) ; } root = q . peek last ( ) ; while ( root = null ) { system . out . print ( root . data + ) ; root = q . poll last ( ) ; if ( root . right = null ) { q . offer first ( root . right ) ; } if ( root . left = null ) { q . offer first ( root . left ) ; } root = q . peek last ( ) ; } }
result no = find column no ( get first child ( ) ) ; if ( result no = = - 1 ) { result no = find column no ( get next sibling ( ) ) ; }
global display _ = global display ;
pattern pattern _ week = pattern . compile ( [ 0 - 9 ] { 4 , 4 } - [ 0 - 9 ] { 2 , 2 } - w [ 0 - 9 ] { 2 , 2 } ) ; matcher matcher _ week = pattern _ week . matcher ( s ) ; if ( matcher _ week . find ( ) ) { string [ ] s _ parsed = matcher _ week . group ( 0 ) . split ( w ) ; int week _ num = integer . parse int ( s _ parsed [ s _ parsed . length - 1 ] ) ; return new string [ ] { get date from week ( week _ num + 1 ) , get date from week ( week _ num + 2 ) } ; } return null ;
file file = new file ( file name ) ; file channel w channel = new file output stream ( file , false ) . get channel ( ) ; w channel . write ( serialized buf ) ; w channel . close ( ) ; serialized buf . position ( 0 ) ;
values . add ( value2 ) ;
if ( ( bytes [ 1 ] & 0xc0 ) = 0x00 ) bits 6 + 7 throw new address format exception ( bits 0x40 and 0x80 must be cleared for ec - multiplied keys . ) ; ec multiply = true ; } else {
update import list for ( sg ) ; }
enum values enums = null ; if ( key type . is enum type ( ) ) { non - enum if we got it as type erased class ( from instance ) @ suppress warnings ( unchecked ) class < enum < ? > > enum class = ( class < enum < ? > > ) key type . get raw class ( ) ; enums = enum values . construct ( enum class , config . get annotation introspector ( ) ) ; } return new enum map serializer ( type . get content type ( ) , static typing , enums , element type serializer , property , element value serializer ) ; }
hive . get conf ( ) . set int var ( hive conf . conf vars . metastore _ fs _ handler _ threads _ count , 0 ) ;
try { string sql = upsert into contestants ( contestant _ number , contestant _ name ) values ( 23 , ' bruce springsteen ' ) ; java . sql . statement query = conn . create statement ( ) ; query . execute query ( sql ) ; system . err . println ( error ( execute query ( upsert ) succeeded , should have failed ) ) ; fail ( ) ; } catch ( sqlexception e ) { }
referenced class index + = count ;
super . install defaults ( ) ;
list < server health state > message for invalid merge = server health service . filter by scope ( health state scope . for partial config repo ( second downstream config repo ) ) ; assert that ( message for invalid merge . is empty ( ) , is ( false ) ) ;
seg . get array ( ) ;
jtx transaction jtx = worker . maybe request transaction ( required ( ) , ctx _ 1 ) ; assert not null ( jtx ) ; db session session1 = session provider . get db session ( ) ; execute update ( session1 , insert into girl values ( 1 , ' sophia ' , null ) ) ;
r . min height [ py - 1 ] + = pixels _ diff ; } } else if ( constraints . temp height > i & & constraints . temp height < next size ) next size = constraints . temp height ; } } return r ; }
throw new org . apache . axis2 . databinding . adbexception ( public ip cannot be null ) ; } else {
note note = notebook . create note ( anonymous ) ;
issue query . builder query = issue query . builder ( ) . sort ( issue query . sort _ by _ file _ line ) . asc ( true ) ; assert that search returns only ( query , f1 _ 1 , f1 _ 2 , f1 _ 3 , f2 _ 1 , f2 _ 2 , f2 _ 3 ) ;
message = consumer . receive ( ) ;
long [ ] expected = new long [ ] { 2 , 3 , 4 , 5 } ; assert arrival intervals ( bsd , expected ) ;
public void test relay chaining slow consumer ( ) { databus relay test util . relay runner r1 = null , r2 = null ;
log . debug ( already processed reduce sink : + sink . get name ( ) ) ; return true ; }
try { dispatcher . rethrow exception ( ) ; fail ( monitor should have thrown an exception after getting error . ) ; } catch ( foreign exception ex ) { assert true ( got an unexpected exception : + ex , ex . get cause ( ) = = extexn . get cause ( ) ) ; log . debug ( got the testing exception ) ; }
return choose local rack ( local machine , excluded nodes , blocksize , max nodes per rack , results ) ; }
batch . draw ( tex , 0 , 0 ) ; batch . end ( ) ;
integer sum sum1 = new integer sum ( 0 ) ; mutable map < string , integer > map1 = interval . from to ( 1 , 100 ) . to map ( string : : value of , functions . get integer pass thru ( ) ) ; parallel iterate . for each ( map1 , new sum procedure ( sum1 ) , new sum combiner ( sum1 ) ) ; assert . assert equals ( 5050 , sum1 . get sum ( ) ) ;
if ( impl . is connection reset ( ) ) { throw new socket exception ( connection reset ) ; }
path usersdir = new path ( local dir , container localizer . usercache ) ;
timeout cluster state listeners . for each ( timeout cluster state listener : : on close ) ;
assert equals ( 5 , buf . component at offset ( 2 ) . capacity ( ) ) ;
holder . item view . set tag ( r . id . fastadapter _ item _ adapter , fast adapter . this ) ;
tester . start page ( page ) ;
interpreter . set register ( cf , big integer . zero , operand size . byte , reil register status . defined ) ; interpreter . set register ( eax , big integer . value of ( 0x7 fffffffl ) , operand size . dword , reil register status . defined ) ; final mock operand tree operand tree1 = new mock operand tree ( ) ;
file input stream fis = new file input stream ( file ) ; file channel fc = fis . get channel ( ) ; byte buffer bb = byte buffer . allocate ( start byte ) ; fc . read ( bb ) ;
final timer timer = new timer ( ) ;
do get request ( remote context path + init servlet?url = + urlencoder . encode ( info context path . to external form ( ) , utf - 8 ) ) ; deployer . undeploy ( remote ) ;
parse utils . require no attributes ( reader ) ; model node add keycloak sub = util . create add operation ( path address . path address ( keycloak extension . path _ subsystem ) ) ; list . add ( add keycloak sub ) ; while ( reader . has next ( ) & & next tag ( reader ) = end _ element ) { if ( reader . get local name ( ) . equals ( realm definition . tag _ name ) ) { read realm ( reader , list ) ; } else if ( reader . get local name ( ) . equals ( secure deployment definition . tag _ name ) ) { read deployment ( reader , list ) ; } else if ( reader . get local name ( ) . equals ( secure server definition . tag _ name ) ) { read secure server ( reader , list ) ; } }
final class loader loader = tx bridge outbound recovery service . class . get class loader ( ) ;
if ( recording wrapper = null ) { recording wrapper . replay ( bucket ords ) ; } }
if ( current state . supersedes ( new cluster state ) | | ( new cluster state . nodes ( ) . get master node id ( ) . equals ( current state . nodes ( ) . get master node id ( ) ) & & current state . version ( ) = = new cluster state . version ( ) ) ) { if the new state has a smaller version , and it has the same master node , then no need to process it logger . debug ( received a cluster state that is not newer than the current one , ignoring ( received { } , current { } ) , new cluster state . version ( ) , current state . version ( ) ) ; return true ; }
array list < string > filenames = new array list < string > ( ) ; for ( file system item file : files ) filenames . add ( file . get name ( ) ) ;
for ( long val : boring values ) { literal schema + = create procedure hex _ literal _ proc _ arith _ + make even digits ( val ) + as \ n + select \ n + ? + x ' + make even digits ( val ) + ' , \ n + x ' + make even digits ( val ) + ' - ? , \ n + ? * x ' + make even digits ( val ) + ' , \ n + x ' + make even digits ( val ) + ' ? , \ n + - x ' + make even digits ( val ) + ' \ n + from t ; ; } try { project . add literal schema ( literal schema ) ; } catch ( exception e ) { fail ( ) ; }
string guava = iterables . get only element ( iterable ) ; using guava string jdk = collection . iterator ( ) . next ( ) ; using jdk string apache = collection utils . extract singleton ( collection ) ; using apache assert ( ordered iterable . size ( ) = 1 ) ; using gs string gs = ordered iterable . get first ( ) ; system . out . println ( single = + jdk + : + guava + : + apache + : + gs ) ; print single = a3 : a3 : a3 : a3
assert equals ( return code . include , mp onefilter list . filter cell ( null ) ) ;
if ( side inputs . is empty ( ) & & non keyed state internals = null ) { bag state < windowed value < input t > > pushed back = non keyed state internals . state ( state namespaces . global ( ) , pushed back tag ) ; iterable < windowed value < input t > > pushed back contents = pushed back . read ( ) ; if ( pushed back contents = null ) { if ( iterables . is empty ( pushed back contents ) ) { string pushed back string = joiner . on ( , ) . join ( pushed back contents ) ; throw new runtime exception ( leftover pushed - back data : + pushed back string + . this indicates a bug . ) ; } } } check finish bundle timer . cancel ( true ) ;
for ( int offset = 0 ; offset < code length ; offset + + ) { check if the replacement instruction , if any , has a different length than the original instruction . instruction replacement instruction = replacements [ offset ] ; if ( replacement instruction = null & & replacement instruction . length ( offset ) = instruction factory . create ( code , offset ) . length ( offset ) ) { return false ; } } return true ;
configuration prefix to skip compare . add ( yarn configuration . yarn _ client _ app _ submission _ poll _ interval _ ms ) ;
api api = ( api ) f . get annotations ( ) [ 0 ] ;
if ( full string . length ( ) > 2 & & full string . starts with ( \ ) & & full string . ends with ( \ ) ) { full string = full string . substring ( 1 , full string . length ( ) - 1 ) . trim ( ) ; } string evaluator part = null ;
assert . assert not null ( server side context data was expected to be non - null , values seen on server side ) ;
renderer . reset position ( renderer position us ) ;
injector . get instance ( key . get ( bacon . class , named ( turky ) ) ) ;
byte [ ] data = new byte [ 8 ] ; data [ 0 ] = 0x07 ; data [ 1 ] = rfxcom base message . packet type . lighting1 . to byte ( ) ; data [ 2 ] = sub type . to byte ( ) ; data [ 3 ] = seq nbr ; data [ 4 ] = ( byte ) sensor id ; data [ 5 ] = unitcode ; data [ 6 ] = command . to byte ( ) ; data [ 7 ] = ( byte ) ( ( signal level & 0x0 f ) < < 4 ) ; return data ;
for ( web resource set class resource : class resources ) { class resource . start ( ) ; } cache . enforce object max size limit ( ) ;
flags & = opcodes . acc _ native ; }
thread . sleep ( idle connection timeout ms + 1000 ) ;
unit nrshape unit shape = ( unit nrshape ) cell . get shape ( ) ; unit nrshape unit shape parent = unit shape . get shape at level ( unit shape . get level ( ) - 1 ) ; if ( parent facet = = null | | parent shape . equals ( unit shape parent ) ) { setup parent ( unit shape parent ) ; }
if ( terminated ) { regexp . append ( . * ) ; }
for ( int i = activities opened . size ( ) - 1 ; i > = 0 ; i - - ) { sleeper . sleep ( minisleep ) ; finish activity ( activities opened . get ( i ) ) ; }
for ( int i = drag layer . get child count ( ) - 1 ; i > = 0 ; i - - ) { view child = drag layer . get child at ( i ) ; if ( child instanceof folder ) { folder folder = ( folder ) child ; if ( folder . get info ( ) . opened ) return folder ; } } return null ;
component key stroke pair ckp = new component key stroke pair ( c , ks ) ; container top container = component key stroke map . get ( ckp ) ;
boolean is idl = idlentity . class . is assignable from ( clz ) ; stub factory = sff . create stub factory ( clz . get name ( ) , is idl , code base , clz , clz . get class loader ( ) ) ; }
throw new mapping exception ( could not parse version unsaved - value : + version unsaved value ) ;
return collection . contains ( non final class2 ) ;
service block packet . skip bits ( 2 ) ; null padding int window style = service block packet . read bits ( 3 ) ; int pen style = service block packet . read bits ( 3 ) ; cue builder . define window ( visible , row lock , column lock , priority , relative positioning , vertical anchor , horizontal anchor , row count , column count , anchor id , window style , pen style ) ;
from ( direct - vm : foo . noprops ) . process ( exchange - > assert null ( exchange . get property ( abc ) ) ) ; from ( direct - vm : foo . props ) . process ( exchange - > assert equals ( def , exchange . get property ( abc , string . class ) ) ) ; } } ; }
json parser buffer parser = unknown tokens . as parser ( ) ; while ( buffer parser . next token ( ) = json token . end _ object ) { string prop name = buffer parser . get current name ( ) ; unknown : let ' s call handler method buffer parser . next token ( ) ; handle unknown property ( buffer parser , ctxt , bean , prop name ) ; } return bean ;
producer template . send body and header ( direct : tweets , tweet , custom header , 12312 ) ; result endpoint . expected message count ( 1 ) ;
height = measure spec . get size ( height measure spec ) ;
check valid unique and assume unique ( schema , msg p , msg p ) ;
if ( parent parent = null & & parent parent . has descendant ( ( abstract config value ) parent ) ) config impl . trace ( * * * * * bug * * * * * trying to push non - child of + parent parent + , non - child was + parent ) ;
byte [ ] entity id bytes = entity . get entity id ( ) . get bytes ( utf _ 8 ) ; byte [ ] entity type bytes = entity . get entity type ( ) . get bytes ( utf _ 8 ) ; byte [ ] domain id bytes = entity . get domain id ( ) . get bytes ( utf _ 8 ) ;
if ( _ surrogate > 0 ) { char second = str . char at ( off + + ) ; - - len ; write ( convert surrogate ( second ) ) ; will have at least one more char ( case of 1 char was checked earlier on ) } int out ptr = _ out ptr ;
appender = null ; in error = false ; string class name = attributes . get value ( class _ attribute ) ;
if ( settings . is picky ( ) ) { visitor = new simple checks adapter ( visitor ) ; } if ( debug stream = null ) { visitor = new trace class visitor ( visitor , debug stream , null ) ; }
assert . assert equals ( 0 * gb , report _ nm1 . get available resource ( ) . get memory size ( ) ) ;
compact ( ) ;
merged . set name ( merged . get name ( ) + : + other . get name ( ) ) ; return merged ; }
object s = get columns ( ) . get ( column ) ;
located block block = namesystem . get block locations ( file . to string ( ) , 0 , 1 ) . get ( 0 ) ; namesystem . mark block as corrupt ( block . get block ( ) , block . get locations ( ) [ 0 ] ) ; update metrics ( ) ; assert equals ( 1 , metrics . corrupt blocks . get ( ) ) ; assert equals ( 1 , metrics . pending replication blocks . get ( ) ) ; assert equals ( 1 , metrics . scheduled replication blocks . get ( ) ) ; fs . delete ( file , true ) ; update metrics ( 10000 ) ;
completable future < acknowledge > trigger future = self gateway . trigger computation latch ( timeout ) ; trigger future . get ( timeout . to milliseconds ( ) , time unit . milliseconds ) ;
split entry . update ( cur split ) ; }
proc name = item _ big _ del ; verify long running queries ( 50000 , 0 , proc name , false ) ; tear down ( ) ; set up ( ) ; verify long running queries ( 50000 , 100 , proc name , false ) ;
if ( packet instanceof home packet ) { if ( interest = = interest type . relayid ) { return ( ( home packet ) packet ) . get relay id ( ) ; } } else rare data fields if ( packet instanceof rare packet ) { if ( interest = = interest type . relayid ) { return ( ( rare packet ) packet ) . get relay id ( ) ; } }
convert grouped properties ( content values , vcard ) ; convert birthdays ( content values , vcard ) ;
ex . set cserror code ( ref . get cserror code ( ) ) ;
assert . assert equals ( web origins , arrays . as list ( http : localhost : 8980 myapp ) , client . get web origins ( ) ) ; assert . assert equals ( consent required , false , client . is consent required ( ) ) ; assert . assert equals ( base url , http : localhost : 8980 myapp , client . get base url ( ) ) ; assert . assert equals ( bearer only , true , client . is standard flow enabled ( ) ) ; assert . assert false ( mappers not empty , client . get protocol mappers ( ) . is empty ( ) ) ;
assert . assert equals ( onetwo , old class path . get class ( onetwo ) . get common superclass ( new class proto ( old class path , onetwo ) ) . get type ( ) ) ; assert . assert equals ( onetwo , new class path . get class ( onetwo ) . get common superclass ( new class proto ( new class path , onetwo ) ) . get type ( ) ) ;
m sleep amount chart . set no data text ( ) ;
this . task info = task info ; this . task id = task id ; }
return new string response ( deactivated + path ( tenant resource . api _ path , tenant name , application resource . api _ path , application name , environment resource . api _ path , environment , region , region , instance , instance name ) ) ;
throw new deployment exception ( sm . get string ( pojo method mapping . duplicate annotation , on close . class , current clazz ) ) ;
int idx = 0 ; for ( short value lv : lva ) { assert equals ( ( short ) idx + + , lv . get value ( ) ) ; }
spill batch repeated ( batch , ( vector map join hash table result ) hash set result ) ; batch . size = 0 ; break ; case nomatch :
throw new null pointer exception ( string . format ( error message template , error message args ) ) ;
immutable map . builder < integer , group > groups builder = new immutable map . builder < > ( ) ; for ( map . entry < integer , list < node > > group : nodes . stream ( ) . collect ( collectors . grouping by ( node : : group ) ) . entry set ( ) ) groups builder . put ( group . get key ( ) , new group ( group . get key ( ) , group . get value ( ) ) ) ; this . groups = groups builder . build ( ) ;
string name = make fragment name ( container . get id ( ) , item id ) ; fragment fragment = m fragment manager . find fragment by tag ( name ) ; if ( fragment = null ) { if ( debug ) log . v ( tag , attaching item + item id + : f = + fragment ) ; m cur transaction . attach ( fragment ) ; } else { fragment = get item ( position ) ; if ( debug ) log . v ( tag , adding item + item id + : f = + fragment ) ; m cur transaction . add ( container . get id ( ) , fragment , make fragment name ( container . get id ( ) , item id ) ) ; } if ( fragment = m current primary item ) { fragment . set menu visibility ( false ) ; set user visibility hint ( fragment , false ) ; } return fragment ;
assert null ( never _ load was loaded in wrapped doc , visitor . lazy doc . get document ( ) . get field ( never _ load ) ) ; } finally {
int meta server num = cluster . get server with meta ( ) ;
if ( psi element instanceof psi file ) { final virtual file virtual file = psi util core . get virtual file ( psi element ) ; if ( virtual file = null ) { final file info file = new virtual file info ( psi element , virtual file ) ; icon = get icon for association ( file , associations . find association for file ( file ) ) ; } } else if ( psi element instanceof psi directory ) { icon = get directory icon ( ( psi directory ) psi element ) ; } return icon ;
return string . format ( % . 4g % s , value , abbreviate ( unit ) ) ;
utils . delete ( temp dir ) ; assert false ( files . exists ( temp dir . to path ( ) ) ) ; }
if ( inst . is missing ( structure . class index ( ) ) ) { out w . write ( ? ) ; } else { out w . write ( structure . attribute ( structure . class index ( ) ) . value ( ( int ) inst . value ( structure . class index ( ) ) ) ) ; } out w . write ( \ n ) ;
if ( original mediator = = null ) { return true ; } return original mediator . transformer applied ( ) ; }
canvas . draw circle ( ball2 . center x ( ) , ball2 . center y ( ) , circle2 . radius , paint ) ;
assert xpath evaluates to ( abstract app schema mock data . gsml _ uri , xsd : import @ namespace , doc ) ; assert xpath evaluates to ( abstract app schema mock data . gsml _ schema _ location _ url , xsd : import @ schema location , doc ) ;
assume true ( platform . detect ( ) = platform . windows ) ; test project build file parser factory build file parser factory = new test project build file parser factory ( cell . get root ( ) , known build rule types ) ;
root . set debug enabled ( true ) ;
this . children = object util . check not null ( children , children ) ;
reset state ( ) ;
shared preferences . editor prefs writer = cookie prefs . edit ( ) ; for ( string name : cookies . key set ( ) ) { prefs writer . remove ( cookie _ name _ prefix + name ) ; } prefs writer . remove ( cookie _ name _ store ) ; prefs writer . commit ( ) ; }
if ( e = null & & e . get status ( ) = status . deleted & & e . get status ( ) = status . gone ) { throw new hibernate exception ( a collection with cascade = \ all - delete - orphan \ was no longer referenced by the owning entity instance : + loaded persister . get role ( ) ) ; }
long local millis = date time zone . utc . get millis keep local ( date time zone . get default ( ) , millis utc ) ; part . bind ( column , new date ( local millis ) ) ; } else { throw new illegal argument exception ( unsupported type + type ) ; } } } batch . execute ( ) ; }
try { configurator . configure ( data collector , this ) ; } catch ( exception exc ) { throw wrapper . orb configurator error ( exc ) ; }
byte [ ] start key = { 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 } ; byte [ ] end key = { 9 , 9 , 9 , 9 , 9 , 9 , 9 , 9 , 9 , 9 } ;
note copied note = notebook repo . get ( 2 a94 m5 j1 z , anonymous ) ;
try { iter . has next ( ) ; fail ( no exception thrown ) ; } catch ( exception e ) { if ( ( e instanceof some checked exception ) ) { throw e ; } }
dest . write string ( visibility ) ;
throwable exception = null ;
final cluster state generator . params params = fixture . generator params ( ) . min storage nodes up ( 3 ) ; final annotated cluster state state = cluster state generator . generated state from ( params ) ;
pipeline . swap ( segments , null , false , false ) ;
s3 policy condition block block = one statement . get condition block ( ) ;
sb . append ( ' ' ) ; } else { sb . append ( ' . ' ) ; } step = c1 * step ; }
path snapshot sub = snapshot test helper . get snapshot path ( dir , s1 , sub . get name ( ) ) ; inode directory snapshot node4 sub = fsdir . get inode ( snapshot sub . to string ( ) ) . as directory ( ) ; assert true ( snapshot node4 sub . is with snapshot ( ) ) ;
add parameters for included sources ( page sources , query ) ; return ; }
if ( string utils . is blank ( operator group ) ) { type keywords . add all ( arrays . as list ( type . prefix _ group ) ) ; } return type keywords ;
result = services . execute operation ( remove stack op ) ;
lr . add field ( user agent , new log field ( log field . type _ string , lp . get next cb ( ) ) ) ;
set counter base for iv ( iv1 , long . max _ value ) ;
rtype . load new version ( 003 , retrieve rename ( the type , the type + 3 ) ) ; result = run constructor ( rtype . get clazz ( ) , ) ; res = run on instance ( rtype . get clazz ( ) , result . return value , get integer ) ; assert equals ( 3 , ( ( integer ) res . return value ) . int value ( ) ) ; res = run on instance ( rtype . get clazz ( ) , result . return value , get string ) ; assert null ( res . return value ) ; }
ret = new string builder ( ) ;
hour = parse int ( date , offset + = 1 , offset + = 2 ) ; if ( check offset ( date , offset , ' : ' ) ) { offset + = 1 ; } minutes = parse int ( date , offset , offset + = 2 ) ; if ( check offset ( date , offset , ' : ' ) ) { offset + = 1 ; }
driver . navigate ( ) . to ( login form url ) ; assert . assert equals ( push the button , driver . get title ( ) ) ;
language mode = language mode . ecmascript _ 2015 ; assert print ( function f ( ) { if ( e1 ) { function goo ( ) { return true } } else foo ( ) } , function f ( ) { if ( e1 ) { function goo ( ) { return true } } else foo ( ) } ) ; assert print ( function f ( ) { if ( e1 ) function goo ( ) { return true } else foo ( ) } , function f ( ) { if ( e1 ) { function goo ( ) { return true } } else foo ( ) } ) ; assert print ( if ( e1 ) { function goo ( ) { return true } } , if ( e1 ) { function goo ( ) { return true } } ) ; assert print ( if ( e1 ) function goo ( ) { return true } , if ( e1 ) { function goo ( ) { return true } } ) ; assert print ( if ( e1 ) a : function goo ( ) { return true } , if ( e1 ) { a : function goo ( ) { return true } } ) ;
if ( bucket name . ends with ( - ) ) throw new invalid bucket name ( dns requiremens , bucket name : + bucket name + should not end with a dash ) ;
populate data ( new hash map < long , dlsn > ( ) , conf local , name , 4 , 10 , false ) ; distributed log manager dlm = create new dlm ( conf local , name ) ;
list < hstore file > candidates = sf create ( 0 ) ; for ( hstore file file : candidates ) { if ( file instanceof mock hstore file ) { mock hstore file mock file = ( mock hstore file ) file ; mock file . set time range tracker ( time range tracker . create ( time range tracker . type . sync , - 1 , - 1 ) ) ; mock file . set entries ( 0 ) ; } }
} return i ; } ; }
query ( build request ( lowerfilt : ( \ quick red fox \ ) , false , request handler name , random ( ) . next boolean ( ) , extended , true , count , 10 , collate , true , max collation tries , 10 , max collations , 1 , collate extended , false , alt term count , 5 , max results , 10 ) ) ;
cgroup base dir = config extension . get cgroup base dir ( conf ) ; root dir = config extension . get cgroup root dir ( conf ) ; if ( string utils . is blank ( root dir ) | | string utils . is blank ( cgroup base dir ) ) { string builder sb = new string builder ( ) ; sb . append ( check configuration file . the setting of ) ; sb . append ( config extension . cgroup _ base _ dir ) . append ( and ) ; sb . append ( config extension . cgroup _ root _ dir ) ; sb . append ( are invalid . ) ; throw new runtime exception ( sb . to string ( ) ) ; } string real root dir = cgroup base dir + + root dir ;
cut < c > that = ( cut < c > ) obj ; try { int compare result = compare to ( that ) ; return compare result = = 0 ; } catch ( class cast exception ignored ) { }
numeric doc values numeric = field cache . default . get numerics ( ar , numeric , field cache . legacy _ int _ parser ) ; assert equals ( 0 , numeric . next doc ( ) ) ; assert equals ( 42 , numeric . long value ( ) ) ;
settings index settings = settings . builder ( ) < 1 > . put ( setting _ number _ of _ shards , 1 ) . put ( setting _ number _ of _ replicas , 0 ) . build ( ) ; string payload = xcontent factory . json builder ( ) < 2 > . start object ( ) . start object ( settings ) < 3 > . value ( index settings ) . end object ( ) . start object ( mappings ) < 4 > . start object ( doc ) . start object ( properties ) . start object ( time ) . field ( type , date ) . end object ( ) . end object ( ) . end object ( ) . end object ( ) . end object ( ) . string ( ) ;
tree suppressible node = suppressible node ( state . get path ( ) ) ; if ( suppressible node = = null ) { return ; } suppress warnings existing annotation = get annotation ( suppressible node , suppress warnings . class ) ;
reset callback status properties ( ) ; return callback ;
if ( from keyboard & & deep shortcut manager . supports shortcuts ( item ) ) { info . add action ( m actions . get ( deep _ shortcuts ) ) ; } if ( delete drop target . supports accessible drop ( item ) ) { info . add action ( m actions . get ( remove ) ) ; }
create table and flush ( htd ) ;
fill right = left view offset < right border ;
volt queue sql ( select sql , onepointone ) ; volt table results1 [ ] = volt execute sql ( true ) ;
args = new args ; } else {
final payment payment3 = payment api . create void ( account , payment . get id ( ) , transaction external key3 , immutable list . < plugin property > of ( ) , call context ) ; assert equals ( payment3 . get external key ( ) , payment external key ) ; assert equals ( payment3 . get payment method id ( ) , account . get payment method id ( ) ) ; assert equals ( payment3 . get account id ( ) , account . get id ( ) ) ; assert equals ( payment3 . get auth amount ( ) . compare to ( auth amount ) , 0 ) ; assert equals ( payment3 . get captured amount ( ) . compare to ( big decimal . zero ) , 0 ) ; assert equals ( payment3 . get purchased amount ( ) . compare to ( big decimal . zero ) , 0 ) ; assert equals ( payment3 . get refunded amount ( ) . compare to ( big decimal . zero ) , 0 ) ; assert equals ( payment3 . get currency ( ) , currency . aed ) ; assert false ( payment3 . is auth voided ( ) ) ; assert equals ( payment3 . get transactions ( ) . size ( ) , 3 ) ; assert equals ( payment3 . get transactions ( ) . get ( 2 ) . get external key ( ) , transaction external key3 ) ; assert equals ( payment3 . get transactions ( ) . get ( 2 ) . get payment id ( ) , payment . get id ( ) ) ;
assert not same ( e1 , e2 ) ; assert not same ( e1 . get configuration ( ) , e2 . get configuration ( ) ) ;
} else if ( number of users supported = = 0 ) { logger . error ( node { } : door lock does not support any user codes , no codes set , get node ( ) . get node id ( ) ) ; } else { sync user codes ( ) ; } }
int count ;
throw new null pointer exception ( list = = null ) ;
m select day = res . get string ( r . string . select _ day ) ; m select year = res . get string ( r . string . select _ year ) ;
return subscriber id . substring ( 0 , math . min ( 6 , subscriber id . length ( ) ) ) + . . . ; } else {
set focusable in touch mode ( true ) ;
metadata . create table ( session , table metadata builder ( new schema table name ( other , orders ) ) . column ( orderkey , bigint ) . build ( ) , false ) ;
client stats stats = periodic stats context . fetch and reset baseline ( ) . get stats ( ) ; system . out . printf ( insert statistics : \ n + throughput % d s , aborts failures % d % d , avg 95 % % latency % . 2f % dms \ n , stats . get txn throughput ( ) , stats . get invocation aborts ( ) , stats . get invocation errors ( ) , stats . get average latency ( ) , stats . k percentile latency ( 0 . 95 ) ) ;
if ( saved state = null ) { restore state ( saved state ) ; saved state = null ; save term buffer ( ) ; return true ; } else if ( input . increment token ( ) ) { return false ; }
return ( supertype = = type . serializable ) | | ( supertype = = type . cloneable ) ;
final int last bottom = get lowest child bottom ( ) ;
m _ world . m _ flags | = world . new _ fixture ; return fixture ; }
int diff ms = ( int ) ( current time ms - last update time ms ) ; if ( diff ms < = 0 ) { return available tokens ; } int threads = threads in use . get ( ) ;
usernames fetcher . number _ of _ users = - 1 ;
prop files manager . add properties ( get application config file location ( module name ) , get appli cation config file name ( profile ) , properties , true , force ) ; }
list < phone > phones = entity manager . create query ( select ph + from phone ph + inner join ph . person pr + where pr . address = : address + and pr . created on > : timestamp , phone . class ) . set parameter ( address , address ) . set parameter ( timestamp , timestamp ) . get result list ( ) ;
final split . set divider location ( 0 . 5 ) ; } else {
int idx mid = value . index of ( , , base ) ; if ( idx mid = = - 1 | | idx mid > = idx closed - 1 | | idx mid < = idx open + 1 ) { throw new wcs20 exception ( invalid scale extent syntax , expecting a comma separate list of axis name ( min , max ) * , wcs20 exception . wcs20 exception code . invalid encoding syntax , scale extent ) ; } int idx next mid = value . index of ( , , idx mid + 1 ) ;
graphics2 d . draw image ( cur image , 1 , i offset , null ) ;
try { unpack the real , inner jar . path jar = temp . get path ( ) . resolve ( main . jar ) ; fat jar . unpack jar to ( class loader , jar ) ; unpack all the native libraries , since the system loader will need to find these on disk . path native libs = temp . get path ( ) . resolve ( nativelibs ) ; files . create directory ( native libs ) ; fat jar . unpack native libraries into ( class loader , temp . get path ( ) ) ; update the appropriate environment variable with the location of our native libraries and start the real main class in a new process so that it picks it up . process builder builder = new process builder ( ) ; builder . command ( get command ( jar , args ) ) ; update environment ( builder . environment ( ) , temp . get path ( ) ) ; builder . inherit io ( ) ; wait for the inner process to finish , and propagate it ' s exit code , before cleaning up the native libraries . system . exit ( builder . start ( ) . wait for ( ) ) ; } finally { temp . close ( ) ; }
connection connection = connection factory . create connection ( username , password ) ;
throw uncheck ( e ) ;
if ( is simple down axis | | ( axis = = - 1 ) ) walker = walker . get next walker ( ) ; else { boolean is last walker = ( null = = walker . get next walker ( ) ) ; if ( is last walker ) { if ( walker . is doc ordered ( ) & & ( axis = = axis . descendant | | axis = = axis . descendantorself | | axis = = axis . descendantsfromroot | | axis = = axis . descendantsorselffromroot ) | | ( axis = = axis . attribute ) ) return true ; } return false ; }
navigable map < string , region state > regions in transition = maps . new tree map ( ) ;
key value kv0 _ 1 = new key value ( row , fami , qual1 , ts , type . put ) ;
if ( input stream . available ( ) > 0 ) { input stream . skip bytes ( input stream . available ( ) ) ; }
_ runner . do send ( msg ) ; get context ( ) . job queue ( ) . add job ( new check lease request status ( ) ) ; } catch ( i2 cpmessage exception ime ) {
expr . evaluate ( b ) ; decimal column vector r = ( decimal column vector ) b . cols [ 2 ] ; assert true ( r . vector [ 0 ] . get hive decimal ( ) . equals ( hive decimal . create ( 2 . 20 ) ) ) ; assert true ( r . vector [ 1 ] . get hive decimal ( ) . equals ( hive decimal . create ( - 2 . 30 ) ) ) ; assert true ( r . vector [ 2 ] . get hive decimal ( ) . equals ( hive decimal . create ( 1 ) ) ) ;
doc new doc = null ;
assert that ( response . get content as string ( ) , is empty string ( ) ) ; }
b1 = base64 alphabet [ base64 data [ data index + + ] ] ; b2 = base64 alphabet [ base64 data [ data index + + ] ] ; if ( b1 = = - 1 | | b2 = = - 1 ) { if found no data just return null throw new base64 decoding exception ( decoding . general ) ; } byte d3 , d4 ;
if ( x + get width ( ) > screen width ) x = screen width - get width ( ) ;
assert that ( expansion failure ( ( location label1 ) , ( srcs ) , ) ) . has message that ( ) . is equal to ( ( location ) not defined ) ;
x non empty directory foo . is writable ( ) ; i . e . stat
controller . add test element ( new test sampler ( three ) ) ;
for ( int i = 0 ; i < expected threads created ; i + + ) { assert in log ( content , azure blob delete thread - + thread . current thread ( ) . get name ( ) + - + i ) ; }
assert normalize ( ' bar ' like ' _ ar ' , is literal ( true ) ) ;
for ( pool pool : bpmn model . get pools ( ) ) { graphic info graphic info = bpmn model . get graphic info ( pool . get id ( ) ) ; process diagram canvas . draw pool or lane ( pool . get id ( ) , pool . get name ( ) , graphic info ) ; }
if ( entity reference . get entity persister ( ) . equals ( entity definition . get entity persister ( ) ) ) { throw new walking exception ( mismatched fetch source from stack on pop ) ; }
find iterable = collection . find ( elem match ( dim _ cm , document . parse ( { gt : 22 , lt : 30 } ) ) ) ;
string name = local name ;
for ( map . entry < string , method > getter to method : property names to getters . entry set ( ) ) { descriptors . add ( new property descriptor ( getter to method . get key ( ) , getter to method . get value ( ) , null ) ) ; } return descriptors ;
job store dir = server . get storage location ( job id , blob key ) . get parent file ( ) ;
jsonarray a2 = new jsonarray ( my enum . values ( ) ) ; assert equals ( [ null , null , null ] , a2 . to string ( ) ) ; }
if ( push views to temp location ( intersecting views , occupied , direction , ignore view , solution ) ) { return true ; }
factory = m _ factories . get ( namespace ) ; if ( factory = null ) { return factory ; }
context . start ( ) ; get mock endpoint ( mock : result ) . expected message count ( 1 ) ; template . send body ( seda : foo , hello world ) ; assert mock endpoints satisfied ( ) ; }
xml tree . insert attribute ( new attribute , this ) ; return this ;
boolean tmp selection item = this . encryption protocol status [ row index ] ;
jmsservice modify event event = new jmsservice modify event ( service , jmsevent type . added ) ; handle service event ( event ) ; }
f step into command = new dsf step into command ( session , null ) ;
assert true ( am client . remote requests table . is empty ( ) ) ; am client . unregister application master ( final application status . succeeded , null , null ) ;
get or create interceptor for sub cluster ( bad sc , test ) ; bad sub cluster . add ( bad sc ) ;
try { string . class . get declared methods ( ) ; } catch ( security exception e ) { throw new unavailable ( accessibility unavailable : + e ) ; } accessibility = true ; }
tokenizer . get white spaces ( ) ; if ( tokenizer . is ended ( ) ) { log . info ( pattern was just empty or whitespace ) ; return null ; } string token = tokenizer . get token ( ) ;
mock batches . when exchange received ( 1 , new processor ( ) { @ override public void process ( exchange exchange ) throws exception { throw new runtime exception ( boom ) ; } } ) ;
parameters = parameters . sub list ( 1 , parameters . size ( ) ) ; entity type entity type = null ;
operation set file transfer file transfer op set = protocol provider . get operation set ( operation set file transfer . class ) ; if ( file transfer op set = null ) { file transfer op set . add file transfer listener ( get contact list panel ( ) ) ; }
class . for name ( android . app . application , * initialize = * false , managed channel provider . class . get class loader ( ) ) ; return true ;
slice _ from ( log ) ;
map < integer , string > m = new tree map < > ( ) ; hash h = murmur hash . get instance ( ) ; int per client rows = ( opts . total rows opts . num client threads ) ; try { for ( int j = 0 ; j < opts . num client threads ; j + + ) { test options next = new test options ( opts ) ; next . start row = j * per client rows ; next . per client run rows = per client rows ; string s = mapper . write value as string ( next ) ; log . info ( client = + j + , input = + s ) ; byte [ ] b = bytes . to bytes ( s ) ; int hash = h . hash ( new byte array hash key ( b , 0 , b . length ) , - 1 ) ; m . put ( hash , s ) ; } for ( map . entry < integer , string > e : m . entry set ( ) ) { out . println ( e . get value ( ) ) ; } } finally { out . close ( ) ; } return input dir ;
assert equals ( supplier . get buffered pages ( ) , 0 ) ;
int m = scc . count ( ) ; std out . println ( m + components ) ;
try { log . info ( outlink db : running ) ; job client . run job ( outlink job ) ; log . info ( outlink db : installing + outlink db ) ; fsutils . replace ( fs , old outlink db , outlink db , true ) ; fsutils . replace ( fs , outlink db , temp outlink db , true ) ; if ( preserve backup & & fs . exists ( old outlink db ) ) fs . delete ( old outlink db , true ) ; log . info ( outlink db : finished ) ; } catch ( ioexception e ) { remove lock file and and temporary directory if an error occurs lock util . remove lock file ( fs , lock ) ; if ( fs . exists ( temp outlink db ) ) { fs . delete ( temp outlink db , true ) ; } log . error ( string utils . stringify exception ( e ) ) ; throw e ; }
row pending mutations = builder . copy ( ) . build ( ) ; if ( pending mutations . is empty ( ) ) return prefetched row ;
output . reserve ( 1 ) ; }
long expected rows = num rows ;
rule resolver . get all rules ( ( ( has runtime deps ) rule ) . get runtime deps ( new source path rule finder ( rule resolver ) ) : : iterator ) . for each ( this : : acquire reference ) ;
assert equals ( 0 , session factory ( ) . get statistics ( ) . get prepare statement count ( ) ) ; for ( employee group group : groups ) { assert false ( hibernate . is initialized ( group ) ) ; } assert equals ( 0 , session factory ( ) . get statistics ( ) . get prepare statement count ( ) ) ;
if ( index > ( _ size > > 1 ) ) { int position = _ size - 1 ; t node = _ tail ; while ( position > index ) { node = node . get previous ( ) ; position - - ; } return node ; } else { int position = 0 ; t node = _ head ; while ( position < index ) { node = node . get next ( ) ; position + + ; } return node ; }
if ( _ state = = state . header ) parsed header ( ) ; else parsed trailer ( ) ;
suite . add test ( new test operation set typing notifications ( test typing notifications event delivery ) ) ; return suite ;
public void test update md5 index ( ) { final string schema str = { \ type \ : \ record \ , \ name \ : \ a \ , \ fields \ : [ + { \ name \ : \ j \ , \ type \ : \ string \ , \ j1 \ : \ v1 \ , \ index type \ : \ v2 \ , \ d \ : + \ v3 \ , \ j3 \ : \ 10000 \ } ] } ;
boolean borrow = ( difference > > 32 = 0 ) ;
verify resize ( jpeg , 512 , 250 , scalte type , 512 , 250 ) ; exactly half verify resize ( jpeg , 511 , 249 , scalte type , 509 , 249 ) ; just under half verify resize ( jpeg , 1080 , 500 , scalte type , 1024 , 500 ) ; larger verify resize ( jpeg , 500 , 500 , scalte type , 500 , 244 ) ; keep same ratio
processed nodes . clear ( ) ;
modifiable solr params p = new modifiable solr params ( ) ; p . add ( collection , rules coll ) ; p . add ( action , modifycollection ) ; p . add ( rule , cores : < 5 ) ; p . add ( rule , node : * , replica : 1 ) ; p . add ( rule , freedisk : > + min gb2 ) ; p . add ( auto add replicas , true ) ; cluster . get solr client ( ) . request ( new generic solr request ( post , collections _ handler _ path , p ) ) ; doc collection rules collection = get collection state ( rules coll ) ;
this . task executor = scheduler factory bean . get config time task executor ( ) ;
for ( type k k : key set ( ) ) { final type v v = get ( k ) ; do an official ' get ' t . put ( k , v ) ; } return t ; } catch ( clone not supported exception e ) {
for ( class < ? > api : test _ clazzes ) { for ( visibility vis : visibility . values ( ) ) { injector . get instance ( key . get ( api , named ( vis . name ( ) ) ) ) ; * if [ aop ] * aop injector . get instance ( key . get ( api , named ( vis . name ( ) ) ) ) ; * end [ aop ] * } }
assert not connected ( test _ sslengine _ set use client mode ( false , false ) ) ;
weight = mgmt . get property key ( weight ) ; uid = mgmt . get property key ( uid ) ; someid = mgmt . get property key ( someid ) ; name = mgmt . get property key ( name ) ; value = mgmt . get property key ( value ) ; friend = mgmt . get edge label ( friend ) ; link = mgmt . get edge label ( link ) ; connect = mgmt . get edge label ( connect ) ; parent = mgmt . get edge label ( parent ) ; child = mgmt . get edge label ( child ) ; spouse = mgmt . get edge label ( spouse ) ; person = mgmt . get vertex label ( person ) ; tag = mgmt . get vertex label ( tag ) ;
builder . append ( str . substring ( start offset , offset ) ) ;
assert equals ( null , dfsutil . get namenode service addr ( conf , null , ns1 - nn1 ) ) ;
object key ; if ( object instanceof activation ) { key = class object type . match _ object type . get class type ( ) ; } else if ( object instanceof fact ) { key = ( ( fact ) object ) . get fact template ( ) . get name ( ) ; } else { key = object . get class ( ) ; } object type conf object type conf = this . type conf map . get ( key ) ;
iwc . set merge scheduler ( new concurrent merge scheduler ( ) ) ; final my index writer w = new my index writer ( d , iwc ) ;
int dir = 0 ;
datagram packet response = new datagram packet ( buffer , buffer . length ) ;
assert labels visited ( immutable set . of ( pkg : x , pkg : z ) , immutable set . of ( pkg : x ) , expect _ error , keep _ going ) ; }
verify ( fs , never ( ) ) . rename ( eq ( status . get path ( ) ) , is a ( path . class ) ) ;
if ( bus = null ) { configurer configurer = bus . get extension ( configurer . class ) ; if ( null = configurer ) { configurer . configure bean ( this ) ; } }
timer . increment time ( 15 ) ; assert equals ( null , f1 . get ( 1000 , time unit . milliseconds ) ) ;
for ( string registered path : this . cache mappings . key set ( ) ) { if ( this . path matcher . match ( registered path , url path ) ) { return this . cache mappings . get ( registered path ) ; } } return null ; }
if ( m _ elem context . m _ start tag open ) { close start tag ( ) ; m _ elem context . m _ start tag open = false ; }
if ( m pending check for tap = = null ) { m pending check for tap = new check for tap ( ) ; } post delayed ( m pending check for tap , view configuration . get tap timeout ( ) ) ;
if ( snapshot enabled ) { log . info ( snapshot feature is not enabled , missing log and hfile cleaners . ) ; path snapshot dir = snapshot description utils . get snapshots dir ( mfs . get root dir ( ) ) ; if ( fs . exists ( snapshot dir ) ) { file status [ ] snapshots = fsutils . list status ( fs , snapshot dir , new snapshot description utils . completed snaphot directories filter ( fs ) ) ; if ( snapshots = null ) { log . error ( snapshots are present , but cleaners are not enabled . ) ; check snapshot support ( ) ; } } }
rule stop state stop = atn . rule to stop state [ rule index ] ;
final bulk scorer and doc candidate = tail . pop ( ) ;
scratch . file ( test rule . bzl , silly _ rule = rule ( , implementation = int , , attrs = { , \ srcs \ : attr . label _ list ( allow _ files = true ) , , } , ) ) ;
aspect path builder . add ( sky key ) ;
double range = max value - min value ; vector values = new vector ( num values ) ; random random = new random ( seed ) ; if ( attr type = = attribute . nominal ) { for ( int i = 0 ; i < num values ; i + + ) { double v = new double ( ( math . abs ( random . next int ( ) ) % ( int ) range ) + ( int ) min value ) ; values . add ( v ) ; } } if ( attr type = = attribute . numeric ) { for ( int i = 0 ; i < num values ; i + + ) { double v = new double ( random . next double ( ) * range + min value ) ; values . add ( v ) ; } }
map < build rule , rule key > result run1 rule keys = get rule keys from build rules ( result run1 . get action graph ( ) . get nodes ( ) , result run1 . get resolver ( ) ) ; map < build rule , rule key > result run2 rule keys = get rule keys from build rules ( result run2 . get action graph ( ) . get nodes ( ) , result run2 . get resolver ( ) ) ; map < build rule , rule key > result run3 rule keys = get rule keys from build rules ( result run3 . get action graph ( ) . get nodes ( ) , result run3 . get resolver ( ) ) ;
this . rtmp output conn params . add ( new string [ ] { b , value? 1 : 0 } ) ; }
case ' x ' : n = 0 ; for ( i = 0 ; ( i < n digits ) & & ( src < end ) ; i + + ) { c = g data . regexp . source [ src + + ] ; int digit = to asciihex digit ( c ) ; if ( digit < 0 ) {
int free list size = free size ( ) * this . poolable size ; if ( this . non pooled available memory + free list size > = size ) {
assert equals ( 2 , store size ( target cluster ) ) ;
assert equals ( hsql3 row rs . absolute ( 0 ) , volt3 row rs . absolute ( 0 ) ) ;
net broadcast tuple tuple = broadcast model . get value ( ) ;
add manifest processor parameter ( activity intent fragment test . class , android manifest min froyo . xml ) ; compile result result = compile files ( to path ( activity intent fragment test . class , fragment . java ) , activity in manifest . class ) ; file generated file = to generated file ( activity in manifest . class ) ; assert compilation successful ( result ) ;
assert equals ( 1 , era . get ( millis ) ) ;
web element source menu entry = menu navigator . get menu item ( driver _ , code , source ) ; source menu entry . click ( ) ;
send request . fee = tx fee ;
dfstest util . wait for replication ( fs , file , block num , 15 * 1000 ) ;
throw new runtime exception ( ef ) ; } finally { if ( original periodic ) nd4j . get memory manager ( ) . toggle periodic gc ( true ) ; nd4j . get memory manager ( ) . set occasional gc frequency ( original freq ) ; } } catch ( exception ex ) {
result [ wp + + ] = src1begin ; result [ wp + + ] = src1end ; src1 + = 2 ; } else if ( src2begin < = src1begin ) {
assert equals ( test file block size , nn rpc . get preferred block size ( test file inode path . to string ( ) ) ) ;
legacy configuration importer importer = new legacy configuration importer ( ) ; importer . set configuration ( geo server ) ; importer . imprt ( resource loader . get base directory ( ) ) ; }
data data = ( data ) object ;
set visibility ( view . visible ) ; }
int dlim = dx + downsampled iw ;
vertices [ m vertex index ] = ( float ) vertex0 . x ;
try { privileged operation check setup op = new privileged operation ( privileged operation . operation type . check _ setup ) ; privileged operation executor privileged operation executor = get privileged operation executor ( ) ; privileged operation executor . execute privileged operation ( check setup op , false ) ; } catch ( privileged operation exception e ) { int exit code = e . get exit code ( ) ; log . warn ( exit code from container executor initialization is : + exit code , e ) ; throw new ioexception ( linux container executor not configured properly + ( error = + exit code + ) , e ) ; } try { resource handler chain = resource handler module . get configured resource handler chain ( conf , nm context ) ; if ( log . is debug enabled ( ) ) { log . debug ( resource handler chain enabled = + ( resource handler chain = null ) ) ; } if ( resource handler chain = null ) { log . debug ( bootstrapping resource handler chain ) ; resource handler chain . bootstrap ( conf ) ; } } catch ( resource handler exception e ) { log . error ( failed to bootstrap configured resource subsystems , e ) ; throw new ioexception ( failed to bootstrap configured resource subsystems ) ; }
distributed executor service des = create des ( cache1 ) ;
is valid = is valid & & validator . is valid ( text , is empty ) ;
execute ( insert into % s ( a , b , c ) values ( ? , ? , ? ) using timestamp ? , 900 , 0 , 2 , 100 l ) ;
app _ 0 _ requests _ 0 . clear ( ) ;
huffman stage decoder . current group = curr group ; huffman stage decoder . current length = curr length ; huffman stage decoder . current alpha = curr alpha ; huffman stage decoder . modify length = modify length ; return ;
default url mappings holder . initialize ( ) ;
whitebox . invoke method ( worker task , send records ) ; assert equals ( false , whitebox . get internal state ( worker task , last send failed ) ) ; assert null ( whitebox . get internal state ( worker task , to send ) ) ; power mock . verify all ( ) ;
if ( field . get type ( ) instanceof text field ) { text field ft = ( text field ) field . get type ( ) ; assert equals ( f + field ' s auto phrase is wrong for ver = + ver , ( v < 1 . 4 f ) , ft . get auto generate phrase queries ( ) ) ; }
if ( _ input ptr > = _ input end ) { load more guaranteed ( ) ; }
fallback = density . ordering . max ( available ) ; } else {
} } else if ( num = = 2 & & crit count = = 2 ) {
if ( copy truncate ) { stored block . set generation stamp ( newgenerationstamp ) ; stored block . set num bytes ( newlength ) ; }
jgen . write end object ( ) ;
indicator ( feats , type _ signature , input . subject type + , + input . object type ) ;
spark . messages ( ) . post ( msg ) ; logger . debug ( successfully sent message ' { } ' , msg . get markdown ( ) ) ; return true ;
add element ( planar configuration , data , child _ policy _ empty ) ; values = new array list ( ) ;
if ( matcher . matches ( ) ) { timestamp = matcher . group ( 1 ) + z ; } date date = date utils . from isodate string ( timestamp ) ; final string trade id = string . value of ( trade . get tid ( ) ) ; return new trade ( null , trade . get amount ( ) , currency pair , trade . get price ( ) , date , trade id ) ;
throw new illegal state exception ( cannot compare without having any requested media types , ex ) ; }
component synthesized component = synthesize annotation ( attributes , component . class , web controller . class ) ; assert not null ( synthesized component ) ;
set < string > groupset = new hash set < string > ( groups ) ;
( ( client impl ) client ) . call procedure with client timeout ( batch timeout override type . no _ timeout , arbitrary duration proc , 0 , time unit . seconds , 2000 ) ;
if ( cache ) { principal principal = request . get user principal ( ) ; if ( principal = = null ) { session session = request . get session internal ( false ) ; if ( session = null ) { principal = session . get principal ( ) ; if ( principal = null ) { if ( log . is debug enabled ( ) ) { log . debug ( we have cached auth type + session . get auth type ( ) + for principal + principal ) ; } request . set auth type ( session . get auth type ( ) ) ; request . set user principal ( principal ) ; } } } } boolean auth required = is continuation required ( request ) ;
twitter . remove user ( user1 ) ; user rdbms user1 after removal = twitter . find user by id ( user id1 ) ;
for ( value first value : all values ) { for ( value second value : all values ) { verify add with correct populator ( lucene populator , native populator , first value , second value ) ; } }
assert equals ( value . _ x , 2 ) ;
subgroups . put ( obj , obj ) ; }
user . set attribute ( attribute , attribute values in context ) ;
mark used parameters ( program method , - 1 l ) ;
ioutils . cleanup ( log , file sys ) ; cluster . restart name node ( ) ; file sys = cluster . get file system ( ) ; assert false ( file sys . get acl status ( dir1 ) . get entries ( ) . is empty ( ) ) ; assert false ( file sys . get acl status ( dir2 ) . get entries ( ) . is empty ( ) ) ; assert false ( file sys . get acl status ( file1 ) . get entries ( ) . is empty ( ) ) ; assert true ( file sys . get acl status ( dir3 ) . get entries ( ) . is empty ( ) ) ; assert true ( file sys . get acl status ( file2 ) . get entries ( ) . is empty ( ) ) ; } finally {
do { key . set value ( random . next long ( ) ) ; } while ( contains ( expected keys , 0 , expected key count , key . long value ( ) ) ) ; node . insert key at ( cursor , key , position , expected key count ) ; insert ( expected keys , expected key count , key . long value ( ) , position ) ; value . set value ( random . next long ( ) ) ;
file package manager props file = new file ( system . get property ( user . home ) + file . separator + package manager . props ) ;
assert . assert not null ( login page . get error ( ) ) ; assert . assert that ( login page . get error ( ) , contains string ( x509 certificate authentication ' s failed . ) ) ;
temp + = ( ( prior _ nom [ i ] m _ num instances ) * ( stdvs _ nom [ i ] m _ num instances ) ) ;
fs input desc . get table info ( ) . get properties ( ) . remove ( org . apache . hadoop . hive . metastore . api . hive _ metastore constants . meta _ table _ partition _ columns ) ; }
if ( i = = 0 ) return 32 ; int n = 1 ; if ( i > > > 16 = = 0 ) { n + = 16 ; i < < = 16 ; } if ( i > > > 24 = = 0 ) { n + = 8 ; i < < = 8 ; } if ( i > > > 28 = = 0 ) { n + = 4 ; i < < = 4 ; } if ( i > > > 30 = = 0 ) { n + = 2 ; i < < = 2 ; } n - = i > > > 31 ; return n ; }
datanode command cmd = bp namenode . block report ( bp registration , bpos . get block pool id ( ) , reports , new block report context ( 1 , 0 , report id , full br lease id , true ) ) ;
final resources res = get resources ( ) ; final int default page color = res . get color ( r . color . default _ circle _ indicator _ page _ color ) ; final int default fill color = res . get color ( r . color . default _ circle _ indicator _ fill _ color ) ; final int default orientation = res . get integer ( r . integer . default _ circle _ indicator _ orientation ) ; final int default stroke color = res . get color ( r . color . default _ circle _ indicator _ stroke _ color ) ; final float default stroke width = res . get dimension ( r . dimen . default _ circle _ indicator _ stroke _ width ) ; final float default radius = res . get dimension ( r . dimen . default _ circle _ indicator _ radius ) ; final boolean default centered = res . get boolean ( r . bool . default _ circle _ indicator _ centered ) ; final boolean default snap = res . get boolean ( r . bool . default _ circle _ indicator _ snap ) ;
row . add ( null ) ;
meta data . builder builder = meta data . builder ( ) ; for ( index id index : snapshot . indices ( ) ) { builder . put ( meta data . index ( index . get name ( ) ) , false ) ; } meta data = builder . build ( ) ; } repository . initialize snapshot ( snapshot . snapshot ( ) . get snapshot id ( ) , snapshot . indices ( ) , meta data ) ; snapshot created = true ; if ( snapshot . indices ( ) . is empty ( ) ) {
assert equals ( num files , desc . num blocks ( ) ) ;
try { engine = ( engine ) event . get lifecycle ( ) ; } catch ( class cast exception e ) { log . error ( sm . get string ( engine config . cce , event . get lifecycle ( ) ) , e ) ; return ; }
assert locked ( cache ( 2 , optimistic _ tx _ cache _ name ) , key info . get key2 ( ) ) ;
robot . assert view state rendered ( loading first page , error first page ) ;
cache topology cache topology = advanced cache ( 1 ) . get component registry ( ) . get state transfer manager ( ) . get cache topology ( ) ;
m result text view = ( text view ) get activity ( ) . find view by id ( r . id . result text view ) ;
this . requestor = requestor ;
texture pink rock = asset manager . load texture ( textures terrain rock rock . png ) ;
rm1 . wait for state ( app1 . get application id ( ) , rmapp state . accepted ) ; assert . assert equals ( 2 , app1 . get app attempts ( ) . size ( ) ) ; rmapp attempt attempt2 = app1 . get current app attempt ( ) ;
tests . put ( map , new uni vocity csv data format ( ) . set as map ( true ) . set header extraction enabled ( true ) ) ;
if ( current hosts . equals ( all hosts ) ) { all hosts = current hosts ; notify the monitors . for ( host change monitor < service instance > monitor : monitors ) { monitor . on change ( all hosts ) ; } }
assert good ( ein menschenfreund . ) ;
assert equals ( 0 , bytes . compare to ( v2 , r . get value ( family , qualifiers [ 2 ] ) ) ) ; assert equals ( r . get column latest cell ( family , qualifiers [ 0 ] ) . get timestamp ( ) , r . get column latest cell ( family , qualifiers [ 2 ] ) . get timestamp ( ) ) ; }
string package name = get project operations ( ) . get top level package ( module . get module name ( ) ) . get fully qualified package name ( ) . concat ( . web . reports ) ;
expect current cdcstate ( cdcstate . permitted ) ;
list < app info > all apps = new array list < > ( m apps . size ( ) ) ; for ( map . entry < string , array list < app info > > entry : section map . entry set ( ) ) { all apps . add all ( entry . get value ( ) ) ; } m apps . clear ( ) ;
char first char = str . char at ( 0 ) ; if ( first char = = ' + ' | | first char = = ' - ' | | ( first char > = ' 0 ' & & first char < = ' 9 ' ) ) { try { return spatial utils . parse point ( str , ctx ) ; } catch ( exception e ) { ignore } } try { return shape reader . read ( str ) ; } catch ( exception e ) { string msg = unable to parse shape given formats + \ lat , lon \ , \ x y \ or as + shape reader . get format name ( ) + because + e ; if ( msg . contains ( str ) ) { msg + = input : + str ; } throw new solr exception ( solr exception . error code . bad _ request , msg , e ) ; }
intent intent = new intent ( from , browser activity . class ) ;
assert clean close ( state , true ) ;
assert equals ( check all en , all en , resources . get message ( locale . english , key . all ) ) ;
if ( ( f preloaded element to selection . get ( element ) ) . boolean value ( ) ) continue ;
validate and set writable permissions ( false , ret ) ; }
| | string at ( ( m _ current - 3 ) , 7 , clachan , ) ) { metaph add ( h ) ; advance counter ( 3 , 2 ) ; return true ; } return false ; }
return this ; }
builder . literal ( search argument . truth value . yes _ no _ null ) ; return ;
super . on measure ( width measure spec , height measure spec ) ;
compress = true ; if ( false . equals ignore case ( environment . get property ( local . tmp . compression ) ) ) compress = false ; try { fw = new file writer ( filename , true ) ; writer = new buffered writer ( fw ) ; } catch ( ioexception e ) { e . print stack trace ( ) ; }
else if ( obj instanceof asn1 sequence ) { return new encrypted private key info ( ( asn1 sequence ) obj ) ; }
if ( old package definition . resolution optional & & package definition . resolution optional ) { return old package definition ; } if ( old package definition . version = = null & & package definition . version = null ) { return package definition ; }
byte [ ] marker = new byte [ 2 ] ;
cluster c = new cluster ( cluster map , null , this . region finder , this . rack manager ) ; if ( this . needs balance ( c ) & & this . overall needs balance ( ) ) return null ; cluster load state cs = new cluster load state ( cluster map ) ;
final list < invoice > invoices = kill bill client . get invoices for account ( account json . get account id ( ) , request options ) ;
assert equals ( null , context . get parameter as ( key , long . class ) ) ; assert equals ( new integer ( 100 ) , context . get parameter as ( key1 , integer . class ) ) ;
pinching = false ; listener . pinch stop ( ) ; panning = true ;
if ( e instanceof keeper exception . session expired exception ) { return ; } this . abortable . abort ( failed delete of + this . rs server name znode , e ) ; }
assert fails to decode ( base32 ( ) , ab = c , unrecognized character : = ) ;
cluster state . builder builder = cluster state . builder ( current state ) ; meta data . builder md builder = meta data . builder ( current state . meta data ( ) ) ; cluster blocks . builder blocks = cluster blocks . builder ( ) . blocks ( current state . blocks ( ) ) ; routing table . builder rt builder = routing table . builder ( current state . routing table ( ) ) ; immutable open map < shard id , restore in progress . shard restore status > shards ; set < string > aliases = new hash set < > ( ) ; if ( renamed indices . is empty ( ) ) {
if ( index = = end ) { return true ; }
st = days of week . tail set ( c dow ) ; if ( st . size ( ) > 0 ) { dow = st . first ( ) ; } int days to add = 0 ;
final cheese cheese = new cheese ( brie , 15 ) ; final default fact handle h1 = new default fact handle ( 1 , cheese ) ; rete . assert object ( h1 , pctx factory . create propagation context ( 0 , propagation context . type . insertion , null , null , null ) , ksession ) ;
if ( current captchas . size ( ) > max _ saved _ captchas ) current captchas . remove ( 0 ) ;
for ( int index = this . m matching beacon type code start offset ; index < = this . m matching beacon type code end offset ; index + + ) { byte value = ( byte ) ( this . get matching beacon type code ( ) > > ( 8 * ( this . m matching beacon type code end offset - index ) ) & 0xff ) ; advertising bytes [ index - 2 ] = value ; }
stop ( ) ; }
boolean gen reduce sink = false ; boolean has order by = false ;
return new bom result ( iso - 10646 - ucs - 4 , 0 ) ;
system . arraycopy ( _ array , index , _ array , index + c . _ limit , _ limit - index ) ;
new dbs [ i ] = ( graph database api ) new test highly available graph database factory ( ) . new embedded database builder ( store dir file ) . set config ( config ( i ) ) . new graph database ( ) ; debug ( started + i + as current version ) ; legacy dbs [ i ] = null ;
try { integer . parse int ( current line ) ; } catch ( number format exception e ) { log . w ( tag , skipping invalid index : + current line ) ; continue ; }
database1 . begin ( txtype . optimistic ) ;
log . info ( total size of input data : + string utils . human readable int ( data size ) ) ; log . info ( total number of input data files : + file count ) ; return new data statistics ( data size , file count , false ) ; }
simple time zone simple time zone = new simple time zone ( - 28800000 , custom america los _ angeles , calendar . march , 9 , 0 , 7200000 , calendar . november , 2 , 0 , 7200000 , 3600000 ) ;
central directory cdir = central directory . view of ( input buffer ) . at ( 0 ) . parse ( ) ; assert with message ( count ) . that ( cdir . get count ( ) ) . is equal to ( 20 ) ; assert with message ( position after parse ) . that ( cdir . buffer . position ( ) ) . is equal to ( exp size ) ; assert with message ( limit after parse ) . that ( cdir . buffer . limit ( ) ) . is equal to ( 10000 ) ; cdir . buffer . flip ( ) ; assert with message ( position after finish ) . that ( cdir . buffer . position ( ) ) . is equal to ( 0 ) ; assert with message ( limit after finish ) . that ( cdir . buffer . limit ( ) ) . is equal to ( exp size ) ; }
_ context . run ( r ) ; if ( exception . get ( ) = null ) throw exception . get ( ) ; return reference . get ( ) ; }
int key size = 0 ;
collection < event fact handle > fact2 = get ksession ( ) . get fact handles ( new class object filter ( get kbase ( ) . get fact type ( package name , intervalled ) . get fact class ( ) ) ) ;
sdf . apply pattern ( yyyy - mm - dd ) ;
node < e > h , p , q ;
if ( value = = 0 . 0 ) { return ; }
sources . add jar ( new file ( . entities . jar ) ) ;
expression = parse ( 1 < = 3 . 0d ) ; assert cant compile ( expression ) ; expression = parse ( t ( integer ) . value of ( 3 ) < = 4 ) ; assert true ( ( boolean ) expression . get value ( ) ) ; assert can compile ( expression ) ; assert true ( ( boolean ) expression . get value ( ) ) ; expression = parse ( t ( integer ) . value of ( 3 ) < = t ( integer ) . value of ( 3 ) ) ; assert true ( ( boolean ) expression . get value ( ) ) ; assert can compile ( expression ) ; assert true ( ( boolean ) expression . get value ( ) ) ; expression = parse ( 5 < = t ( integer ) . value of ( 3 ) ) ;
mock endpoint result = get mock endpoint ( mock : result ) ; result . expected message count ( 1 ) ; result . message ( 0 ) . body ( ) . is instance of ( list . class ) ; list < string > body = new array list < string > ( ) ;
if ( is window late ( window ) ) { continue ; } is skipped element = false ; evicting window state . set current namespace ( window ) ;
in order inorder = in order ( spy , controller ) ;
assert equals ( 1 , person total call durations . size ( ) ) ; } ) ; }
( text is removed | | m span starts [ i ] > start | | m span ends [ i ] < m gap start ) ) { remove span ( i ) ; continue ; do not increment i , spans will be shifted left in the array }
draw context . clear entities ( world , start _ x - 1 , start _ y - 1 , start _ z - 1 , start _ x + this . building width , start _ y + this . building height , start _ z + this . building length ) ;
return clusters [ 0 ] ;
object result = client . execute ( method name , in . get body ( list . class ) ) ;
hour = java cal . get ( calendar . hour _ of _ day ) ;
if ( this . handler mappings = = null ) { this . handler mappings = get default strategies ( context , handler mapping . class ) ; if ( logger . is debug enabled ( ) ) { logger . debug ( no handler mappings found in servlet ' + get servlet name ( ) + ' : using default ) ; } }
result = table . get row or before ( first row , hconstants . catalog _ family ) ; assert true ( result . contains column ( hconstants . catalog _ family , null ) ) ; assert true ( bytes . equals ( result . get value ( hconstants . catalog _ family , null ) , zero ) ) ;
throw new unsupported operation exception ( constants . the _ integers _ could _ not _ be _ compared ) ;
for ( int i = 0 ; i < methods . length ; i + + ) { string return type = methods [ i ] . get return type descriptor ( ) ; don ' t generate private * or loosely return typed * methods if ( methods [ i ] . has modifier ( private ) * | | return type = = null * ) continue ; int modifiers = get asmmodifiers ( methods [ i ] . get modifiers ( ) ) ; if ( is interface ) modifiers | = ( acc _ public | acc _ abstract ) ; generate method ( class name , fq class name , methods [ i ] . get name ( ) , return type , methods [ i ] . get param type descriptors ( ) , modifiers , cw ) ; boolean is static = ( modifiers & acc _ static ) > 0 ; boolean overridden = class contains method ( super class , methods [ i ] . get name ( ) , methods [ i ] . get param type descriptors ( ) ) ; if ( is static & & overridden ) generate super delegate method ( super class name , methods [ i ] . get name ( ) , return type , methods [ i ] . get param type descriptors ( ) , modifiers , cw ) ; } return cw . to byte array ( ) ;
set < byte array > failed keys = sets . new hash set ( ) ; failed keys . add ( key byte array ) ; set < byte array > slop keys = make slop keys ( key byte array , failing node id list ) ; set < slop > registered slops = get all slops ( slop keys ) ; if ( registered slops . size ( ) = = 0 ) { fail ( should have seen some slops . but could not find any . ) ; } else if ( registered slops . size ( ) = 1 ) { fail ( number of slops registered = 1 ) ; }
throw new sketch exception ( msg . replace ( lcurly , { ) . replace ( rcurly , } ) , error file , error line , re . get column ( ) , false ) ;
return gradle test kit ;
try { display . make current ( ) ; } catch ( lwjglexception e ) { throw new runtime exception ( e ) ; } gl clear ( gl _ color _ buffer _ bit ) ;
string class name = get class name from uri ( _ namespace of funct ) ;
case none : {
assert equals ( num entries , iterables . size ( retriever ) ) ;
try { service reference < ? > [ ] references = this . context . get all service references ( metadata service . class . get name ( ) , null ) ; for ( service reference < ? > ref : references ) { return ( metadata service ) this . context . get service ( ref ) ; } return null ; } catch ( invalid syntax exception e ) { logger . warning ( cannot load metadata service on member details scanner impl . ) ; return null ; }
final replica . state lir state = get leader initiated recovery state ( collection , shard id , core . get core descriptor ( ) . get cloud descriptor ( ) . get core node name ( ) ) ;
for ( int i = 0 ; i < broker _ count ; i + + ) { broker web service ports [ i ] = port manager . next free port ( ) ; broker native broker ports [ i ] = port manager . next free port ( ) ; service configuration config = new service configuration ( ) ; config . set broker service port ( broker native broker ports [ i ] ) ; config . set cluster name ( use ) ; config . set web service port ( broker web service ports [ i ] ) ; config . set zookeeper servers ( 127 . 0 . 0 . 1 + : + zookeeper _ port ) ; config . set broker service port ( broker native broker ports [ i ] ) ; config . set load manager class name ( simple load manager impl . class . get name ( ) ) ; config . set advertised address ( localhost + i ) ; ; pulsar services [ i ] = new pulsar service ( config ) ; pulsar services [ i ] . start ( ) ; broker urls [ i ] = new url ( http : 127 . 0 . 0 . 1 + : + broker web service ports [ i ] ) ; lookup addresses [ i ] = pulsar services [ i ] . get advertised address ( ) + : + config . get web service port ( ) ; pulsar admins [ i ] = new pulsar admin ( broker urls [ i ] , ( authentication ) null ) ; } create namespace policies ( pulsar services [ 0 ] ) ;
flushables . flush quietly ( out err . get output stream ( ) ) ; flushables . flush quietly ( out err . get error stream ( ) ) ; system . set out ( saved out ) ;
check in nanos ( reservoir , now + 11 , collision _ buffer * 2 + 1 , 10 , 10 , 10 ) ;
int new event size = event v1 factory . compute event length ( key , dbus event info ) ; byte buffer serialization buffer = byte buffer . allocate ( new event size ) ; serialization buffer . order ( _ buf . order ( ) ) ; int size = dbus event v1 . serialize event ( key , serialization buffer , dbus event info ) ; if ( size = new event size ) throw new databus runtime exception ( event size doesn ' t match after conversion from v2 to v1 ) ;
if ( real _ next > = real _ stack . size ( ) ) return ;
m current scale = 0 ;
tmp key . init ( background painter instance , xstate ) ;
assert equals ( 1 , zk map . size ( ) ) ;
set relationship type ( namespaces . comments ) ;
gwcinitializer gwc initializer = geo server extensions . bean ( gwcinitializer . class ) ;
verify zone status ( zone , null , 0 ) ;
mock . expected bodies received ( expected body ) ; template . request ( direct : start , new processor ( ) { public void process ( exchange exchange ) throws exception { message in = exchange . get in ( ) ; in . set body ( value ) ; } } ) ; mock . assert is satisfied ( ) ; }
current sort field = f ;
if ( t instanceof socket exception & & socket _ closed _ message . matcher ( string . value of ( t . get message ( ) ) ) . matches ( ) ) { t = new closed channel exception ( ) ; }
add permissions xml ( ear , null , permissions xml ) ; ear . add as libraries ( create library ( ) ) ; return ear ;
annotation metadata builder roo dto annotation = new annotation metadata builder ( roo java type . roo _ dto ) ;
return false ; } int size ; if ( b . is directory entry ( ) ) { size = is directory ; } else { size = ( ( document node ) b ) . get size ( ) ; } if ( size = a sizes . get ( b name ) ) {
if ( lookup index > = lookups . length ) { if ( logger . is debug enabled ( ) ) logger . debug ( no more addresses for + account ) ; lookups = null ; return false ; }
assert equals ( ready , state iterator . next ( ) ) ;
string class name = probable class . get simple name ( ) ; if ( no compare | | clone ( ) . starts with ( child ) ) { string method label = < html > clone ( ) : + class name + - < font color = 777777 > + class name + < font > < html > ; candidates . add ( new completion candidate ( clone ( ) , method label , clone ( ) , completion candidate . predef _ method ) ) ; }
odocument v doc a _ db2 = database2 . load ( v doc a _ rid ) ; v doc a _ db2 . field ( name , doc a _ v2 ) ; database2 . save ( v doc a _ db2 ) ;
archive entry next entry ;
marshaller registration . register marshallers ( proto stream marshaller . get serialization context ( client ( 0 ) ) ) ; }
number text field < integer > core pool size = new number text field < integer > ( core pool size , integer . class ) ; core pool size . set minimum ( 1 ) ; form . add ( core pool size ) ; number text field < integer > max pool size = new number text field < integer > ( max pool size , integer . class ) ;
add raw measure ( project _ view _ 5 _ ref , ncloc _ language _ distribution _ key , < null > = 3 ; foo = 10 ) ; under test . execute ( ) ;
byte [ ] encrypted cek = get encrypted cek ( ) ;
verify delegation token in state store ( zk tester , token0 , tokens with renewal . get ( token0 ) , 0 ) ;
list < method metadata > all declared methods = member holding type details . get methods ( ) ;
translate transition arrive = new translate transition ( duration . millis ( 1200 ) , controls box ) ;
tmpl . add ( min sdk version , 9 ) ;
collections . sort ( result pairs ) ;
for ( synchronous queue < versioned < slop > > slop queue : slop queues . values ( ) ) { try { slop queue . put ( end ) ; } catch ( interrupted exception e ) { logger . warn ( error putting poison pill , e ) ; } } for ( future result : consumer results ) { try { result . get ( ) ; } catch ( exception e ) { logger . warn ( exception in consumer , e ) ; } }
for ( final interceptor binding meta data binding : method bindings ) { if ( binding . is total ordering ( ) ) { for ( final string interceptor : binding . get interceptor order ( ) ) { method level interceptors . add ( new interceptor description ( interceptor ) ) ; } } }
add vm opt ( b , testcase , randomized context . current ( ) . get target class ( ) . get simple name ( ) ) ;
region manager manager = check region manager ( plugin , world ) ;
if ( jchar > = 0x dc00 ) { return coder result . malformed for length ( 1 ) ; } int jchar2 = ( in . get ( ) & 0x ffff ) ;
join right key values with null ( iterator2 . get values ( ) , join function , collector ) ; it2 empty = iterator2 . next key ( ) ; return true ;
assert equals ( hello , world? , decode string ( a gvsb g8s ihdvcmxk py e = ) ) ;
byte buffer [ ] nio buffers = in . nio buffers ( ) ; int nio buffer cnt = in . nio buffer count ( ) ; long expected written bytes = in . nio buffer size ( ) ; socket channel ch = java channel ( ) ;
from ( direct : start ) . stream caching ( ) . choice ( ) . when ( ) . xpath ( message text ( ) = ' xx ' ) . to ( mock : x ) . when ( ) . xpath ( message text ( ) = ' yy ' ) . to ( mock : y ) . end ( ) ; } } ; }
data frame predictions = model . transform ( test data ) ;
assert equals ( 127 , bs . next set bit ( 127 ) ) ;
assert is directory ( fs , path ) ;
b . bind ( port ) . sync ( ) . channel ( ) . close future ( ) . sync ( ) ; } finally {
assert master address eventually ( get address ( member3 ) , member4 ) ;
ioexception ioe2 = new ioexception ( input stream error ) ;
glide . with ( context ) . load ( podcast . image url ) . placeholder ( r . color . light _ gray ) . disk cache strategy ( disk cache strategy . none ) . fit center ( ) . dont animate ( ) . into ( view holder . cover view ) ;
string builder encoded value = new string builder ( value . length ( ) * 2 ) ; int length = value . length ( ) ;
use configuration ( - - noexperimental _ android _ include _ library _ resource _ jars ) ; action deploy jar action = get generating action ( get file configured target ( java r android : bin _ deploy . jar ) . get artifact ( ) ) ;
m pipe body bounds . set ( arc bounds . center x ( ) + m gas tube width 2 . 0f - m pipe body width 2 . 0f , arc bounds . center y ( ) - m pipe body height , arc bounds . center x ( ) + m gas tube width 2 . 0f + m pipe body width 2 . 0f , arc bounds . center y ( ) ) ;
producer . send ( message . get bytes ( ) ) ;
segment metadata impl segment metadata = extract segment metadata ( raw table name , committing segment name str ) ; old seg metadata . set crc ( long . value of ( segment metadata . get crc ( ) ) ) ; old seg metadata . set start time ( segment metadata . get time interval ( ) . get start millis ( ) ) ; old seg metadata . set end time ( segment metadata . get time interval ( ) . get end millis ( ) ) ; old seg metadata . set time unit ( time unit . milliseconds ) ; old seg metadata . set index version ( segment metadata . get version ( ) ) ; old seg metadata . set total raw docs ( segment metadata . get total raw docs ( ) ) ; old seg metadata . set partition metadata ( get partition metadata from segment metadata ( segment metadata ) ) ; final znrecord old zn record = old seg metadata . to znrecord ( ) ; final string old znode path = zkmetadata provider . construct property store path for segment ( realtime table name , committing segment name str ) ;
send window update ( 1 , 200 ) ;
int vert count = ( circle samples + 1 ) * ( radial samples + 1 ) ;
string hive var = stab = sales table ; icol = customer id ;
if ( system . nano time ( ) - start of cycle - instance emit batch time . to nanos ( ) > 0 ) { break ; } if ( collector . get total data emitted in bytes ( ) - total data emitted in bytes before cycle > instance emit batch size . as bytes ( ) ) { break ; }
byte [ ] expected region end = expected bounds . get ( start boundary index + 1 ) ;
byte buffer bb = byte buffer . allocate ( ( int ) ( raf . length ( ) - raf . get file pointer ( ) ) ) ;
for ( int i = 0 ; i < tasks . size ( ) ; i + + ) { asset descriptor desc = tasks . get ( i ) . asset desc ; if ( desc . file name . equals ( file name ) & & desc . type . equals ( type ) ) throw new gdx runtime exception ( asset with name ' + file name + ' already in task list , but has different type ( expected : + class reflection . get simple name ( type ) + , found : + class reflection . get simple name ( desc . type ) + ) ) ; }
_ encap . write _ long ( _ member count ) ;
if ( m device . wait ( until . has object ( accept button selector ) , ui _ timeout ) ) { m device . find object ( accept button selector ) . click ( ) ; }
m _ install but . set enabled ( enable install ) ;
av = mv . visit parameter annotation ( i , ljava lang synthetic ; , false ) ; if ( av = null ) { av . visit end ( ) ; } }
this . service factory bean = service factory bean ; }
protocol mapper model prot mapper model = foo client . get protocol mapper by name ( oidclogin protocol . login _ protocol , foo ) ; john consent . get granted protocol mappers ( ) . remove ( prot mapper model ) ;
streams copy . get ( i ) . cancel ( status ) ;
collection . map reduce ( default _ map , default _ reduce , default _ collection , map reduce command . output type . reduce , null ) ;
cache . remove ticket from cache ( get token ( ) ) ;
m recycler view touch action guard manager . attach recycler view ( m recycler view ) ;
if ( post . is discover post ( ) ) { reader post discover data discover data = post . get discover data ( ) ; if ( discover data = null & & discover data . get discover type ( ) = = reader post discover data . discover type . editor _ pick ) { if ( discover data . get blog id ( ) = 0 & & discover data . get post id ( ) = 0 ) { reader activity launcher . show reader post detail ( get activity ( ) , discover data . get blog id ( ) , discover data . get post id ( ) ) ; return ; } else if ( discover data . has permalink ( ) ) { if we don ' t have a blog id post id , we sadly resort to showing the post in a web view activity - this will happen for non - jp self - hosted reader activity launcher . open url ( get activity ( ) , discover data . get perma link ( ) ) ; return ; } } }
message . set message payload ( payload ) ; if ( request nonce timer = null ) { logger . warn ( node { } : request nonce timer = null but generating a new request , node . get node id ( ) ) ; } request nonce timer = new nonce timer ( nonce timer type . requested , node ) ; request nonce message = message ; return message ;
assert true delayed5sec ( new assert task ( ) { @ override public void run ( ) throws exception { assert equals ( 0 , listener . objects . size ( ) ) ; } } ) ;
boolean basecompile = config . compile ( project ) ; assert true ( basecompile ) ; misc utils . copy file ( project . get path to deployment ( ) , configuration . get path to catalog for test ( catalogupdate - cluster - base . xml ) ) ;
list < located block > lblocks2 = dfstest util . get all blocks ( in2 ) ; for ( located block blk : lblocks2 ) { assert true ( is block token expired ( blk ) ) ; }
( ( bitmap type request ) load ) . center crop ( ) ; } } else if ( holder . get scale type ( ) = = image holder . scale type . fit _ center ) {
assert equals ( in _ cp _ config _ value , hadoop conf . get ( in _ cp _ config _ key , null ) ) ; }
int pass = 1 ;
future < boolean > update1 = update flush wait ( item id , load barrier , null , flush latch1 , commit latch1 ) ; future < boolean > update2 = update flush wait ( item id , load barrier , pre flush latch , flush latch2 , commit latch2 ) ; await or throw ( flush latch1 ) ; assert cache contains ( tombstone . class ) ; pre flush latch . count down ( ) ; await or throw ( flush latch2 ) ;
assert equals ( 4 , reader . get num images ( true ) ) ; iiometadata image metadata = reader . get image metadata ( 0 ) ;
heartbeat thread . join ( 3000 ) ; } catch ( interrupted exception ignored ) {
size + = next . get size ( ) ;
command . neo store command cmd = mock ( command . neo store command . class ) ;
if ( to type . is interface ( ) & & org . gjt . sp . jedit . bsh . this . class . is assignable from ( from type ) & & capabilities . can generate interfaces ( ) ) return check only ? valid _ cast : ( ( org . gjt . sp . jedit . bsh . this ) from value ) . get interface ( to type ) ;
final string payment state name = payment smhelper . get errored state for transaction ( transaction type . authorize ) . to string ( ) ; test listener . push expected event ( next event . payment _ plugin _ error ) ; payment dao . update payment and transaction on completion ( account . get id ( ) , null , payment . get id ( ) , transaction type . authorize , payment state name , payment state name , payment . get transactions ( ) . get ( 0 ) . get id ( ) , transaction status . unknown , requested amount , account . get currency ( ) , foo , bar , internal call context ) ; test listener . assert listener status ( ) ;
compiler . report ( jserror . make ( class node , polymer pass errors . polymer _ class _ properties _ not _ static ) ) ;
generation manager . checkpoint ( ) ;
long column index = get column index ( field name ) ; if ( index added ) { table . remove search index ( column index ) ; } throw ( runtime exception ) e ; }
super ( paged file , default value . length , length , base ) ;
camel context . get management strategy ( ) . remove event notifier ( event notifier ) ;
url = new url ( get jetty url ( ) + foo bar?authenticated = bar & op = renewdelegationtoken & token = + dt ) ; conn = ( http urlconnection ) url . open connection ( ) ; conn . set request method ( put ) ; assert . assert equals ( http urlconnection . http _ forbidden , conn . get response code ( ) ) ;
for ( string udtf alias : blank qb . get aliases ( ) ) { qb . add alias ( udtf alias ) ; }
fire group event ( new group , server stored group event . group _ created _ event ) ;
write namespace declarations ( ) ;
simple feature type builder tb = new simple feature type builder ( ) ;
expired _ flag = true ;
for ( odistributed lifecycle listener l : listeners ) try { l . on node left ( node left name ) ; } catch ( exception e ) {
assert true ( smack configuration . is smack initialized ( ) ) ; }
if ( character . is whitespace ( line . char at ( before ) ) ) { log ( ast . get line no ( ) , before , msg _ ws _ not _ preceded , open _ angle _ bracket ) ; }
resource symbols symbol values = create symbol file ( r . txt , int layout stubbable _ activity 0x7f020000 ) ; resource symbols symbols in library = create symbol file ( lib . r . txt ) ; path out = temp . resolve ( classes ) ; files . create directories ( out ) ; rclass generator writer = rclass generator . with ( out , symbol values . as initializers ( ) , final fields ) ; writer . write ( com . foo , symbols in library . as initializers ( ) ) ; path package dir = out . resolve ( com foo ) ;
master key next master key for node = this . container token secret manager . get next key ( ) ;
string find age by gtelteclause = select p from person redis p where p . age < = : max and p . age > = : min ; query = em . create query ( find age by gtelteclause ) ; query . set parameter ( min , 32 ) ; query . set parameter ( max , 35 ) ; results = query . get result list ( ) ;
string find by id = select p from person oracle no sql p where p . person id = 2 ; results = execute select query ( find by id ) ; assert . assert equals ( 1 , results . size ( ) ) ; assert . assert equals ( 2 , results . get ( 0 ) . get person id ( ) ) ; assert . assert equals ( person2 , results . get ( 0 ) . get person name ( ) ) ; assert . assert equals ( 20 , results . get ( 0 ) . get age ( ) . int value ( ) ) ; string find = select p from person oracle no sql p where p . person name = ' person4 ' and p . age > 30 ;
em . get transaction ( ) . begin ( ) ; str2 = em . find ( str test entity . class , str2 . get id ( ) ) ; coll1 = em . find ( double set ref coll entity . class , coll1 . get id ( ) ) ; coll1 . get collection ( ) . add ( str2 ) ;
hregion server server = cluster . get region server ( table region index ) ; print out regions ( server , initial regions : ) ; int region count = server . get online regions ( ) . size ( ) ;
prior files . remove ( this file ) ; } } }
hazelcast component helper . copy headers ( exchange ) ; }
if ( _ is includable field ( f ) ) { continue ; }
else { continue ; }
from ( direct : start ) . to ( mybatis : select account by id?statement type = select one & input header = + test _ case _ input _ header _ name + & output header = + test _ case _ output _ header _ name ) . to ( mock : result ) ;
if ( non standard delimiter = null ) { sql statement builder . set delimiter ( non standard delimiter ) ; }
client state machine . this . get screen helper ( ) . clear fragment ( missing _ mcp _ port _ error ) ;
renew all leases ( ) ;
assert equals ( 1 , m _ pbd . num open segments ( ) ) ; }
setup vector dimension ( resource info . elevation , elevation , dimension presentation . list , null , units , unit _ symbol ) ; string base = base feature info + & elevation = 1 . 0 3 . 0 ;
gen framebuffers ( 1 , gl multi fbo ) ;
data b db upd via string = new data b ( ) ; data b db upd via node = new data b ( ) ; assert equals ( 1 , db upd via string . da . i ) ; assert equals ( 3 , db upd via string . k ) ; mapper . reader for updating ( db upd via string ) . read value ( json bstring ) ; assert equals ( 5 , db upd via string . da . i ) ; assert equals ( 13 , db upd via string . k ) ; assert equals ( 1 , db upd via node . da . i ) ;
m _ kernel = kernel . make copy ( m _ svm . get kernel ( ) ) ;
assert equals ( 100 , gwc . get ( ) . get config ( ) . get gutter ( ) ) ; }
client response resp = client . call procedure ( sum b1 _ r ) ;
string collplan = new string ( planned statement . core . collector fragment , constants . utf8 encoding ) ;
if ( prev tab index = = index of component ( hidden component ) ) {
final job . builder builder = job . new builder ( ) . set command ( lists . new array list ( server , foo - service . yaml ) ) . set env ( env ) . set ports ( ports ) . set registration ( registration ) . set volumes ( volumes ) . set creating user ( test _ user ) ;
namespaces ns = new namespaces ( c , http : acme . com cheese ) ; from ( direct : start ) . filter ( ) . xpath ( c : person [ @ name = ' james ' ] , ns ) . to ( mock : result ) ;
for ( file f : files ) { if ( f . get name ( ) . equals ( only name ) ) { return true ; } } log . trace ( done file : { } does not exist , done file name ) ; return false ; }
return ( bits [ i page _ size ] [ i % page _ size ] & bitmask ) = 0 ;
attribute class attribute = old header . class attribute ( ) ;
int in scope = page context . page _ scope ; try { if ( to scope = null ) { in scope = tag utils . get instance ( ) . get scope ( to scope ) ; } } catch ( jsp exception e ) { log . warn ( to scope was invalid name so we default to page _ scope , e ) ; }
assert equals ( driver strategy . sorted _ group _ reduce , reduce node . get driver strategy ( ) ) ; assert equals ( driver strategy . sorted _ group _ combine , combine node . get driver strategy ( ) ) ;
links . add ( create link ( get , uri builder . clone ( ) . path ( workspace service . class , get by key ) . build ( workspace . get id ( ) ) . to string ( ) , link _ rel _ self ) ) ;
final byte [ ] javax mail payload bytes = javax mail payload . copy bytes ( ) ;
sets [ i ] = set ;
local query runner . create catalog ( tpch , new tpch connector factory ( 1 ) , immutable map . of ( ) ) ; return local query runner ; }
m in custom drag = false ;
if ( node . lo kid = null ) { write recursively ( out , node . lo kid ) ; }
source panel _ = new simple panel ( ) ;
build operation progress listeners . get source ( ) . status changed ( event ) ;
final path enc file2 = new path ( zone , myfile2 ) ;
byte array output stream s = new byte array output stream ( ) ; file f = new file ( tmp ) ;
create graph ( a to b , b to c , c to d , d to e ) ;
volt queue sql ( compare , m _ ts val , m _ ts val , m _ ts val , m _ ts val , m _ ts val , m _ ts val , m _ ts val , m _ str ts , m _ bin val , m _ pt , m _ poly ) ; break ;
while ( c . is array ( ) ) { c = c . get component type ( ) ; }
try { stop operation ( ) ; } catch ( exception stp ex ) { this . stop exception = stp ex ; } } else {
int edited sample count = 0 ;
assert equals ( test _ value _ 1 , continuing request . get cookies ( ) [ 0 ] . get value ( ) ) ; assert equals ( test _ value _ 2 , continuing request . get cookies ( ) [ 1 ] . get value ( ) ) ; assert equals ( default _ max _ age , continuing request . get cookies ( ) [ 1 ] . get max age ( ) ) ;
exchange . set property ( response _ class , global objects . class ) ; break ; case get _ basic _ info :
if ( null = image view & & ( image view instanceof photo view ) ) { if ( image view . get scale type ( ) = scale type . matrix ) { throw new illegal state exception ( the image view ' s scale type has been changed since attaching a photo view attacher ) ; } }
utils . show long toast ( context , image uri . to string ( ) ) ;
conf . set queues ( p1 , new string [ ] { x1 , x2 } ) ; conf . set capacity ( x1 , 80f ) ; conf . set maximum capacity ( x1 , 100f ) ; conf . set user limit factor ( x1 , 2f ) ; conf . set capacity ( x2 , 20f ) ; conf . set maximum capacity ( x2 , 100f ) ; conf . set user limit factor ( x2 , 2f ) ; conf . set queues ( p2 , new string [ ] { y1 , y2 } ) ; conf . set capacity ( y1 , 80f ) ;
if ( input . is direct ( ) & & supports unsafe ( ) ) { long address = direct byte buffer access loader . get address ( input ) ; if ( address = 0 ) { address + = input . position ( ) ; input . position ( input . limit ( ) ) ; update ( address , length ) ; return ; } } array = new byte [ length ] ;
data map a = new data map ( ) ; data map b = new data map ( ) ; data map c = new data map ( ) ; a . put ( b , b ) ;
ordered fields and methods = order fields and methods by dependency ( unordered fields and methods , properties inventory ) ;
best brokers . add all ( candidates ) ;
num live nodes . set ( status . num live ) ; num live excluded nodes . set ( status . num live excluded ) ; num live decommissioning in progress nodes . set ( status . num live decommissioning in progress ) ; num live decommissioned . set ( status . num live decommissioned ) ; num dead nodes . set ( status . num dead ) ;
session . set max text message buffer size ( 10000 ) ; session . add message handler ( string handler ) ; final client client = new client ( session ) ; final room room = get room ( true ) ;
final int partition key length = in . read int ( ) ;
final int num edits = 1000 ;
result size + = length owalpage v1 . calculate record size ( owalpage v1 . max _ entry _ size ) * owalpage . page _ size ; int left size = ( int ) length % owalpage v1 . calculate record size ( owalpage v1 . max _ entry _ size ) ;
date string = 2009 - 03 - 11 t17 : 57 : 00 z ;
annotation mirror annotation = element util . get annotation ( element , objective cname . class ) ; if ( annotation = null ) { return ( string ) element util . get annotation value ( annotation , value ) ; } type element outer class = element util . get declaring class ( element ) ;
byte buffer . position ( byte buffer . position ( ) - 1 ) ; }
filter = filter factory . get all filter ( ) . get ( i ) ; try { if ( filter . is enabled ( ) ) { filter . on http response receive ( http message ) ; } } catch ( exception e ) {
if ( pos . index of ( % ) > 0 ) { int size = orientation = = orientation . horizontal ? get offset width ( ) : get offset height ( ) ; float percentage = float . parse float ( pos . substring ( 0 , pos . length ( ) - 1 ) ) ; pos = percentage 100 * size + px ; } string attribute name ;
assert equals ( expected get result . source as string ( ) , parsed embedded get result . source as string ( ) ) ;
standard resolver . add ( new bean name elresolver ( new standard bean name resolver ( local beans ) ) ) ; standard resolver . add ( custom resolvers ) ;
boolean is rtl = ( ( m current mode = = orientation . portrait ) ^ localization utils . is layout rtl ( ) ) ; if ( m stack tabs [ clicked ] . get layout tab ( ) . check close hit test ( x , y , is rtl ) ) {
inject namespaces ( element , binder ) ; log . trace ( parsing route context done , returning { } , element , ctx ) ;
sign in with email and password ( email , password ) ;
aliases = create aliases ( collections . singleton list ( node3 ) ) ; map = allocate ( host provisioner , aliases ) ; assert correct number of host ( map , 1 ) ;
right * = phase y ; left * = phase y ; add bar ( left , top , right , bottom ) ; } } }
blocks = dfs client . namenode . get block locations ( file1 . to string ( ) , 0 , long . max _ value ) ; while ( blocks . get ( 0 ) . is corrupt ( ) = true ) { try { log . info ( waiting until block is marked as corrupt . . . ) ; thread . sleep ( 1000 ) ; } catch ( interrupted exception ie ) { } blocks = dfs client . namenode . get block locations ( file1 . to string ( ) , 0 , long . max _ value ) ; } replica count = blocks . get ( 0 ) . get locations ( ) . length ; assert true ( replica count = = 1 ) ; cluster . shutdown ( ) ;
spell opts . tokens = query converter . convert ( super ) ; result = checker . get suggestions ( spell opts ) ; assert true ( result is null and it shouldn ' t be , result = null ) ; suggestions = result . get ( spell opts . tokens . iterator ( ) . next ( ) ) ; assert true ( suggestions size should be 0 , suggestions . size ( ) = = 0 ) ; spell opts . tokens = query converter . convert ( caroline ) ;
if ( a prefix = = xmlsymbols . prefix _ xmlns | | a qname = = xmlsymbols . prefix _ xmlns ) { namespaces . add ( a prefix = = xmlsymbols . prefix _ xmlns ? attributes . get local name ( i ) : xmlsymbols . empty _ string ) ; }
int p5 = get precedence ( ops . not ) ;
sd . write ( ) ; injection handler . process event io ( injection event . fsimage _ upgrade _ after _ save _ image ) ; file prev dir = sd . get previous dir ( ) ;
assert true ( am client . remote requests table . is empty ( ) ) ; am client . unregister application master ( final application status . succeeded , null , null ) ;
endpoints . add all ( file endpoints ) ; simple expressions . add all ( file simple expressions ) ; route ids . add all ( file route ids ) ;
filter post processor fpp = new filter post processor ( asset manager ) ;
if ( get ( document font name ) = null ) { log . info ( document font name + already mapped . ) ; if ( last seen number of physical fonts = = physical fonts . get physical fonts ( ) . size ( ) ) {
secondary region . replay walregion event marker ( region events . get ( 1 ) ) ;
http2 connect ( ) ; send settings ( 0 , false , new setting value ( 0x2 , 0x2 ) ) ;
return new object bank wrapper < > ( flags , new object bank < > ( new resettable reader iterator factory ( in ) , reader and writer ) , known lcwords ) ; }
gb constraints = new grid bag constraints ( ) ; gb constraints . anchor = grid bag constraints . east ; gb constraints . fill = grid bag constraints . horizontal ; gb constraints . gridy = 2 ; gb constraints . gridx = 1 ; gb layout . set constraints ( ef holder , gb constraints ) ; aligned p . add ( ef holder ) ; jlabel relative lab = new jlabel ( use relative file paths , swing constants . right ) ;
tcpreceiver thread . sock = server socket channel . open ( ) ;
populate db and indexes ( node count , multi threaded ) ;
final u d = dimension checker . get dimension ( v ) ;
if ( mapper = null & & mapper . get from native converter ( ptypes [ i ] ) = null ) { direct = false ; break ; } }
assert . assert true ( unwrap exception ( e ) instanceof file already exists exception ) ; }
throw new human readable exception ( e , % s : javac unable to load annotation processor : % s , target . get fully qualified name ( ) , name ) ; }
final int pointer index = ev . find pointer index ( m active pointer id ) ; if ( pointer index = = - 1 ) return true ;
client . set data ( paths . status deployment group ( dg . get name ( ) ) , deployment group status . new builder ( ) . set state ( failed ) . build ( ) . to json bytes ( ) ) ;
r = run unguarded ( clazz , run m ) ;
if ( expect . length = result . length ) { fail ( label + length expect = + expect . length + result = + result . length ) ; }
assert not equals ( entry . get key ( ) , orig , now ) ;
lock blocking index populator . release ( ) ;
count down latch split = new count down latch ( 1 ) ; count down latch proceed = new count down latch ( 1 ) ; region split listener list = new region split listener ( split , proceed ) ; cluster . get master ( ) . executor service . register listener ( event type . rs _ zk _ region _ split , list ) ; log . info ( splitting table ) ;
item1 . get colors ( ) . clear ( ) ; item1 . get category ( ) . set example item ( item1 _ 1 ) ; s = open session ( ) ; tx = s . begin transaction ( ) ; item1 merged = ( item ) s . merge ( item1 ) ;
init mock cursor with two tags ( m mock cursor ) ;
final org . apache . camel . component . linkedin . api . model . company result = request body and headers ( direct : getcompanybyid , null , headers ) ; assert not null ( get company by id result , result ) ;
vertices [ x1 ] = left ;
for ( int i = listeners . length - 2 ; i > = 0 ; i - = 2 ) { if ( listeners [ i ] = = ancestor listener . class ) { lazily create the event : ancestor event ancestor event = new ancestor event ( source , id , ancestor , ancestor parent ) ; ( ( ancestor listener ) listeners [ i + 1 ] ) . ancestor removed ( ancestor event ) ; } } }
mortar scope root = this ;
term term = new term ( path , good ) ;
try { listener . added tag ( this , child ) ; } catch ( final exception exception ) { cutility functions . log exception ( exception ) ; } }
mibreakpoint dmdata breakpoint1 = ( mibreakpoint dmdata ) get breakpoint ( ref ) ;
assert jq ( req ( fq , filt , q , { func } + f2 , group , true , group . query , id : [ 2 to 5 ] , fl , id , rows , 3 , group . main , true ) , response = = { ' num found ' : 4 , ' start ' : 0 , ' docs ' : [ { ' id ' : ' 3 ' } , { ' id ' : ' 4 ' } , { ' id ' : ' 2 ' } ] } ) ;
if ( data set . get shadow color same as candle ( ) ) { if ( open > close ) m render paint . set color ( data set . get decreasing color ( ) = = color template . color _ none ? data set . get color ( j ) : data set . get decreasing color ( ) ) ; else if ( open < close ) m render paint . set color ( data set . get increasing color ( ) = = color template . color _ none ? data set . get color ( j ) : data set . get increasing color ( ) ) ; else m render paint . set color ( data set . get neutral color ( ) = = color template . color _ none ? data set . get color ( j ) : data set . get neutral color ( ) ) ; } else { m render paint . set color ( data set . get shadow color ( ) = = color template . color _ none ? data set . get color ( j ) : data set . get shadow color ( ) ) ; } m render paint . set style ( paint . style . stroke ) ;
string column name = check identifier start ( statement matcher . group ( 3 ) , ddl statement . statement ) ;
public int get line ( ) { return bufline [ bufpos ] ; }
command sb . append ( classpath ) ;
string redirect uri = oauth . get redirect uri ( ) ; oauth . redirect uri ( http : invalid ) ;
int stream . range ( 0 , range ) . boxed ( ) . for each ( i - > cache . put ( i , i + - value ) ) ; assert equals ( range , cache . size ( ) ) ; cache set < map . entry < integer , string > > entry set = cache . entry set ( ) ; primitive iterator . of int iterator = create stream ( entry set ) . flat map to int ( e - > int stream . of ( e . get key ( ) , e . get value ( ) . length ( ) ) ) . iterator ( ) ;
return delegate creator . call ( args ) ; } catch ( throwable t ) {
iterable < row > results = group by query runner test helper . run query ( factory , runner , query ) ;
log . debugf ( rebalancing with nodes % s % s , address ( 2 ) , address ( 3 ) ) ;
parse optional parameters ( request , info object , raw kvp ) ; if ( request . get layers ( ) . size ( ) = request . get styles ( ) . size ( ) ) { string msg = layers . size ( ) + layers requested , but found + request . get styles ( ) . size ( ) + styles specified . ; throw new service exception ( msg , get class ( ) . get name ( ) ) ; }
gb info . grp set rqr additional mrjob = gb info . grp sets . size ( ) > hc . get int var ( hive conf . conf vars . hive _ new _ job _ grouping _ set _ cardinality ) ;
string builder sb = new string builder ( ) ; for ( string k : signables . key set ( ) ) { sb . append ( k ) . append ( \ n ) ; sb . append ( signables . get ( k ) ) . append ( \ n ) ; } string result = sb . to string ( ) ; return result ;
while ( connections . is empty ( ) ) { try { thread . sleep ( 100 ) ; } catch ( interrupted exception e ) { } } client excecutor factory . unref and cleanup ( ) ;
final file config dir = new file ( filename ) . get parent file ( ) ;
props . set property ( process properties . path _ home , temp folder . new folder ( ) . get absolute path ( ) ) ;
throw new illegal argument exception ( unknown json config ) ;
iterator < resource server token services > iter = token services . values ( ) . iterator ( ) ;
try { iprivacy service client = get client ( ) ; if ( client = null ) return ( client . get version ( ) = = c current version ) ; } catch ( security exception ignored ) { } catch ( throwable ex ) { util . bug ( null , ex ) ; } return false ;
web socket frame [ ] frames = new web socket frame [ ping count + 1 ] ;
excluded nodes list = new array list < node > ( excluded nodes . size ( ) ) ;
assert equals ( should not have invalidated . , 1 , counter . calls ) ;
base . set values ( 4 , - 10 , 2345 , - 74 , 4 . 0001 ) ;
cluster context . elected ( role1 , for quorum , failing instance , 9 ) ;
bubble data data = new bubble data ( data sets ) ;
int utf16 length = sequence . length ( ) ; int utf8 length = utf16 length ; int i = 0 ;
variable value ptr = fn . new variable ( new pointer type ( value type ) ) ; global value ptr ptr = new global ( symbols . global value ptr symbol ( method ) , _ private , new null constant ( i8 _ ptr ) ) ; module builder . add global ( value ptr ptr ) ; fn . add ( new load ( value ptr , new constant bitcast ( value ptr ptr . ref ( ) , new pointer type ( value ptr . get type ( ) ) ) ) ) ; label null label = new label ( ) ;
captured command . get ( ) . run ( ) ; assert equals ( captured delay . get ( ) , test task . get status ( ) . get throttled ( ) ) ; }
return scale info ( logged task , logged attempt , locality , logged locality , rack local over node local , rack remote over node local ) ; } } else { throw new illegal argument exception ( attempt result is not succeeded , failed or killed : + logged attempt . get result ( ) ) ; } }
frame handler ( ) . data ( false , 5 , buffer , ( int ) buffer . size ( ) ) ;
test error reporter . assert has encountered all errors ( ) ; test error reporter . assert has encountered all warnings ( ) ; return result ; }
buffer [ m3 ] = ( byte ) ( ( buffer [ m3 ] & 131 ) + ( _ h < < 2 ) ) ; 10000011
( ( text view ) certificate view . find view by id ( com . android . internal . r . id . serial _ number ) ) . set text ( get serial number ( m x509 certificate ) ) ;
return path ; } }
out . put next entry ( input file provider . get zip entry ( filename ) ) ;
if ( serial device . is empty ( ) ) { serial device . close ( ) ; serial devices . remove ( serial device . get port ( ) ) ; }
false value = parser . parse expression ( true ) . get value ( boolean . class ) ;
file file = local file . get location ( ) . to file ( ) ;
double t = ( double ) index reader . get sum total term freq ( text field name ) ( double ) index reader . get doc count ( text field name ) ;
cluster . wait for hastate ( 0 , haservice state . active ) ;
expected rows = 1 ; expected keys = this . cols per row ; f = new row filter ( compare op . less , new binary comparator ( bytes . to bytes ( test row one - 2 ) ) ) ; s = new scan ( ) ; s . set filter ( f ) ; verify scan no early out ( s , expected rows , expected keys ) ;
this . doc highlighter . remove all highlights ( ) ;
out . println ( ) ; }
assert null ( new meta contact . get display name ( ) + was still in its old location after moving it . , parent meta group . get meta contact ( new meta contact . get meta uid ( ) ) ) ; assert not null ( new meta contact . get display name ( ) + was not in the new location after moving it . , fixture . meta cl service . get root ( ) . get meta contact ( new meta contact . get meta uid ( ) ) ) ;
legend matrix [ rn ] . add node ( node ) ;
{ match range ( ' \ u200 c ' , ' \ u200 d ' ) ; } break ; case 10 :
action result default action result = lookup and register if missing ( madvoc config . get default action result ( ) ) ; if ( string results . is empty ( ) ) {
if ( str pos > = str len ) { break ; }
log . warn ( we still have a missing fee + ( e . missing = null ? e . missing . to friendly string ( ) : ) ) ; amount = amount . subtract ( e . missing ) ;
lazy simple ser de test sd = new lazy simple ser de ( ) ; ser de utils . initialize ser de ( test sd , conf , tbl props , null ) ; writable s3 = test sd . serialize ( s , hrsd . get object inspector ( ) ) ;
unmarshaller . set listener ( new unmarshaller . listener ( ) { public void before unmarshal ( object target , object parent ) { if ( target instanceof client side cell set model ) { ( ( client side cell set model ) target ) . set cell set model listener ( listener ) ; } } public void after unmarshal ( object target , object parent ) { if ( target instanceof client side cell set model ) { ( ( client side cell set model ) target ) . set cell set model listener ( null ) ; } } } ) ;
assert u ( del q ( * : * ) ) ; long new dels q = ( ( gauge < number > ) metrics . get ( dels qname ) ) . get value ( ) . long value ( ) ; long new cumulative dels q = ( ( meter ) metrics . get ( cumulative dels qname ) ) . get count ( ) ; assert equals ( new dels q , 1 , new dels q ) ; assert equals ( new cumulative dels q , 1 , new cumulative dels q - cumulative dels q ) ;
client . test jq ( params ( q , no _ match _ s : no _ matches , json . facet , { process empty : true , + pages : { type : query , domain : { + to children + } } + , books : { type : query , domain : { + to parents + } } + } ) , facets = = { count : 0 + , pages : { count : 0 } + , books : { count : 0 } + } ) ;
assume true ( platform . detect ( ) = platform . windows ) ; test project build file parser factory build file parser factory = new test project build file parser factory ( cell . get root ( ) , known build rule types ) ;
package manager package manager = get package manager ( ) ; intent main intent = package manager . get launch intent for package ( get package name ( ) ) ; start activity ( main intent ) ; }
for ( thread thread : threads ) thread . start ( ) ;
overwrite stopwords ( stopwordc \ n ) ; h . get core container ( ) . reload ( collection ) ; up . process ( get solr core ( ) ) ;
dir2 . delete ( ) ;
if ( stream = = null ) { throw new null pointer exception ( stream parameter must not be null . ) ; }
vs = new max float function ( new value source [ ] { bogus _ float _ vs , bogus _ int _ vs , bogus _ double _ vs } ) ; assert none exist ( vs ) ; }
char [ ] buf = path . get buffer ( ) ; if ( context version . resources = null & & buf [ path end - 1 ] = ' ' ) { string path str = path . to string ( ) ; web resource file ;
jdk to orb input stream bridge . set value handler ( this ) ;
pq . clear ( ) ; for ( phrase positions pp : nr pps ) { if ( pp . position > end ) { end = pp . position ; } pq . add ( pp ) ; } return end ; }
return i & - i ;
if ( matcher . matches ( ) ) { timestamp = matcher . group ( 1 ) + z ; } date date = date utils . from isodate string ( timestamp ) ; final string trade id = string . value of ( trade . get tid ( ) ) ; return new trade ( null , trade . get amount ( ) , currency pair , trade . get price ( ) , date , trade id ) ;
ddl = create table t ( id integer not null , num integer , + constraint tblimit1 limit partition rows 6 ) ; + alter table t add limit partition rows 7 ; ; check ddlerror message ( ddl , null ) ;
client = get client ( ) ;
stats = tracker . get stats ( fadvised file name ) ;
eval time series ( labels , network predictions , mask array ) ; return ;
persistent properties = property map . values ( ) . to array ( new grails domain class property [ property map . size ( ) ] ) ;
configuration conf = new configuration ( ) ; conf . set ( dfs . permissions . supergroup , supergroup ) ; unix user group information . save to conf ( conf , unix user group information . ugi _ property _ name , dfs _ ugi ) ; dfs = new mini dfscluster ( conf , 1 , true , null ) ; file system fs = dfs . get file system ( ) ;
auth type = auth type . value of ( user info parts [ 2 ] ) ;
new re trace ( re trace . stack _ trace _ expression , verbose , mapping file ) . retrace ( reader , writer ) ;
assert equals ( 1 , count locks ( lock manager ) ) ;
if ( g0 = end ) { shift gap ( end ) ; }
if ( input buffer = null ) { input buffer . receive reset ( ) ; }
for ( int i = 0 ; i < 2 ; i + + ) { inet socket address inet address = new inet socket address ( 127 . 0 . 0 . 1 , base port + i ) ; assert false ( routing service . address2 shard id . contains key ( inet address ) ) ; } for ( int i = 0 ; i < num hosts ; i + + ) { inet socket address inet address = new inet socket address ( 127 . 0 . 0 . 1 , base port + 2 + i ) ; assert true ( routing service . address2 shard id . contains key ( inet address ) ) ; int shard id = routing service . address2 shard id . get ( inet address ) ; socket address sa = routing service . shard id2 address . get ( shard id ) ; assert not null ( sa ) ; assert equals ( inet address , sa ) ; }
final int top level idx = cpg . add methodref ( get class name ( ) , top level , top level sig ) ;
op set pers presence . fire provider status change event ( op set pers presence . get presence status ( ) , parent provider . get jabber status enum ( ) . get status ( jabber status enum . offline ) ) ;
return get item ( position ) . hash code ( ) ;
show day ( 2 ) ;
frame init = array utils . frame ( ard ( ard ( 13 . 2 , 236 , 58 , 21 . 2 ) , ard ( 10 . 0 , 263 , 48 , 44 . 5 ) , ard ( 8 . 1 , 294 , 80 , 31 . 0 ) , ard ( 8 . 8 , 190 , 50 , 19 . 5 ) ) ) ; glrm job = null ;
if ( e . to string ( ) . contains ( the security policy should have prevented ) ) { assert . fail ( the security policy for the rule should have prevented this from executing . . . ) ; } else {
coordinator . set rate history ( uploaded , downloaded ) ;
totally covered . clear ( ) ;
if ( frame buffer object = null ) { frame buffer object . dispose ( ) ; }
throw yre ;
tail epsilon remover opt = new tail epsilon remover ( atn ) ;
try { anomalies = adhoc anomaly detection ( aux function id , start time for correlated anomalies , end time for correlated anomalies , dimension map ) ; } catch ( exception e ) { anomalies = collections . empty list ( ) ; log . warn ( failed to fetch data for the auxiliary anomaly function ( { } ) ; omitting anomalies from this function . , aux function id , e ) ; }
int end position = import tree . get end position ( unit . end positions ) ;
super . begin function template ( node , name node ) ;
byte [ ] bytes = message . get raw content ( ) ; if ( bytes = null ) { try { if ( message instanceof sipmessage ) { return ( ( sipmessage ) message ) . get message content ( ) ; } else { return new string ( bytes , utf - 8 ) ; } } catch ( unsupported encoding exception e ) { } } return null ;
m app . get dbaccess helper ( ) . add song eqvalues ( song id , m fifty hertz level , m one thirty hertz level , m three twenty hertz level , m eight hundred hertz level , m two kilohertz level , m five kilohertz level , m twelve point five kilohertz level , m virtualizer level , m bass boost level , m reverb setting ) ;
json obj . put ( host count , host count + participant count + 1 ) ;
mock output stream byte output stream = new mock output stream ( length ) ; buffered output stream slice output output = new buffered output stream slice output ( byte output stream ) ; for ( int i = 0 ; i < offsets . length - 1 ; i + + ) { output . write bytes ( input array , offsets [ i ] , offsets [ i + 1 ] - offsets [ i ] ) ; }
return m context . get resources ( ) . get boolean ( r . bool . abc _ action _ bar _ embed _ tabs _ pre _ jb ) ; }
top visible index = selected element index ;
em . clear ( ) ; actor composite actor1 = em . find ( actor composite . class , new actor id ( a , 1 ) ) ; actor composite actor2 = em . find ( actor composite . class , new actor id ( a , 2 ) ) ; assert actors ( actor1 , actor2 ) ;
assert rows ( execute ( select a , b , s from % s group by a ) , row ( 1 , 2 , 1 ) , row ( 2 , 2 , 2 ) , row ( 4 , 8 , null ) , row ( 3 , null , 3 ) ) ; assert rows ( execute ( select a , b , s from % s group by a , b ) , row ( 1 , 2 , 1 ) , row ( 1 , 4 , 1 ) , row ( 2 , 2 , 2 ) , row ( 2 , 4 , 2 ) , row ( 4 , 8 , null ) , row ( 3 , null , 3 ) ) ;
assert script stack ( e , \ \ ujjjj , ^ - - - - here ) ;
nic . is virtual ( ) & & nic name . starts with ( vnif ) & & nic name . starts with ( vnbr ) & & nic name . starts with ( peth ) & & nic name . starts with ( vif ) & & nic name . starts with ( virbr ) & & nic name . contains ( : ) ) { final string [ ] info = net utils . get nic params ( nic name ) ; if ( info = null & & info [ 0 ] = null ) { _ private nic = nic ; s _ logger . info ( designating private to be nic + nic name ) ; break ; } }
assert equals ( 0 , instance . compare ( inprogress _ 1 _ 3 , inprogress _ 1 _ 3 ) ) ;
exchange empty = new default exchange ( exchange ) ; empty . get in ( ) . set body ( ) ; log . trace ( writing done file : [ { } ] , done file name ) ;
database = databases . open no authenticate ( i db url , user ) ;
principal = authenticate ( context , username , credentials ) ;
if ( b ) { return ; }
access controller . do privileged ( ( privileged action < object > ) ( ) - > { session destroyed impl ( se ) ; return null ; } ) ;
return model generator . to service model ( super model , zone , config server hosts , slobrok monitor manager ) ; }
illegal utf83 byte field . set text ( ) ; }
return reflection height - 0 . 5f ;
case 0xd8 : case 0xd9 : case 0xda : case 0xdb : case 0xdc : case 0xdd : case 0xde : case 0xdf : case 0xe0 :
sign in listener . on sign in ( result ) ;
string ref addr addr = new string ref addr ( nns , jndi name ) ;
uri = http : + uri ; }
exception = orb . get pihandler ( ) . invoke client piending point ( reply message . system _ exception , se ) ;
query [ ] queries = new query [ 1 ] ; queries [ 0 ] = new query ( query string , aq , score ) ; queries [ 0 ] . set extraction techniques ( extraction _ techniques ) ; return queries ;
ogg packet . trim payload ( ) ; return extractor . result _ continue ;
job . _ work = 1000 ; dkv . put ( aml ) ; job . update ( 30 , data import and parse complete ) ; }
assert equals ( 15 , exec ( int x = 60 ; x > > > = 2 l ; return x ; ) ) ; assert equals ( - 60 > > > 2 , exec ( int x = - 60 ; x > > > = 2 l ; return x ; ) ) ; }
if ( build . version . sdk _ int > = build . version _ codes . kitkat ) { set translucent status ( true ) ; system bar tint manager m tint manager = new system bar tint manager ( this ) ; m tint manager . set status bar tint enabled ( true ) ; m tint manager . set navigation bar tint enabled ( true ) ; m tint manager . set tint color ( 0x f00099 cc ) ; m tint manager . set tint drawable ( uielements helper . get general action bar background ( this ) ) ; get action bar ( ) . set background drawable ( uielements helper . get general action bar background ( this ) ) ; }
rs1 . get configuration manager ( ) . notify all observers ( conf ) ;
start route ( route id ) ; return ;
test error ( line _ joiner . join ( class a { , method1 ( ) { } , method1 ( ) { } , } ) , strict mode check . duplicate _ class _ methods ) ;
if ( property . char at ( 0 ) = = ' ' & & property . char at ( property . length ( ) - 1 ) = = ' ' ) { property = property . substring ( 1 , property . length ( ) - 1 ) ; } algorithms in property = property . split ( , ) ;
if ( this . cached plans = null ) { return this . cached plans ; }
assert true ( throttle . higher than ( no ) ) ;
keys . put ( show _ quick _ doc , new key builder ( ) . shift ( ) . char code ( key code map . f2 ) . build ( ) ) ;
client socket = socket . accept ( server socket ) ;
j = overlap [ j ] ;
m translate y = 0 ; m sections distance y = integer . max _ value ; } }
n create default ( ) ;
b tree node . insert key at ( cursor , key , pos , key count ) ; b tree node . insert value at ( cursor , value , pos , key count ) ; tree node . set key count ( cursor , key count + 1 ) ; return ; no split has occurred
expected rows = 3 ;
final odocument tmp = new odocument ( get class name ( label ) ) . set tracking changes ( false ) ;
htable descriptor htd = this . admin . get table descriptor ( table name ) ;
int index = - 1 ;
close guard . set reporter ( close guard reporter ) ;
cfw . add ( byte code . aconst _ null ) ; cfw . add ( byte code . areturn ) ; } else if ( param and var count = = 1 ) {
map < string , string > map = get layer info ( c , layer ) ;
j = overlap [ j ] ;
string req = get request ( ) ;
fixture . meta cl service . create meta contact ( mcl slick fixture . mock provider , parent meta group , new contact id ) ;
marshalled entry in store = store . load ( key ) ;
warmup ( req , channels ) ; long start time = system . nano time ( ) ;
student oracle no sqlchar student max = new student oracle no sqlchar ( ) ;
number number = ( number ) node . get attribute value ( max features ) ;
target node selection failure = new module version resolve exception ( dependency metadata . get selector ( ) , t ) ; return ; }
try { graph . execute job ( op chain , user ) ; fail ( exception expected ) ; } catch ( final runtime exception runtime e ) { final in order in order = in order ( hook1 , hook2 ) ; in order . verify ( hook1 ) . post execute ( result1 , captor . get value ( ) , context ) ; in order . verify ( hook2 ) . post execute ( result2 , captor . get value ( ) , context ) ; in order . verify ( hook1 ) . on failure ( result2 , captor . get value ( ) , context , e ) ; in order . verify ( hook2 ) . on failure ( result2 , captor . get value ( ) , context , e ) ; final list < operation > ops = captor . get value ( ) . get operations ( ) ; assert equals ( 1 , ops . size ( ) ) ; assert same ( operation , ops . get ( 0 ) ) ; }
string [ ] sentence = open nlp . tokenize ( text ) ; string [ ] pos tags = open nlp . tag pos ( sentence ) ; string [ ] chunk tags = open nlp . tag chunks ( sentence , pos tags ) ; string np = null ;
else if ( state = no _ token _ state ) { arg list . add ( curr arg . to string ( ) ) ; }
holder . text . set text size ( 24 ) ; v . set tag ( holder ) ;
assert . assert true ( failed to deploy : + result , operations . is successful outcome ( result ) ) ;
if ( f dtdcontent model handler = null ) { oc = xmldtdcontent model handler . occurs _ zero _ or _ more ; f dtdcontent model handler . occurrence ( oc , null ) ; }
validator . validate modified role service ( config , old config ) ; fail ( ) ;
id = ;
if ( bb . position ( ) + task header size ( ) > default _ buffer _ size ) { compile ( ) ; return 0 ; } else { return default _ buffer _ size - ( bb . position ( ) + task header size ( ) ) ; }
url location = get class ( ) . get resource ( main . fxml ) ;
response size + = call . get response cell size ( ) ; } metrics . dequeued call ( q time ) ; metrics . processed call ( processing time ) ; metrics . total call ( total time ) ; metrics . received request ( request size ) ; metrics . sent response ( response size ) ;
web hdfs file system . log . info ( redirect = + redirect ) ;
for ( int row = 0 ; row < rows ; row + + ) { for ( int col = 0 ; col < cols ; col + + ) { assert true ( seeker . seek ( mark , tab ) ) ; assert equals ( data [ row ] [ col ] , seeker . extract ( mark , extractors . string ( ) ) . value ( ) ) ; } assert true ( mark . is end of line ( ) ) ; }
perform http call ( domain test support . master address , 8080 ) ; perform http call ( domain test support . slave address , 8630 ) ; }
range map entry < k , v > range map entry = map entry above to truncate . get value ( ) ; if ( range map entry . get upper bound ( ) . compare to ( range to remove . upper bound ) > 0 ) {
system . out . println ( terminate instances ) ;
if ( e . get key code ( ) = = key event . vk _ escape ) { if ( save menu visible ) { cancel save button . post click ( ) ; } else { cancel button . post click ( ) ; } return ; }
mv . visit maxs ( 0 , 0 ) ;
raw rank profile raw child = new raw rank profile ( rank profile registry . get rank profile ( search , child ) , attribute fields ) ;
if ( paused ) return ; if ( is wifi required & & file download utils . is network not on wifi type ( ) ) { throw new file download network policy exception ( ) ; } } while ( true ) ; } finally {
cluster state changed . signal all ( ) ;
return bf . get bean ( default _ transaction _ manager _ name , platform transaction manager . class ) ;
log . info ( inlink db : running ) ;
byte first byte = input . get ( ) ;
region = locator . get region location ( key . get ( ) ) . get region info ( ) . get start key ( ) ;
k = k | ( data [ i _ 4 + 0 ] & 0xff ) ;
bus handler . push expected events ( next event . invoice , next event . payment , next event . invoice _ payment ) ; clock . add months ( 1 ) ; assert listener status ( ) ; final list < invoice > invoices = invoice user api . get invoices by account ( account . get id ( ) , false , call context ) ;
multiple writers script writers = new multiple writers ( ) ; script writers . add writer ( writer ) ; script writers . add writer ( writers ) ; return script writers ; }
try { this . query helper . test queries from string ( query str , 2 ) ; } catch ( exception e ) { throw throwables . propagate ( e ) ; } log . info ( shutting down kafka supervisor ) ;
tree map < object , object > tm1 = new tree map < object , object > ( m1 ) ; tree map < object , object > tm2 = new tree map < object , object > ( m2 ) ; iterator < entry < object , object > > i1 = tm1 . entry set ( ) . iterator ( ) ; iterator < entry < object , object > > i2 = tm2 . entry set ( ) . iterator ( ) ; while ( i1 . has next ( ) ) { map . entry < object , object > entry1 = i1 . next ( ) ; map . entry < object , object > entry2 = i2 . next ( ) ; int c = compare ( entry1 . get value ( ) , entry2 . get value ( ) ) ; if ( c = 0 ) { return c ; } else { c = compare ( entry1 . get value ( ) , entry2 . get value ( ) ) ; if ( c = 0 ) { return c ; } } } return 0 ;
http server [ ] servers = new http server [ num servers ] ; count down latch start server latch = new count down latch ( num servers ) ; set < http server > connected servers = new concurrent hash set < > ( ) ; for ( int i = 0 ; i < num servers ; i + + ) { http server server = vertx . create http server ( new http server options ( ) . set host ( default _ http _ host ) . set port ( default _ http _ port ) ) ; server . request handler ( req - > { connected servers . add ( server ) ; req . response ( ) . end ( ) ; } ) ; server . listen ( ar - > { assert true ( ar . succeeded ( ) ) ; start server latch . count down ( ) ; } ) ; servers [ i ] = server ; } await latch ( start server latch ) ; count down latch req latch = new count down latch ( requests ) ;
from ( direct : start ) . to ( mybatis : select account by id?statement type = select one & input header = + test _ case _ input _ header _ name + & output header = + test _ case _ output _ header _ name ) . to ( mock : result ) ;
buffer . put ( ( byte ) ( payload [ i ] ^ mask [ counter + + % 4 ] ) ) ;
b = 0x09e3779b9 l ;
indarray original = vec . get word vector matrix ( doc _ 16392 ) . dup ( ) ; indarray inferred a1 = vec . infer vector ( this is my work ) ; indarray inferred b1 = vec . infer vector ( this is my work . ) ; indarray inferred c1 = vec . infer vector ( this is my day ) ; indarray inferred d1 = vec . infer vector ( this is my night ) ; log . info ( a : { } , arrays . to string ( inferred a1 . data ( ) . as float ( ) ) ) ;
if ( delay ms > 0 ) { new timer ( ) { public void run ( ) { if ( was hidden _ ) show progress ( ) ; } } . schedule ( delay ms ) ; } else { show progress ( ) ; }
if ( resume ) { mean . add value ( delta time ) ; } else { delta time = 0 ; } boolean lrunning = false ;
final transformer tran = super . get transformer ( ) ; if ( tran = null ) { final error listener err handler = tran . get error listener ( ) ;
map < task id , task > tasks = job . get tasks ( ) ;
transaction util . do in hibernate ( this : : session factory , session - > { session . update ( foo ) ; } ) ; assert equals ( integer . value of ( 0 ) , bar . get version ( ) ) ; assert equals ( integer . value of ( 0 ) , foo . get version ( ) ) ;
setup bounds ( feature type ) ;
rule key input change = new default rule key factory ( 0 , hash cache , path resolver , rule finder ) . build ( archive . from ( target , project filesystem , rule finder , default _ archiver , immutable list . of ( ) , default _ ranlib , immutable list . of ( ) , archive contents . normal , default _ output , immutable list . of ( fake source path . of ( different ) ) , * cacheable * true ) ) ;
m damped freq = m natural freq * math . sqrt ( 1 - m damping ratio * m damping ratio ) ;
int bytes read = math . min ( data read - ( int ) extra bytes head , len ) ; system . arraycopy ( data buffer , ( int ) extra bytes head , buf , off , bytes read ) ; return bytes read ;
t = zksplit log . get file name ( t ) ;
handle . add last right tuple ( this ) ; }
fn node . its needs activation = true ; }
query cache event data event data = create end of sequence event ( partition id ) ;
object [ ] args = new object [ string list . length + 1 ] ;
out . write boolean ( this . origin = null ) ; if ( this . origin = null ) { bytes . write byte array ( out , this . origin . get versioned bytes ( ) ) ; } out . write boolean ( this . payload = null ) ; if ( this . payload = null ) { bytes . write byte array ( out , this . payload ) ; } }
array list < string > bye = null ; hash map < string , integer > another = null ; array list < object > struct = new array list < object > ( ) ; struct . add ( null ) ; struct . add ( bye ) ; struct . add ( another ) ; properties schema = new properties ( ) ;
assert . assert equals ( 7 , state descriptor . type . values ( ) . length ) ;
b = b serializer . deserialize ( in , - 1 ) ;
ctx . send upstream ( e ) ;
assert true ( unwrap exception ( e ) instanceof ioexception ) ;
splice ( e new , e org . lnext ) ;
int letter count = test util . next int ( random ( ) , 2 , 10 ) ;
offset + = next . get size ( ) ;
int chunk = node index > > chunk _ shift ;
java . util . date delayed date = delayed date ( timeout millis ( ) ) ;
s = open session ( ) ;
if ( class doc . is class ( ) ) { groovy constructor doc [ ] constructors = class doc . constructors ( ) ; if ( constructors = null & & constructors . length = = 0 ) { add default constructor to doc
reset ( task storage ) ; expect ( task storage . get active tasks ( ) ) . and return ( tasks ) . any times ( ) ; for ( task task : tasks ) { expect ( task storage . get status ( task . get id ( ) ) ) . and return ( optional . of ( task status . running ( task . get id ( ) ) ) ) . any times ( ) ; expect ( task storage . get task ( task . get id ( ) ) ) . and return ( optional . of ( task ) ) . any times ( ) ; } replay ( task storage ) ; supervisor . run internal ( ) ;
most current versions map . remove ( current goodversion ) ;
object [ ] array = ( object [ ] ) value ;
message store plugin context context = new message store plugin context ( message store config , broker stats manager , message arriving listener , broker config ) ;
int exact item pos = ( int ) ( available scroll height * touch fraction ) ;
int chunk size = bytes per checksum + checksum size ;
m paint . reset ( ) ; m paint . set filter bitmap ( false ) ; m paint . set xfermode ( s xfermode ) ;
relationship rel = this . get relationships part ( ) . add part ( targetpart , mode , get package ( ) . get content type manager ( ) , proposed rel id ) ;
assert . assert true ( test utils . condition valid ( new indexing service condition ( ) { @ override public boolean is valid ( ) { return remote task runner . get removed worker cleanups ( ) . contains key ( worker . get host ( ) ) ; } } ) ) ;
if ( template . get parameter types ( ) . length = method . get parameter types ( ) . length ) continue ; equal = true ;
final stop watch watch = ( counter = null & & counter . is statistics enabled ( ) ) ? new stop watch ( ) : null ;
for ( int i = 0 ; i < interfaces . length ; i + + ) { output . write utf ( interfaces [ i ] . get name ( ) ) ; } }
return new generic entity reader ( context , filters , data to retrieve ) ;
thread context . put ( request _ headers , headers ) ; final security context security context ; if ( auth header = null & & auth header . starts with ( basic ) ) { final string base64 user pass = auth header . substring ( auth header . index of ( ' ' ) + 1 ) ; final string user pass = decode base64 ( base64 user pass ) ; final string [ ] split = user pass . split ( : ) ; if ( split . length = 2 ) { throw new bad request exception ( invalid credentials in authorization header ) ; } security context = create security context ( split [ 0 ] , split [ 1 ] , secure , security context . basic _ auth , host , grizzly request . get remote addr ( ) , headers ) ; } else { security context = create security context ( null , null , secure , null , host , grizzly request . get remote addr ( ) , headers ) ; } request context . set security context ( security context ) ;
inclusive stop filter = new inclusive stop filter ( bytes . to bytes ( inclusive stop filter ) ) ; assert true ( inclusive stop filter . are serialized fields equal ( protobuf util . to filter ( protobuf util . to filter ( inclusive stop filter ) ) ) ) ; }
assert false ( file utils . delete path if empty ( fs , file path ) ) ;
assert . assert equals ( 0 , tasks . size ( ) ) ;
if ( override ) { bound key codes . add ( key code ) ; } else { bound key codes . remove ( key code ) ; }
list < float > bids price data = get price data ( bitcoinium orderbook . get bids ( ) ) ; collections . reverse ( bids price data ) ; list < float > bids volume data = get volume data ( bitcoinium orderbook . get bids ( ) ) ; collections . reverse ( bids volume data ) ; x axis bid data = bids price data ; y axis bid data = bids volume data ;
thread handle . ping ( ) ; int bufsize = sock . get receive buffer size ( ) ; byte message [ ] = new byte [ bufsize ] ; datagram packet packet = new datagram packet ( message , bufsize ) ; sock . receive ( packet ) ;
final model node subsystem add op = util . create add operation ( subsystem address ) ;
this . metadata cache . put ( store def . get name ( ) , new versioned < object > ( store def str , value . get version ( ) ) ) ; }
final int off = lower index ; indent : 4 exp : 4 final int len = upper index - lower index ; indent : 4 exp : 4 input indentation from guava2 < k , v > outer = null ; indent : 4 exp : 4 return outer ; indent : 4 exp : 4
ctxt . generate java source ( } else { ) ;
this . set state ( transaction state . completed ) ;
return instance ; indent : 8 exp : 8 } indent : 4 exp : 4
mod cluster config resource definition . proxy _ list . parse and set parameter ( value , conf , reader ) ; break ; default : throw unexpected attribute ( reader , i ) ; } }
ctm0 . set rebalancing enabled ( true ) ;
streaming assertions = assertion factory . new streaming assert ( table , europe _ uk ) ; streaming assertions . assert min transaction id ( 1 l ) ; streaming assertions . assert max transaction id ( 1 l ) ; streaming assertions . assert expected file count ( 1 ) ; read records = streaming assertions . read records ( ) ;
xsltattribute def select attr def node = new xsltattribute def ( null , select , xsltattribute def . t _ expr , false , xsltattribute def . error , node ( ) ) ;
string xpath base = csw : record [ dc : identifier = ' urn : uuid : 784e2afd - a9fd - 44a6 - 9a92 - a3848371c8ec ' ] ;
for ( int i = 0 ; i < 32767 ; i + + ) { if ( i > = 0 ) { counter3 + + ; } else { counter4 + + ; } counter5 + + ; }
string sort by = get and remove parameter ( parameters , sort by , string . class ) ; if ( is not empty ( sort by ) & & endpoint helper . is reference parameter ( sort by ) ) { we support nested sort groups so they should be chained string [ ] groups = sort by . split ( ; ) ; iterator < string > it = cast utils . cast ( object helper . create iterator ( groups ) ) ; comparator < exchange > comparator = create sort by comparator ( it ) ; endpoint . set sort by ( comparator ) ; } set properties ( endpoint . get configuration ( ) , parameters ) ; set properties ( endpoint , parameters ) ;
restart nm ( max _ tries ) ; check num of local dirs ( ) ; verify ( del service , times ( 1 ) ) . delete ( arg that ( new file deletion matcher ( del service , null , new path ( resource localization service . nm _ private _ dir + _ del _ ) , null ) ) ) ;
delete orphans ( new create table response listener ( result , statement ) , statement . table ident ( ) ) ; return result ;
name = object name . get instance ( org . apache . camel : context = camel - 1 , type = endpoints , name = \ mock : x \ ) ; assert false ( should not be registered , mbean server . is registered ( name ) ) ; name = object name . get instance ( org . apache . camel : context = camel - 1 , type = endpoints , name = \ mock : y \ ) ; assert false ( should not be registered , mbean server . is registered ( name ) ) ; name = object name . get instance ( org . apache . camel : context = camel - 1 , type = endpoints , name = \ mock : z \ ) ; assert false ( should not be registered , mbean server . is registered ( name ) ) ;
r1 . unpause ( ) ; thread . sleep ( 8000 ) ; log . info ( some events , more time then timeout . events = + counting consumer . get num data events ( ) + ; resets = + counting consumer . get num resets ( ) ) ; assert . assert equals ( counting consumer . get num resets ( ) , 0 ) ; r1 . pause ( ) ; stop events
string delete query = delete from person ;
throw e ; }
if ( offset + length < = n2 . get start position ( ) & & offset > = astnodes . get exclusive end ( n1 ) ) { return true ; }
string table = get properties ( ) . get property ( tablename _ property , tablename _ property _ default ) ; try { htable interface ht = h conn . get table ( table ) ; ht . get table descriptor ( ) ; } catch ( ioexception e ) { throw new dbexception ( e ) ; } }
if ( m item count = total item count & & visible item count > 0 ) { m item count = total item count ; m long list = m item count visible item count > = min _ pages ; } if ( m always show ) { m long list = true ; } if ( m long list ) { if ( m state = state _ none ) { set state ( state _ none ) ; } return ; } if ( total item count - visible item count > 0 & & m state = state _ dragging ) { m thumb y = get thumb position for list position ( first visible item , visible item count , total item count ) ; if ( m changed bounds ) { reset thumb pos ( ) ; m changed bounds = false ; } } m scroll completed = true ; if ( first visible item = = m visible item ) { return ; }
load new version ( r impl , 2 ) ;
if ( return class type . get parameter count ( ) = = 1 & & inheritance util . is inheritor ( return resolved , common class names . java _ util _ list ) ) { final psi type list type = return class type . get parameters ( ) [ 0 ] ; return is dom element inheritor ( list type ) ; } return false ; }
dbc . add test ( new test config ( test _ order _ by , select a , b from aaa order by a , b ; , false , order by output ) ) ;
int derivative state = ( int ) math . signum ( math . abs ( m over scroll offset ) - math . abs ( overscroll ) ) ; if ( derivative state = m over scroll derivative & & derivative state = = 1 & & overscroll < 0 ) { m over scroll counter + + ; } else if ( overscroll > 0 | | m current mode = = orientation . landscape ) { m over scroll counter = 0 ; } m over scroll derivative = derivative state ; m over scroll offset = overscroll ;
configuration conf = util . get configuration ( ) ; secure test util . enable security ( conf ) ;
args = new string [ ] { - replace labels on node , - fail on unknown nodes } ;
application attempt id app attempt id = app . get app attempts ( ) . key set ( ) . iterator ( ) . next ( ) ; assert . assert not null ( app attempt id ) ; final custom am am = new custom am ( app attempt id , response . get client to amtoken master key ( ) . array ( ) ) ; am . init ( conf ) ; am . start ( ) ;
float inv = 1f dir dot norm ; float t = diff dot norm * inv ; if ( do planar ) { store . set ( origin ) . add local ( direction . x * t , direction . y * t , direction . z * t ) ; } else {
app . get context ( ) . get event handler ( ) . handle ( new task attempt event ( map task1 . get attempts ( ) . values ( ) . iterator ( ) . next ( ) . get id ( ) , task attempt event type . ta _ done ) ) ;
list < byte array > positive test key list = sample keys from partition ( admin client , 1 , rw store def with replication . get name ( ) , arrays . as list ( 1 ) , 20 ) ; rebalance and check ( rebalance kit . plan , rebalance kit . controller , arrays . as list ( 0 , 1 , 2 ) ) ;
return false ; } else if ( is upper ( last type ) & & is alpha ( type ) ) {
for ( int i = 0 ; i < 50 ; i + + ) { partition count = r . next int ( 1000 ) + 1 ; config bytes = legacy hashinator . get configure bytes ( partition count ) ; h1 = new hashinator lite ( partition count ) ; h2 = the hashinator . get hashinator ( hashinator type . legacy . hashinator class , config bytes , false ) ; tandem test same long hash ( h1 , h2 , partition count ) ; config bytes = elastic hashinator . get configure bytes ( partition count , elastic hashinator . default _ total _ tokens ) ; h1 = new hashinator lite ( hashinator lite type . elastic , config bytes , false ) ; h2 = the hashinator . get hashinator ( hashinator type . elastic . hashinator class , config bytes , false ) ; tandem test same long hash ( h1 , h2 , partition count ) ; }
i = line start + line length ;
else { logger . fine ( header and setup now on single page : ) ; replace second page and renumber page seqs ( vorbis header sizes , new comment length , new second page data length , second page header , new comment , raf , raf temp ) ; }
boolean validated = resumable upload resource manager . validate upload ( upload id , total byte to upload , start position , end position , total file size ) ;
reader simple post view . on simple post click listener listener = new reader simple post view . on simple post click listener ( ) { @ override public void on simple post click ( view v , long site id , long post id ) { show related post detail ( site id , post id , is global ) ; } } ;
new remoting permission ( create endpoint ) , new remoting permission ( connect ) ,
list < gplay card section > sections = new array list < gplay card section > ( ) ;
adp = new add dynamic partitions ( txn mgr . get current txn id ( ) , default , tab1 , collections . singleton list ( p = two ) ) ; adp . set operation type ( data operation type . update ) ; txn handler . add dynamic partitions ( adp ) ; txn mgr . commit txn ( ) ; txnid : id txn update4 assert . assert equals ( write _ set mismatch : + txn db util . query to string ( conf , select * from write _ set ) , 1 , txn db util . count query agent ( conf , select count ( * ) from write _ set where ws _ partition = ' p = one ' and ws _ operation _ type = ' u ' and ws _ table = ' tab1 ' ) ) ; assert . assert equals ( write _ set mismatch : + txn db util . query to string ( conf , select * from write _ set ) , 1 , txn db util . count query agent ( conf , select count ( * ) from write _ set where ws _ partition = ' p = two ' and ws _ operation _ type = ' u ' and ws _ table = ' tab1 ' ) ) ;
edit policy . edit schedule ( ) ;
return ? ; }
simple date format sdf = new simple date format ( hh : mm ) ;
set flag ( native . epollout ) ;
catalog cat = get geo server ( ) . get catalog ( ) ; workspace info ws = ws name = null ? cat . get workspace by name ( ws name ) : cat . get default workspace ( ) ; data store info ds = cat . get factory ( ) . create data store ( ) ; ds . set workspace ( ws ) ;
annotations directory out . write int ( index map . adjust annotation set ( directory in . read int ( ) ) ) ;
string cause = ex . get cause ( ) . get message ( ) ;
test thread local transfer ( new thread local transfer callable ( new local workspace thread local transfer ( ) ) { @ override void assert thread local cleaned ( ) { assert null ( local workspace . get ( ) ) ; } @ override void assert thread local applied ( ) { assert same ( ws , local workspace . get ( ) ) ; } } ) ;
assert . assert equals ( hello world , new client . get ( queue1 open ) ) ;
string bootstrap time = get property from client info ( bootstrap time ) ;
final top docs top hits ; if ( sort = = null ) { if ( use from ) { top score doc collector c = top score doc collector . create ( num hits ) ; searcher . search ( query , c ) ; from = test util . next int ( random ( ) , 0 , num hits - 1 ) ; size = num hits - from ; top docs temp top hits = c . top docs ( ) ; if ( from < temp top hits . score docs . length ) {
finished = true ;
implied _ argument = week _ of _ year ;
security . remove provider ( entrust . get name ( ) ) ;
processor = new subscribe method processor ( endpoint ) ;
if ( local import volume = = null ) { throw new org . apache . axis2 . databinding . adbexception ( property cannot be null ) ; } local import volume . serialize ( my _ qname , factory , xml writer ) ; }
view . set visibility ( view . invisible ) ; }
cli . send line ( deployment = + war file . get name ( ) + : add ( enabled = true , content = { url = + war file . to uri ( ) . to url ( ) . to external form ( ) + } ) ) ;
if ( referenced class = = null ) { return descriptor ; }
incorporate counter ( new set field ( this ) { @ override void set ( long val ) { metrics . set cumulative cpu usage ( val ) ; } } , counters , cpu _ milliseconds ) ;
resource management service r = gui activator . get resources ( ) ; int hgap = r . get settings int ( impl . gui . dial _ pad _ horizontal _ gap ) ; int vgap = r . get settings int ( impl . gui . dial _ pad _ vertical _ gap ) ; int width = r . get settings int ( impl . gui . dial _ pad _ width ) ; int height = r . get settings int ( impl . gui . dial _ pad _ height ) ; dial pad panel = new jpanel ( new grid layout ( 4 , 3 , hgap , vgap ) ) ;
try { get editor input ( ) . release ( ) ; } catch ( throwable e ) { log . warn ( error releasing content input , e ) ; } }
case variable tunnel build message . message _ type :
stmt . execute ( create table + non ascii table name + ( key int , value string ) + row format delimited fields terminated by ' | ' ) ;
m . set c1 ( 3 ) ;
am1 . allocate ( 127 . 0 . 0 . 1 , 1024 , num _ containers , new array list < container id > ( ) ) ; nm . node heartbeat ( true ) ;
thread1 . join ( ) ;
if ( latency us > max _ latency _ us ) { log . w ( tag , ignoring impossibly large audio latency : + latency us ) ; latency us = 0 ; } } catch ( exception e ) {
assert junit . assert true ( error in tx1 . , tx1 . get ( 15 , seconds ) ) ; assert junit . assert true ( error in tx2 . , tx2 . get ( 15 , seconds ) ) ;
return earlier or later ? instant + diff : instant ; }
if ( preferred wizard name = null & & wizard . get class ( ) . get name ( ) . equals ( preferred wizard name ) ) { pref wiz = wizard ; }
get result = engine . get ( new get ( true , doc ) , searcher factory ) ;
content = this . term3 + + this . term2 ; } doc . add ( new text field ( this . field , content , field . store . yes ) ) ; writer . add document ( doc ) ; }
ldapstorage provider ldap fed provider = ldaptest utils . get ldap provider ( session , ldap model ) ; ldaptest utils . add ldapuser ( ldap fed provider , test realm , user6 , user6 fn , user6 ln , user6 @ email . org , null , 126 ) ; ldapobject ldap user5 = ldap fed provider . load ldapuser by username ( test realm , user5 ) ;
int index = request uri . index of ( ' ? ' ) ; if ( index > - 1 ) { request uri = request uri . substring ( 0 , index ) ; } holder = samlrequest parser . parse request redirect binding ( saml request ) ;
if ( suffix length = = constants . schema _ validation _ feature . length ( ) & & feature id . ends with ( constants . schema _ validation _ feature ) ) { return feature state . recognized ; }
if ( unknown exception info = null ) { write map entry ( os , ue info id , unknown exception info , gv ) ; sc map . put ( ue info id , unknown exception info ) ; } }
mock . set result minimum wait time ( 500 ) ;
among _ var = find _ among _ b ( a _ 2 , 11 ) ;
string builder sb = new string builder ( this . to string template ) ; sb . append ( next expiration = ) ; sb . append ( this . next expiration ) ; sb . append ( timer state = ) ; sb . append ( this . timer state ) ; sb . append ( info = ) ; sb . append ( this . info ) ; sb . append ( ] ) ; return sb . to string ( ) ; }
boolean saw new base = false ;
envelope aggregate bbox = get map . get bbox ( ) ;
string parent element base uri = ( this . parent node ( ) = null ) ? this . parent node ( ) . get base uri ( ) : null ;
comments list fragment . set comment status filter ( m current comment status type ) ;
set data with retries ( zk bread crumb path , app data , old breadcrumb stat . get version ( ) ) ;
time begin = my begin list . item ( 0 ) ; double begin offset = begin . get resolved offset ( ) + offset ; if ( begin offset > max offset ) { this element can ' t be started . return timeline ; } timeline entry my begin = new timeline entry ( begin offset , seq , timeline entry . action _ begin ) ; timeline . add ( my begin ) ; time list my end list = seq . get end ( ) ;
doc write request write request = new index request ( index , type , id ) . source ( requests . index _ content _ type , foo , bar ) ; doc write response response = new index response ( shard id , type , id , 1 , 17 , 1 , random boolean ( ) ) ; bulk item request request = new bulk item request ( 0 , write request ) ; request . set primary response ( new bulk item response ( 0 , doc write request . op type . index , response ) ) ; assert that ( replica item execution mode ( request , 0 ) , equal to ( replica item execution mode . normal ) ) ;
string token = ( ( hive connection ) hs2 conn ) . get delegation token ( mini hive kdc . hive _ test _ user _ 2 , mini hive kdc . hive _ service _ principal ) ; fail ( mini hive kdc . hive _ test _ super _ user + shouldn ' t be allowed to retrieve token for + mini hive kdc . hive _ test _ user _ 2 ) ;
final flush request flush request = new flush request ( ) ; flush request . force ( false ) ; flush request . wait if ongoing ( false ) ; index shard . flush ( flush request ) ; index shard . refresh ( test ) ;
return test string . code points ( ) . filter ( ch - > ch = = ' . ' ) . count ( ) ; }
new native code generator ( ) . generate ( src , bin , jni ) ;
. to ( exchange pattern . in out , activemq : queue : name requestor?reply to = queue : named reply queue )
camel context . create producer template ( ) . send body ( { { entry } } , test message ) ; spring application . exit ( context ) ;
if ( component . add local candidate ( candidate ) ) { candidates . add ( candidate ) ; } local address second = null ; relayed address second = null ; return candidates ; }
path trash dir = trash . get current trash dir ( file ) ;
market data service market data service = bitcurex . get market data service ( ) ; generic ( market data service , pair ) ;
string owner = aws client to instance to owner . get ( aws client ) . get ( instance id ) ; if ( owner = = null ) { owner = janitor metadata . get ( basic simian army context . global _ owner _ tagkey ) ; } if ( owner = = null ) { owner = get tag value ( basic simian army context . global _ owner _ tagkey , tags ) ; } string email domain = get owner email domain ( ) ; if ( owner = null & & owner . contains ( @ ) & & string utils . is not blank ( email domain ) ) { owner = string . format ( % s @ % s , owner , email domain ) ; } return owner ;
set content view ( r . layout . content _ frame ) ; get support fragment manager ( ) . begin transaction ( ) . replace ( r . id . content _ frame , new sample list fragment ( ) ) . commit ( ) ;
test deep file creation base ( deep file creation test , deep , deep file creation , ( short ) 0700 , ( short ) 0700 ) ;
try { final string entity name = get entity name resolver ( ) . resolve entity name ( object ) ; if ( entity name = = null ) { throw new illegal argument exception ( could not resolve entity - name [ + object + ] ) ; } get session factory ( ) . get metamodel ( ) . entity persister ( entity name ) ; } catch ( hibernate exception e ) { throw new illegal argument exception ( not an entity [ + object . get class ( ) + ] , e ) ; }
suppliers . memoize ( ( ) - > location expander . build location map ( rule context , label map , options . contains ( options . allow _ data ) ) ) , options . contains ( options . exec _ paths ) ) ; }
ltm . remove ( left tuple ) ;
if ( worker1 . equals ( worker - 1 ) ) assert . assert equals ( worker - 2 , worker2 ) ; else assert . assert equals ( worker - 1 , worker2 ) ; } finally { http client utils . close quietly ( response ) ; } }
int set values size = values . size ( ) ; int bind args size = ( where args = = null ) ? set values size : ( set values size + where args . length ) ; object [ ] bind args = new object [ bind args size ] ; int i = 0 ; for ( string col name : values . key set ( ) ) { sql . append ( ( i > 0 ) ? , : ) ; sql . append ( col name ) ; bind args [ i + + ] = values . get ( col name ) ; sql . append ( = ? ) ; } if ( where args = null ) { for ( i = set values size ; i < bind args size ; i + + ) { bind args [ i ] = where args [ i - set values size ] ; } } if ( text utils . is empty ( where clause ) ) { sql . append ( where ) ; sql . append ( where clause ) ; } sqlite statement statement = new sqlite statement ( this , sql . to string ( ) , bind args ) ;
if ( mqtt client = null ) { try { mqtt client . close ( ) ; } catch ( mqtt exception e ) { e . print stack trace ( ) ; } }
throw object helper . wrap runtime camel exception ( e ) ;
type extractor . validate if writable ( basic type info . string _ type _ info , string . class ) ;
if ( info stream = null ) { info stream . print ( test : field norms . . . . . . . . . ) ; } norms producer norms reader = reader . get norms reader ( ) ; if ( norms reader = null ) { norms reader = norms reader . get merge instance ( ) ; } for ( field info info : reader . get field infos ( ) ) { if ( info . has norms ( ) ) { check numeric doc values ( info . name , norms reader . get norms ( info ) ) ; + + status . tot fields ; } } msg ( info stream , string . format ( locale . root , ok [ % d fields ] [ took % . 3f sec ] , status . tot fields , ns to sec ( system . nano time ( ) - start ns ) ) ) ;
{ return false ; }
assert . assert equals ( num _ files + 3 , cluster . get name node ( ) . get namesystem ( ) . get files and directories total ( ) ) ;
switch ( xmlns ) { case protocol provider service jabber impl . urn _ xmpp _ jingle _ ice _ udp _ 1 : transport manager = new ice udp transport manager ( peer ) ; break ; case protocol provider service jabber impl . urn _ xmpp _ jingle _ raw _ udp _ 0 : transport manager = new raw udp transport manager ( peer ) ; break ; default : throw new illegal argument exception ( unsupported jingle + transport + xmlns ) ; } synchronized ( transport manager sync root ) { transport manager sync root . notify ( ) ; }
assert that thrown by ( ( ) - > this . assign executor . assign executor ( 1 , flow . get execution id ( ) ) ) . is instance of ( executor manager exception . class ) . has message containing ( non - existent executor ) ; }
is main thread layout best = false ; } else {
formatter . set property data source ( new object property ( null , expected class ) ) ; formatter . get value ( ) ; calls format
put . add ( new key value ( row , family , qualifier , value ) ) ; cell set model model = build model from put ( put ) ;
m password toggle view . set padding ( m edit text . get padding left ( ) , m edit text . get padding top ( ) , m edit text . get padding right ( ) , m edit text . get padding bottom ( ) ) ;
if ( connector . get allow trace ( ) & & req . method ( ) . equals ignore case ( trace ) ) { wrapper wrapper = request . get wrapper ( ) ; string header = null ; if ( wrapper = null ) { string [ ] methods = wrapper . get servlet methods ( ) ; if ( methods = null ) { for ( int i = 0 ; i < methods . length ; i + + ) { if ( trace . equals ( methods [ i ] ) ) { continue ; } if ( header = = null ) { header = methods [ i ] ; } else { header + = , + methods [ i ] ; } } } } res . set status ( 405 ) ; res . add header ( allow , header ) ; res . set message ( trace method is not allowed ) ; request . get context ( ) . log access ( request , response , 0 , true ) ; return false ; } do connector authentication authorization ( req , request ) ;
contact details = contact info op set . get details ( contact , birth date detail . class ) ; string birth date detail = ;
. add all ( classpaths . get non classpath deps ( ) )
builder params . set cert path checkers ( params . get pkixparameters ( ) . get cert path checkers ( ) ) ; builder params . set sig provider ( params . sig provider ( ) ) ;
service block packet . skip bits ( 16 ) ; } else { handle set pen attributes ( ) ; } break ; case command _ spc : if ( current cue builder . is defined ( ) ) {
byte buffer header = gen . generate header bytes ( frame ) ; byte buffer payload = frame . get payload ( ) ; out . put ( header ) ; out . put ( payload ) ;
shader = g . get poly shader ( g . lights , false ) ; shader . bind ( ) ; rendering fill = false ;
set new member name ( member , new name ) ; }
for ( string local : maps [ tip . get id within job ( ) ] . get split locations ( ) ) { node datanode = jobtracker . get node ( local ) ; int new level = this . max level ; if ( tracker = null & & datanode = null ) { new level = get matching level for nodes ( tracker , datanode ) ; } if ( new level < level ) { level = new level ; an optimization if ( level = = 0 ) { break ; } } } return level ;
stax parser util . validate ( start element , assertion ) ;
int pos = buffer util . flip to fill ( header ) ; try {
final int w = m float view . get width ( ) ; final int h = m float view . get height ( ) ; int x = m float loc . x ;
list < hregion info > regions = admin . get table regions ( table _ name ) ; collections . sort ( regions , new comparator < hregion info > ( ) { public int compare ( hregion info r1 , hregion info r2 ) { return bytes . compare to ( r1 . get start key ( ) , r2 . get start key ( ) ) ; } } ) ; int num regions = admin . get table regions ( table _ name ) . size ( ) ; int num regions after merge = num regions - 2 ;
int data ;
use configuration ( - - fdo _ optimize = does not exist ) ;
alt result = ctx . digest ( ) ; }
class < ? > actual arg type = am . get raw parameter type ( 0 ) ;
return new array list ( logger _ names ) ; }
plugin implementation lookup ; if ( plugin id . get namespace ( ) = = null ) { plugin id qualified = plugin id . with namespace ( default plugin manager . core _ plugin _ namespace ) ; lookup = unchecked get ( id mappings , new plugin id lookup cache key ( qualified , class loader ) ) . or null ( ) ; if ( lookup = null ) { return lookup ; } } return unchecked get ( id mappings , new plugin id lookup cache key ( plugin id , class loader ) ) . or null ( ) ;
recognizer . other ( c ) ;
if ( assume _ check . matches ( tree , state ) ) { return no _ match ; } return make description ( assume , tree ) ;
expression e = javascript compiler . compile ( sqrt ( _ score ) + ln ( popularity ) ) ; simple bindings bindings = new simple bindings ( ) ; bindings . add ( new sort field ( popularity , sort field . type . int ) ) ; bindings . add ( new sort field ( _ score , sort field . type . score ) ) ; rescorer rescorer = e . get rescorer ( bindings ) ; hits = rescorer . rescore ( searcher , hits , 10 ) ;
response = get as servlet response ( rest oseo collections sentinel2 layer ) ;
annotated field field = _ fields . value ; linked < annotated field > next = _ fields . next ; for ( ; next = null ; next = next . next ) { annotated field next field = next . value ; class < ? > field class = field . get declaring class ( ) ; class < ? > next class = next field . get declaring class ( ) ; if ( field class = next class ) { if ( field class . is assignable from ( next class ) ) { next is more specific field = next field ; continue ; } if ( next class . is assignable from ( field class ) ) { getter more specific continue ; } } throw new illegal argument exception ( multiple fields representing property \ + get name ( ) + \ : + field . get full name ( ) + vs + next field . get full name ( ) ) ; } return field ;
throw new illegal state exception ( can ' t collect more than [ + integer . max _ value + ] ids ) ;
string defaults descriptor = context . get defaults descriptor ( ) ; if ( defaults descriptor = null & & defaults descriptor . length ( ) > 0 ) { resource dft resource = resource . new system resource ( defaults descriptor ) ; if ( dft resource = = null ) dft resource = context . new resource ( defaults descriptor ) ; context . get meta data ( ) . set defaults ( dft resource ) ; }
final file root dir io file = new file ( root path . to uri ( ) . get path ( ) ) ;
items . stream ( ) . for each ( item - > scheduler . execute ( ( ) - > pool . put ( item ) ) ) ; shutdown callback . get ( shutdown _ timeout , time _ unit ) ;
throughput threshold pages = - 1 ;
vt = client . call procedure ( @ ad hoc , select sum ( v _ sum _ age ) , min ( v _ min _ age ) , max ( v _ max _ rent ) , + sum ( v _ count _ rent ) from + tb + where v _ g1 > 10 ; ) . get results ( ) [ 0 ] ; validate table of longs ( vt , new long [ ] [ ] { { 154 , 22 , 14 , 6 } } ) ; vt = client . call procedure ( @ ad hoc , select v _ g1 , sum ( v _ sum _ age ) , min ( v _ min _ age ) , max ( v _ max _ rent ) , + sum ( v _ count _ rent ) from + tb + where v _ g1 > 10 group by v _ g1 order by 1 ; ) . get results ( ) [ 0 ] ; validate table of longs ( vt , new long [ ] [ ] { { 20 , 45 , 22 , 7 , 2 } , { 30 , 109 , 24 , 14 , 4 } } ) ; vt = client . call procedure ( @ ad hoc , select v _ g1 , sum ( v _ sum _ age ) , min ( v _ min _ age ) , max ( v _ max _ rent ) , + sum ( v _ count _ rent ) from + tb + where v _ g2 > 1 group by v _ g1 order by 1 ; ) . get results ( ) [ 0 ] ;
message out = exchange . get out ( ) ; out . set body ( result . to string ( ) ) ; out . set headers ( exchange . get in ( ) . get headers ( ) ) ; out . set attachments ( exchange . get in ( ) . get attachments ( ) ) ; }
list < user session model > loaded sessions = load persisted sessions paginated ( true , 10 , 1 , 2 ) ;
assert equals ( gs . get settings ( cite workspace ) . get metadata ( ) . get ( restutils . root _ key , string . class ) , root ) ;
header symlink tree symlink tree = cxx preprocessables . create header symlink tree build rule ( target , filesystem , root , links , header mode . symlink _ tree _ only ) ;
try { when cache manager . get object persister ( string . class ) ; fail ( no data class persistence manager should have been found as none had been registered ) ; } catch ( exception ex ) { then assert true ( true ) ; }
if ( fval = null ) { list < indexable field > fields = new array list < > ( 2 ) ; fields . add ( fval ) ; fields . add ( docval ) ; return fields ; } fval = docval ; }
for ( on . handle sub : this . subs ) { sub . destroy ( ) ; } this . subs = null ; }
key gen . init ( sym _ keylength ) ;
for ( int i = 0 ; i < 2 ; i + + ) { assert . assert true ( waiting for server + i + being up , client base . wait for server up ( 127 . 0 . 0 . 1 : + client ports [ i ] , connection _ timeout * 2 ) ) ; }
final method permissions meta data wild card method permissions = assembly descriptor . get method permissions by ejb name ( * ) ; if ( wild card method permissions = null ) { handle method permissions ( component description , wild card method permissions ) ; }
int c = f current entity . ch [ f current entity . position ] ;
assert equals ( cn0 _ 1 . get node ( ) , n1 ) ;
new peeking iterator tester < t > ( list ) . test ( ) ;
assert equals ( 127 , bs . next set bit ( 127 ) ) ;
rpc manager rm2 = testing util . extract component ( backup owner cache , rpc manager . class ) ; controlled rpc manager crm2 = new controlled rpc manager ( rm2 ) ;
data collector . cancel ( ) ;
if ( debug ) dprint ( new _ context ( ) ) ; naming context data store impl = ( naming context data store ) this ; synchronized ( impl ) { return impl . new context ( ) ; } }
assert that ( map ) . contains exactly ( key , value , ( object ) args ) ; }
assert that ( map ) . contains exactly ( key , value , ( object ) args ) ; }
regex + = ) ;
parser = new geo json parser ( invalid geometry invalid coordinates string ( ) ) ;
remainder = if starts with return remainder ( skip ( , function ) ;
qjm = create spying qjm ( ) ; spies = qjm . get logger set for tests ( ) . get loggers for tests ( ) ; future throws ( new ioexception ( mock failure ) ) . when ( spies . get ( 1 ) ) . accept recovery ( mockito . < segment state proto > any ( ) , mockito . < url > any ( ) ) ; future throws ( new ioexception ( mock failure ) ) . when ( spies . get ( 2 ) ) . accept recovery ( mockito . < segment state proto > any ( ) , mockito . < url > any ( ) ) ; try { qjm . recover unfinalized segments ( ) ; fail ( should have failed to recover ) ; } catch ( quorum exception qe ) { generic test utils . assert exception contains ( mock failure , qe ) ; } finally { qjm . close ( ) ; }
get capabilities ( ) . test with fail ( data ) ; m _ train instances = data ;
entity manager em = get entity manager ( ) ;
nd4j . get random ( ) . set seed ( 123 ) ; indarray weights expected = nd4j . randn ( ' f ' , shape ) . muli ( fast math . sqrt ( 2 . 0 fan in ) ) ; assert equals ( weights expected , weights actual ) ;
if ( next panel = = null ) throw new wizard panel not found exception ( ) ; wizard page old panel = current panel ;
final roster entry added entry = roster . get entry ( contact jid ) ;
throw first action execution exception ; } throw new action execution exception ( first action execution exception . get message ( ) , first action execution exception . get cause ( ) , action , root causes . build ( ) , catastrophe , first action execution exception . get exit code ( ) ) ; }
for ( int i = 0 ; i < 5 ; i + + ) { thread . sleep ( 100 ) ; if ( start block count = cache . get block count ( ) | | start block hits = cache . get stats ( ) . get hit count ( ) | | start block miss = cache . get stats ( ) . get miss count ( ) ) { start block count = cache . get block count ( ) ; start block hits = cache . get stats ( ) . get hit count ( ) ; start block miss = cache . get stats ( ) . get miss count ( ) ; i = - 1 ; } }
map join persistable table container table container = map join tables [ tag ] ;
tester . groups ( ) . assert that user is only member of ( null , user . get login ( ) ) ; }
remove interceptor which is out the phases ( server . get endpoint ( ) . get service ( ) . get in interceptors ( ) , remaining _ in _ phases , get in interceptor names ( ) ) ;
add to replicas map ( volume map , finalized dir , lazy write replica map , true ) ;
connection . set token bytes ( null ) ;
return rotate left ( value * 0x c2 b2 ae3 d27 d4 eb4 fl , 31 ) * 0x9 e3779 b185 ebca87 l ;
log . log ( level . info , potential trouble determining caller of reflective method , e ) ; } }
v translate . x = ( get width ( ) 2 ) - ( scale * ( s width ( ) 2 ) ) ;
verify delegation token renew ( null , bar ) ;
string builder result = new string builder ( 80 ) ; while ( true ) { int c = in . read ( ) ; if ( c = = - 1 ) { throw new eofexception ( ) ; } else if ( c = = ' \ n ' ) { break ; } result . append ( ( char ) c ) ; } int length = result . length ( ) ;
log . info ( setting up app master command ) ;
set properties ( options , parameters ) ;
feature collection = get collection by parent identifier ( parent id ) ;
bus handler . push expected events ( next event . invoice , next event . payment , next event . invoice _ payment ) ; clock . add days ( 1 ) ; assert listener status ( ) ;
if ( phi < 2 * database descriptor . get phi convict threshold ( ) | | parent repair sessions . is empty ( ) ) return ; set < uuid > to remove = new hash set < > ( ) ;
iterator < integer > iter = set . iterator ( ) ; assert false ( iter . has next ( ) ) ; log . info ( test clear - done ) ;
if ( node . is visible to user ( ) ) { return false ; }
while ( projector calibrator . get image count ( ) % camera calibrators . length < camera number ) { projector calibrator . wait ( ) ; } projector calibrator . add markers ( imaged projector markers , prewrapped proj markers ) ; projector calibrator . notify ( ) ; }
incomplete reduce tasks = total reduce tasks ;
final char sequence title = text utils . ellipsize ( m text , m title paint , available width , text utils . truncate at . end ) ;
m _ sons = null ;
min index = math . min ( min index , r ) ; max index = math . max ( max index , r ) ; }
if ( x . is string ) return x ; for ( int c = 0 ; c < r ; c + + ) if ( x . next [ c ] = null ) return x ; return null ;
return collections . singleton list ( named op ) ;
entity manager . get entity manager factory ( ) . get cache ( ) . evict all ( ) ; }
return ( int ) ( size * 1024 ) ;
while ( rev it . has previous ( ) ) { rev it . previous ( ) ; rev it . remove ( ) ; } return immutable list . copy of ( line ) ; } } } }
languages . add ( model ) ; } }
trust manager factory tmfactory = trust manager factory . get instance ( trust manager factory . get default algorithm ( ) ) ;
assert . assert equals ( navigate url , listener . navigate complete2 string ) ; listener . navigate complete2 called = false ; listener . navigate complete2 string = null ;
{ null content as empty < string [ ] > result = mapper . read value ( json , new type reference < null content as empty < string [ ] > > ( ) { } ) ; assert equals ( 1 , result . values . length ) ; assert equals ( , result . values [ 0 ] ) ; }
user1 . run as ( new privileged exception action < void > ( ) { @ override public void run ( ) throws exception { try ( connection connection = connection factory . create connection ( conf ) ) { try ( table t = connection . get table ( test _ table . get table name ( ) ) ) { increment inc = new increment ( test _ row1 ) ; inc . set time range ( 0 , 123 ) ; inc . add column ( test _ family1 , test _ q1 , 2 l ) ; t . increment ( inc ) ; t . increment column value ( test _ row1 , test _ family1 , test _ q2 , 1 l ) ; } } return null ; } } ) ; verify user denied for increment multiple versions ( user2 , test _ row1 , test _ q2 ) ;
test types ( * * \ n + * @ constructor \ n + * @ struct \ n + * \ n + function square ( side ) { \ n + this . side = side ; \ n + } \ n + square . prototype = * * @ struct * { \ n + area : function ( ) { return this . side * this . side ; } \ n + } ; \ n + square . prototype . id = function ( x ) { return x ; } ; ) ;
if ( ( ex . get cause ( ) instanceof assertion error ) ) { throw ex ; }
update j ( json ( [ { ' id ' : ' 2 ' , ' big _ decimal _ tf ' : 100000000000000000000000000001234567890 . 0987654321 } ] ) , params ( commit , true ) ) ;
update ui ( false ) ; } }
this . message typeface style = parcel . read int ( ) ;
byte [ ] ecd1 = epki . get encrypted data ( ) ; byte [ ] ecd2 = epki . get encrypted data ( ) ; assert not same ( encrypted private key info data . encrypted data , ecd1 ) ; assert not same ( encrypted private key info data . encrypted data , ecd2 ) ; assert not same ( ecd1 , ecd2 ) ; performed = true ; } catch ( no such algorithm exception allowed failure ) { } }
list results ;
public shape values source make shape value source ( ) { return new bbox value source ( this ) ;
string queue = conf . get ( mrjob config . queue _ name , job conf . default _ queue _ name ) ;
throw new internal error ( md5 not supported on this platform ) ;
assert that ( actual flags ) . overriding error message ( expected flags < % s > but was < % s > . , flags to string ( flags ) , flags to string ( actual flags ) ) . is equal to ( flags ) ; return myself ;
for ( int i = data . size ( ) - 1 ; i > = 0 ; i - - ) { final app info application info = data . get ( i ) ; final component name component = application info . intent . get component ( ) ; if ( user . equals ( application info . user ) & & package name . equals ( component . get package name ( ) ) ) { if ( find activity ( matches , component ) ) { removed . add ( application info ) ; data . remove ( i ) ; } } }
unsafe parts . add ( p ) ; } }
log . info ( error trying to remove + to remove + from + this . get class ( ) . get simple name ( ) , e ) ;
int gname = cpg . add interface methodref ( dom _ intf , get node name , ( i ) ljava lang string ; ) ;
if ( arrays . equals ( limits , product . limits ) ) { return false ; } return catalog name = null ? catalog name . equals ( product . catalog name ) : product . catalog name = = null ;
root class . set declared version ( prop ) ;
virtual table vt = new virtual table ( vt name , virtual table ) ;
alpha = ( int ) ( ( 1 - ratio ) * 255 ) ;
if ( http message = = null ) { return null ; }
return coverage results . to array ( new grid coverage2 d [ ] { } ) ; }
if ( template meta data = null ) { md builder . put ( update open close on partition template ( template meta data , false ) ) ; }
f listeners = new array list ( ) ;
if ( last tx id = fsedit log loader . txid _ ignore & & last tx id > avatar node . get last written tx id ( ) ) { string msg = standby : quiescing - standby could not successfully ingest the edits up to : + last tx id + , last consumed txid : + avatar node . get last written tx id ( ) ; log . fatal ( msg ) ; throw new standby state exception ( msg ) ; } log . info ( standby : quiescing ingest - consumed transactions up to : + avatar node . get last written tx id ( ) ) ; return current segment tx id ;
assert true ( new caddress ( 0x876543ab l ) . to hex string ( ) . equals ignore case ( 876543ab ) ) ;
result = process engine configuration . get command executor ( ) . execute ( new get variable names command ( sub process task . get execution id ( ) , false ) ) ;
init code . append ( original code . substring ( last added , d . get start ( ) ) ) ;
int new capacity = alloc ( ) . calculate new capacity ( writer index + min writable bytes , max capacity ) ;
for ( int i = fields . length - 1 ; i > = 0 ; i - - ) { if ( name . equals ( fields [ i ] . get name ( ) ) ) { return fields [ i ] ; } }
classifier zero r = new weka . classifiers . rules . zero r ( ) ;
return null ; } else if ( len > 0 ) {
method method = klass . get method ( make context , map . class ) ; log . debug ( object : + obj + method : + method ) ; transport = ( icontext ) method . invoke ( obj , storm _ conf ) ; } log . info ( transport factory make context done . . . ) ; } catch ( exception e ) {
write file ( fs , my file , 10 ) ;
store . rebuild id generator ( ) ;
test plan follower ( false ) ;
object store . drop database ( db name1 ) ;
out . println ( < a href = \ . . reqparams . html \ > ) ; out . println ( < img src = \ . . images code . gif \ height = 24 + width = 24 align = right border = 0 alt = \ view code \ > < a > ) ; out . println ( < a href = \ . . index . html \ > ) ; out . println ( < img src = \ . . images return . gif \ height = 24 + width = 24 align = right border = 0 alt = \ return \ > < a > ) ; out . println ( < h3 > + title + < h3 > ) ; string first name = request . get parameter ( firstname ) ;
pb buf . position ( limit ) ;
assume true ( job api . v _ 26 . is supported ( instrumentation registry . get target context ( ) ) ) ; job config . force api ( job api . v _ 26 ) ; int job id = schedule job ( ) ;
return ( statement inspector ) interceptor : : on prepare statement ; }
line + = newline ;
list . add ( create add operation ( path address . path address ( extension , org . jboss . as . jacorb ) ) ) ;
assert equals ( added module ; changed image base ; removed module ; , m _ listener . events ) ;
buffered bytes = 0 ;
discovery nodes new nodes = discovery nodes . builder ( state . nodes ( ) ) . add ( discovery node ) . build ( ) ;
typed array a = context . obtain styled attributes ( attrs , r . styleable . line page indicator , def style , 0 ) ; m centered = a . get boolean ( r . styleable . line page indicator _ centered , default centered ) ; m line width = a . get dimension ( r . styleable . line page indicator _ line width , default line width ) ; m gap width = a . get dimension ( r . styleable . line page indicator _ gap width , default gap width ) ; set stroke width ( a . get dimension ( r . styleable . line page indicator _ stroke width , default stroke width ) ) ; m paint unselected . set color ( a . get color ( r . styleable . line page indicator _ unselected color , default unselected color ) ) ;
if ( is selected ) return image utils . get scaled rounded icon ( avatar bytes , width , height ) ;
mgr = new mock node label manager ( ) ;
string main file name = null ;
if ( user buf len < = 0 ) { return true ; } else { set input from saved data ( ) ; } }
for ( blob store listener listener : listeners . get listeners ( ) ) { delegate . add listener ( listener ) ; }
find iterable < document > iterable = db . get collection ( restaurants ) . find ( new document ( grades . score , new document ( gt , 30 ) ) ) ;
create week days cells ( ) ;
file writer out = new file writer ( config _ file _ path ) ; out . write ( < ?xml version = \ 1 . 0 \ ? > \ n ) ; out . write ( < configuration > \ n ) ; out . close ( ) ; config manager config manager = new config manager ( types , conf ) ;
assert equals ( total list , p list ) ;
annotation visitor . visit annotation ( clazz , method , code attribute , annotations [ index ] ) ;
tree . add field path ( bar . quz ) ; assert equals ( bar . baz , bar . quz , foo , tree . to string ( ) ) ;
explicit failover ( ) ;
config . set ( dfs . hosts . exclude , { hadoop . tmp . dir } dfs hosts exclude ) ; file exclude file = new file ( config . get ( dfs . hosts . exclude , exclude ) ) ; if ( exclude file . exists ( ) ) { if ( exclude file . get parent file ( ) . mkdirs ( ) ) throw new ioexception ( nnthroughput benchmark : cannot mkdir + exclude file ) ; } new file output stream ( exclude file ) . close ( ) ;
assert true ( docs and positions enum . start offset ( ) = = - 1 | | docs and positions enum . start offset ( ) = = 0 ) ;
if ( result width = = view width ) { result x = 0 ; result y = ( int ) math . round ( ( view height - result height ) 2 ) ; } else if ( result height = = view height ) { result x = ( int ) math . round ( ( view width - result width ) 2 ) ; result y = 0 ; } else { result x = ( int ) math . round ( ( view width - result width ) 2 ) ; result y = ( int ) math . round ( ( view height - result height ) 2 ) ; } final rect result = new rect ( result x , result y , result x + ( int ) math . ceil ( result width ) , result y + ( int ) math . ceil ( result height ) ) ;
try { zip file zf = new zip file ( file ) ; string slashname = rtype . get slashed name ( ) + . class ; zip entry ze = zf . get entry ( slashname ) ; long lmt = ze . get time ( ) ; last modified time ( ) . to millis ( ) ; jar entry je = new jar entry ( rtype , slashname , lmt ) ; zf . close ( ) ; set < jar entry > jar entries = watched jar contents . get ( file ) ; if ( jar entries = = null ) { jar entries = new hash set < jar entry > ( ) ; watched jar contents . put ( file , jar entries ) ; } jar entries . add ( je ) ; if ( global configuration . is runtime logging & & log . is loggable ( level . info ) ) { log . info ( watching jar file entry . jar = + file + file = + rtype . get slashed name ( ) + lmt = + lmt ) ; } } catch ( ioexception e ) { e . print stack trace ( ) ; }
x509 cert impl impl certs [ ] = new x509 cert impl [ certificates . length ] ;
log . info ( starting checkpoint manager server ) ;
if ( logger . is debug enabled ( ) ) logger . debug ( query _ log _ event has unknown status vars ( first has code : + code + ) , skipping the rest of them ) ;
assert q ( req ( def type , edismax , q , myalias : zapp ) , nor ) ; assert q ( req ( def type , edismax , q , myalias : zapp , f . myalias . qf , name ) , oner ) ;
int count = 0 ; final bytes ref fstenum < object > fst enum = new bytes ref fstenum < > ( fst ) ; while ( fst enum . next ( ) = null ) { count + + ; } assert equals ( 1 , count ) ; assert not null ( util . get ( fst , new bytes ref ( str ) ) ) ;
int depth = 1 + gd . get max ( ) . get z ( ) - gd . get min ( ) . get z ( ) ; int width = 1 + gd . get max ( ) . get x ( ) - gd . get min ( ) . get x ( ) ; int ind = ( pos . get x ( ) - gd . get min ( ) . get x ( ) ) + ( pos . get z ( ) - gd . get min ( ) . get z ( ) ) * width + ( pos . get y ( ) - gd . get min ( ) . get y ( ) ) * width * depth ; return ind ; }
final set < validation mode > modes = validation mode . get modes ( cfg service . get settings ( ) . get ( mode _ property ) ) ; if ( modes . size ( ) > 1 ) { log . multiple validation modes ( validation mode . loggable ( modes ) ) ; } if ( modes . size ( ) = = 1 & & modes . contains ( validation mode . none ) ) { we have nothing to do ; just return return ; } final class loader service class loader service = service registry . get service ( class loader service . class ) ;
final data data shadow = this . data ; byte [ ] pos = data shadow . send mtfvalues2 _ pos ; for ( int i = n groups ; - - i > = 0 ; ) { pos [ i ] = ( byte ) i ; } for ( int i = 0 ; i < n selectors ; i + + ) { final byte ll _ i = data shadow . selector [ i ] ; byte tmp = pos [ 0 ] ; int j = 0 ; while ( ll _ i = tmp ) { j + + ; byte tmp2 = tmp ; tmp = pos [ j ] ; pos [ j ] = tmp2 ; } pos [ 0 ] = tmp ; data shadow . selector mtf [ i ] = ( byte ) j ; }
input stream is = qjm . get image input stream ( start tx id + iteration ) . get input stream ( ) ; byte [ ] contents = new byte [ written . length ] ; is . read ( contents ) ; assert true ( arrays . equals ( written , contents ) ) ; return hash ; }
final list < state > all states = new array list < state > ( ) ; final state initial = new state ( ( byte ) 0 , ) ; for ( string value : original items ) { add states ( initial , value , all states ) ; }
channel future connect = bootstrap . connect ( host , port ) ; channel channel ; if ( connect . await ( timeout . to millis ( ) , time unit . milliseconds ) ) { channel = connect . channel ( ) ; } else { throw new timeout exception ( connection failed ) ; } channel . write and flush ( request ) ;
driver manager . register driver ( new fake derby ( ) ) ;
if ( e . get message ( ) . starts with ( invalid global max attempts configuration ) ) throw e ;
html style text watcher watcher = new html style text watcher ( ) ; spannable content = new spannable string builder ( < b > stuff < b > ) ; watcher . update spans ( content , new html style text watcher . span range ( 0 , 3 ) ) ; assert equals ( 1 , content . get spans ( 0 , 3 , foreground color span . class ) . length ) ;
break ; case chargeback : try { invoice api . record chargeback reversal ( payment control context . get payment id ( ) , payment control context . get transaction external key ( ) , internal context ) ; } catch ( final invoice api exception e ) { log . warn ( on failure call failed for attempt id = ' { } ' , transaction type = ' { } ' , payment control context . get attempt payment id ( ) , transaction type , e ) ; } break ; default : throw new illegal state exception ( unexpected transaction type + transaction type ) ;
log . info ( difference to reproducible mode : + math . abs ( mean - repro _ error ) stddev + standard deviations ) ;
models . add ( model ) ; this line = next line ; next line = get next non empty line ( scanner , count ) ; }
final solr client client = get solr client ( ) ; final map < string , string > query param map = new hash map < string , string > ( ) ; query param map . put ( q , * : * ) ; query param map . put ( fl , id , name ) ;
final alert alert = alert service . factory ( result ) ; alert service . save ( alert ) ; alert notifications sender . send ( result , stream , alert , alert condition ) ;
assert equals ( blocksize , blocks . get file length ( ) ) ; assert equals ( 1 , block list . size ( ) ) ;
assert buffer result equals ( types , get future ( future , no _ wait ) , empty results ( task _ instance _ id , 1 , true ) ) ;
responses responses = fetch responses from discovery protocol ( collections . singleton list ( dest ) ) ;
int dot = prefix . last index of ( ' . ' ) ; prefix = dot = - 1 ? prefix . substring ( 0 , dot ) : ; } t result = map . get ( name ) ; if ( result = null ) return result ; }
jar _ = package job jar ( ) ; if ( jar _ = null ) { job conf _ . set jar ( jar _ ) ; } if ( ( cache archives = null ) | | ( cache files = null ) ) { get uris ( cache archives , cache files ) ; boolean b = distributed cache . check uris ( file uris , archive uris ) ; if ( b ) fail ( link _ uri ) ; }
int longest steps count = list . m _ step count ;
if ( view = null & & view instanceof empty view ) { return view ; } view = new empty view ( get context ( ) ) ; view . set layout params ( new linear layout . layout params ( layout params . match _ parent , layout params . wrap _ content ) ) ; return view ;
exec utils . add argument ( cmd , command line utils . get first segment of command ( ) ) ;
do collection = true ; }
mock endpoint result = get mock endpoint ( mock : result ) ; result . expected message count ( 1 ) ; result . message ( 0 ) . body ( ) . is instance of ( list . class ) ; list < string > body = new array list < string > ( ) ;
hystrix plugins . get instance ( ) ; system . set property ( hystrix . plugin . hystrix dynamic properties . implementation , com . netflix . hystrix . strategy . properties . hystrix dynamic properties system properties ) ; hystrix plugins plugins = setup mock service loader ( ) ; assert true ( plugins . get dynamic properties ( ) instanceof hystrix dynamic properties system properties ) ; hystrix dynamic properties p = plugins . get dynamic properties ( ) ;
return super . get compile include path ( ) . plus ( get file operations ( ) . files ( new callable < file collection > ( ) { @ override public file collection call ( ) throws exception { cpp component tested = tested component . get or null ( ) ; if ( tested = = null ) { return get file operations ( ) . files ( ) ; } return ( ( default cpp component ) tested ) . get all header dirs ( ) ; } } ) ) ;
top docs hits = searcher . search ( query , 10 ) ; assert equals ( 3 , hits . total hits ) ; assert equals ( 3 , r . document ( hits . score docs [ 0 ] . doc ) . get ( id ) ) ; assert equals ( 1 , r . document ( hits . score docs [ 1 ] . doc ) . get ( id ) ) ; assert equals ( 2 , r . document ( hits . score docs [ 2 ] . doc ) . get ( id ) ) ; double values source source = double values source . from long field ( popularity ) ;
consistency level . assure sufficient live nodes ( keyspace , target replicas ) ; if ( repair decision = read repair decision . none ) { tracing . trace ( read - repair { } , repair decision ) ; read repair metrics . attempted . mark ( ) ; }
int modulus length bits = ( ( rsakey ) signing key ) . get modulus ( ) . bit length ( ) ;
first value buffer = buffer . slice ( ) ;
inode node = dir . get file inode ( path ) ; if ( node = null & & node . is under construction ( ) ) { paths to save + + ; } else if ( node = = null ) {
override component name = model . get artifact id ( ) . replace ( camel - , ) ; } create component configuration source ( pkg , model , override component name ) ; create component auto configuration source ( pkg , model , aliases , override component name ) ; create component spring factory source ( pkg , model ) ; } }
object key = new object ( ) ; int hash = map . hash ( key ) ; object old value = new object ( ) ; object new value = new object ( ) ; atomic reference array < reference entry < object , object > > table = segment . table ; int index = hash & ( table . length ( ) - 1 ) ; dummy entry < object , object > entry = dummy entry . create ( key , hash , null ) ;
byte buffer buffer = buffer pool . get ( size ) ;
return ( inet address [ ] ) cached result ;
for ( int i = pattern parts . length ; i < path parts . length ; i + + ) { if ( puts > 0 | | i > 0 ) { builder . append ( this . path separator ) ; } builder . append ( path parts [ i ] ) ; } return builder . to string ( ) ;
list < item bo > deleted items = parse deleted items ( items , release items ) ; item bos . add all ( deleted items ) ; modified item cnt + = deleted items . size ( ) ; namespace bo . set item modified cnt ( modified item cnt ) ;
ioutils . cleanup ( log , file sys ) ; cluster . restart name node ( ) ; file sys = cluster . get file system ( ) ; assert false ( file sys . get acl status ( dir1 ) . get entries ( ) . is empty ( ) ) ; assert false ( file sys . get acl status ( dir2 ) . get entries ( ) . is empty ( ) ) ; assert false ( file sys . get acl status ( file1 ) . get entries ( ) . is empty ( ) ) ; assert true ( file sys . get acl status ( dir3 ) . get entries ( ) . is empty ( ) ) ; assert true ( file sys . get acl status ( file2 ) . get entries ( ) . is empty ( ) ) ; } finally {
if ( scheduling mode = = scheduling mode . respect _ partition _ exclusivity & & accessible to partition ( ps . get partition ( ) ) ) { if ( log . is debug enabled ( ) ) { log . debug ( skip this queue = + get queue path ( ) + , because it is not able to access partition = + ps . get partition ( ) ) ; } activities logger . queue . record queue activity ( activities manager , node , get parent name ( ) , get queue name ( ) , activity state . rejected , activity diagnostic constant . not _ able _ to _ access _ partition + node . get partition ( ) ) ; if ( root queue ) { activities logger . node . finish skipped node allocation ( activities manager , node ) ; } return csassignment . null _ assignment ; }
field refs . add ( mocked struct field ( first , type name 1 , this is a comment ) ) ;
log . debug ( { } another pod ( { } ) is the current leader and it is still active , log prefix ( ) , this . latest leader info . get leader ( ) ) ; return false ;
log . debug ( fs state after snapshot : ) ; util . get hbase cluster ( ) . get master ( ) . get master file system ( ) . log file system state ( log ) ; snapshot testing utils . confirm snapshot valid ( util , protobuf util . create hbase protos snapshot desc ( snapshots . get ( 0 ) ) , table _ name , test _ fam ) ;
set up child ( child ) ; return child ;
byte txt [ ] = new byte [ 100 * 1024 ] ;
if ( service . pid . equals ( key ) ) { continue ; } matcher matcher = extract _ plugwise _ config _ pattern . matcher ( key ) ;
find row ( mr , ' c ' , 44 , - 1 ) ; find row ( mr , ' c ' , 45 , - 1 ) ; find row ( mr , ' c ' , 46 , - 1 ) ; find row ( mr , ' c ' , 43 , - 1 ) ; mr . flush ( true ) ; find row ( mr , ' c ' , 44 , - 1 ) ; find row ( mr , ' c ' , 45 , - 1 ) ; find row ( mr , ' c ' , 46 , - 1 ) ; find row ( mr , ' c ' , 43 , - 1 ) ; } finally {
i = new input source ( ( input stream ) null ) ;
long start ts = system . nano time ( ) ; check ( sequence [ 13 ] , buffer . get next non blocked ( ) ) ; check ( sequence [ 14 ] , buffer . get next non blocked ( ) ) ; check ( sequence [ 18 ] , buffer . get next non blocked ( ) ) ; check ( sequence [ 19 ] , buffer . get next non blocked ( ) ) ; validate alignment time ( start ts , buffer ) ;
em . get transaction ( ) . begin ( ) ; lukasz = em . find ( person . class , lukasz . get id ( ) ) ; lukasz . set name ( lucas ) ; em . merge ( lukasz ) ; em . get transaction ( ) . commit ( ) ;
assert true ( arrays . equals ( data , source . read ( ) ) ) ;
if ( spdy syn reply frame . is invalid ( ) | | is remote initiated id ( stream id ) | | spdy session . is remote side closed ( stream id ) ) { issue stream error ( ctx , e . get remote address ( ) , stream id , spdy stream status . invalid _ stream ) ; return ; }
sch . configure blocking ( true ) ; sslcontext ssl context = link . init sslcontext ( true ) ;
for ( int i = 0 ; i < poly lats . length - 1 ; i + + ) { final int index = poly lats . length - 2 - i ; points . add ( new geo point ( planet model . wgs84 , from degrees ( poly lats [ index ] ) , from degrees ( poly lons [ index ] ) ) ) ; }
parent . mark cancelled ( ) ; internal dispose empty shared slot ( parent ) ; } } }
last restart time = switch to running timestamp - restarting timestamp ;
if ( visibility = = gone | | visibility = = invisible ) { stop animation ( ) ; } else { start animation ( ) ; } }
if ( channel factory = null ) { channel factory . release external resources ( ) ; channel factory = null ; }
assert equals ( 1 , reader . read short ( ) ) ; reader . mark ( ) ; assert equals ( 1 , reader . read int ( ) ) ; assert equals ( 1 l , reader . read long ( ) ) ; assert equals ( 1 . 0f , reader . read float ( ) , 0 ) ; assert equals ( 16 , reader . bytes past mark ( null ) ) ;
rc history . remove ( 0 ) ; } rc last snapshot = current snapshot ; } } ; sched handle = scheduler . schedule with fixed delay ( request counter , period , period , time unit . seconds ) ; }
for ( iterator i = params . key set ( ) . iterator ( ) ; i . has next ( ) ; ) { string key = ( string ) i . next ( ) ; object value = find ( info , key ) . get value ( ) ; if ( value = null ) { map . put ( key , value ) ; } } return collections . synchronized map ( map ) ; }
for ( bitmap icon : icons ) { m cmd params . set icon ( icon ) ; } break ;
wait ns . set ( 0 ) ;
create edits ( 20 ) ; h . disabled = false ;
if ( property access exceptions = null ) { property access exception [ ] pae array = property access exceptions . to array ( new property access exception [ property access exceptions . size ( ) ] ) ; throw new property batch update exception ( pae array ) ; }
if ( this . session monitor = null & & this . session monitor . is alive ( ) ) { this . session monitor . interrupt ( ) ; } }
compact equals ( sf create ( 99 , 99 , 99 , 99 , 30 , 26 , 26 , 29 , 25 , 25 ) , 30 , 26 , 26 ) ;
try { mapper . read value ( quote ( value ) , byte [ ] . class ) ; fail ( should not pass ) ; } catch ( mismatched input exception e ) { verify exception ( e , failed to decode ) ; verify exception ( e , as base64 ) ; verify exception ( e , illegal character ' ' ) ; }
helper can inline reference to function ( can inline result . no , function foo ( ) { return ; } ; foo ( ) ; , foo , inline _ direct ) ; }
hdfs . set safe mode ( safe mode action . safemode _ enter , false ) ;
query factory qf = search . get query factory ( remote cache ) ; query query = qf . from ( user pb . class ) . having ( notes ) . like ( % 567 % ) . and ( ) . having ( surname ) . eq ( test surname ) . build ( ) ; list < user > list = query . list ( ) ; assert not null ( list ) ;
logs . d ( more - - - - - ) ; handler handler = new handler ( ) ; handler . post delayed ( new runnable ( ) { public void run ( ) { logs . d ( more - - - - ) ;
vertex1 . get current execution attempt ( ) . fail ( new exception ( test failure ) ) ;
new project . add proxy user ( creator . get user id ( ) ) ; try { update project setting ( new project ) ; } catch ( final project manager exception e ) { e . print stack trace ( ) ; throw e ; } }
bson output . write int32 ( flag position , get flag bits ( ) ) ; } else {
if ( files . exists ( get path ( . watchmanconfig ) ) ) { write contents to path ( { \ ignore _ dirs \ : [ \ buck - out \ , \ . buckd \ ] } , . watchmanconfig ) ; } is set up = true ;
return math . min ( integer . max _ value , ( int ) decorated cache . cache entry set ( ) . stream ( ) . filter ( cache filters . predicate ( tombstone . exclude _ tombstones ) ) . count ( ) ) ;
r . close ( ) ; if ( iter = = 0 ) {
result endpoint . expected bodies received ( body1 , body2 , body3 , body3 ) ; result endpoint . assert is satisfied ( ) ;
assert true ( fs volume impl . get reserved for replicas ( ) = = 1000 ) ;
if ( m cursor = null ) m cursor . close ( ) ;
flog . info ( startup : non - file image managers : ) ; for ( image manager im : non file image managers ) { remote storage state st = im . analyze image storage ( ) ; flog . info ( - > image manager : + im + state : + st . get storage state ( ) ) ; if ( st . get storage state ( ) = = storage state . inconsistent ) { throw ioexception ( image manager has inconsistent state : + im + , state : + st . get storage state ( ) ) ; } remote image states . put ( im , st ) ; } flog . info ( startup : non - file journal managers : ) ;
return ( sign * 0 . 0d ) ; } else if ( power of two > 1025 ) {
final byte [ ] expanded buffer = new byte [ expanded buffer size ] ; system . arraycopy ( buffer , 0 , expanded buffer , 0 , buffer . length ) ; buffer = expanded buffer ; }
int max blocks = 10 ; tree map . entry set ( ) . stream ( ) . flat map ( e - > e . get value ( ) . stream ( ) ) . for each ( e - > { double max delay = e . get ( max delay ) ; if ( max delay < = max blocks & & fee [ 0 ] = = 0 ) fee [ 0 ] = math utils . round double to long ( e . get ( max fee ) ) ; } ) ; fee [ 0 ] = math . min ( math . max ( fee [ 0 ] , fee request service . btc _ min _ tx _ fee ) , fee request service . btc _ max _ tx _ fee ) ; log . info ( fee + fee [ 0 ] ) ; return fee [ 0 ] ;
granule source source = reader . get granules ( name , true ) ;
record = proxy . read from client ( ) ; assert . assert equals ( tlsrecord . type . alert , record . get type ( ) ) ; record = proxy . read from client ( ) ; assert . assert null ( record ) ; server . close ( ) ;
sub . set ( 1 , two ) ; fail ( it should throws concurrent modification exception . ) ; } catch ( concurrent modification exception e ) {
m fragment manager . add on back stack changed listener ( new fragment manager . on back stack changed listener ( ) { public void on back stack changed ( ) { set layout ( ) ; } } ) ; }
final map < uuid , subscription base bundle > bundles per id = new hash map < uuid , subscription base bundle > ( ) ;
annotations directory out . write int ( index map . adjust annotation set ( directory in . read int ( ) ) ) ; }
int length = m items . size ( ) ;
try ( ar archive input stream stream = new ar archive input stream ( new file input stream ( filesystem . resolve ( output ) . to file ( ) ) ) ) { assert that ( stream . get next ar entry ( ) , matchers . null value ( ) ) ; }
get model controller client ( ) . execute ( operations . create add operation ( test _ overlay _ address . append ( deployment , deployment _ name ) . to model node ( ) ) ) ;
string [ ] scientists = { einstein , darwin , copernicus , pasteur , curie , faraday , newton , bohr , galilei , maxwell } ; for ( int i = 1 ; i < = 1000 ; i + + ) { int index = i % scientists . length ; formatter f = new formatter ( ) ; string doc = f . format ( { \ _ id \ : \ % d \ , \ scientist \ : \ % s \ , \ fixed field \ : \ fixed value \ } , i , scientists [ index ] ) . to string ( ) ; iohelper . close ( f ) ; test collection . insert one ( ( basic dbobject ) json . parse ( doc ) ) ; } assert equals ( data pumping of 1000 entries did not complete entirely , 1000 l , test collection . count ( ) ) ; }
if ( offset > ref node . get child nodes ( ) . get length ( ) ) { throw new domexception ( domexception . index _ size _ err , dommessage formatter . format message ( dommessage formatter . dom _ domain , index _ size _ err , null ) ) ; }
conf . set int ( hbase . regionsever . info . port , - 1 ) ;
vectorized row batch batch = new vectorized row batch ( 2 , vectorized row batch . default _ size ) ; bytes column vector v = new bytes column vector ( vectorized row batch . default _ size ) ; batch . cols [ 0 ] = v ; bytes column vector out v = new bytes column vector ( vectorized row batch . default _ size ) ; out v . init buffer ( ) ; batch . cols [ 1 ] = out v ;
{ er _ cant _ get _ snapshot _ length , the method get snapshot length cannot be called on the xpath result of xpath expression ' ' { 0 } ' ' because its xpath result type is { 1 } . this method applies only to types unordered _ node _ snapshot _ type and ordered _ node _ snapshot _ type . } , { er _ non _ iterator _ type , the method iterate next cannot be called on the xpath result of xpath expression ' ' { 0 } ' ' because its xpath result type is { 1 } . this method applies only to types unordered _ node _ iterator _ type and ordered _ node _ iterator _ type . } ,
parse ( rawdata ) ; }
order2 = new order ( ) ;
set < gpu device > assigned resources = null ; if ( resource mappings = null ) { assigned resources = new hash set < > ( ) ; for ( serializable s : resource mappings . get assigned resources ( resource information . gpu _ uri ) ) { assigned resources . add ( ( gpu device ) s ) ; } } if ( assigned resources = = null | | assigned resources . is empty ( ) ) { when no gpu resource assigned , don ' t need to update docker command . return collections . empty set ( ) ; }
from ( direct : upsert sobject ) . to ( salesforce : upsert sobject?s object name = line _ item _ _ c & s object id name = name & raw payload = true & format = + format ) ;
for ( artifact artifact : provider . get transitive extra action artifacts ( ) ) { artifact owner owner = artifact . get artifact owner ( ) ; if ( owner instanceof aspect key ) { if ( aspect classes . contains ( ( ( aspect key ) owner ) . get aspect class ( ) ) ) { artifacts . add ( artifact ) ; } } } return artifacts . build ( ) ;
content values values = new content values ( 1 ) ;
distribution bits = integer . parse int ( lines [ 1 ] ) ; bucket cursor = long . parse long ( lines [ 2 ] ) ; finished bucket count = long . parse long ( lines [ 3 ] ) ; total bucket count = long . parse long ( lines [ 4 ] ) ; if ( total bucket count = = finished bucket count ) { return ; we ' re done here }
if ( value equivalence . is empty ( ) ) { return false ; }
return e ; }
if ( is used ( program class ) ) { mark as used ( program field ) ;
thread local map = inheritable thread locals field . get ( threads [ i ] ) ;
double entailing weight = weight * entailing _ weight ;
if ( diff scale < math utils . long _ powers _ of _ ten . length & & math . max ( this . bit length , subtrahend . bit length + long _ powers _ of _ ten _ bit _ length [ diff scale ] ) + 1 < 64 ) { return value of ( this . small value - subtrahend . small value * math utils . long _ powers _ of _ ten [ diff scale ] , this . scale ) ; } return new big decimal ( this . get unscaled value ( ) . subtract ( multiplication . multiply by ten pow ( subtrahend . get unscaled value ( ) , diff scale ) ) , this . scale ) ; } else { case s2 > s1 : [ u1 * 10 ^ ( s2 - s1 ) - u2 , s2 ]
rich map function < ? , ? > function = new rich map function < tuple2 < string value , int value > , tuple2 < string value , int value > > ( ) { private static final long serial version uid = 1 l ; @ override public tuple2 < string value , int value > map ( tuple2 < string value , int value > value ) throws exception { return null ; } } ;
execute ( update % s set d = ? where a = ? and b = ? and c = ? , null , ( byte ) 1 , ( short ) 2 , 3 ) ;
case 2 : activation activator get active servers { int result [ ] = null ; result = this . get active servers ( ) ; out = rh . create reply ( ) ; com . sun . corba . se . spi . activation . server ids helper . write ( out , result ) ; break ; }
if ( ( distrib phase . toleader . equals ( distrib phase ) ? 0 : max errors ) < known errors . size ( ) ) { note : even if max errors wasn ' t exceeded , we need to throw an error when we have any errors if we ' re a leader that was forwarded to by another node so that the forwarding node knows we encountered some problems and can aggregate the results first err tracker . throw first ( ) ; }
ft = tester . new form tester ( form ) ;
return builder ;
int n distribution params = layer . get output distribution ( ) . distribution input size ( n in ) ; int pxz weight count = decoder layer sizes [ decoder layer sizes . length - 1 ] * n distribution params ; indarray pxz weight view = gradient view . get ( ndarray index . point ( 0 ) , ndarray index . interval ( so far , so far + pxz weight count ) ) ; so far + = pxz weight count ; indarray pxz bias view = gradient view . get ( ndarray index . point ( 0 ) , ndarray index . interval ( so far , so far + n distribution params ) ) ; indarray pxz weights reshaped = create weight matrix ( decoder layer sizes [ decoder layer sizes . length - 1 ] , n distribution params , null , null , pxz weight view , false ) ;
if ( select item = = null & & configuration . is enabled ( ) ) { select item = handler block . tab item ; }
int image size width px = ( int ) ( ( icon _ size * scale ) + 0 . 5 ) ; int image size height px = ( int ) ( ( icon _ size * scale ) + 0 . 5 ) ; container . set layout params ( new linear layout . layout params ( image size width px , image size height px ) ) ;
remove groups ( groups ) ;
g . set color ( header . get background ( ) ) ; g . fill rect ( dragged cell rect . x , dragged cell rect . y , dragged cell rect . width , dragged cell rect . height ) ; paint cell ( g , dragged cell rect , dragged column index ) ; }
get char encoder ( ) ;
now playing intent . set component ( new component name ( com . wm . remusic , com . wm . remusic . activity . playing activity ) ) ; now playing intent . add flags ( intent . flag _ activity _ new _ task ) ; pending intent click intent = pending intent . get broadcast ( this , 0 , now playing intent , pending intent . flag _ update _ current ) ; pending intent click = pending intent . get activity ( this , 0 , now playing intent , pending intent . flag _ update _ current ) ; final bitmap bitmap = image utils . get artwork quick ( this , get album id ( ) , 160 , 160 ) ;
create channel ( new fake name resolver factory ( ) , no _ interceptor ) ;
pub . start publishing ( key ) ; bus . await init ( key , null , bus _ update _ timeout , time unit . seconds ) ;
n = s [ ( l > > 24 ) & 0xff ] ;
res dc0 = resources dc0 resource . find by name ( premium resource ) . get ( 0 ) ; assert . assert equals ( protected ultra premium * , res dc0 . get uri ( ) ) ; atomic integer i = new atomic integer ( 0 ) ;
i = par . go to end of last header ( ) . go to next line ( ) . get pos ( ) ; par = new sip parser ( message ) ;
indent level = new indent level ( get line start ( parent ast ) ) ;
throw new illegal state exception ( ) ; case default :
partition ( 0 ) . assert key available for write ( k0 , - 1 ) ;
log . debugf ( unable to release jdbc result set [ % s ] , e . get message ( ) ) ;
for ( int i = 0 ; i < lg jobs . length ; i + + ) { storage . get full job ( lg jobs [ i ] . get id ( ) ) ; }
uri jdbc uri = uri . create ( uri . substring ( uri _ jdbc _ prefix . length ( ) ) ) ;
if ( profile id = = null | | profile id . length ( ) = = 0 | | ( ( query width = = image request . unspecified _ dimension ) & & ( query height = = image request . unspecified _ dimension ) ) ) { set blank profile picture ( ) ; } else if ( changed | | force ) { send image request ( true ) ; } }
orchestrator . add listener ( listener1 ) ;
x . load new version ( 004 , retrieve rename ( invokespecial . x , invokespecial . x ) ) ;
int offset = field offsets [ field index ] ;
phone number phone number = phone number utils . get phone number ( phone ) ;
boolean test = true ; indent : 8 exp : 8 if ( test ) { indent : 8 exp : 8 system . get property ( foo ) ; indent : 12 exp : 12
test listener . push expected events ( next event . change , next event . cancel , next event . block ) ; final default entitlement changed base entitlement = ( default entitlement ) base entitlement . change plan ( new plan specifier ( assault - rifle , billing period . monthly , price list set . default _ pricelist _ name ) , null , immutable list . < plugin property > of ( ) , call context ) ; assert listener status ( ) ;
if ( modifier . is public ( method . get modifiers ( ) ) ) { return false ; }
throw new illegal argument exception ( cannot create observables from unmanaged realm objects ) ; }
conf . set strings ( yarn configuration . nm _ docker _ allowed _ container _ networks , networks ) ;
try { create dir ( full app dir , appperms , true , user ) ; init app dir status = true ; } catch ( ioexception e ) { log . warn ( unable to create app directory + full app dir . to string ( ) , e ) ; } }
date time date time = new date time ( ms since epoch , date time zone . utc ) ;
package manager pm = m context . get package manager ( ) ; boolean installed = false ; try { pm . get package info ( com . google . android . music , package manager . get _ activities ) ; installed = true ; } catch ( name not found exception e1 ) {
exception ex = em . last exception ;
message < ? > message = message builder . with payload ( invalid value . get bytes ( ) ) . build ( ) ; thrown . expect ( method argument not valid exception . class ) ;
data . m scale = tentative scale ;
final temporary job nginx = jobs . job ( ) . image ( nginx ) . port ( http , 80 , ports . local port ( http ) ) . registration ( nginx , http , http ) . deploy ( ) ;
conn . close ( ) ; try { meta . get jdbcminor version ( ) ; fail ( sqlexception not thrown ) ; } catch ( sqlexception e ) { ok } }
if ( alloc response . get updated containers ( ) = null ) { for ( updated container updated container : alloc response . get updated containers ( ) ) { system . out . println ( got update . . ) ; updated containers . put ( updated container . get container ( ) . get id ( ) , updated container ) ; } } if ( iterations left > 0 ) {
on view ( with id ( r . id . button _ take _ photo ) ) . perform ( click ( ) ) ;
this . last response = ( sipresponse ) rel response ;
assert exists ( out file ) ; }
final operation set connection info op set connection info = protocol provider . get operation set ( operation set connection info . class ) ;
job created job = youtube reporting . jobs ( ) . create ( job ) . execute ( ) ;
if ( ( operation = = folder operation . move & & messaging controller . is move capable ( message ) ) | | ( operation = = folder operation . copy & & messaging controller . is copy capable ( message ) ) ) { final toast toast = toast . make text ( get activity ( ) , r . string . move _ copy _ cannot _ copy _ unsynced _ message , toast . length _ long ) ; toast . show ( ) ; return false ; } }
string three lines then cr2 lf = \ n \ r \ n \ r \ n \ n final \ n \ n \ n line ;
assert true ( cache . heap size ( ) < ( max size * lru block cache . default _ acceptable _ factor ) ) ;
if ( request . get parameter ( f ) = = null ) { return ; } string [ ] item names = request . get parameter ( f ) . split ( , ) ;
vector < socket permission > permissions = new vector < > ( perms . size ( ) ) ; synchronized ( this ) { permissions . add all ( perms ) ; } object output stream . put field pfields = out . put fields ( ) ; pfields . put ( permissions , permissions ) ; out . write fields ( ) ; }
drawer . set background color ( drawer color ) ;
. filter ( objects : : non null ) . to list ( ) ) ; return builder . build ( ) ; }
m ptr frame layout . set pull to refresh ( false ) ;
conf . set int ( hive conf . conf vars . hive _ exec _ input _ listing _ max _ threads . varname , two _ threads ) ; conf . set int ( deprecated _ mapred _ dfsclient _ parallelism _ max , one _ thread ) ; assert equals ( zero _ executors , utilities . get max executors for input listing ( conf , zero _ locations ) ) ; assert equals ( one _ executor , utilities . get max executors for input listing ( conf , one _ location ) ) ; assert equals ( two _ executors , utilities . get max executors for input listing ( conf , two _ locations ) ) ; assert equals ( two _ executors , utilities . get max executors for input listing ( conf , three _ locations ) ) ; }
session . get transaction ( ) . begin ( ) ; session . delete ( visitor1 ) ; session . delete ( visitor2 ) ; session . delete ( visitor3 ) ; session . delete ( zoo ) ; session . get transaction ( ) . commit ( ) ; session . close ( ) ;
m scene2 = scene . get scene for layout ( m scene root , r . layout . transition _ everwhere _ scene2 , this ) ;
buf . append ( ' ' ) . append ( _ current state ) ;
assert equals ( should have not invalidated cache . , 1 , counter . calls ) ;
hash map < string , string > args = new hash map < > ( ) ; args . put ( hl , true ) ; args . put ( df , t _ text ) ; args . put ( hl . fl , ) ; test harness . local request factory sum lrf = h . get request factory ( , 0 , 200 , args ) ; assert u ( adoc ( t _ text , a long day ' s night , id , 1 ) ) ;
test init data = new drm init data ( data _ universal , data _ 2 , data _ 1 ) ;
e = expect throws ( solr exception . class , ( ) - > { collection admin request . create alias ( testalias , doesnotexist ) . process ( cluster . get solr client ( ) ) ; } ) ; assert equals ( solr exception . error code . bad _ request , solr exception . error code . get error code ( e . code ( ) ) ) ; assert true ( e . get message ( ) . contains ( can ' t create collection alias for collections = ' doesnotexist ' , ' doesnotexist ' is not an existing collection or alias ) ) ;
map of copied hfiles . put ( table name , staging dir ) ; } return map of copied hfiles ; } finally {
assert not anumber ( 1x ) ;
response = read ( conn . get error stream ( ) ) ; }
if ( m double sided ) { gles20 . gl enable ( gles20 . gl _ cull _ face ) ; } else if ( m back sided ) { gles20 . gl cull face ( gles20 . gl _ back ) ; }
m _ client . call procedure ( @ ad hoc , alter table foo drop constraint blerg ; ) ; indexes = m _ client . call procedure ( @ statistics , index , 0 ) . get results ( ) [ 0 ] ; do { indexes = m _ client . call procedure ( @ statistics , index , 0 ) . get results ( ) [ 0 ] ; tables = m _ client . call procedure ( @ statistics , table , 0 ) . get results ( ) [ 0 ] ; } while ( indexes . get row count ( ) = 0 ) ;
try { ioutils . rename ( temp , f ) ; } finally { if ( temp . exists ( ) ) { temp . delete ( ) ; } }
frame fr2 = new frame ( id , src . _ names . clone ( ) , svecs ) ;
the switcher . master is elected ( new high availability member change event ( high availability member state . pending , high availability member state . to _ master , instance id , listening at ) ) ;
return ( ( configurable listable bean factory ) parent ) . is autowire candidate ( bean name , descriptor ) ;
sequence = insert revision ( new rev , doc numeric id , sequence , current , data ) ; if ( sequence < = 0 ) { return new tdstatus ( tdstatus . internal _ server _ error ) ; } if ( i = = 0 ) {
return addr = = null ? null : address ( addr , 0 , addr . length ) ;
assert equals ( 2 , md . columns ( ) . size ( ) ) ;
if ( m buffers . size ( ) > 0 ) { byte buffer first = m buffers . get first ( ) ; if ( first . position ( ) > = b . remaining ( ) ) { first . position ( first . position ( ) - b . remaining ( ) ) ; first . mark ( ) ; first . put ( b ) ; first . reset ( ) ; reclaim ( b ) ; return ; } }
close ( channel , succeeded future ( channel ) , true ) ;
for ( entry < string , property index > c : columns . entry set ( ) ) { property index indexed column = c . get value ( ) ; if ( indexed column . get index type ( ) = null & & indexed column . get index type ( ) . to lower case ( ) . equals ( constants . composite ) ) { index info index info = new index info ( c . get key ( ) , indexed column . get max ( ) , indexed column . get min ( ) , indexed column . get index type ( ) , indexed column . get name ( ) ) ; table info . add to indexed column list ( index info ) ; } } set attributes = entity type . get attributes ( ) ;
if ( drag percent > 1 . 0f & & m end of refreshing ) { rotate angle = ( drag percent % 1 ) * 10 ; drag percent = 1 . 0f ; } float offset x = ( ( m screen width * drag percent ) 2 ) - m jet width center ;
card view native card view = ( card view native ) get activity ( ) . find view by id ( r . id . carddemo _ header _ std ) ;
int nr of process definitions = 5 ; for ( int i = 0 ; i < nr of process definitions ; i + + ) { repository service . create deployment ( ) . add classpath resource ( org activiti engine test api runtime one task process . bpmn20 . xml ) . deploy ( ) ; } assert equals ( nr of process definitions , repository service . create process definition query ( ) . count ( ) ) ; assert equals ( nr of process definitions , repository service . create process definition query ( ) . active ( ) . count ( ) ) ; assert equals ( 0 , repository service . create process definition query ( ) . suspended ( ) . count ( ) ) ;
string response ; response = get response entity ( req ) ; assert that ( response , contains string ( request _ tag ) ) ; assert that ( response , not ( contains string ( proxy _ tag ) ) ) ; response = get response entity ( req param ) ; assert that ( response , contains string ( request _ tag ) ) ; assert that ( response , not ( contains string ( proxy _ tag ) ) ) ; response = get response entity ( singleton ) ; assert that ( response , contains string ( singleton _ tag ) ) ; assert that ( response , contains string ( proxy _ tag ) ) ; response = get response entity ( req ) ; assert that ( response , contains string ( request _ tag ) ) ; assert that ( response , not ( contains string ( proxy _ tag ) ) ) ; response = get response entity ( singleton ) ; assert that ( response , contains string ( singleton _ tag ) ) ; assert that ( response , contains string ( proxy _ tag ) ) ; response = get response entity ( singleton param ) ;
p = find by id ( person kvstore . class , 1 , em ) ;
this . message event . get channel ( ) . write ( response ) ;
user attribute attr = ( user attribute ) config attrib ed . get value ( ) ;
values = build single value ( keyname ) ; }
datagram socket socket = new datagram socket ( ) ;
boolean s end = ( s > = string . length ( ) ) ;
final shared preferences . editor editor = prefs . edit ( ) . put boolean ( pref _ _ legacy _ already _ converted _ to _ 4 _ 8 _ 0 , true ) ; pref utils . save ( editor ) ; }
assert equals ( source2 , coder utils . decode from byte array ( coder , coder utils . encode to byte array ( coder , source2 ) ) ) ;
assert equals ( moo , jobb . get ( testprops ) ) ;
file status status = mock ( file status . class ) ; when ( status . is directory ( ) ) . then return ( true ) ; when ( local fs . get file status ( local path ) ) . then return ( status ) ; shared cache uploader spied = create spied uploader ( resource , local path , user , conf , scm client , fs , local fs ) ; path actual path = spied . get actual path ( ) ;
string time = null ;
encoded . write bytes ( data , data . reader index ( ) , data len ) ; return encoded ; } }
final collection < postgre table foreign key > all foreign keys = get container ( ) . constraint cache . get typed objects ( monitor , get container ( ) , postgre table foreign key . class ) ; for ( postgre table foreign key constraint : all foreign keys ) { if ( constraint . get associated entity ( ) = = this ) { refs . add ( constraint ) ; } } return refs ; }
object checker . fail ( optional value , matcher ) ; }
if ( prefetched pages . get ( ) > = max _ mmap _ prefetch _ pages ) { return ; } final long prefetch slowdown page limit = ( long ) ( prefetch _ slowdown _ pct * max _ mmap _ prefetch _ pages ) ;
copy . destination ( ) . set op type ( create ) ; bulk by scroll response response = copy . get ( ) ; assert that ( response , matcher ( ) . batches ( 1 ) . version conflicts ( 1 ) . failures ( 1 ) . created ( 99 ) ) ; for ( failure failure : response . get bulk failures ( ) ) { assert that ( failure . get message ( ) , contains string ( version conflict engine exception [ [ test ] [ ) ) ; } }
store info store = resource . get store ( ) ; namespace info ns = catalog . get namespace by prefix ( store . get workspace ( ) . get name ( ) ) ; string name = resource . get name ( ) ;
t node = cookie config . get ( http - only ) ; if ( t node = null ) { boolean http only = boolean . parse boolean ( t node . to string ( false , true ) ) ; switch ( context . get meta data ( ) . get origin ( cookie - config . http - only ) ) { case not set : {
method m1 = test method . class . get declared method ( invoke instance test , new class [ 0 ] ) ;
assert equals ( object . class , subtype . contained type ( 0 ) . get raw class ( ) ) ;
test failure ( view . update compacting ( empty set ( ) , of ( r2 ) ) , cur ) ;
for ( test subscriber < integer > sub : subscribers ) { sub . await terminal event ( 1000 , time unit . milliseconds ) ; system . out . println ( subscriber received : + sub . get on next events ( ) ) ; sub . assert completed ( ) ; sub . assert no errors ( ) ; sub . assert values ( 5 ) ; }
map < string , string > cmd conf map = parse args ( cmd args ) ;
time window fz = time window . from ( monday , wednesday - thursday , 0 , 17 - 19 , utc ) ;
err = strm . deflate ( jzlib . z _ partial _ flush ) ;
consumer < integer > source next = mock ( consumer . class ) ;
if ( remote views = = null ) { throw new illegal argument exception ( remote views cannot be null . ) ; }
assert parse ( true , true ) ; assert parse ( false , false ) ; assert parse ( true , true ) ; assert parse ( false , false ) ; assert parse ( false , fa lse ) ;
assert equals ( group _ + ( starting _ sequence + grps increment ) , group data . get name ( ) ) ;
db . pm = pm ; uri . builder b = new uri . builder ( ) ; db . m uri = uri util . parse default file ( m filename ) ; db . set loaded ( ) ; app . clear shutdown ( ) ;
init shadow paint ( ) ; canvas . draw circle ( ( float ) size * m density 2 , ( float ) size * m density 2 , ( stroke _ radius + stroke _ width 2 + shadow _ width ) * m density , m shadow paint ) ;
tl = new transfer listener ( files [ 3 ] . get name ( ) , false , false ) ; mock ftop set . add file transfer listener ( tl ) ; mock ftop set . receive file ( files [ 3 ] , test contact ) ; wait seconds ( 200 ) ; mock ftop set . remove file transfer listener ( tl ) ; control date2 = new date ( ) ;
dx = - dx ;
style ( view holder . image loved on , m starred ? 1 : 0 ) ;
check sub cluster policy configuration ( request . get policy configuration ( ) ) ;
jar url = new file ( file ) . to uri ( ) . to url ( ) ; } else { jar url = new file ( file ) . to url ( ) ; } } else { try {
pc . set target ( new test bean ( ) ) ; aop proxy aop = create aop proxy ( pc ) ; itest bean tb = ( itest bean ) aop . get proxy ( ) ; try { note : exception param below isn ' t used tb . get age ( ) ; fail ( should have wrapped exception raised by interceptor ) ; } catch ( undeclared throwable exception thrown ) { assert equals ( exception matches , unexpected exception , thrown . get undeclared throwable ( ) ) ; } catch ( exception ex ) { ex . print stack trace ( ) ; fail ( didn ' t expect exception : + ex ) ; }
rule definition dto custom rule = rule testing . new custom rule ( template rule ) . set name ( old name ) . set description ( old description ) . set severity ( severity . minor ) . set status ( rule status . beta ) . get definition ( ) ; db . rules ( ) . insert ( custom rule ) ; db . rules ( ) . insert rule param ( custom rule , param - > param . set name ( regex ) . set type ( string ) . set description ( reg ex ) . set default value ( null ) ) ; db session . commit ( ) ;
array list < awsresource > resources = new array list < > ( ) ; test rdsjanitor resource tracker tracker = new test rdsjanitor resource tracker ( ) ; when ( tracker . get jdbc template ( ) . query ( matchers . any string ( ) , matchers . any ( object [ ] . class ) , matchers . any ( row mapper . class ) ) ) . then return ( resources ) ; string id = i - 12345678901234567 ;
value = run ( clazz , run it ) ;
final string json = new string ( jsonserialiser . serialise ( aggregator , true ) ) ;
assert bytecode has pattern ( def x = 1 ; int y = 2 ; return x + y , ( ?s ) . * invokedynamic add . * arguments : \ \ s + + \ \ d + + , \ \ s + + def bootstrap . binary _ operator + , \ \ s + + 0 + . * ) ; assert bytecode has pattern ( int x = 1 ; def y = 2 ; return x + y , ( ?s ) . * invokedynamic add . * arguments : \ \ s + + \ \ d + + , \ \ s + + def bootstrap . binary _ operator + , \ \ s + + 0 + . * ) ; }
if ( proxy socket = = null ) { throw new protocol exception ( unable to create tunnel through proxy server . ) ; } socket socket = factory . create socket ( proxy socket , host , port , true * auto close * ) ; success = true ;
atmosphere request impl . builder b = ( new atmosphere request impl . builder ( ) . request ( request ) . method ( method type ) . content type ( content type = = null ? request . get content type ( ) : content type ) . attributes ( m ) . path info ( path info ) . context path ( request . get context path ( ) ) . servlet path ( request . get servlet path ( ) ) . request uri ( request uri ) . request url ( request . request url ( ) ) . destroyable ( destroyable ) . headers ( request . headers map ( ) ) . session ( resource . session ( ) ) ) ; return b ;
string new md5 = md5 . get md5 ( new file ) ; file bs diff file = get output path ( new file ) . to file ( ) ; if ( old file = = null | | old file . exists ( ) ) { file operation . copy file using stream ( new file , bs diff file ) ; write log files ( new file , null , null , new md5 ) ; return true ; }
if ( rows to qualifier . contains key ( row id ) ) { rows to qualifier . put ( row id , new array list < > ( ) ) ; }
leaving index = leader index = get leader id ( qu ) ;
add blur handler ( handler . get key ( ) ) ; }
int [ ] surface attribs = { egl14 . egl _ none } ; eglsurface egl surface = egl14 . egl create window surface ( m egldisplay , m eglconfig , surface , surface attribs , 0 ) ; check egl error ( egl create window surface ) ; if ( egl surface = = null ) { throw new runtime exception ( surface was null ) ; } return egl surface ;
try { service . start ( other contender ) ; fail ( should fail with an exception ) ; } catch ( illegal state exception e ) { expected }
if ( benchmark props . contains key ( store _ name ) ) { throw new voldemort exception ( missing storename ) ; } string socket url = benchmark props . get string ( url ) ;
c . core . main table = table ; c . core . main index = index ; table . add constraint ( c ) ;
m _ data = new instances ( data ) ; m _ data . delete with missing class ( ) ; super . build classifier ( m _ data ) ;
provider . process binding configuration ( test . items , new rollershutter item ( item1 ) , string . format ( > [ % 1 s : 0 : trigger = up , transformation = 1 ] , > [ % 1 s : 0 : trigger = down , transformation = - 1 ] + , > [ % 1 s : 1 : trigger = move , transformation = 1 ] , > [ % 1 s : 1 : trigger = stop , transformation = 0 ] , slave _ name ) ) ;
string json = mapper . write value as string ( person ) ;
dictionary source id function . reset ( ) ; if ( page . get position count ( ) = = 0 ) { return empty _ page _ processor _ output ; }
final list < string > names = function descr . get parameter names ( ) ; final list < string > types = function descr . get parameter types ( ) ; for ( int i = 0 , size = names . size ( ) ; i < size ; i + + ) { try { type resolver . resolve type ( types . get ( i ) ) ; } catch ( final class not found exception e ) { errors . add ( new function error ( function descr , e , unable to resolve type + types . get ( i ) + while building function . ) ) ; break ; } } vars . put ( text , function descr . get text ( ) ) ;
string exit msg = string . format ( retained heap limiter forcing exit due to gc thrashing : tenured space + % s out of % s ( > % s % % ) occupied after back - to - back full gcs , space . get used ( ) , space . get max ( ) , occupied heap percentage threshold ) ;
list < string > dynamic part keys = job info . get dynamic partitioning keys ( ) ;
if ( value instanceof scriptable ) { return script runtime . default object to string ( ( scriptable ) value ) ; } else { return value . to string ( ) ; }
double max = ( ( ( ( double ) nanos _ per _ call ) nanos _ in _ one _ sec ) * iter count ) * tolerance ;
protocol . add response ( discover items , verification . corresponding sender receiver , verification . request type get ) ;
ogg page header second page header = ogg page header . read ( raf ) ;
if ( node = = null ) { return 0 ; } if ( command class name = null ) { this is a report item , handle it with the report info converter . if ( command class name . equals ignore case ( info ) ) { return info converter . get refresh interval ( ) ; } if ( node . get node id ( ) = = this . controller . get own node id ( ) & & command class name . equals ignore case ( switch _ all ) ) { return 0 ; } command class = node . resolve command class ( command class . get command class ( command class name ) , binding configuration . get endpoint ( ) ) ; if ( command class = = null ) { logger . warn ( no command class found for item = { } , command class name = { } , using 0 refresh interval . , item name , command class name ) ; return 0 ; } } else { command class = resolve converter ( provider . get item ( item name ) , node , binding configuration . get endpoint ( ) ) ; }
new caret = advance ? other run start : other run limit ;
perform sized long failure test ( new byte [ ] { } ) ;
f domconfig properties . set property ( domconstants . s _ dom3 _ properties _ ns + domconstants . dom _ validate _ if _ schema , domconstants . dom3 _ default _ false ) ;
assert equals ( tool runner . run ( job , new random writer ( ) , rw args ) , 0 ) ; }
system . out . printf ( doing a full select and using the index for ordering . \ n ) ; volt table table1 = client . call procedure ( query name ) . get results ( ) [ 0 ] ; assert equals ( max _ rows 2 , table1 . get row count ( ) ) ;
views . set view visibility ( r . id . text1 , view . gone ) ; views . set text view text ( r . id . text2 , error state ) ; } else {
namespaces . put ( ns prefix , ns value ) ; } } }
start route ( route id ) ; return ;
boolean u odd = ( k = = s1 ) ; mutable big integer t = u odd ? v : u ; int tsign = u odd ? - 1 : 1 ; int lb ;
if ( get parent ( ) = null ) { get parent ( ) . notify subtree accessibility state changed ( this , this , accessibility event . content _ change _ type _ subtree ) ; }
final list < persistent collection change data > merged changes = new array list < > ( ) ;
samples = sample provider . get activity samples ( 0 , 0 ) ; assert equals ( 0 , samples . size ( ) ) ; samples = sample provider . get activity samples ( - 1 , 1 ) ; assert equals ( 0 , samples . size ( ) ) ; samples = sample provider . get activity samples ( 1 , - 1 ) ; assert equals ( 0 , samples . size ( ) ) ;
int k = use _ sign _ of _ a * sa + use _ sign _ of _ c * sc ; int q = flip ( k ) ; opposite of k return a * k + b * q ; }
env . execute ( streaming word count ) ; }
get map request get map = params . get get map request ( ) ;
styles . get style ( ) . add ( s ) ;
conf . set ( angel conf . angel _ action _ type , mlconf . angel _ ml _ predict ( ) ) ; kmeans runner runner = new kmeans runner ( ) ;
collection < server name > dead region servers = status . get dead server names ( ) ; errors . print ( number of dead region servers : + dead region servers . size ( ) ) ; if ( details ) { for ( server name name : dead region servers ) { errors . print ( + name ) ; } }
ket = cursor ; switch ( among _ var ) { case 0 : break lab1 ; case 1 :
session connection b = client . get zeppelin connection ( bbbb , anonymous , anonymous ) ; assert not null ( connection b ) ; assert true ( connection b . is open ( ) ) ; assert equals ( client . count connected notes ( ) , 2 ) ;
my volley . init ( this ) ; timber . plant ( new timber . debug tree ( ) ) ;
if ( _ generate origin ) { out . open tag ( context - param ) . tag ( param - name , origin ) . tag ( param - value , _ origin attribute ) . close tag ( ) ; }
assert analyzes to ( a , Ø®ÙØ±Ø¯Ù Ø´Ø¯Ù Ø¨Ø§Ø´Ø¯ , new string [ ] { Ø®ÙØ±Ø¯Ù } ) ;
do action ( action . decrease , auto tune factor , progress ) ;
component model full name mapper model = ldaptest utils . get subcomponent by name ( app realm , ldap model , full name ) ; app realm . remove component ( full name mapper model ) ; first name mapper . set id ( null ) ;
while ( results [ 0 ] . advance row ( ) ) { long completed = results [ 0 ] . get long ( end _ time ) ; assert true ( end _ time was not filled , completed = 0 ) ; } }
xmlattr stack . get xmlns attr ( result ) ;
final float dist = ( float ) math . hypot ( to . left - from . left , to . top - from . top ) ; final resources res = get resources ( ) ; final float max dist = ( float ) res . get integer ( r . integer . config _ drop anim max dist ) ;
if ( ( n [ 0 ] . id . length ( ) = = 0 ) & & ( n [ 0 ] . kind . length ( ) = = 0 ) ) throw new org . omg . cos naming . naming context package . invalid name ( ) ;
try { pre check permission ( ) ; } catch ( access control exception ace ) { runtime . get runtime ( ) . exit ( - 1 ) ; } catch ( ioexception ioe ) { runtime . get runtime ( ) . exit ( - 1 ) ; }
if ( self block . is water ( ) | | self block . is ice ( ) ) { render type = chunk mesh . render type . water _ and _ ice ; } if ( self block . is double sided ( ) ) { render type = chunk mesh . render type . billboard ; } if ( block appearance . get part ( block part . center ) = null ) { vector4f color offset = self block . calc color offset for ( block part . center , self biome ) ; block appearance . get part ( block part . center ) . append to ( chunk mesh , x , y , z , color offset , render type , vertex flag ) ; } boolean [ ] draw dir = new boolean [ 6 ] ;
rebalance plan = cluster test utils . make plan ( z1z3 current , z1z3 stores , z1z3 current , z1z3 stores ) ; assert equals ( rebalance plan . get plan ( ) . size ( ) , 0 ) ; assert equals ( rebalance plan . get primaries moved ( ) , 0 ) ; assert equals ( rebalance plan . get partition stores moved ( ) , 0 ) ; assert equals ( rebalance plan . get partition stores moved xzone ( ) , 0 ) ;
remove interceptor which is out the phases ( server . get endpoint ( ) . get service ( ) . get in interceptors ( ) , remaining _ in _ phases , get in interceptor names ( ) ) ;
assert true ( embedder . offer ( trailer ) ) ; assert true ( embedder . finish ( ) ) ; http message aggrated message = embedder . poll ( ) ; assert not null ( aggrated message ) ; assert equals ( chunk1 . get content ( ) . readable bytes ( ) + chunk2 . get content ( ) . readable bytes ( ) , http headers . get content length ( aggrated message ) ) ; assert equals ( aggrated message . get header ( x - test ) , boolean . true . to string ( ) ) ; assert equals ( aggrated message . get header ( x - trailer ) , boolean . true . to string ( ) ) ; check content buffer ( aggrated message ) ;
verify ( task , times ( 1 ) ) . call ( ) ;
return super . get encoded ( ) ;
memory . poke long array ( ptr , values , 0 , values . length , true ) ; assert longs equal ( values , ptr , true ) ; assert longs equal ( swapped values , ptr , false ) ;
group by . add to expressions ( group by column name ) ;
assert not null ( context . get registry ( map . class ) ) ; assert null ( context . get registry ( jndi registry . class ) ) ; context . stop ( ) ;
if ( ( ( secs ^ r ) & ( nanos ^ r ) ) < 0 ) { return ( secs < 0 ) ? long . min _ value : long . max _ value ; } return r ;
blocking queue < bad key status > bq = new array blocking queue < bad key status > ( 5 ) ; executor service bad key writer service = executors . new single thread executor ( ) ; bad key writer bad key writer = new bad key writer ( file name , bq ) ;
assert equals ( foo coder1 max1 , foo coder1 min ) ;
assert . assert true ( map . lookup ( key data , platform . byte _ array _ offset , record length bytes ) . is defined ( ) ) ;
if ( . project . equals ( entry path . last segment ( ) ) ) { iproject description desc = workspace . load project description ( entry path ) ; zip project name = desc . get name ( ) ; }
if ( method . is synthetic ( ) ) { continue ; } add action resource method ( action resource model , method ) ;
if ( is serializable ( t ) ) { can send the exception itself this . exception = t ; } else { we ' ll compose all of the info into a string string writer s out = new string writer ( ) ; print writer out = new print writer ( s out ) ; t . print stack trace ( out ) ; out . close ( ) ; this . error = s out . to string ( ) ; }
return ctxt . handle missing instantiator ( get value class ( ) , this , null , no creator with arguments specified ) ;
authenticate payload payload = new authenticate payload ( get username ( ) , get password ( ) ) ; m dispatcher . dispatch ( authentication action builder . new authenticate action ( payload ) ) ; }
while ( iterator . has next ( ) ) { hregion r = iterator . next ( ) ; if ( region replica util . is default replica ( r . get region info ( ) ) ) { iterator . remove ( ) ; } } return online regions ;
if ( total verts > = 65536 ) { make sure we create an unsigned int buffer so we can fit all of the meshes format for buf [ vertex buffer . type . index . ordinal ( ) ] = vertex buffer . format . unsigned int ; } else { format for buf [ vertex buffer . type . index . ordinal ( ) ] = vertex buffer . format . unsigned short ; }
int weight idx = fr2 . find ( _ parms . _ weights _ column ) ;
throw new ioexception ( unknown type id when skipping bytes ) ; } }
int tmp2 = tri indices [ index1 ] ; tri indices [ index1 ] = tri indices [ index2 ] ; tri indices [ index2 ] = tmp2 ; }
path lock = new path ( host db , lock _ name ) ; if ( fs . exists ( current ) ) { fs . mkdirs ( current ) ; } lock util . create lock file ( fs , lock , false ) ; multiple inputs . add input path ( job , current , sequence file input format . class ) ; if ( top hosts = null ) { multiple inputs . add input path ( job , top hosts , key value text input format . class ) ; }
context resource link crl = new context resource link ( ) ;
this . preferences = new gcpreferences ( ) ; this . preferences . set to ( preferences ) ; show model metrics panel = preferences . is show model metrics panel ( ) ;
if ( wmts = null & & wmts . get hints ( ) = null & & objects . equals ( wmts . get hints ( ) . get ( xmlhandler hints . entity _ resolver ) , entity resolver ) ) { wmts cache . remove ( id ) ; wmts = null ; } if ( wmts = = null ) { synchronized ( wmts cache ) { wmts = ( web map tile server ) wmts cache . get ( id ) ; if ( wmts = = null ) { httpclient client = get httpclient ( expanded store ) ; string capabilities url = expanded store . get capabilities url ( ) ; url server url = new url ( capabilities url ) ; wmts = new web map tile server ( server url , client , null ) ; if ( string utils . is not empty ( info . get header name ( ) ) & & string utils . is not empty ( info . get header value ( ) ) ) { wmts . get headers ( ) . put ( info . get header name ( ) , info . get header value ( ) ) ; } wmts cache . put ( id , wmts ) ; } } } return wmts ;
thread . sleep ( async _ event _ completion _ wait ) ; assert equals ( get available permits ( sub ref ) , recv queue size ) ; for ( int i = 0 ; i < recv queue size 2 ; i + + ) { string message = my - message - + i ; producer . send ( message . get bytes ( ) ) ; msg = consumer . receive ( ) ; consumer . acknowledge ( msg ) ; }
result = multiply fraction ( result , numerator , denominator ) ;
settings . push enabled ( connection . local ( ) . allow push to ( ) ) ;
return m _ chars . get string ( offset , length ) ;
cached live nodes = get cached live nodes from local state ( actual live nodes . size ( ) ) ; assert equals ( iter + iter + + actual live nodes . size ( ) + = + cached live nodes . size ( ) , actual live nodes , cached live nodes ) ; } finally { for ( live node trasher thrasher : thrashers ) {
file utils . clean directory ( new dir . to file ( ) ) ;
ultimate recycler view . set refreshing ( false ) ;
rec1 = create record ( rec1fields ) ; rec2 = create record ( rec2fields ) ; rec2 . update binary represenation ( ) ; rec1 . union fields ( rec2 ) ; check unioned record ( rec1 , rec1fields , rec2fields ) ;
if ( get line start ( get main ast ( ) ) = = get main ast ( ) . get column no ( ) ) { result = super . get indent impl ( ) ; } else { result = new indent level ( get line start ( get main ast ( ) ) ) ; } return result ; }
log . e ( tag , on start command + this . to string ( ) ) ;
final interval interval = opt interval . get ( ) ; final shard spec shard spec = shard specs . get shard spec ( interval , input row ) ; sequence name = appenderators . get sequence name ( interval , find version ( versions , interval ) , shard spec ) ; } else {
exists already . set type ( part . get relationship type ( ) ) ; load part ( part , exists already ) ;
err ( \ n get an mbean server connection ) ; mbsc = jmxc . get mbean server connection ( ) ; }
m received invoke key down = false ; return true ;
assert . assert equals ( set0 , join . get input1 ( ) . get local strategy keys ( ) ) ; assert . assert equals ( set0 , join . get input2 ( ) . get local strategy keys ( ) ) ; assert . assert true ( arrays . equals ( join . get input1 ( ) . get local strategy sort order ( ) , join . get input2 ( ) . get local strategy sort order ( ) ) ) ; assert . assert equals ( set01 , reducer . get input ( ) . get local strategy keys ( ) ) ; assert . assert equals ( set01 , reducer . get keys ( 0 ) ) ; assert . assert true ( arrays . equals ( reducer . get input ( ) . get local strategy sort order ( ) , reducer . get sort orders ( 0 ) ) ) ; return true ;
dest . write string ( normal ) ;
floating action button . close ( true ) ; } } ) ; final floating action button floating action button3 = ( floating action button ) find view by id ( r . id . menu _ new _ cloud ) ; floating action button3 . set color normal ( icon skin ) ; floating action button3 . set color pressed ( icon skin ) ; floating action button3 . set on click listener ( new view . on click listener ( ) { @ override public void on click ( view view ) { main activity helper . add ( main activity helper . new _ cloud ) ;
album2 . get album name ( ) ;
boolean is ssoauthentication = true . equals ( session . get attribute ( sso _ auth ) ) ; if ( is ssoauthentication ) { client session . set note ( sso _ auth , true ) ; } else { int auth time = time . current time ( ) ; user session . set note ( auth _ time , string . value of ( auth time ) ) ; client session . remove note ( sso _ auth ) ; } return protocol . authenticated ( user session , client session ) ;
assert equals ( in _ cp _ config _ value , hadoop conf . get ( in _ cp _ config _ key , null ) ) ; }
list < element > expected filtered results = arrays . as list ( get edge ( a3 , a3 , false ) , get edge ( a3 , b3 , false ) , get edge ( a3 , d3 , false ) , get edge ( a3 , c3 , false ) , get edge ( a5 , b5 , false ) , get edge ( a3 , a3 , true ) , get edge ( a3 , b3 , true ) , get edge ( a3 , c3 , true ) , get edge ( a3 , d3 , true ) , get entity ( a5 ) ) ;
if ( s end | | p end ) { break ; }
jar url = new file ( file ) . to uri ( ) . to url ( ) ; } else { jar url = new file ( file ) . to url ( ) ; } } else { try {
page builder . reset ( ) ;
node . set included in scope ( model . get session ( ) . is included in scope ( node ) , true ) ; node . set excluded from scope ( model . get session ( ) . is excluded from scope ( node ) , true ) ; this . apply filter ( node ) ; handle event ( parent , node , event type . add ) ; } else if ( href map . get ( ref . get history id ( ) ) = node ) {
in . mark ( 10 ) ;
fatal error handler . rethrow error ( ) ;
document doc = xmlunit . build test document ( file utils . read file to string ( config file ) ) ; xmlassert . assert xpath evaluates to ( ft1 - id , transform feature type id , doc ) ; }
version + = 5 ;
return new dbinary ( build target , project filesystem , params . with extra deps ( immutable sorted set . of ( native linkable ) ) , executable builder . build ( ) , native linkable . get source path to output ( ) ) ; }
class loader loader1 = new subverted class loader ( new url [ ] { get class ( ) . get resource ( ) } , this . get class ( ) . get class loader ( ) ) ; class cheese class = loader1 . load class ( org . drools . compiler . cheese ) ; knowledge builder configuration kbuilder conf = knowledge builder factory . new knowledge builder configuration ( null , loader1 ) ;
if ( entry . get value ( ) . is online ( ) & & entry . get value ( ) . is offline ( ) ) { filter sites . add ( entry . get key ( ) ) ; }
if ( is anonymous ( ) ^ other . is anonymous ( ) ) { return is anonymous ( ) ? - 1 : 1 ; } return spec . compare to ( other . spec ) ; }
if ( fast expand tab = null ) { float delta = calculate offset to make tab visible ( fast expand tab , can expand selected tab , allow left expand , true ) ; if ( m should cascade tabs ) { if the scrolling strip stacker is being used and the new tab button is visible , go directly to the new scroll offset rather than animating . animating the scroll causes the new tab button to disappear for a frame . boolean should animate = m new tab button . is visible ( ) & & m animations disabled for testing ; set scroll for scrolling tab stacker ( delta , should animate , time ) ; } else if ( delta = 0 . f ) { m scroller . start scroll ( m scroll offset , 0 , ( int ) delta , 0 , time , expand _ duration _ ms ) ; } } m update host . request update ( ) ;
this . registry . add remote registry dto ( message , this . converter , - 1 ) ; assert equals ( 1 , this . registry . get user count ( ) ) ; this . registry . purge expired registries ( ) ;
runtime . get runtime ( ) . add shutdown hook ( new thread ( streams - pipe - shutdown - hook ) { @ override public void run ( ) { streams . close ( ) ; latch . count down ( ) ; } } ) ; try { streams . start ( ) ; latch . await ( ) ; } catch ( throwable e ) { system . exit ( 1 ) ; }
written + = 12 ;
distribution bits = integer . parse int ( lines [ 1 ] ) ; bucket cursor = long . parse long ( lines [ 2 ] ) ; finished bucket count = long . parse long ( lines [ 3 ] ) ; total bucket count = long . parse long ( lines [ 4 ] ) ; if ( total bucket count = = finished bucket count ) { return ; we ' re done here }
return convert array to array ( value ) ;
d arr [ d + + ] = ca [ ( i > > > 18 ) & 0x3f ] ; d arr [ d + + ] = ca [ ( i > > > 12 ) & 0x3f ] ; d arr [ d + + ] = ca [ ( i > > > 6 ) & 0x3f ] ; d arr [ d + + ] = ca [ i & 0x3f ] ;
cv . visit field insn ( getstatic , fq class name , bshstatic + class name , lorg gjt sp jedit bsh this ; ) ;
player . rotate ( 0 , 2 * tpf , 0 ) ;
peer . read ( ) ;
this . standard analyzer = new throwable analyzer ( ) ;
palette combo box = new palette combo box ( palette utils . get sequencial palettes ( ) ) ;
logger . debug ( could not serialize variable value + variable event . get variable value ( ) ) ;
sphere sphere = new sphere ( 64 , 64 , 20 ) ;
golab0 : while ( true ) { lab1 : do { if ( ( in _ grouping ( g _ v , 97 , 252 ) ) ) { break lab1 ; } break golab0 ; } while ( false ) ; if ( cursor > = limit ) { return false ; } cursor + + ; }
byte [ ] read array = read buffer . array ( ) ; while ( left > 0 ) { int put length = math . min ( targets [ index ] . remaining ( ) , left ) ; targets [ index ] . put ( read array , read count - left , put length ) ; index + + ; left - = put length ; } return read count ;
map < string , long > updated counters = new hash map < > ( ) ;
{ m lbrac ( ) ; } break ;
if ( user object = null & & user object instanceof serializable ) { t values = new object [ 2 ] ; t values [ 0 ] = user object ; t values [ 1 ] = user object ; } else t values = new object [ 0 ] ; s . write object ( t values ) ; }
object instance ;
try { service reference < ? > [ ] references = context . get all service references ( type management service . class . get name ( ) , null ) ; for ( service reference < ? > ref : references ) { type management service = ( type management service ) context . get service ( ref ) ; return type management service ; } return null ; } catch ( invalid syntax exception e ) { logger . warning ( cannot load type management service on dbre database listener impl . ) ; return null ; }
meta block index writer = new hfile block index . block index writer ( ) ; log . debug ( initialized with + cache conf ) ; if ( is schema configured ( ) ) { schema configuration changed ( ) ; }
if ( use default cache ) {
return operations . determinize ( automaton , max graph expansions ) ;
for ( int i = 0 ; i < number of cases ; i + + ) { string [ ] current line split = br . read line ( ) . split ( ) ; int command = integer . parse int ( current line split [ 0 ] ) ; int number = integer . parse int ( current line split [ 1 ] ) ;
throw new dex exception2 ( class with type index + class def . get type index ( ) + extends itself ) ; } else {
field field = singleton registry . instance . get class ( ) . get declared field ( singleton objects ) ; field . set accessible ( true ) ; map < string , object > singleton objects = ( map < string , object > ) field . get ( singleton registry . instance ) ; singleton objects . clear ( ) ; }
data = new instances ( data ) ;
buf . append ( get push data name ( opcode ) ) . append ( [ ) . append ( utils . hex . encode ( data ) ) . append ( ] ) ; } else {
if ( new events = null & & new events . is empty ( ) ) { constructor treats empty events list as invalid value throw exception if serialized list is empty throw new invalid object exception ( empty list of events ) ; } else if ( new events = = null ) { new events = collections . empty list ( ) ; }
mapped roles . add all ( jboss web meta data . get security role names ( ) ) ; } else { mapped roles . add ( role ) ; } } info . add roles ( mapped roles , http methods ) ;
assert equals ( oliver , a token . get ( 0 ) . get readings ( ) . get ( 1 ) . get lemma ( ) ) ; assert equals ( olive , a token . get ( 0 ) . get readings ( ) . get ( 2 ) . get lemma ( ) ) ; assert equals ( work , a token . get ( 1 ) . get readings ( ) . get ( 0 ) . get lemma ( ) ) ;
thread db server1 = new thread ( ) { @ override public void run ( ) { servers [ 0 ] = db server ( db1 _ dir , get local url ( ) , asynch - dserver - config - 0 . xml ) ; } } ; db server1 . start ( ) ; db server1 . join ( ) ;
finalize display name vars ( vars ) ; display name = display name template engine . transform ( template , vars ) ;
public void action performed ( action event e ) { indent : 12 exp : 12
ci . set text ( new char [ ] { } , 0 , 0 ) ; assert equals ( character iterator . done , ci . last ( ) ) ; assert equals ( ci . get end index ( ) , ci . get index ( ) ) ;
hbase ser de ser de = new hbase ser de ( ) ; configuration conf = new configuration ( ) ; properties tbl = create properties i _ i ( ) ; ser de utils . initialize ser de ( ser de , conf , tbl , null ) ; deserialize and serialize ( ser de , r , p , expected fields data ) ; ser de = new hbase ser de ( ) ; conf = new configuration ( ) ;
if ( replication bit config . get source type ( ) = = replication bit setter static config . source type . column & & is replication field ( database field name , replication bit config ) ) { set replicated ( state machine helper . verify replication status ( replication value pattern , field value , replication bit config . get missing value behavior ( ) ) ) ; } } }
for ( entry < string , string > f : fields . entry set ( ) ) doc . field ( f . get key ( ) , f . get value ( ) ) ; doc . save ( ) ; i response . send ( 201 , ok , ohttp utils . content _ text _ plain , record + doc . get identity ( ) + updated successfully . , null ) ; } else if ( del . equals ( operation ) ) {
complex resource key < group membership key , group membership param > complex key = build complex key ( 1 , 1 , 10 , string1 ) ; complex key group membership group membership = build complex key group membership ( complex key . get key ( ) , alfred @ test . linkedin . com , alfred , hitchcock ) ; request < empty record > create request = builders . create ( ) . input ( group membership ) . build ( ) ;
message = message . split ( \ n ) [ 0 ] ;
java binary object inspector bin inspector = primitive object inspector factory . java byte array object inspector ;
assert equals ( 1 l + 1 l , exec ( return 1 l + 1 l ; ) ) ;
if ( table = = null ) { return ; }
constraints . remove ( desc ) ;
master procedure testing utility . validate table creation ( util . get hbase cluster ( ) . get master ( ) , table name , regions , cf1 , cf2 ) ; }
if ( out = null ) { try { out . close ( ) ; } catch ( exception e ) {
hazelcast instance impl hz instance impl = mock ( hazelcast instance impl . class ) ;
final string string to compare = point in geo3 dshape query : field = point : shape : geo standard circle : { planetmodel = planet model . wgs84 , center = [ lat = 0 . 7 ;
bytes data [ pos ] = b ;
float scale before = scale ; v translate before . set ( v translate ) ; boolean handled = on touch event internal ( event ) ;
input stream is = bitcoin average ticker jsontest . class . get resource as stream ( marketdata example - ticker - data . json ) ;
new request . set header ( ( header ) this . from header . clone ( ) ) ;
log . info ( allocator state after all the tasks : + a . test dump ( ) ) ;
sort buffer switcher = new jcheck box ( j edit . get property ( options . view . bufferswitcher . sort buffers ) ) ; sort buffer switcher . set selected ( j edit . get boolean property ( bufferswitcher . sort buffers , true ) ) ; sort buffer switcher . add action listener ( new action listener ( ) { public void action performed ( action event evt ) { sort buffer switcher by name . set enabled ( sort buffer switcher . is selected ( ) ) ; } } ) ; add component ( sort buffer switcher ) ;
string tmp id ;
float min = 0 ; float pref = 0 ; float max = 0 ; int n = get view count ( ) ;
instructions . add ( constant expression . null ) ;
assert that ( cap . authenticate ( token ) ) . is equal to ( null ) ;
assert equals ( max pages , saved pages . size ( ) ) ;
m binding = data binding util . set content view ( this , r . layout . activity _ main ) ; init status view ( ) ;
local access control list tracker = true ; } else {
rule copy = ( rule ) pages . pop ( ) ;
assert that ( e ) . has message that ( ) . contains ( could not load external package ) ;
sleep ( 100 ) ;
a = a * 255 . 0f ; r = ( float ) math . pow ( r , 1 . 0 2 . 2 ) * 255 . 0f ; g = ( float ) math . pow ( g , 1 . 0 2 . 2 ) * 255 . 0f ; b = ( float ) math . pow ( b , 1 . 0 2 . 2 ) * 255 . 0f ; return math . round ( a ) < < 24 | math . round ( r ) < < 16 | math . round ( g ) < < 8 | math . round ( b ) ;
string route id = ( string ) mbean server . get attribute ( on , route id ) ; assert equals ( my route , route id ) ; string xml = ( string ) mbean server . invoke ( on , dump route as xml , null , null ) ;
{ m right _ curly ( ) ; if ( state . failed ) return ; } break ; case 37 :
l . status | = label . pushed ;
set < resource worker slot > workers = new hash set < > ( ) ;
else if ( value instanceof select & & ( ( select < ? > ) value ) . get select ( ) . size ( ) = = 1 ) return dsl . field ( ( select < record1 < t > > ) value ) ; [ 4771 ] any other query part type is not supported here else if ( value instanceof query part ) throw field expected ( value ) ; else return val ( value ) ;
logger . warn ( caught exception while obtaining resource timeout , e ) ;
assert true ( namenode is not in safe mode , cluster . get name node ( ) . is in safe mode ( ) ) ;
task attempt = task attempts . get ( 1 ) ;
render state rs = technique def . get render state ( ) ; if ( rs = null ) { out . write ( render state { \ n ) ; write render state ( rs , out ) ; out . write ( } \ n \ n ) ; }
validated page = utilities . bound to range ( validated page , 0 , get page count ( ) - 1 ) ;
event . set exception ( ex ) ; event . set event execution type ( event execution type . execute _ failure ) ; event bus instance . get instance ( ) . post ( event ) ; executor exception handler . handle exception ( ex ) ; return null ; }
testing client . testing ( ) . remove user session ( test , session id ) ;
assert that ( url ( ) ) . contains ( profiles ) ;
if ( command instanceof on off type ) { if ( command . equals ( on off type . off ) ) { send command ( mac address , ; fan ; wintermode ; off ) ; } else if ( command . equals ( on off type . on ) ) { send command ( mac address , ; fan ; wintermode ; on ) ; } }
child class . c method ( ) ;
assert equals ( 0 , fem . get replacement array ( ) . length ) ;
via header via = ( via header ) unregister request . get header ( via header . name ) ; if ( via = null ) via . remove parameter ( branch ) ;
final navi node first node = get mappings ( ) . get node ( get raw view ( ) . get graph ( ) . get nodes ( ) . get ( 0 ) ) ; all nodes . remove ( first node ) ; show nodes ( lists . new array list ( first node ) , all nodes ) ; }
class loader loader1 = new subverted class loader ( new url [ ] { get class ( ) . get resource ( ) } , this . get class ( ) . get class loader ( ) ) ; class cheese class = loader1 . load class ( org . drools . compiler . cheese ) ; knowledge builder configuration kbuilder conf = knowledge builder factory . new knowledge builder configuration ( null , loader1 ) ;
stop ( ) ;
edit plugin [ ] plugins = j edit . get plugins ( ) ; for ( edit plugin plugin : plugins ) { jmenu item menu item = plugin . create browser menu items ( ) ; if ( menu item = null ) vec . add ( menu item ) ;
try { get connection ( connection idx ) . get account manager ( ) . create account ( username , password ) ; created user idx . add ( connection idx ) ; } catch ( xmppexception e ) { e . print stack trace ( ) ; fail ( e . get message ( ) ) ; }
if ( pinfo . partition key = = null ) { all matched = false ; }
ioe = ioex ;
integer id3 genre id = genre types . get instance of ( ) . get id for value ( genre id ) ;
handle raw text ( cursor , limit , elements ) ;
sink . add test ( documenttypegetdoctype . class ) ;
hstore mock store = mockito . mock ( hstore . class ) ; mockito . do return ( 2000 l ) . when ( mock store ) . get size ( ) ; mockito . do return ( true ) . when ( mock store ) . can split ( ) ; stores . add ( mock store ) ;
comment snippet comment snippet = new comment snippet ( ) ; comment snippet . set text original ( text ) ;
consumer . set start scheduler ( false ) ;
get druid connection ( ch . id ) ; return conn props ;
int total size = ( int ) ok http response . body ( ) . content length ( ) ;
set default gravity ( right view , m right gravity ) ; set default text gravity ( right view , m right text gravity ) ; set default drawable ( right view . get center text view ( ) , m right tv drawable left , m right tv drawable right , m text view drawable padding , m right tv drawable width , m right tv drawable height ) ; set default background ( right view . get center text view ( ) , m right text background ) ; set default string ( right view , m right top text string , m right text string , m right bottom text string ) ; add view ( right view ) ; }
node n = null ; character c = seq . char at ( length ) ; int index = prev . child index ( c ) ;
assert equals ( double . negative _ infinity , records fetch lag max . value ( ) , epsilon ) ;
converted annotated classes . put ( annotation , annotated ) ;
string level fanout size = options . contains key ( level _ fanout _ size _ option ) ? options . get ( level _ fanout _ size _ option ) : string . value of ( default _ level _ fanout _ size ) ; try { int fanout size = integer . parse int ( level fanout size ) ; if ( fanout size < 1 ) { throw new configuration exception ( string . format ( % s must be larger than 0 , but was % s , level _ fanout _ size _ option , fanout size ) ) ; } } catch ( number format exception ex ) { throw new configuration exception ( string . format ( % s is not a parsable int ( base10 ) for % s , size , level _ fanout _ size _ option ) , ex ) ; } unchecked options . remove ( level _ fanout _ size _ option ) ;
if ( lines . get ( 0 ) . size ( ) = = 0 ) { lines . remove ( 0 ) ; }
update mnemonic binding ( b ) ; lazy action map . install lazy action map ( c , basic button listener . class , button . action map ) ;
unique col values = new long [ cardinality ] ; for ( int i = 0 ; i < unique col values . length ; i + + ) { unique col values [ i ] = ( long ) ( math . random ( ) * long . max _ value ) ; } col values = new long [ n rows ] ; for ( int i = 0 ; i < col values . length ; i + + ) { col values [ i ] = unique col values [ ( int ) ( math . random ( ) * cardinality ) ] ; } }
map < string , list < string > > annotations = new hash map < > ( ) ;
get field creator provider ( type name ) . create list field ( type name , field type , field name , cardinality . get cardinality ( ) , null , not null , size min , size max , mapped by , fetch , comment , join column name , referenced column name , join table , join columns , referenced columns , inverse join columns , inverse referenced columns , permit reserved words , aggregation , orphan removal , shell context . is force ( ) , format expression , format message ) ;
check create statement supported ( result set type , result set concurrency , result set holdability ) ; return create statement ( ) ; }
final partial druid query left partial query = left . get partial druid query ( ) ; final filter where filter = left partial query . get where filter ( ) ; final filter new where filter ; if ( where filter = null ) { new where filter = where filter . copy ( where filter . get trait set ( ) , where filter . get input ( ) , make and ( immutable list . of ( where filter . get condition ( ) , make or ( conditions ) ) ) ) ; } else { new where filter = logical filter . create ( left partial query . get scan ( ) , make or ( conditions ) ) ; } partial druid query new partial query = partial druid query . create ( left partial query . get scan ( ) ) . with where filter ( new where filter ) . with select project ( left partial query . get select project ( ) ) . with select sort ( left partial query . get select sort ( ) ) ;
assert listener status ( ) ; }
for ( int o = 0 ; o < ls [ 2 ] . _ a . length ; o + + ) { double a = ref . _ nn . outputs [ o ] ; double b = ls [ 2 ] . _ a [ o ] ; if ( trainer = = neural net . execution mode . single thread ) { compare val ( a , b , abseps , releps ) ; } }
define fetching strategy ( ) ; collection . set batch size ( batch size ) ; collection . set mutable ( property . is annotation present ( immutable . class ) ) ;
while ( is done ( ) ) { logger . info ( current worker status is : { } . , worker ) ; } print ( leaving run ( ) ) ;
if ( intent . action _ view . equals ( intent . get action ( ) ) ) util . import pro license ( new file ( intent . get data ( ) . get path ( ) ) ) ;
assert equals ( 1 , deleted entities ) ; } ) ; do in jpa ( this : : entity manager factory , entity manager - > { session session = entity manager . unwrap ( session . class ) ;
assert that ( param . get name ( ) ) . is equal to ( regex ) ;
if ( tika impl . class . get class loader ( ) instanceof urlclass loader ) { url [ ] urls = ( ( urlclass loader ) tika impl . class . get class loader ( ) ) . get urls ( ) ; set < url > set = new linked hash set < > ( arrays . as list ( urls ) ) ; if ( set . size ( ) = urls . length ) { throw new assertion error ( duplicate jars : + arrays . to string ( urls ) ) ; } add read permissions ( perms , set ) ; }
final string json = new string ( jsonserialiser . serialise ( aggregator , true ) ) ;
list < integer > responses = lumberjack util . send messages ( port , null ) ;
socket nn sock = new socket ( nn addr . get address ( ) , nn addr . get port ( ) ) ; cache . put ( nn sock ) ; assert same ( read the write , nn sock , cache . get ( nn addr ) ) ; cache . put ( nn sock ) ;
ctxt . generate java source ( java . lang . reflect . method + method name + = + pd name + [ + index + ] . get write method ( ) ; ) ;
cpg = class gen . get constant pool ( ) ; il = method gen . get instruction list ( ) ; final int index = cpg . add methodref ( _ class name , < init > , ( + translet _ intf _ sig + dom _ intf _ sig + node _ iterator _ sig + z ) v ) ;
test bean dave1 = oom . protected override singleton ( ) ; assert equals ( david , dave1 . get name ( ) ) ; test bean dave2 = oom . protected override singleton ( ) ; assert equals ( david , dave2 . get name ( ) ) ; assert same ( dave1 , dave2 ) ;
for ( string value : get values ( ) ) { buf . element ( value , value ) ; }
rest ( users ) . put ( { id } ) . to ( mock : input ) ; } } ; }
hregion info region = assigned regions on live rs . remove ( 0 ) ; master . assignment manager . region offline ( region ) ; master . assignment manager . regions in transition . put ( region . get encoded name ( ) , new region state ( region , region state . state . opening , system . current time millis ( ) , dead server name ) ) ; zkassign . create node offline ( zkw , region , dead server name ) ; zkassign . transition node opening ( zkw , region , dead server name ) ;
mock rm rm = new mock rm ( get configuration with queue labels ( conf ) ) { @ override public rmnode labels manager create node label manager ( ) { return mgr ; } } ; rm . get rmcontext ( ) . set node label manager ( mgr ) ; rm . start ( ) ; mock nm nm1 = rm . register node ( h1 : 1234 , 8000 ) ; rm . register node ( h2 : 1234 , 8000 ) ; rm . register node ( h3 : 1234 , 8000 ) ; container id container id2 ;
thread [ ] threads = new thread [ num threads ] ; for ( int i = 0 ; i < num threads ; i + + ) { threads [ i ] = new thread ( new runnable ( ) { @ override public void run ( ) { data set ds ; while ( ( ds = data sets . poll ( ) ) = null ) { indarray output = inf . output ( ds ) ; outputs . add ( pair . make pair ( ds . get labels ( ) , output ) ) ; } } } ) ; } for ( int i = 0 ; i < num threads ; i + + ) { threads [ i ] . start ( ) ; }
final int num sub tree = in . read int ( ) ; for ( int i = 0 ; i < num sub tree ; i + + ) { process directory with snapshot ( in , v , skip blocks ) ; } }
it = ( input type . input type convolutional ) input type util . get output type cnn layers ( input type , kernel , stride , padding , dilation , convolution mode . truncate , d out , - 1 , layer name , convolution layer . class ) ;
assert equals ( - 1 . 3 , searcher . doc ( td . score docs [ 0 ] . doc ) . get ( value ) ) ;
marshaller . unmarshal ( new stream source ( 1 ) ) ; argument captor < saxsource > source captor = argument captor . for class ( saxsource . class ) ; verify ( unmarshaller ) . unmarshal ( source captor . capture ( ) ) ; saxsource result = source captor . get value ( ) ;
map context arguments = new map context ( ) ; arguments . put ( a . b , 1d ) ; arguments . put ( a . b . c , 0 . 1d ) ; arguments . put ( a . c , 0 . 01d ) ; arguments . put ( a , 19d ) ; double result = gbdt . evaluate ( arguments ) . as double ( ) ;
check mbean permission ( ( string ) null , null , null , get domains ) ;
path file2 = new path ( filestatus2 . dat ) ;
int entry num = 0 ;
remote endpoint remote = session . get remote ( ) ;
thrown = false ;
string builder selectors arg builder = new string builder ( ) ;
body provider . mark ( integer . max _ value ) ; ( ( basic http entity enclosing request ) m http request ) . set entity ( new input stream entity ( body provider , body length ) ) ; }
string faulty heuristicdefinition = \ mutual _ information \ : { \ include _ negatives \ : false , \ some _ unknown _ field \ : false } ; string expected error = unknown field [ some _ unknown _ field ] ; check parse exception ( heuristic parser mapper , faulty heuristicdefinition , expected error ) ; faulty heuristicdefinition = \ chi _ square \ : { \ unknown _ field \ : true } ;
buffered writer writer = files . new buffered writer ( output file path , charset ) ; indent : 12 exp : 12 zip file zf = new zip file ( indent : 12 exp : 12 zip file name ) indent : 16 exp : 16
context . stop route ( bar ) ; context . start route ( bar ) ; out = template . request body and header ( direct : start , null , id , 456 , string . class ) ;
if ( service instanceof startup listener ) { startup listener listener = ( startup listener ) service ; add startup listener ( listener ) ; } if ( service instanceof camel context aware ) { camel context aware aware = ( camel context aware ) service ; aware . set camel context ( this ) ; }
long v = random int between ( 14 , 17 ) ;
string rendered = function . render ( null , arg list ( leading , ' - ' , from , trim source ) , null ) ;
case ' x ' :
headers header . put ( 4 , ( byte ) ( headers header . get ( 4 ) & flags . end _ headers ) ) ;
final receive port < object > take0 rp = channels . take ( ( receive port < object > ) take source ch , 0 ) ;
int not registered key group index = 2 ; registry . notify kv state unregistered ( vertices [ 0 ] . get job vertex id ( ) , new key group range ( not registered key group index , not registered key group index ) , name ) ; fail ( did not throw expected exception ) ; } catch ( illegal argument exception expected ) {
get mock endpoint ( mock : result ) . message ( 0 ) . header ( exchange . redelivery _ counter ) . is equal to ( 3 ) ; get mock endpoint ( mock : result ) . message ( 0 ) . header ( exchange . redelivery _ max _ counter ) . is null ( ) ; template . send body and header ( direct : start , a , id , 123 ) ;
threads . sleep ( 2 * refresh period ) ; final atomic boolean running = new atomic boolean ( true ) ;
if ( ( buffer [ diff ] & 128 ) = = 128 & & ( buffer [ diff + 2 ] & 32 ) = = 32 & & ( pts . get ( id ) = = null | | ( pts . get ( id ) = null & & end ) ) ) { pts . put ( id , integer . value of ( get ts ( buffer , diff + 3 ) ) ) ; } } }
iterator < mount table > it = records . iterator ( ) ;
do done ( original , sub exchange , pairs , callback , false , true ) ; return ; } total . increment and get ( ) ; }
cache . add index ( _ _ key , true ) ; for ( int i = 17 ; i < put count ; i + + ) { map . remove ( i ) ; }
map < string , object > map = ejb resource . get map ( ) ; if ( map = = null | | map . size ( ) = = 0 ) { throw new illegal state exception ( map from the resource is empty . ) ; } if ( boolean . true . equals ( map . get ( resource keys . roleref _ perm _ check ) ) ) { don ' t do this in the real world return authorization context . permit ; }
remove user ( ) ; users = admin client . realm ( realm ) . users ( ) . search ( null , null , null ) ; assert . assert equals ( 0 , users . size ( ) ) ;
try { admin client . store mgmt ops . verify or add store ( new store def , process _ name , false , service ) ; assert . fail ( non existent store create with flag disabled should have thrown error ) ; } catch ( voldemort exception ex ) { pass } for ( integer node id : cluster . get node ids ( ) ) { store definition retrieved = retrieve store on node ( new store name , node id ) ; if ( node id = = failed _ node _ id ) { assert equals ( mismatched store def should be left intact , incompatible def , retrieved ) ; } else { assert null ( store should not exist in this node , retrieved ) ; } }
string connector address = vm . get agent properties ( ) . get property ( connector _ address ) ;
list < member is available > expected = new linked list < > ( ) ;
else { int index = - 1 ; try { index = integer . parse int ( arg ) ; } catch ( number format exception ex ) { parser context . get reader context ( ) . error ( constructor argument ' + arg name + ' specifies an invalid integer , attr ) ; } if ( index < 0 ) { parser context . get reader context ( ) . error ( constructor argument ' + arg name + ' specifies a negative index , attr ) ; } if ( cvs . has indexed argument value ( index ) ) { parser context . get reader context ( ) . error ( constructor argument ' + arg name + ' with index + index + already defined using < constructor - arg > . + only one approach may be used per argument . , attr ) ; } cvs . add indexed argument value ( index , value holder ) ;
string html = < doctype html > < html lang = \ en \ > < head > < head > < body > < div > initial element < div > < body > < html > ; string expected text = new element ; string clone expect = new element in clone ; document original = jsoup . parse ( html ) ;
string normalized product id ;
for ( long old = query execution max time . get ( ) ; ( is longest query = time > old ) & & ( query execution max time . compare and set ( old , time ) ) ; old = query execution max time . get ( ) ) { nothing to do here given the odd loop structure . . . }
assert . assert equals ( 1 , response1 . get ( ) . id ) ;
if ( default port = = 0 ) { cand port = get free port ( ) ; } else { cand port = default port ; }
collections . sort ( discovered partitions , new kafka topic partition . comparator ( ) ) ;
environment env = create environment ( context ) ; object tm = env . get ( environment name . transaction _ manager ) ; transaction manager txm = new jta transaction manager ( env . get ( environment name . transaction ) , env . get ( environment name . transaction _ synchronization _ registry ) , tm ) ;
if ( next run ns < system . nano time ( ) ) { notify job consumer ( ) ; return 0 l ; } long diff = ( long ) math . ceil ( ( double ) ( next run ns - system . nano time ( ) ) ns _ per _ ms ) ; ensure consumer on time ( diff ) ; return diff ;
if ( ic . equals ( parts [ i ] ) ) { string color name = parts [ i + 1 ] . replace all ( , ) ; remove spaces from color ' s name so we can look it up node . set color ( color name ) ; break ; } } }
program method . attributes accept ( program class , this ) ; }
string writer writer = new string writer ( ) ; e . print stack trace ( new print writer ( writer ) ) ; fatal error ( writer . to string ( ) ) ; return true ; }
hystrix request context . set context on current thread ( parent thread state ) ;
s prepend shortcut label = menu . get context ( ) . get resources ( ) . get string ( com . android . internal . r . string . prepend _ shortcut _ label ) ;
offset = long . value of ( 0 ) ;
this . checkpoint manager server = new checkpoint manager server ( topology name , topology id , checkpoint mgr id , stateful storage , checkpoint manager server loop , server host , server port , server socket options ) ; }
assert true ( ( a & 0x ff000000 ) = 0 ) ; assert true ( ( a & 0x00 ff0000 ) = 0 ) ; assert true ( ( a & 0x0000 ff00 ) = 0 ) ; assert true ( ( a & 0x000000 ff ) = 0 ) ; assert true ( ( c & 0x ff000000 ) = 0 ) ;
return authentication info ; } finally {
word2 vec w2v = read word2 vec from text ( tmp file syn0 , tmp file syn1 , tmp file c , tmp file h , configuration ) ;
case inc : return prefix expression ( node , types . plus _ plus ) ; case dec : return prefix expression ( node , types . minus _ minus ) ;
final string [ ] recognized features = { parser _ settings , validation , namespaces , external _ general _ entities , external _ parameter _ entities , } ;
tester . advance input watermark ( new instant ( 149 ) ) ; assert true ( tester . should fire ( first window ) ) ; assert false ( tester . should fire ( second window ) ) ; assert false ( tester . should fire ( merged window ) ) ;
return new required field response ( true ) ;
boolean same = eid = = previous eid & & previous group id = = group id ;
model loaded model = null ;
closure closure = new closure ( ) { public java . lang . object evaluate ( ) { return new dyn any factory impl ( orb ) ; } } ; closure future = closure factory . make future ( closure ) ;
if ( expanded store = = null ) throw new illegal argument exception ( store is missing from configuration ) ; else throw new illegal argument exception ( don ' t know how to deal with this store + expanded store ) ;
file [ ] names ; names = dir . list files ( ) ; file f ; int name count = ( names = = null ) ? 0 : names . length ;
assert ( ast . get child ( 1 ) = null ) ; qbexpr qbexpr2 = new qbexpr ( alias + subquery _ tag _ 2 ) ; do phase1 qbexpr ( ( astnode ) ast . get child ( 1 ) , qbexpr2 , id + subquery _ tag _ 2 , alias + subquery _ tag _ 2 , inside view ) ; qbexpr . set qbexpr2 ( qbexpr2 ) ; }
ref time show ( is show day , true , is show minute , is show second , is show millisecond ) ; is re layout = true ; } else if ( is show hour & & m day = = 0 & & m hour = = 0 ) {
if ( start white space = - 1 ) { decoded text . append ( text . substring ( start white space , end white space ) ) ; start white space = - 1 ; }
assert array equals ( pass , . to char array ( ) ) ; }
try { result point [ ] corner points = new white rectangle detector ( image ) . detect ( ) ; point a = corner points [ 0 ] ; point b = corner points [ 1 ] ; point c = corner points [ 2 ] ; point d = corner points [ 3 ] ; } catch ( not found exception e ) { this exception can be in case the initial rectangle is white in that case , surely in the bull ' s eye , we try to expand the rectangle . int cx = image . get width ( ) 2 ; int cy = image . get height ( ) 2 ; point a = get first different ( new point ( cx + 7 , cy - 7 ) , false , 1 , - 1 ) . to result point ( ) ; point b = get first different ( new point ( cx + 7 , cy + 7 ) , false , 1 , 1 ) . to result point ( ) ; point c = get first different ( new point ( cx - 7 , cy + 7 ) , false , - 1 , 1 ) . to result point ( ) ; point d = get first different ( new point ( cx - 7 , cy - 7 ) , false , - 1 , - 1 ) . to result point ( ) ; }
s2a . close ( ) ;
if ( spdy syn stream frame . is truncated ( ) ) { spdy syn reply frame spdy syn reply frame = new default spdy syn reply frame ( stream id ) ; spdy syn reply frame . set last ( true ) ; spdy headers . set status ( spdy version , spdy syn reply frame , http response status . request _ header _ fields _ too _ large ) ; spdy headers . set version ( spdy version , spdy syn reply frame , http version . http _ 1 _ 0 ) ; channels . write ( ctx , channels . future ( channel ) , spdy syn reply frame ) ; }
get mock endpoint ( mock : a ) . expected message count ( 1 ) ; get mock endpoint ( mock : b ) . expected message count ( 1 ) ; template . send body and header ( activemq2 : queue : inbox , a , uid , 123 ) ;
start = this . start end [ 0 ] ; end = row . get next unset ( this . start end [ 1 ] + 1 ) ;
if ( defn . get local assignment ( ) = null ) return true ;
power dog local api binding config config = new power dog local api binding config ( ) ;
if ( child end < = limit ) { break ; } index - - ; } }
via header via = msg . get via header ( ) ; string proto = via . get protocol ( ) . to lower case ( ) ; print log ( using transport + proto , log level . medium ) ;
assert flags ( filesystem , libraries exexample exexample exexample model . m , target . with flavors ( internal flavor . of ( iphonesimulator - x86 _ 64 ) , internal flavor . of ( compile - pic - + sanitize ( exexample exexample model . m . o ) ) ) , paths . get ( exexample exexample model . m . o ) , * is library * true , file to entry , includes ) ;
t retval = items [ start ] ; items [ start ] = null ; start = ( start + 1 ) % size ;
result . multiply ( gen array [ i ] . a ) ; result . add ( gen array [ i ] . c ) ; bit _ map & = ( 1 l < < i ) ; } }
add workspace namespace ( ws ) ;
view empty search = adapter . on create view holder ( this , all apps grid adapter . view _ type _ empty _ search ) . m content ;
service info service = new wmsinfo impl ( ) ; ( ( wmsinfo impl ) service ) . set id ( wms - test ) ; service . set name ( wms ) ; service . set maintainer ( foo ) ; service = database . add ( service ) ;
string traverse xlink depth = request . get traverse xlink depth ( ) ;
string builder sb = ( msg = = null ) ? new string builder ( ) : new string builder ( msg ) ;
if ( available > = max stack size ) break ; pixel stack [ top + + ] = ( byte ) first ; prefix [ available ] = ( short ) old _ code ; suffix [ available ] = ( byte ) first ; available + + ; if ( ( ( available & code _ mask ) = = 0 ) & & ( available < max stack size ) ) { code _ size + + ; code _ mask + = available ; } old _ code = in _ code ;
_ map = new concurrent hash map < k , v > ( initial entries , 0 . 8f , 4 ) ;
if ( double . is infinite ( not expected ) & & not expected = = actual | | math . abs ( not expected - actual ) < = delta ) { assert . fail ( item name + should not be equal : < + not expected + ' > ' ) ; }
issuer x500 principals = parse issuer names ( temp names ) ;
assert false ( action . is executed ) ; haven ' t executed yet assert false ( response called . get ( ) ) ; assert false ( failure called . get ( ) ) ; verify ( execution service ) . execute bulk request ( bulk docs itr . capture ( ) , failure handler . capture ( ) , completion handler . capture ( ) ) ; completion handler . get value ( ) . accept ( exception ) ; assert true ( failure called . get ( ) ) ;
em . get transaction ( ) . begin ( ) ; te = em . find ( str test entity . class , id ) ; test revision listener . data = data2 ; te . set str ( y ) ; em . get transaction ( ) . commit ( ) ; timestamp3 = system . current time millis ( ) ; }
if ( capacity < record protocol . get record size ( 2 ) ) { return new sslengine result ( sslengine result . status . buffer _ overflow , handshake status , 0 , 0 ) ; }
canonical request . append ( host : ) . append ( host ) . append ( ' \ n ' ) ; canonical request . append ( ' \ n ' ) ;
future = e . get channel ( ) . write ( output ) ;
if ( source = = null ) throw new null pointer exception ( cannot decode null source array . ) ; if ( off < 0 | | off + len > source . length ) throw new illegal argument exception ( string . format ( source array with length % d cannot have offset of % d and process % d bytes . , source . length , off , len ) ) ; if ( len = = 0 ) return new byte [ 0 ] ; else if ( len < 4 ) throw new illegal argument exception ( base64 - encoded string must have at least four characters , but length specified was + len ) ;
string [ ] options = new string [ options vec . size ( ) ] ;
string key class name = tbl . get property ( composite _ rowid _ class ) ; if ( key class name = null ) { log . info ( loading composite row id class + key class name ) ; class < ? > key class = java utils . load class ( key class name ) ; class < ? extends accumulo composite row id > composite row id class = key class . as subclass ( accumulo composite row id . class ) ; return new composite accumulo row id factory ( composite row id class ) ; } return new default accumulo row id factory ( ) ;
exec result body = template word count . request body ( ( object ) test , exec result . class ) ;
throw new unsupported encoding exception ( csn ) ;
assert . assert not equals ( default owner , nonexistent owner ) ; assert . assert not equals ( default group , nonexistent group ) ; m thrown . expect ( ioexception . class ) ; m thrown . expect message ( could not update owner ) ; s tfs . set owner ( file c , nonexistent owner , nonexistent group ) ; }
trim ( sb ) ; sb . append ( > ) ; }
check alter table succeed ( alter table baz drop column num2 ; ) ;
paros database db = new paros database ( ) ; db . set database param ( new database param ( ) ) ; db . open ( file . get absolute path ( ) ) ; map < string , string > cur map = new hash map < > ( ) ; map < string , string > cmp map = new hash map < > ( ) ;
int expected value = 712828289 ; int key = 99812822 ;
bound to number . remove ( 123 , null ) ;
new append no lf ( s , start , pos ) ; }
hregion info splita = new hregion info ( td . get table name ( ) , bytes . to bytes ( aaa ) , bytes . to bytes ( ccc ) ) ; thread . sleep ( 1001 ) ;
message . add field ( testfield , testvalue ) ;
final transaction manager tm = jta platform standard testing impl . instance . transaction manager ( ) ;
final int total _ file _ size = 5 * 1024 ;
cl = prev ;
for ( int i = 1 ; i < nn count ; i + + ) { dfs cluster . restart name node ( i ) ; }
force factory injection ( rest client factory . class , null ) ; force factory injection ( system service factory . class , null ) ; app log . v ( t . tests , null factories set ) ; }
if ( visible methods count = = 0 ) { library class . methods = empty _ library _ methods ; } else { library class . methods = new library method [ visible methods count ] ; system . arraycopy ( reusable methods , 0 , library class . methods , 0 , visible methods count ) ; }
template . send body and header ( get ftp url ( ) , hello world , exchange . file _ name , hello . txt ) ;
bag bag = new tree bag ( arrays . as list ( input _ text . split ( ) ) ) ;
if ( dom instanceof multi dom ) { dom = ( ( multi dom ) dom ) . get dtm ( node ) ; } if ( dom instanceof domenhanced for dtm ) { _ enhanced dom = ( domenhanced for dtm ) dom ; } else if ( dom instanceof domadapter ) { dom idom = ( ( domadapter ) dom ) . get domimpl ( ) ; if ( idom instanceof domenhanced for dtm ) { _ enhanced dom = ( domenhanced for dtm ) idom ; } }
xml xml parent = ( xml ) parent ;
assert process ended ( pi . get id ( ) ) ; }
assert . assert true ( exists ( fc2 , test path ) ) ;
channel configuration = bit array . read bits ( 4 ) ;
query = new rule query ( ) . set query text ( not present ) ;
file . println ( < tr > < td > < a > < td > < tr > ) ; file . println ( < table > ) ; }
if ( clock1 . get ( l entry ) < clock2 . get ( l entry ) ) { is equal = false ; is greater = false ; }
network delay view . set enabled ( false ) ;
data output buffer data = new data output buffer ( ) ;
clear mappings ( key code ) ;
verify ( tsdb , times ( 1 ) ) . delete ( eq ( key ) , eq any order ( new byte [ ] [ ] { qual1 , qual2 , qual3 } ) ) ; }
assert false ( thread . has data ) ; assert null ( second tracker . get data ( false ) ) ; assert null ( local tracker . get data ( false ) ) ; log . info ( successfully made unavailable ) ;
date time dt11 = new date time ( 1723 , 11 , 2 , 0 , 0 , 0 , 0 , coptic _ utc ) ; date time dt12 = new date time ( 1723 , 12 , 2 , 0 , 0 , 0 , 0 , coptic _ utc ) ; date time dt13 = new date time ( 1723 , 13 , 2 , 0 , 0 , 0 , 0 , coptic _ utc ) ; date time dt01 = new date time ( 1724 , 1 , 2 , 0 , 0 , 0 , 0 , coptic _ utc ) ; duration field fld = dt11 . month of year ( ) . get duration field ( ) ;
return position impl . get ( head map . first entry ( ) . get key ( ) , - 1 ) ;
heapify ( ) ;
int result = integer . min _ value ; for ( column c : m columns ) { int top = c . get top ( ) ; result = math . max ( result , top ) ; } return result ; }
assert equals ( 1 , page . get ( 0 ) . get id ( ) ) ; assert equals ( 10 , page . size ( ) ) ; assert equals ( 183 , page . get total ( ) ) ; page info < country > page info = page . to page info ( ) ;
nd4j . get affinity manager ( ) . attach thread to device ( thread , device id ) ; thread . set daemon ( true ) ;
schedule application attempt ( ) ; container status cs = scheduler utils . create abnormal container status ( builder utils . new container id ( application attempt . get app attempt id ( ) , 1 ) , scheduler utils . lost _ container ) ;
producer . close ( ) ;
clear region plan from balancer plan ( region info ) ; synchronized ( this . regions in transition ) { if ( this . regions in transition . remove ( region info . get encoded name ( ) ) = null ) { this . regions in transition . notify all ( ) ; } }
t . set identity ( ) ; t . m11 = qt . w * qt . w - qt . x * qt . x ; t . m22 = t . m11 ; t . m21 = 2 * qt . x * qt . w ; t . m12 = - t . m21 ; t . m00 = qt . w * qt . w + qt . x * qt . x ; m . mul ( m , t ) ; t . transpose ( ) ; m . mul ( t , m ) ; }
if ( keep alive ) { last content future . add listener ( channel future listener . close ) ; }
else if ( get end point ( ) . is open ( ) ) fill interested ( ) ;
is vectorized = hive conf . get bool var ( conf , conf vars . llap _ io _ nonvector _ wrapper _ enabled ) & & ( utilities . get plan path ( conf ) = null ) ;
byte buffer . put int ( sys flag ) ;
byte chunk result = get url ( http : localhost : + get port ( ) + bug49922 target ) ; assert equals ( target , result . to string ( ) ) ; }
assert true ( distributor . is child ( stream aid , connection . connection stream ( ) . id ( ) , default _ priority _ weight ) ) ; assert equals ( 1 , distributor . num children ( stream aid ) ) ;
this . spout output collector = output collector ;
end x1 = next center x - dot radius - ( joining fraction * gap ) ;
if ( open file queue _ . is empty ( ) ) return ;
if ( is striped ) { striped block doesn ' t support seek to new source in2 . seek ( 0 ) ; } else { assert true ( in2 . seek to new source ( 0 ) ) ; }
string subject = mmp . get subject ( ) = null ? mmp . get subject ( ) . get string ( ) : null ;
final jmethod setter with mode = template class . method ( jmod . public , template class , setter name ) ; add accessor doc ( template class , setter with mode , schema field , setter ) ; set deprecated annotation and javadoc ( setter with mode , schema field ) ; jvar param = setter with mode . param ( type , value ) ; jvar mode param = setter with mode . param ( _ set mode class , mode ) ; jinvocation inv = setter with mode . body ( ) . invoke ( put + wrapped or direct ) . arg ( field field ) . arg ( jexpr . dotclass ( type ) ) ; data class arg ( inv , data class ) . arg ( param ) . arg ( mode param ) ; setter with mode . body ( ) . _ return ( jexpr . _ this ( ) ) ; }
default _ offset = bytes . read int ( ) ;
fold ( x = [ , , 1 ] . length , x = 3 ) ;
result = db . get metadata ( ) . get command cache ( ) . get ( db . get user ( ) , i command . get text ( ) , i command . get limit ( ) ) ;
edge . set error ( dependency not found . ) ;
i1 < < = i2 ; }
string constant . referenced member accept ( this ) ;
if ( direction path = = null ) { throw new ioexception ( the pin doesn ' t support ' set direction ' operation ) ; } files . write ( direction path , new direction . get bytes ( ) ) ;
mul ( t3 , t4 , t2 ) ; * 2 ^ 200 - 2 ^ 0 *
mortar scope root = this ;
servlet action context servlet action context = ( servlet action context ) context ;
int serverside multiplier = c . get int ( hbase . client . serverside . retries . multiplier , 10 ) ;
dp . release ( snapshot ) ;
string old name = a b ; string new name = a c ; folder old folder = folder view . rename folder ( old name , new name ) ;
cluster . restart name node ( 1 ) ; cluster . transition to standby ( 0 ) ; cluster . transition to active ( 1 ) ; assert true ( fs . mkdirs ( new path ( foo4 ) ) ) ; assert ctimes equal ( cluster ) ;
kpswitch conflict util . show panel ( panel layout ) ; switch to panel = true ;
file . delete ( ) ; }
errors errors = new errors ( ) ; key < ? > implementation key = key . get ( implementation type ) ; try { for ( method method : factory type . get raw type ( ) . get methods ( ) ) { key < ? > return type = get key ( factory type . get return type ( method ) , method , method . get annotations ( ) , errors ) ; if ( implementation key . equals ( return type ) ) { collector . add binding ( return type , implementation type ) ; } } } catch ( errors exception e ) { throw new configuration exception ( e . get errors ( ) . get messages ( ) ) ; }
other date = ( date ) left value ;
session s = open session ( ) ; s . begin transaction ( ) ; s . create query ( from human h where h . nick name = ' 1 ' | | ' ov ' | | ' tha ' | | ' few ' ) . list ( ) ; s . get transaction ( ) . commit ( ) ; s . close ( ) ; }
databus sources . remove persisted entries ( ) ;
_ encoding from = from ;
assert equals ( locale . english , icu . locale from icu locale id ( en ) ) ;
add manifest processor parameter ( activity intent fragment test . class , android manifest min froyo . xml ) ; compile result result = compile files ( to path ( activity intent fragment test . class , support fragment . java ) , to path ( activity intent fragment test . class , support activity compat . java ) , activity in manifest . class ) ; file generated file = to generated file ( activity in manifest . class ) ; assert compilation successful ( result ) ;
log . debug ( synchronous processing : message [ { } ] , destination [ { } ] , exchange . get in ( ) . get body ( ) , get endpoint ( ) . get endpoint uri ( ) ) ; try { get processor ( ) . process ( exchange ) ; } catch ( exception e ) { exchange . set exception ( e ) ; } finally { callback . done ( true ) ; } } else {
replace ( curs , xml ) ; } else { insert child ( curs , xml ) ; } }
if ( file . ends with ( . ) ) file = file . substring ( 0 , file . length ( ) - 1 ) ; return file ;
if ( member specification . annotation type = null ) { writer . print ( configuration constants . annotation _ keyword ) ; writer . println ( class util . external type ( member specification . annotation type ) ) ; writer . print ( ) ; }
get mock endpoint ( mock : result ) . message ( 0 ) . header ( exchange . redelivery _ counter ) . is equal to ( 3 ) ; template . send body and header ( direct : start , a , id , 123 ) ;
assert q ( req ( q , * : * , facet , true , facet . range , non doc values field , facet . range . start , instant . of epoch milli ( min ) . to string ( ) , facet . range . end , instant . of epoch milli ( max plus gap ) . to string ( ) , facet . range . gap , gap . to string ( ) , facet . range . method , dv ) , test strings ) ;
if ( _ num waiting . get ( ) > max ) { _ log . log always ( log . warn , refusing connection since we have exceeded our max of + max + and there are + _ num waiting + waiting already ) ; _ num waiting . decrement and get ( ) ; return null ; }
get region services ( ) . block updates ( ) ; try { if ( log . is debug enabled ( ) ) { log . debug ( in - memory flush : pushing active segment into compaction pipeline ) ; } push active to pipeline ( this . active ) ; } finally { get region services ( ) . unblock updates ( ) ; }
s = new scanner ( 03 , 456 ) ; s . use locale ( locale . english ) ; assert false ( s . has next long ( ) ) ; try { s . next long ( ) ; fail ( ) ; } catch ( input mismatch exception expected ) { } s = new scanner ( 03456 ) ;
function < integer , integer > adder = new function < integer , integer > ( ) { @ override public integer apply ( integer from ) { return from + holder . value ; } } ;
if ( material index = = - 1 ) { material index = 0 ; } material jme mat ;
list results ;
delete query . builder ( ) . table ( table ) . affects tags ( ) . build ( ) ;
assert query fails ( select * from lineitem l where true in ( select 1 in ( select 2 * l . orderkey ) ) , unsupported _ correlated _ subquery _ error _ msg ) ; }
order . verify ( spy sub ) . cancel ( any string ( ) , any ( exception . class ) ) ; order . verify ( spy sub ) . cleanup ( any ( exception . class ) ) ; }
project definition module1 _ module1 = module1 . get sub projects ( ) . get ( 0 ) ; assert that ( module1 _ module1 . get key ( ) ) . is equal to ( com . foo . project : module1 : module1 ) ; assert that ( module1 _ module1 . get name ( ) ) . is equal to ( foo sub module 1 ) ; }
buffer = new buffer ( ) ; buffer . append ( 12345 ) ; buffer . append ( new simple uploadable ( 21 ) ) ; buffer . append ( x ) ; assert equals ( 27 , buffer . size ( ) ) ; baos = new byte array output stream ( ) ; hpl = new simple progress listener ( ) ;
azure blob storage test account test acct = new azure blob storage test account ( fs , account , container , use container suffix as container name ) ; return test acct ; }
future < record metadata > request2 = accumulator . append ( tp0 , time . milliseconds ( ) , key . get bytes ( ) , value . get bytes ( ) , null , null , max _ block _ timeout ) . future ; sender . run ( time . milliseconds ( ) ) ; assert equals ( 2 , transaction manager . sequence number ( tp0 ) . long value ( ) ) ; assert equals ( 0 , transaction manager . last acked sequence ( tp0 ) ) ; assert false ( request2 . is done ( ) ) ;
int [ ] intervals = conf . get ints ( dfsconfig keys . dfs _ metrics _ percentiles _ intervals _ key ) ; return ms . register ( new name node metrics ( process name , session id , intervals , jm ) ) ;
tm . set rollback only ( ) ;
else if ( result mapping . contains key ( key ) & & filtered mappings . contains ( key ) & & cm . get ( key ) . equals ( result mapping . get ( key ) ) ) { filtered mappings . add ( key ) ; result mapping . remove ( key ) ; } } } }
list < key value > kvs = new array list < key value > ( ) ; key value kv = new key value ( row , column _ family , column _ qualifier _ 2 , val _ 1 ) ; kvs . add ( new key value ( row , column _ family , column _ qualifier _ 2 , val _ 1 ) ) ; kvs . add ( new key value ( row , column _ family , column _ qualifier , val _ 1 ) ) ; kvs . add ( new key value ( row , column _ family , column _ qualifier _ 2 , val _ 1 ) ) ; filter . filter row ( kvs ) ;
m _ sub exp queue = new queue ( ) ;
configuration new conf = new configuration ( conf ) ; name node . initialize generic keys ( new conf , ns2 , nn1 ) ; assert equals ( global - default , new conf . get ( key ) ) ;
m _ logger . debug ( string . format ( sql : % s \ n , sql ) ) ; ;
assert not equals ( rs , hash set ) ; assert not equals ( hash set , rs ) ; hash set . add ( 3 ) ; assert equals ( rs , hash set ) ; assert equals ( hash set , rs ) ;
ioe . print stack trace ( ) ; return null ;
hash map < string , datanode info > host map = new hash map < string , datanode info > ( ) ; for ( located block b : blocks . get located blocks ( ) ) { for ( datanode info i : b . get locations ( ) ) { host map . put ( i . get network location ( ) + + i . get name ( ) , i ) ; } } for ( map . entry < string , datanode info > entry : host map . entry set ( ) ) { datanode descriptor result = null ; datanode info i = entry . get value ( ) ; result = new datanode descriptor ( i , i . get network location ( ) , i . get host name ( ) , i . get capacity ( ) , i . get dfs used ( ) , i . get remaining ( ) , i . get namespace used ( ) , i . get xceiver count ( ) ) ; if ( this . is good target ( result , blocksize , integer . max _ value , this . consider load , new array list < datanode descriptor > ( ) ) ) { i dont care about per rack load . datanode descriptor [ ] r = { result } ; return r ; } }
return index ; }
model node op = new model node ( ) ; op . get ( op _ addr ) . add ( host , master ) ; op . get ( op ) . set ( reload ) ; op . get ( admin - only ) . set ( false ) ; domain master lifecycle util . execute await connection closed ( op ) ;
for ( metrics base m : registry . get metrics list ( ) ) { m . push metric ( metrics record ) ; }
cache . cache block ( single blocks [ i ] . cache key , single blocks [ i ] ) ; expected cache size + = single blocks [ i ] . cache block heap size ( ) ;
for ( int i = 0 ; i < 100 ; i + + ) { connection source conn source = binding . get read connection source ( ) ; connection conn = conn source . get connection ( ) ; assert equals ( connection . get description ( ) . get connection id ( ) , connection . get description ( ) . get connection id ( ) ) ; conn . release ( ) ; conn source . release ( ) ; } connection . release ( ) ;
execution execution = runtime service . create execution query ( ) . execution id ( process instance . get id ( ) ) . message event subscription name ( new message ) . single result ( ) ;
assert equals ( find datanode should be 0 , 0 , block info list . get ( i ) . find datanode ( dd ) ) ;
return this . to node id = node id ; } offsets . set ( rebase ( node id ) , high cache id ) ; high cache id + = count ; }
try { bytes read = is . read ( b , 6 , 5 ) ; fail ( test 3 : index out of bounds exception expected . ) ; } catch ( index out of bounds exception e ) { expected }
if ( lp . view type = item _ view _ type _ header _ or _ footer & & child . get top ( ) < non header tops [ lp . column ] ) { non header tops [ lp . column ] = child . get top ( ) ; }
if ( key serializer definition . has compression ( ) | | value serializer definition . has compression ( ) ) { store = new compressing store ( store , new compression strategy factory ( ) . get ( key serializer definition . get compression ( ) ) , new compression strategy factory ( ) . get ( value serializer definition . get compression ( ) ) ) ; }
log . trace ( handled expected exception , e ) ; s . get transaction ( ) . rollback ( ) ; } finally { s . close ( ) ; } }
cancel poll state ( ) ; return ; } if ( err = command exception . error . op _ not _ allowed _ before _ reg _ nw ) { loge ( handle poll state result : ril returned an error where it must succeed + ar . exception ) ; } } else try {
lines = lines . stream ( ) . map ( p - > p . replace all ( \ \ [ . * \ \ ] , ) ) . collect ( collectors . to list ( ) ) ; set < string > edges = immutable set . copy of ( lines . sub list ( 1 , lines . size ( ) - 1 ) ) ;
read into buff ( zip4j raf , short buff ) ; file header . set version made by ( raw . read short little endian ( short buff , 0 ) ) ;
width = m max width > 0 ? m max width : get preferred width ( ) ;
response future . get ( ) ; fail ( expected to get a timeout exception since the queued producer batch should have been expired ) ; } catch ( execution exception e ) {
web app . set copy web dir ( false ) ; web app . set copy web inf ( false ) ; web app . set generate quick start ( true ) ;
dst . write _ long ( seq length ) ;
on view ( with id ( r . id . list ) ) . check ( matches ( with list size ( usernames supplier . number _ of _ users ) ) ) ;
final int n = element count ( ) ; for ( syntax tree node item : _ contents ) { method gen . mark chunk start ( ) ; item . translate ( class gen , method gen ) ; method gen . mark chunk end ( ) ; }
assert . assert true ( false ) ;
form = ( decimal format ) number format . get instance ( locale . us ) ; symbols = new decimal format symbols ( ) ; number = form . parse ( - + symbols . get infinity ( ) , new parse position ( 0 ) ) ;
if ( model . is enabled ( ) ) { * * * paint the text normally * g . set color ( b . get foreground ( ) ) ; swing utilities2 . draw string underline char at ( c , g , text , mnemonic index , text rect . x + get text shift offset ( ) , text rect . y + fm . get ascent ( ) + get text shift offset ( ) ) ; } else { * * * paint the text disabled * * * g . set color ( b . get background ( ) . brighter ( ) ) ; swing utilities2 . draw string underline char at ( c , g , text , mnemonic index , text rect . x , text rect . y + fm . get ascent ( ) ) ; g . set color ( b . get background ( ) . darker ( ) ) ; swing utilities2 . draw string underline char at ( c , g , text , mnemonic index , text rect . x - 1 , text rect . y + fm . get ascent ( ) - 1 ) ; }
util . log ( this , log . warn , system ready ) ; break ; case finish booting :
logger . fatal ( your abstract smack interoperability layer + implementation + cannot be instantiated properly . + please fix the implementation ) ;
scheduler node . release container ( containers . get ( 0 ) . get container id ( ) , true ) ; scheduler node . release container ( containers . get ( 2 ) . get container id ( ) , true ) ; scheduler node . release container ( containers . get ( 1 ) . get container id ( ) , true ) ; allocate containers ( scheduler node ) ;
while ( source reader . next ( src rel path , src file status ) ) {
if ( is custom font ) space item text . set typeface ( custom font ) ;
persist person ( 1 , person1 , 10 , new address otooracle no sql ( 1 . 1 , address 1 ) ) ;
order last order = orders . get ( orders . size ( ) - 1 ) ;
. set should store result ( false )
entity manager entity manager = entity manager factory ( ) . create entity manager ( ) ; assert false ( setup problem , jta status helper . is active ( testing jta platform impl . instance . get transaction manager ( ) ) ) ; try { entity manager . join transaction ( ) ; fail ( expected join transaction ( ) to fail since there is no active jta transaction ) ; } catch ( transaction required exception expected ) { }
client . get params ( ) . set connection manager timeout ( timeout ms ) ; client . execute method ( get method ) ; return get method ; } } ) ; }
mock task . handle ( new task tattempt event ( get last attempt ( ) . get attempt id ( ) , task event type . t _ add _ spec _ attempt ) ) ; launch task attempt ( get last attempt ( ) . get attempt id ( ) ) ; commit task attempt ( get last attempt ( ) . get attempt id ( ) ) ; mock task . handle ( new task tattempt event ( get last attempt ( ) . get attempt id ( ) , task event type . t _ attempt _ succeeded ) ) ;
if ( type = null ) { re = array . new instance ( type , args . length ) ; for ( int i = 0 ; i < args . length ; i + + ) { array . set ( re , i , args [ i ] ) ; } return re ; } return args ;
log . warn ( no family for + cell ) ;
hbck = do fsck ( conf , false ) ; assert no errors ( hbck ) ; } finally {
if ( allow lazy persist & & v . is transient storage ( ) ) { datanode . get metrics ( ) . incr ram disk blocks write fallback ( ) ; } replica in pipeline new replica info ;
plan nodes = compile to fragments ( select c , sd from + ( select c , sum ( d ) as sd from p1 group by c ) t1 ) ; assert equals ( 2 , plan nodes . size ( ) ) ; pn = plan nodes . get ( 0 ) . get child ( 0 ) ; check seq scan ( pn , t1 , c , sd ) ; assert not null ( pn . get inline plan node ( plan node type . projection ) ) ; pn = pn . get child ( 0 ) ; assert true ( pn instanceof hash aggregate plan node ) ; pn = pn . get child ( 0 ) ; assert true ( pn instanceof receive plan node ) ; pn = plan nodes . get ( 1 ) ;
return new fields query ( q , field names , field _ operator ) ;
property names = get property names ( true , str . substring ( w - 1 ) ) ;
boolean interrupted = false ; for ( executor e : executors copy ) { if ( ( e instanceof executor service ) ) { continue ; } executor service es = ( executor service ) e ; for ( ; ; ) { shutdown now ( es ) ; try { if ( es . await termination ( 100 , time unit . milliseconds ) ) { break ; } } catch ( interrupted exception ex ) { interrupted = true ; } } } if ( interrupted ) { thread . current thread ( ) . interrupt ( ) ; }
if ( pinfo . partition key = = null ) { all matched = false ; }
put storage stats ( storage node1 , 0 , 1 , 2 , 3 ) ;
int derivative state = ( int ) math . signum ( math . abs ( m over scroll offset ) - math . abs ( overscroll ) ) ; if ( derivative state = m over scroll derivative & & derivative state = = 1 & & overscroll < 0 ) { m over scroll counter + + ; } else if ( overscroll > 0 | | m current mode = = orientation . landscape ) { m over scroll counter = 0 ; } m over scroll derivative = derivative state ; m over scroll offset = overscroll ;
case varchar : case string : return new slice dictionary column writer ( column index , type , compression , buffer size , is dwrf ) ; case list : {
final word data lemma = lemma list . get ( lemma list index + + ) ;
return utf8 string . from bytes ( new byte [ 0 ] ) ; } else {
assert equals ( 2 , name node . get name node metrics ( ) . images failed . get ( ) ) ;
return this . region . memstore flush size ;
zip entry ze = zis . get next entry ( ) ;
v _ 5 = limit - cursor ; lab4 : do {
rect f rect = new rect f ( text . get bounding box ( ) ) ;
node = sm . get root node ( ) . get sink propagator ( ) . get first left tuple sink ( ) ;
this . capacity = light weight gset . compute capacity ( 2 . 0 , blocks map ) ;
assert equals ( 0 , slow searcher listener . number of times called . get ( ) ) ; assert false ( query should be waiting for warming to finish , query succeeded . get ( ) ) ;
generation manager . checkpoint ( ) ;
if ( opts . num client threads < = 0 ) { throw new illegal argument exception ( number of clients must be > 0 ) ; }
now = system . current time millis ( ) ; timeout = now + 60000 ; do { thread . sleep ( 20 ) ; } while ( num calls . get ( ) < max concurrent attempts + 1 & & now < timeout ) ; assert equals ( max concurrent attempts + 1 , num calls . get ( ) ) ;
case net error . err _ cert _ common _ name _ invalid : case net error . err _ cert _ date _ invalid : case net error . err _ cert _ authority _ invalid : case net error . err _ cert _ contains _ errors : case net error . err _ cert _ no _ revocation _ mechanism : case net error . err _ cert _ unable _ to _ check _ revocation : case net error . err _ cert _ revoked : case net error . err _ cert _ invalid : case net error . err _ cert _ weak _ signature _ algorithm : case net error . err _ cert _ non _ unique _ name : return error _ ok ;
object [ ] serde row = new object [ column count ] ;
synchronized ( get _ members _ map ) { list < members notification > set = get _ members _ map . get ( group ) ; if ( set = = null ) get _ members _ map . put ( group , set = new array list < > ( ) ) ; set . add ( callback ) ; }
check xpath count ( result , md : domains md : dimension domain [ md : size = ' 0 ' ] , 2 ) ;
point first spline point = spline points . get ( 0 ) ;
string current service = context . get parameters ( ) . get ( service ) ;
populate near cache ( second context , data structure methods . get , default _ record _ count , null ) ;
for ( server name backup : initial . get backup masters ( ) ) { try {
if ( ctxt . get effective major version ( ) < 2 ) { default is elignored = true ; default defered syntax allowed as literal = true ; return ; } if ( ctxt . get effective major version ( ) = = 2 ) { if ( ctxt . get effective minor version ( ) < 5 ) { default defered syntax allowed as literal = true ; } if ( ctxt . get effective minor version ( ) < 4 ) { default is elignored = true ; return ; } } jsp config descriptor jsp config = ctxt . get jsp config descriptor ( ) ;
expected = test _ string ;
this . set enabled ( false ) ;
m touch state = touch _ state _ scrolling ;
when ( backup strategy implementation . perform incremental backup ( any ( ) , any ( ) , any ( ) ) ) . then return ( new potentially erroneous state < > ( backup stage outcome . failure , null ) ) ;
fs . set xattr ( path , name1 , value1 ) ; xattrs = fs . get xattrs ( path ) ; assert . assert equals ( xattrs . size ( ) , 1 ) ; assert . assert array equals ( value1 , xattrs . get ( name1 ) ) ; fs . remove xattr ( path , name1 ) ;
entry . set value ( true ) ;
if ( this span . get local name ( ) . equals ( span ) ) { xml utils . tree copy ( this span , xhtml block ) ;
function < entry < string , long > , string > to string func = e - > string . format ( % dx % s , e . get value ( ) , e . get key ( ) ) ;
ips = inet address . get all by name ( hostname ) ;
assert equals ( 0 , root . get from ( ) ) ; assert equals ( 35 , root . get to ( ) ) ; assert equals ( 6 , root . num children ( ) ) ;
assert equals ( ordering on the second field should be continous . , previous max . f1 - 1 , tuple2 . f0 . f1 . long value ( ) ) ; } previous max = tuple2 . f1 ; } }
test _ util . get hbase admin ( ) . disable table ( table ) ; delete region ( conf , tbl . get table descriptor ( ) , bytes . to bytes ( b ) , bytes . to bytes ( c ) , false , true , false ) ; don ' t rm from fs test _ util . get hbase admin ( ) . enable table ( table ) ; hbase fsck hbck = do fsck ( conf , false ) ; assert errors ( hbck , new error _ code [ ] { error _ code . not _ in _ meta _ or _ deployed , error _ code . hole _ in _ region _ chain } ) ;
event bus . tag = word press - event ; event bus . builder ( ) . log no subscriber messages ( false ) . send no subscriber event ( false ) . throw subscriber exception ( true ) . install default event bus ( ) ; rest client utils . set user agent ( get user agent ( ) ) ;
int mods = method . get modifiers ( ) ;
if ( phone . debug _ phone ) { log ( on connected in or out : connect time = + connect time ) ; } if ( is incoming ) { outgoing calls only process next post dial char ( ) ; }
integer value = ( integer ) s . create query ( select mod ( s . count , 2 ) from simple as s where s . id = 10 ) . list ( ) . get ( 0 ) ;
if ( is shift jis ( charset name ) ) { final string vendor shift jis = s vendor shift jis map . get ( vendor ) ; if ( vendor shift jis = null ) { return vendor shift jis ; } }
expression exp = ( expression ) args . get ( 0 ) ;
artifact action input = create source artifact ( foo action - input . txt ) ; artifact action output = create derived artifact ( foo action - output . txt ) ; atomic integer execution counter = new atomic integer ( 0 ) ; scratch . file ( action input . get path ( ) . get path string ( ) , foo ) ;
for ( int i = 0 ; i < commands . length ; i + + ) { icommand command = commands [ i ] ; if ( command . get builder name ( ) . equals ( autotools configuration builder . builder _ id ) ) { ignore it } else { if ( command . get builder name ( ) . equals ( old _ autotools _ builder _ id ) ) { icommand new command = description . new command ( ) ; new command . set builder name ( builder _ id ) ; command = new command ; } make sure that the autotools builder precedes the managed builder or the remote synchronized builder . if ( command . get builder name ( ) . equals ( builder _ id ) | | command . get builder name ( ) . equals ( remote _ builder _ id ) ) { add autotools configuration builder just before builder icommand new command = description . new command ( ) ; new command . set builder name ( autotools configuration builder . builder _ id ) ; command list . add ( new command ) ; } command list . add ( command ) ; } } return command list . to array ( new icommand [ command list . size ( ) ] ) ;
assert equals ( 4 , size ) ;
dom builder . set end ( il . append ( new aload ( dom builder . get index ( ) ) ) ) ; index = cpg . add interface methodref ( translet _ output _ interface , end document , ( ) v ) ; il . append ( new invokeinterface ( index , 1 ) ) ;
slice _ from ( r ) ; break ; case 5 :
verify ( event publisher ) . post update ( ui8 item2 , new decimal type ( 0x00 ) ) ;
method declaration declaration = ( method declaration ) declaring node ;
_ type factory = type factory . default instance ( ) ; simple mix in resolver mixins = new simple mix in resolver ( null ) ;
late reverb gain = eax db to amp ( e [ 14 ] ) ; convert
admin . major compact ( table _ name ) ; quick poll ( new callable < boolean > ( ) { public boolean call ( ) throws exception { return fs . list status ( store path ) . length = = 1 ; } } , 5000 ) ; } finally {
map < string , string > params = new hash map < string , string > ( ) ; params . put ( open hab alias , zway bridge handler . get zway bridge configuration ( ) . get open hab alias ( ) ) ; params . put ( open hab item name , open habitem . get name ( ) ) ; device command command = new device command ( open hab connector , remove open hab item , params ) ; string message = zway bridge handler . get zway api ( ) . get device command ( command ) ;
bitwise shift checker ( 50 , long . max _ value , 3 ) ; bitwise shift checker ( 51 , 3 , long . max _ value ) ; bitwise shift checker ( 52 , long . max _ value , - 3 ) ; bitwise shift checker ( 53 , - 3 , long . max _ value ) ; bitwise shift checker ( 54 , long . min _ value + 1 , 6 ) ; bitwise shift checker ( 55 , 6 , long . min _ value + 1 ) ; bitwise shift checker ( 56 , long . min _ value + 1 , - 6 ) ; bitwise shift checker ( 57 , - 6 , long . min _ value + 1 ) ; try { bitwise shift checker ( 19 , 3 , 63 ) ; fail ( ) ; } catch ( exception ex ) { assert true ( ex . get message ( ) . contains ( would produce int64 _ min , which is reserved for sql null values ) ) ; } try { bitwise shift checker ( 20 , - 3 , 63 ) ; fail ( ) ; } catch ( exception ex ) { assert true ( ex . get message ( ) . contains ( would produce int64 _ min , which is reserved for sql null values ) ) ; } client client = get client ( ) ;
push executor = more executors . listening decorator ( execs . new blocking single threaded ( appenderator _ merge _ % d , 1 ) ) ;
fruit [ 0 ] = new fruit ( ) ; array store exception
mark down ( server , cluster , uri1 ) ; properties = store . get ( cluster ) ; assert not null ( properties ) ; assert equals ( properties . get partition data map ( uri2 ) . get ( default partition accessor . default _ partition _ id ) . get weight ( ) , 1 . 5d ) ; assert equals ( properties . uris ( ) . size ( ) , 1 ) ;
if ( old data = null ) { on release resources ( old data ) ; todo on release resources ( ) is empty }
raw id3 = build single frame tag ( tit2 , new byte [ ] { id3 _ text _ encoding _ utf _ 8 } ) ;
if ( f dtdgrammar = null ) f dtdgrammar . element ( element name , augs ) ;
itr . remove ( ) ; } else {
if ( dst = = null ) { dst = create compatible dest raster ( src ) ; } else if ( height = dst . get height ( ) | | width = dst . get width ( ) ) { throw new illegal argument exception ( width or height of rasters do not + match ) ; } else if ( num bands = dst . get num bands ( ) ) { make sure that the number of bands are equal throw new illegal argument exception ( number of bands in src + num bands + does not equal number of bands in dest + dst . get num bands ( ) ) ; }
apn context = m apn contexts . get ( phone . apn _ type _ hipri ) ;
final boolean expect commas = session factory ( ) . get dialect ( ) . supports tuple counts ( ) ; assert equals ( expect commas , had commas ) ;
if ( colon idx = = uri spec len - 1 | | uri spec . char at ( colon idx + 1 ) = = ' ' ) { throw new malformed uriexception ( scheme specific part cannot be empty . ) ; }
try { h . fatal error ( null ) ; fail ( null pointer exception expected ) ; } catch ( saxexception e ) { fail ( null pointer exception expected ) ; } catch ( null pointer exception e ) { expected }
cache < string , string > cache = start cache ( ) ;
font f = ( ( styled document ) d ) . get font ( v . get attributes ( ) ) ; container c = get container ( ) ; font metrics fm = ( c = null ) ? c . get font metrics ( f ) : toolkit . get default toolkit ( ) . get font metrics ( f ) ; int width = get characters per tab ( ) * fm . char width ( ' w ' ) ; int tb = ( int ) get tab base ( ) ; return ( float ) ( ( ( ( int ) x - tb ) width + 1 ) * width + tb ) ; }
return ( name . starts with ( _ task ) | | name . starts with ( tmp prefix ) ) ;
if ( equals ( m2 params , m1 params ) ) return true ; for ( int i = 0 ; i < m1 params . length ; i + + ) { itype binding m1 param = m1 params [ i ] ; if ( contains type variables ( m1 param ) ) m1 param = m1 param . get erasure ( ) ; try
result = overwrite ranges ( current action . sub list ( 0 , current action . size ( ) - 1 ) ) ;
version version21 = version . new instance ( 2 , 1 ) ; shuffle . store version ( version21 ) ; assert . assert equals ( version21 , shuffle . load version ( ) ) ; shuffle . close ( ) ; shuffle = new shuffle handler ( ) ; shuffle . set auxiliary local path handler ( path handler ) ; shuffle . set recovery path ( new path ( tmp dir . to string ( ) ) ) ; shuffle . init ( conf ) ; try { shuffle . start ( ) ; assert . fail ( incompatible version , should expect fail here . ) ; } catch ( service state exception e ) { assert . assert true ( exception message mismatch , e . get message ( ) . contains ( incompatible version for state db schema : ) ) ; }
tree tree = dv . get tree ( ) ; tree . remove child ( node ) ; dv . add document ( update , null ) ; tree . structure component ( ) ; tree . expand node ( tree . get tree node ( ) ) ; return ;
node new callee = node util . new qname ( compiler , base class node . get qualified name ( ) + . call , callee , goog . base ) ; n . replace child ( callee , new callee ) ; compiler . report change to enclosing scope ( new callee ) ; } else {
if ( user function instanceof wrapping function ) { user function = ( ( wrapping function < ? > ) user function ) . get wrapped function ( ) ; } else { break ; }
chunk = m _ array [ m _ last chunk ] = new char [ m _ chunk size ] ; } available = m _ chunk size ; m _ first free = 0 ; } }
job conf job conf = mr . create job conf ( ) ;
list < string > stores = new array list ( ) ;
jt conf . set ( fs . default . name , file : ) ; jt conf . set ( mapred . jobtracker . task scheduler , job queue task scheduler . class . get name ( ) ) ; simulator job tracker sjob tracker = simulator job tracker . start tracker ( jt conf , 0 ) ; system . out . println ( created the simulator job tracker successfully ) ; sjob tracker . offer service ( ) ; fake job client jbc = new fake job client ( sjob tracker , no maps , no reduces ) ;
doc = xml dom parser . parse ( src , true ) ;
can wrap any = utilities . get use vectorized input file format ( conf , this ) ;
conf = new configuration ( false ) ;
mmap reader . close ( ) ;
byte [ ] body buffer = body output stream . to byte array ( ) ;
block reader block reader = null ; try { block reader = block reader test util . get block reader ( cluster . get file system ( ) , lblock , 0 , test _ file _ len ) ; block reader . read fully ( buf , 0 , test _ file _ len ) ; } finally { if ( block reader = null ) block reader . close ( ) ; } byte expected [ ] = dfstest util . calculate file contents from seed ( seed , test _ file _ len ) ;
temp = t1 [ hash1 ] ; t1 [ hash1 ] = x ; x = temp ; int hash2 = h2 ( x , 0 , x . length ) ;
replace obsolete tabs ( ts1 ) ; replace obsolete tabs ( ts2 ) ;
if ( this . get class ( ) = new instance . get class ( ) ) { throw new udfargument exception ( invalid copy between + this . get class ( ) . get name ( ) + and + new instance . get class ( ) . get name ( ) ) ; }
for ( schema aware aware : schema aware ) { aware . inform ( this ) ; }
css = css . replace ( * + placeholder + * , ) ;
int grid child top ; int grid child bottom ; if ( flow down ) { grid child top = get lowest positioned bottom ( ) ; grid child bottom = grid child top + get child height ( child ) ; } else { grid child bottom = get highest positioned top ( ) ; grid child top = grid child bottom - get child height ( child ) ; } for ( int i = 0 ; i < m column count ; i + + ) { update column top if needed ( i , grid child top ) ; update column bottom if needed ( i , grid child bottom ) ; }
rc . stats ( ) ;
if ( expected len = clone group . get clone unit length ( ) ) { return false ; }
headers . put ( invoked , invoked ) ; if ( invoked = = 1 ) { return mock : a ; } else if ( invoked = = 2 ) { return mock : b , mock : c ; } else if ( invoked = = 3 ) { return direct : foo ; } else if ( invoked = = 4 ) { return mock : result ; }
return math . acos ( ( ( big decimal ) value ) . double value ( ) ) ; }
assert true ( eval1 f1 = = eval2 f1 & & eval1 acc = = eval2 acc ) ; evaluation eval via method = model . evaluate ( new list data set iterator ( collections . singleton list ( test ) ) ) ;
iheader header = item = null ? get header of ( item ) : null ;
input = input . replace ( _ _ entity _ import _ _ , entity . get fully qualified type name ( ) ) ;
db . new transaction ( ) ;
fake clock . forward nanos ( 9 ) ; assert null ( internal subchannel . obtain active transport ( ) ) ; verify ( mock transport factory , times ( transports created ) ) . new client transport ( addr , authority , user _ agent , no _ proxy ) ; assert equals ( transient _ failure , internal subchannel . get state ( ) ) ; assert no callback invoke ( ) ;
for ( option option : options ) { if ( name . equals ignore case ( option . get long name ( ) ) ) { return option ; } } for ( option option : options ) { if ( name . equals ignore case ( option . get short name ( ) ) ) { return option ; } } for ( option option : options ) { if ( name . equals ignore case ( option . get arg name ( ) ) ) { return option ; } }
for ( long n = 0 ; n < = long math . floor _ sqrt _ max _ long ; n + + ) { long actual = ( long ) math . sqrt ( n * n ) ; assert true ( actual = = n ) ; }
assert that ( backlog bytes metrics . gauges ( ) , is iterable with size . < metric result < gauge result > > iterable with size ( 1 ) ) ;
command = safe string interner . safe intern ( element . get attribute ( itool . command ) ) ;
c = get random ( ' \ u2440 ' , ' \ u245 f ' ) ;
sb . append ( css . substring ( m . start ( ) , m . end ( ) ) ) ; append index = m . end ( ) ; } }
reconnect task . command . run ( ) ;
return new broadcast list resource ( ) . set arguments ( user id or uid , topic ) ;
if ( original = = null ) { latest . bits | = method member . is _ new ; new or changed methods . add ( latest ) ; } else { if ( original . equals ( latest ) ) { check more than just name descriptor new or changed methods . add ( latest ) ; }
payment accounts combo box . set on action ( payment accounts combo box selection handler ) ; currency combo box . set on action ( currency combo box selection handler ) ; }
command executor = shell command executor factory . create executor ( executor context ) ;
update binary represenation ( ) ;
detach all views from parent ( ) ;
run ( insert into table + db name + . ptned _ tmp partition ( b = 10 ) values ( ' + ptn _ data _ 1 [ 0 ] + ' ) , driver ) ;
procedure testing utility . wait no procedure running ( proc executor ) ; proc env . assert sorted exec list ( num _ procs ) ; }
long s ; long n = to ; s = math . min ( a - from , b - a ) ; vec swap ( x , from , b - s , s ) ; s = math . min ( d - c , n - d - 1 ) ; vec swap ( x , b , n - s , s ) ;
preconditions . check state ( action . execute unconditionally ( ) , action ) ; try { ( ( skyframe aware action ) action ) . establish skyframe dependencies ( env ) ; } catch ( skyframe aware action . exception base e ) { throw new action execution exception ( e , action , false ) ; } }
included = true ; } else { player . send message ( chat color . yellow + healed by + plugin . to name ( sender ) + . ) ; } }
new query = query . get text ( ) + \ n + keyword _ limit + + ( offset . long value ( ) + length . long value ( ) ) ;
while ( light data index < num lights * 3 ) { light data . set vector4 in array ( 0f , 0f , 0f , 0f , light data index ) ; light data index + + ; } return cur index ; }
if ( theme instanceof standard chart theme ) { standard chart theme sct = ( standard chart theme ) theme ; if ( sct . get name ( ) . equals ( legacy ) ) { bar renderer . set default bar painter ( new standard bar painter ( ) ) ; xybar renderer . set default bar painter ( new standard xybar painter ( ) ) ; } else { bar renderer . set default bar painter ( new gradient bar painter ( ) ) ; xybar renderer . set default bar painter ( new gradient xybar painter ( ) ) ; } }
column family store cfs = keyspace . open ( keyspace ( ) ) . get column family store ( current table ( ) ) ; cfs . force blocking flush ( ) ; cfs . disable auto compaction ( ) ; cfs . force major compaction ( ) ; assert equals ( 1 , cfs . get live sstables ( ) . size ( ) ) ; sstable reader reader = cfs . get live sstables ( ) . iterator ( ) . next ( ) ; try ( isstable scanner scanner = reader . get scanner ( ) ) { try ( unfiltered row iterator row iterator = scanner . next ( ) ) { only 1 partition data assert false ( scanner . has next ( ) ) ; list < unfiltered > expected unfiltereds = new array list < > ( ) ; row iterator . for each remaining ( expected unfiltereds : : add ) ; test different throttle for ( integer throttle : arrays . as list ( 2 , 3 , 4 , 5 , 11 , 41 , 99 , 1000 , 10001 ) ) { try ( isstable scanner scanner for throttle = reader . get scanner ( ) ) { assert true ( scanner for throttle . has next ( ) ) ; try ( unfiltered row iterator row iterator for throttle = scanner for throttle . next ( ) ) { assert false ( scanner for throttle . has next ( ) ) ; verify throttle iterator ( expected unfiltereds , row iterator for throttle , new throttled unfiltered iterator ( row iterator for throttle , throttle ) , throttle ) ; } } } } }
string s val = field . get text content ( ) ; object val = data type parser . s _ parse data type ( d type , s val ) ; pair < field , object > pair = new pair < field , object > ( f , val ) ; return pair ; }
assert equals ( 5000 , properties . metrics rolling statistical window in milliseconds ( ) . get ( ) . int value ( ) ) ;
updates . sort ( ) ;
return find by first analysis ( ) ;
if ( is empty buckets ) { num buckets = 0 ; } else { num buckets = conf . get num buckets ( ) ; bucket object inspectors = get object inspector array ( reduce sink bucket type infos ) ; bucket vector extract row = new vector extract row ( ) ; bucket vector extract row . init ( reduce sink bucket type infos , reduce sink bucket column map ) ; bucket field values = new object [ reduce sink bucket type infos . length ] ; } if ( is empty partitions ) { non partition random = new random ( 12345 ) ; } else { partition object inspectors = get object inspector array ( reduce sink partition type infos ) ; partition vector extract row = new vector extract row ( ) ; partition vector extract row . init ( reduce sink partition type infos , reduce sink partition column map ) ; partition field values = new object [ reduce sink partition type infos . length ] ; }
for ( oidentifiable from id : from ids ) { final overtex v = to vertex ( from id ) ; if ( v = null ) { for ( oedge e : v . get edges ( odirection . out , label ) ) { edges . add ( e ) ; } } }
ask . set capability ( resources . create resource ( - 1024 , - 1 ) ) ;
debug = true . equals ignore case ( ( string ) options . get ( debug ) ) ; debug native = true . equals ignore case ( ( string ) options . get ( debug native ) ) ; if ( debug native = = true ) { debug = true ; }
log . severe ( channel . read returned negative + wrote ) ;
q = em . create query ( with first composite col clause ) ; q . set parameter ( user id , mevivs ) ; results = q . get result list ( ) ; assert . assert not null ( results ) ; assert . assert equals ( 1 , results . size ( ) ) ;
if ( m delegate = = null ) return super . dispatch key event ( event ) ; if ( event . get key code ( ) = = key event . keycode _ back ) { key event . dispatcher state state = get key dispatcher state ( ) ; if ( state = null ) { if ( event . get action ( ) = = key event . action _ down & & event . get repeat count ( ) = = 0 ) { state . start tracking ( event , this ) ; return true ; } else if ( event . get action ( ) = = key event . action _ up & & event . is canceled ( ) & & state . is tracking ( event ) ) { on back pressed ( ) ; return true ; } } }
pipeline pipeline = new pipeline ( ) . set stages ( new pipeline stage [ ] { label indexer , feature indexer , gbt , label converter } ) ;
assert equals ( 2 , categories . length ) ; assert array equals ( new int [ ] { 5 , 6 } , categories ) ;
assert . assert true ( dut . offer ( 104 l , init4 ) ) ; assert . assert true ( dut . offer ( 2 l , sentinel2 ) ) ; assert . assert true ( dut . offer ( 105 l , init5 ) ) ; assert . assert null ( dut . poll ( ) ) ; assert . assert equals ( init4 , dut . drain ( ) ) ; assert . assert equals ( init5 , dut . drain ( ) ) ; assert . assert null ( dut . drain ( ) ) ; }
post delayed ( this , item _ caption _ cycle _ delay ) ;
final int num buckets = get initial table size ( this . available memory . size ( ) , this . segment size , partition fan out , this . avg record len ) ; init table ( num buckets , ( byte ) partition fan out ) ; final type comparator < bt > build type comparator = this . build side comparator ;
assert null ( doc . field type ( integer ) ) ;
assert not equals ( failed on round + cnt , null , ds ) ;
return collect filters ( m action to filter . get ( matching . get action ( 0 ) ) , matching ) ;
for ( string f : field names ) { query ( q , * : * , sort , f + desc ) ; query ( q , * : * , sort , f + asc ) ; }
url = rewrite url ;
fire channel open ( channel ) ;
try { zkc . curator with super auth ( ) . create ( ) . for path ( barrier ) ; } catch ( node exists exception ignore ) { ignored }
set . add ( obj ) ; return ; }
return bytes with instance creation captured ( bytes , this _ class , record request mapping handler mapping instance ) ; }
minecraft forge . event _ bus . register ( this . event wrapper ) ;
get endpoint ( ) . set content resolved from resource ( true ) ;
get activity ( ) . finish ( ) ; return ;
if ( list snapshots ) { simple date format df = new simple date format ( yyyy - mm - dd ' t ' hh : mm : ss ) ; system . out . printf ( % - 20s | % - 20s | % s % n , snapshot , creation time , table name ) ; for ( snapshot description desc : get snapshot list ( conf ) ) { system . out . printf ( % - 20s | % 20s | % s % n , desc . get name ( ) , df . format ( new date ( desc . get creation time ( ) ) ) , desc . get table name as string ( ) ) ; } return 0 ; } root dir = fsutils . get root dir ( conf ) ;
if ( download ( base url + native + native version + + jar file , bin dir + jar file ) ) { try { checksum = verify sha256 checksum ( new file ( bin dir , jar file ) . get absolute path ( ) , sha256 ) ; } catch ( exception e ) {
if ( ( ( ( tx containing this . get lock time ( ) < transaction . locktime _ threshold ) & & ( n lock time . compare to ( transaction . locktime _ threshold _ big ) ) < 0 ) | | ( ( tx containing this . get lock time ( ) > = transaction . locktime _ threshold ) & & ( n lock time . compare to ( transaction . locktime _ threshold _ big ) ) > = 0 ) ) ) throw new script exception ( script error . script _ err _ unsatisfied _ locktime , locktime requirement type mismatch ) ;
final atomic boolean interrupted = new atomic boolean ( ) ;
connection factory . set copy message on send ( false ) ;
if ( discovery service = = null ) { discovery service = new zone minder discovery service ( this , 30 ) ; }
if ( p my tri info . assigned group [ 0 ] = = null & & p my tri info . assigned group [ 1 ] = = null & & p my tri info . assigned group [ 2 ] = = null ) { p my tri info . flag & = ( orient _ preserving ) ; p my tri info . flag | = ( p group . orient preservering ? orient _ preserving : 0 ) ; } }
final int cell count = width size m min cell size ; final int cell size remaining = width size % m min cell size ; if ( cell count = = 0 ) { give up , nothing fits . set measured dimension ( width size , 0 ) ; return ; }
resources . subtract from ( pending resource , container . get resource ( ) ) ;
workspace info ws info = get catalog ( ) . get workspace by name ( gs ) ;
response = static content . instance . serve html file ( this ) ;
if ( last output stream status . is active ( ) & & channel statuses [ channel index ] . stream status . is active ( ) ) { long watermark millis = watermark . get timestamp ( ) ; if the input watermark ' s value is less than the last received watermark for its input channel , ignore it also . if ( watermark millis > channel statuses [ channel index ] . watermark ) { channel statuses [ channel index ] . watermark = watermark millis ; previously unaligned input channels are now aligned if its watermark has caught up if ( channel statuses [ channel index ] . is watermark aligned & & watermark millis > = last output watermark ) { channel statuses [ channel index ] . is watermark aligned = true ; } now , attempt to find a new min watermark across all aligned channels find and output new min watermark across aligned channels ( ) ; } }
set unicode ( true ) ;
if ( principal = = null ) { principal = principal factory . create principal ( request . get context ( ) . get realm ( ) , account . get principal ( ) , account . get roles ( ) ) ; session . set principal ( principal ) ; session . set auth type ( keycloak - saml ) ; } request . set user principal ( principal ) ;
rmapp app2 = rm1 . submit app ( 1 * gb , app , user , null , default ) ; mock am am2 = mock rm . launch and register am ( app2 , rm1 , nm1 ) ; am1 . allocate ( * , 4 * gb , 1 , new array list < container id > ( ) ) ;
array list < double > float long = get double list ( hf , range facets , long _ ld , double , median ) ; array list < double > float long test = calculate number stat ( transform lists ( float long test start , 0 , 29 , 4 , true , true , true , true , true ) , median ) ; assert equals ( get raw response ( ) , float long , float long test ) ;
if ( check exists ( job name , job group ) ) { logger . info ( > > > > > > > > > add job fail , job already exist , job group : { } , job name : { } , job group , job name ) ; return false ; }
map < integer , string > m = new tree map < > ( ) ; hash h = murmur hash . get instance ( ) ; int per client rows = ( opts . total rows opts . num client threads ) ; try { for ( int j = 0 ; j < opts . num client threads ; j + + ) { test options next = new test options ( opts ) ; next . start row = j * per client rows ; next . per client run rows = per client rows ; string s = mapper . write value as string ( next ) ; log . info ( client = + j + , input = + s ) ; byte [ ] b = bytes . to bytes ( s ) ; int hash = h . hash ( new byte array hash key ( b , 0 , b . length ) , - 1 ) ; m . put ( hash , s ) ; } for ( map . entry < integer , string > e : m . entry set ( ) ) { out . println ( e . get value ( ) ) ; } } finally { out . close ( ) ; } return input dir ;
for ( string file : module . get files ( ) ) { start log . debug ( adding module specified file : % s , file ) ; add file ( module , file ) ; } }
string is send typing notif property = service . gui . send _ typing _ notifications _ enabled ; string is send typing notif = config service . get string ( is send typing notif property ) ; if ( is send typing notif = = null ) is send typing notif = util activator . get resources ( ) . get settings string ( is send typing notif property ) ;
missing required attributes . remove ( ejb3 subsystem xmlattribute . value ) ;
if ( plural attribute . collection type . collection . equals ( plural attribute . get collection type ( ) ) ) { fetch = construct join ( ( collection attribute < x , y > ) plural attribute , jt ) ; } else if ( plural attribute . collection type . list . equals ( plural attribute . get collection type ( ) ) ) { fetch = construct join ( ( list attribute < x , y > ) plural attribute , jt ) ; } else if ( plural attribute . collection type . set . equals ( plural attribute . get collection type ( ) ) ) { fetch = construct join ( ( set attribute < x , y > ) plural attribute , jt ) ; } else { fetch = construct join ( ( map attribute < x , ? , y > ) plural attribute , jt ) ; }
string payload substring = data . substring ( cmd type . get response ( ) . length ( ) ) ;
order ( orders . size ( ) > 1 ? bucket order . compound ( orders ) : orders . get ( 0 ) ) ; return this ;
synchronized ( lock ) { if ( last used = null ) { managers in use . remove ( last used ) ; lock . notify all ( ) ; } }
filter all ( ) ; if ( is filtered ( ) ) {
bundle arguments = get arguments ( ) ; sublime options options = null ;
map view type from ( item ) ;
result + = integer . parse int ( parts [ idx ] ) * 3600000 l ; idx + + ; }
notify attribute assigned ( name , value , old value ) ; }
locale l = locale . us ; time zone utc = time zone . get time zone ( utc ) ; calendar c = calendar . get instance ( utc , l ) ; c . set ( calendar . month , calendar . february ) ; c . set ( calendar . day _ of _ month , 10 ) ; c . set ( calendar . hour _ of _ day , 0 ) ; long this year = c . get time in millis ( ) ;
main activity . m drawer layout . set status bar background color ( color drawable . get color ( ) ) ;
list < ? extends nic > nics = _ network mgr . get nics ( vm . get id ( ) ) ; nic nic in same network = null ; for ( nic nic : nics ) { if ( nic . get network id ( ) = = load balancer . get network id ( ) ) { nic in same network = nic ; break ; } } if ( nic in same network = = null ) { invalid parameter value exception ex = new invalid parameter value exception ( vm + instance id + cannot be added because it doesn ' t belong in the same network . ) ; ex . add proxy object ( vm , instance id , instance id ) ; throw ex ; } if ( s _ logger . is debug enabled ( ) ) { s _ logger . debug ( adding + vm + to the load balancer pool ) ; }
get get = new get ( row ) ; get . add column ( familiy , qualifier ) ; result result = region . get ( get , null ) ; assert equals ( 1 , result . size ( ) ) ; key value kv = result . raw ( ) [ 0 ] ;
verify ( in flight map , times ( 1 ) ) . get ( 0 l ) ;
block manager test util . check heartbeat ( cluster . get name node ( ) . get namesystem ( ) . get block manager ( ) ) ;
device support = create btdevice support ( device ) ;
if ( caching enabled changed | | caching max age changed ) { caching info changed = true ; save = true ; } }
val1 . set value ( 45 l ) . set exists ( true ) ;
input stream input = new file input stream ( database _ file ) ; byte array output stream output = new byte array output stream ( ) ; try { copy the input stream to the output stream ioutils . copy ( input , output ) ; } catch ( exception exception ) { throw new runtime exception ( error reading sqlite database file to byte array . , exception ) ; } return output . to byte array ( ) ;
response = client . execute ( new http get ( url1 ) ) ; try { assert . assert equals ( http servlet response . sc _ ok , response . get status line ( ) . get status code ( ) ) ; assert . assert equals ( 2 , integer . parse int ( response . get first header ( value ) . get value ( ) ) ) ; } finally { http client utils . close quietly ( response ) ; }
if ( m view . dispatch unhandled move ( focused , direction ) ) { finish key event ( event , send done , true ) ; return ; } } } }
result = services a . execute operation ( read dist cache string jdbcstore string keyed table op ) ;
result . get ( model description constants . op _ addr ) . set ( address ) ; return result ; }
fs . delete recursively ( core . cluster state directory ( ) ) ;
m list background view = find view by id ( r . id . list _ background ) ;
rb channel . close ( ) ;
test harness . process watermark ( new watermark ( 2500 ) ) ; expected output . add ( new stream record < > ( new tuple3 < > ( key1 - 1 , 1500 l , 4500 l ) , 4499 ) ) ;
gles20 . gl enable vertex attrib array ( ma texture coord loc ) ; gl util . check gl error ( gl enable vertex attrib array ) ;
ctxt . generate declaration ( double array iterator , private iterator to iterator ( final double [ ] a ) { \ n + return ( new iterator ( ) { \ n + int index = 0 ; \ n + public boolean has next ( ) { \ n + return index < a . length ; } \ n + public object next ( ) { \ n + return new double ( a [ index + + ] ) ; } \ n + public void remove ( ) { } \ n + } ) ; \ n + } ) ;
ids processed but not acknowledged . remove all ( checkpoint . f1 ) ;
url = clu classloader . get resource ( resource name . substring ( 1 ) ) ; }
exec = new async execution ( callable , scheduler , future , config for ( new retry policy ( ) . retry on ( illegal argument exception . class ) ) ) ;
operation set basic telephony < ? > op set telephony = provider . get operation set ( operation set basic telephony . class ) ;
is create group disabled = config service . get boolean ( net . java . sip . communicator . impl . gui . main . contactlist . + create _ group _ disabled , false ) ;
binder . bind ( remote task factory . class ) . to ( http remote task factory . class ) . in ( scopes . singleton ) ; new exporter ( binder ) . export ( remote task factory . class ) . with generated name ( ) ; binder . bind ( remote task stats . class ) . in ( scopes . singleton ) ;
shift left ( unused index , bin count - 1 ) ; bin count - - ; }
pivot x = width 2 ;
options text . append ( \ n \ n general options : \ n \ n ) ;
mock . all messages ( ) . body ( ) . is not equal to ( kaboom ) ; mock . expected bodies received in any order ( hello world , goodday world , bye world , hi world ) ; try { template . send body ( direct : start , hello world , goodday world , kaboom , bye world , hi world ) ; fail ( should thrown an exception ) ; } catch ( camel execution exception e ) { illegal argument exception cause = assert is instance of ( illegal argument exception . class , e . get cause ( ) ) ; assert equals ( forced , cause . get message ( ) ) ; }
oresult set res = db . query ( select from cdr where search _ class ( ' rrc . 20161229193002 . prod _ r4 . eno . data ' ) = true ) ; assertions . assert that ( res ) . has size ( 2 ) ;
jdbc mutable acl service . delete acl ( get top parent oid ( ) , false ) ;
if ( is security enabled ( ) & & timeline v1 service enabled ) { add timeline delegation token ( app context . get amcontainer spec ( ) ) ; }
int irow = row ; int icol = col 2 + 1 ; next : while ( irow < 10 ) { while ( icol < 5 ) { if ( ( clone & mask [ irow ] [ icol ] ) = = 0 ) break next ; icol + + ; } irow + + ; icol = 0 ; }
if ( session tracking modes . contains ( session tracking mode . ssl ) ) { if ( session tracking modes . size ( ) > 1 ) { throw new illegal argument exception ( sm . get string ( application context . set session tracking . iae . ssl , get context path ( ) ) ) ; } } this . session tracking modes = session tracking modes ;
if ( is enabled ( deserialization feature . fail _ on _ unknown _ properties ) ) { p . skip children ( ) ; return true ; }
{ error msg . template _ undef _ err , la plantilla ' ' { 0 } ' ' no est \ u00e0 definida en aquest full d ' ' estils . } ,
phi insn phi = ( phi insn ) insn ; for ( ssa basic block pred : phi . pred blocks for reg ( reg v , ssa meth ) ) { block n = pred ; next function = next function . live _ out _ at _ block ; handle tail recursion ( ) ; }
for ( class < ? > finder : dynamic finders ) { bind finder ( finder ) ; }
if ( create shard context ( ) . get mapper service ( ) . full name ( date _ field _ name ) = null ) { if ( random boolean ( ) ) { query . time zone ( random date time zone ( ) . get id ( ) ) ; } if ( random boolean ( ) ) { query . format ( yyyy - mm - dd ' t ' hh : mm : ss . ssszz ) ; } }
slice _ del ( ) ; break ; } return true ; }
view view to deselect = null ;
array list < string > ls ;
if ( delay + duration < total duration ) { animator animator = view animation utils . create circular reveal ( child , reveal center x , reveal center y , to radius , to radius ) ; animator . set start delay ( delay + duration ) ; animator . set duration ( total duration - ( delay + duration ) ) ; animations . add ( animator ) ; }
variable info [ ] result ; if ( counter > 0 ) { result = new variable info [ counter ] ; system . arraycopy ( variables , 0 , result , 0 , counter ) ; } else { result = new variable info [ 0 ] ; }
set text ( drawee span string builder , buffer type . spannable ) ;
string list wrapper wr = new string list wrapper ( a , null , b ) ; assert equals ( apos to quotes ( { ' list ' : [ ' a ' , null , ' b ' ] } ) , mapper . write value as string ( wr ) ) ;
get conf ( ) . set boolean ( get conf key ( contract options . is _ case _ sensitive ) , false ) ;
tb . set name ( roddy ) ; errors = new bean property binding result ( tb , tb ) ; test validator . validate ( tb , errors ) ; assert false ( errors . has field errors ( name ) ) ; }
execution status status = status tracker . get status ( execution id ) ;
if ( fields ( ) . contains key ( uri ) ) { trigger decoding object o = get field ( uri ) ; set id ( o . to string ( ) ) ; return super . get id ( ) ; } return get index uri ( ) ;
mock nm nm2 = new mock nm ( host1 + : 2351 , 8 * gb , rm . get resource tracker service ( ) ) ; nm2 . register node ( ) ;
final e existing = remove ( index , element ) ;
future . get ( ) ; } catch ( execution exception e ) { log . debug ( exception in concurrent task , e ) ; if ( exception = = null ) { exception = e ; } } }
conf . set int ( yarn configuration . federation _ cache _ time _ to _ live _ secs , 0 ) ; return conf ;
int i = 1 ;
lab15 : do { v _ 5 = limit - cursor ; lab16 : do {
rect paint . set color ( underline color ) ;
if ( m dismiss button = null ) { if ( text utils . is empty ( m dismiss button . get text ( ) ) ) { m dismiss button . set visibility ( gone ) ; } else { m dismiss button . set visibility ( visible ) ; } }
protocol provider . get connection ( ) . remove packet interceptor ( this ) ;
new socket permission ( * , accept , listen ) ) , permissions . xml ) ; return ear ; }
l . set form ( legend form . line ) ;
expected amount = twenty _ four ;
test same ( this . ext = 2 ) ;
for ( fsapp attempt app : apps with demand ) { if ( resources . is none ( pending ) ) { resource app min share = app . get pending demand ( ) ; resources . subtract from non negative ( app min share , app . get fairshare starvation ( ) ) ; if ( resources . greater than ( policy . get resource calculator ( ) , scheduler . get cluster resource ( ) , app min share , pending ) ) { resources . subtract from non negative ( app min share , pending ) ; pending = none ( ) ; } else { resources . subtract from non negative ( pending , app min share ) ; } app . set minshare starvation ( app min share ) ; context . get starved apps ( ) . add starved app ( app ) ; } else { reset minshare starvation in case we had set it in a previous iteration app . reset minshare starvation ( ) ; } }
integer integer fact = ( new random ( ) ) . next int ( integer . max _ value - 1 ) + 1 ;
array list < attribute > new structure = new array list < attribute > ( ) ; for ( mining field meta info m : m _ mining meta ) { new structure . add ( m . get field as attribute ( ) ) ; } for ( derived field meta info d : m _ derived meta ) { new structure . add ( d . get field as attribute ( ) ) ; }
if ( source instanceof domsource ) { result = new domresult ( ) ; } else if ( source instanceof saxsource ) { result = new saxresult ( ) ; } else if ( source instanceof st axsource | | source instanceof stream source ) { result = null ; } if ( source = null ) {
logger . error ( database connection error { } , e . get message ( ) ) ; } else if ( e instanceof runtime exception ) {
s = new scanner ( 03 , 456 ) ; s . use locale ( locale . english ) ; try { s . next big integer ( ) ; fail ( ) ; } catch ( input mismatch exception expected ) { } s = new scanner ( 03456 ) ;
file handle src = gdx . files . internal ( data 8 . 12 . mp3 ) ;
len + = ( ( len + line length - 1 ) line length ) * chunk separator length ;
utilities . clear work map ( job conf ) ;
string broker id = token user session . get note ( details . identity _ provider ) ; broker id = broker id = = null ? token user session . get note ( identity provider . external _ identity _ provider ) : broker id ; if ( broker id = = null | | broker id . equals ( get config ( ) . get alias ( ) ) ) { event . detail ( details . reason , requested _ issuer has not linked ) ; event . error ( errors . invalid _ request ) ; return exchange not linked no store ( uri info , authorized client , token user session , token subject ) ; } return exchange session token ( uri info , event , authorized client , token user session , token subject ) ; } else {
response = _ connector . get response ( get ctx index . html http 1 . 0 \ r \ n \ r \ n ) ; assert true ( response . starts with ( http 1 . 1 403 forbidden ) ) ; }
broadcast input readers [ i ] = new mutable record reader < ioreadable writable > ( get environment ( ) . get input gate ( current reader offset ) , get environment ( ) . get task manager info ( ) . get tmp directories ( ) ) ;
assert invalid ( buffer cell . live ( c , 0 , bb ( 1 ) , cell path . create ( byte buffer util . bytes ( ( long ) 4 ) ) ) ) ;
dimension default value setting default value setting = new dimension default value setting ( ) ; default value setting . set strategy type ( strategy . minimum ) ; setup feature custom dimension ( scanning _ angle _ dimension , scanning angle , default value setting ) ; feature type info time elevation custom = get catalog ( ) . get feature type by name ( time _ elevation _ custom . get local part ( ) ) ;
while ( active buffer = = null | | active buffer . has remaining ( ) ) { if ( log . is debug enabled ( ) ) log . debug ( waiting { } ms to read , timeout ms ) ; if ( timeout ms < 0 ) {
bounding box lbbox = new bounding box ( current box ) ;
if ( is currently loading ) { return ; }
session . get session factory ( ) . get cache ( ) . evict query region ( query . cache . person ) ; end : : caching - management - evict - native - example [ ] } ) ; }
int num heap arenas = 0 ; this . alloc = new pooled byte buf allocator ( prefer direct , num heap arenas , num direct arenas , page size , max order ) ; }
if ( activity = = null ) activity = application status . get last tracked focused activity ( ) ; if ( tab = = null & & activity instanceof chrome tabbed activity ) { tab = ( ( chrome tabbed activity ) activity ) . get activity tab ( ) ; } context app context = context utils . get application context ( ) ;
assert equals ( expected status . get path ( ) . to uri ( ) . get path ( ) , actual status . get path ( ) . to uri ( ) . get path ( ) ) ;
if ( count > 0 ) {
sub item . set header ( expandable item ) ; expandable item . add sub item ( sub item ) ; }
overdrag percent = math . abs ( 1 . 0f - m percent ) ;
for ( int idx = 0 ; idx < width * height ; + + idx ) { row [ idx ] = false ; }
for ( int i = 0 ; i < 2 ; i + + ) { driver . process ( topic1 , expected keys [ i ] , x + expected keys [ i ] ) ; }
if ( m running ) return ;
validator . validate add user group service ( config ) ; fail ( ) ; } catch ( security config exception ex ) {
file mocked path = easy mock . create mock ( file . class ) ;
byte [ ] output = new byte [ uncompressed size + size _ of _ long ] ; int output offset = 0 ; int input offset = 0 ; int cumulative uncompressed block length = 0 ; while ( total decompressed count < uncompressed size ) { if ( total decompressed count = = cumulative uncompressed block length ) { cumulative uncompressed block length + = integer . reverse bytes ( input . get int ( input offset ) ) ; input offset + = size _ of _ int ; } int compressed chunk length = integer . reverse bytes ( input . get int ( input offset ) ) ; input offset + = size _ of _ int ; int decompression size = decompress ( lzo decompressor , input , input offset , compressed chunk length , output , output offset ) ; total decompressed count + = decompression size ; output offset + = decompression size ; input offset + = compressed chunk length ; }
op service . invoke on all partitions ( service _ name , new key load status operation factory ( map name , exception ) ) ; }
string result = get as string ( wms?bbox = + bbox + & styles = & layers = + layers + & format = + open layers map output format . mime _ type + & request = get map + & width = 550 + & height = 250 + & srs = epsg : 4326 & version = 1 . 3 . 0 ) ; assert true ( result . index of ( forbidden format ) > 0 ) ; wms . get get map mime types ( ) . clear ( ) ; wms . set get map mime type checking enabled ( false ) ; get geo server ( ) . save ( wms ) ;
mockito . do return ( arrays . as list ( r1 , r2 , r3 , lr1 ) ) . when ( rs ) . get regions ( ) ; chore . chore ( ) ;
dom . set style attribute ( get element ( ) , position , relative ) ; resizable widget collection . get ( ) . add ( this ) ; redraw ( ) ; }
unit = translate type ( example , @ deprecated class example { } ) ;
else { update . add ( params . make range tombstone ( metadata . comparator , clustering ) ) ; } }
assert equals ( number , number _ type . to string ( ) ) ; assert true ( number _ type . has display name ( ) ) ; assert equals ( number , number _ type . get display name ( ) ) ; asserts . assert resolves to same ( number _ type ) ;
if ( node . lo kid = null ) { write recursively ( out , node . lo kid ) ; }
db dir = new file ( props . get property ( orbconstants . db _ dir _ property , props . get property ( user . dir ) + file sep + default db dir ) ) ;
meta table accessor . add location ( put , sn , sn . get startcode ( ) , - 1 , 2 ) ; meta . put ( put ) ;
backup info . set failed msg ( e . get message ( ) ) ;
for ( int i = 0 ; i < 10 ; i + + ) { try { client . put ( key + i , val + i ) ; assert equals ( must read value back , val + i , client . get ( key + i ) . get value ( ) ) ; } catch ( exception e ) { fail ( should be not see any failures ) ; } }
return sslengine result . handshake status . need _ wrap ;
session . will passivate ( ) ; if ( log . is debug enabled ( ) ) log . debug ( session passivating id = { } , id ) ; _ session data store . store ( id , session . get session data ( ) ) ; if ( get eviction policy ( ) = = evict _ on _ session _ exit ) {
init drawable ( ) ;
if ( state machine . transition to running ( ) ) {
get bdb database ( ) . delete ( transaction , key entry ) ;
if ( insn arr . length = 0 ) { for ( insn node insn : insn arr ) { if ( insn = null ) { int line = insn . get source line ( ) ; if ( line = 0 ) { mth . set source line ( line - 1 ) ; } break ; } } }
add function ( decls , exslt _ date _ time , new function impl ( date , xpath type . string , optional _ string ) ) ;
if ( none . equals ( name ) ) { disable creation ( ) ; }
mandatory positionals = 1 , parameters = { @ param ( name = proto _ toolchain _ attr , positional = false , named = true , type = string . class ) } ) public static java info get runtime toolchain provider ( skylark rule context skylark rule context , string proto toolchain attr ) throws eval exception { transitive info collection runtime = get proto toolchain provider ( skylark rule context , proto toolchain attr ) . runtime ( ) ; return java info . builder . create ( ) . add provider ( java compilation args provider . class , java info . get provider ( java compilation args provider . class , runtime ) ) . build ( ) ;
if ( adaptor = null ) { adaptor . remove mib ( this , context name ) ; }
am1 . allocate ( null , null ) ;
string maker maker = new string maker ( url ) ;
string operation key = create operation key ( info . get name ( ) , signature . length ) ; operations . put ( operation key , signature ) ; }
return new hcat output format writer ( cntxt , sp ) ; }
assert equals ( 1 + ( needs merging ? 1 : 0 ) , commits . size ( ) ) ;
inclusive stop filter = new inclusive stop filter ( bytes . to bytes ( inclusive stop filter ) ) ; assert true ( inclusive stop filter . are serialized fields equal ( protobuf util . to filter ( protobuf util . to filter ( inclusive stop filter ) ) ) ) ; }
system . arraycopy ( bucket , i + 1 , bucket , i , bucket . length - i - 1 ) ; bucket [ bucket . length - 1 ] = null ; n - - ; return true ; } }
localizer event matcher matches l0 req = new localizer event matcher ( container0 , creds0 , vis0 , localizer event type . request _ resource _ localization ) ; verify ( localizer bus ) . handle ( arg that ( matches l0 req ) ) ; assert equals ( resource state . downloading , local . get state ( ) ) ;
test cases . put ( user - agent : linux 2 . 6 . 35 upn p 1 . 0 nds _ mhf dlnadoc 1 . 50 , samsung smt - g7400 ) ;
return state = = state . reading ? buf . limit ( ) : buf . position ( ) ;
compile result result = compile files ( bundle sparse array compile success activity . class ) ;
log . warn ( ex . get message ( ) , ex ) ; } finally {
ioutils . spins ( dir ) ;
m controller . set hierarchy ( null ) ;
synchronized ( oid table ) { reinitialize mapping table locked ( ) ; alg name = name table . get ( algid ) ; } return ( alg name = = null ) ? algid . to string ( ) : alg name ;
break ; stop acquiring more rows for this batch
timber . e ( e , exception while logging ) ;
if ( parameters . contains key ( configuration ) ) { map < string , object > config props = introspection support . extract properties ( parameters , config . ) ; endpoint helper . set reference properties ( this . get camel context ( ) , config props , parameters ) ; endpoint helper . set properties ( this . get camel context ( ) , config props , parameters ) ; }
for ( int i = 0 ; i < old len - offset ; i + + ) { entities [ position + i ] = null ; }
int x = ( int ) ( f * 20 ) ;
session = open session ( ) ;
assert equals ( info1 , dfs2 . rolling upgrade ( rolling upgrade action . query ) ) ; dfs2 . mkdirs ( baz ) ; log . info ( restart cluster 2 ) ;
val1 . set value ( 30 ) . set exists ( true ) ; val2 . set exists ( false ) ; func . get double ( ) ; assert false ( func . exists ( ) ) ;
s . default read object ( ) ; key universe = get key universe ( key type ) ;
assert that ( under test . select groups ( db . get session ( ) , builder ( ) . organization uuid ( organization dto . get uuid ( ) ) . membership ( in ) . build ( ) , user3 . get id ( ) , 0 , 10 ) ) . is empty ( ) ;
for ( i = 0 ; i < keys . size ( ) ; i + + ) { key = keys . get ( i ) . to string ( ) ; value = m _ info . get ( key ) . to string ( ) ; if ( key . equals ( line . separator ) ) value = utils . back quote chars ( value ) ; result + = key + : + value + \ n ; } return result ;
for ( ; _ int < = _ int size ; _ int + + ) { int bits = _ bits [ _ int ] ; if ( bits = 0 ) { any bits set? for ( ; _ bit < 32 ; _ bit + + ) { if ( ( bits & _ masks [ _ bit ] ) = 0 ) { if ( + + _ pos = = pos ) { _ node = ( ( _ int < < 5 ) + _ bit ) - 1 ; return ( _ node ) ; } } } _ bit = 0 ; } } return ( 0 ) ;
if ( col < 2 ) { return false ; } else { return true ; }
if ( use end border _ & & i = = lines _ . size ( ) - 1 ) end rows _ . add ( i ) ; if ( new state = state ) {
rq1 . remove queue ( queue2 ) ;
if ( _ local = = null ) { _ local = method gen . add local variable2 ( get escaped name ( ) , _ type . to jctype ( ) , null ) ; }
system . arraycopy ( current names , 0 , current names = new char [ current ] [ ] , 0 , current ) ;
switch case new node = new switch case ( ) ; if ( node . pat = null ) { new node . set expression ( ( expression ) convert ( node . get expression ( ) ) ) ; } else { new node . set is default ( true ) ; } return new node ;
written bytes = output . sample data ( input , sample current nal bytes remaining , false ) ;
list < string > expected xmls = new array list < > ( ) ;
assert true ( persister . get property nullability ( ) [ entity metamodel . get property index ( address ) ] ) ;
inc = new increment ( rows [ 0 ] ) ;
return chunk size ;
for ( float z = top left . z ; z < bottom left . z ; z + = 1 ) { for ( float x = top left . x ; x < top right . x ; x + = 1 ) { if ( x < 0 | | z < 0 | | x > = size | | z > = size ) continue ; t = get triangle ( x , z ) ; if ( t = null & & bbox . collide with ( t , results ) > 0 ) return 1 ; } } return 0 ;
byte array output stream stream = new unsafe byte array output stream ( length < 32 ? 32 : length + 32 ) ; try { bitcoin serialize to stream ( stream ) ; } catch ( ioexception e ) { cannot happen , we are serializing to a memory stream . } if ( serializer . is parse retain mode ( ) ) { a free set of steak knives if there happens to be a call to this method we gain an opportunity to recache the byte array and in this case it contains no bytes from parent messages . this give a dual benefit . releasing references to the larger byte array so that it it is more likely to be gc ' d . and preventing double serializations . e . g . calculating merkle root calls this method . it is will frequently happen prior to serializing the block which means another call to bitcoin serialize is coming . if we didn ' t recache then internal serialization would occur a 2nd time and every subsequent time the message is serialized . payload = stream . to byte array ( ) ; cursor = cursor - offset ; offset = 0 ; recached = true ; length = payload . length ; return payload ; }
if ( slice = null ) { collection < replica > all replicas for shard = slice . get replicas ( ) ; if ( all replicas for shard = = null ) { throw new solr exception ( solr exception . error code . bad _ request , no replicas found in shard collection : + shard + + collection name ) ; } if ( all replicas for shard . size ( ) = = 1 ) { throw new solr exception ( solr exception . error code . bad _ request , there is only one replica available in shard collection : + shard + + collection name + . cannot delete that . ) ; } if ( all replicas for shard . size ( ) < = count ) { throw new solr exception ( solr exception . error code . bad _ request , there are lesser num replicas requested to be deleted than are available in shard collection : + shard + + collection name + requested : + count + available : + all replicas for shard . size ( ) + . ) ; } }
if ( utils . is rtl ( ) ) {
files . delete ( workspace . get path ( dep1 dep1 _ new . h ) ) ; files . delete ( workspace . get path ( dep2 dep2 _ new . h ) ) ; workspace . build and return output ( target . get fully qualified name ( ) ) ; verify headers ( workspace , dep1 exported symlink tree folder , dep1 dep1 . h ) ; verify headers ( workspace , dep2 exported symlink tree folder , dep2 dep2 . h ) ;
query builder max ee rev qb = root query builder . new sub query builder ( versions middle entity name , middle _ entity _ alias _ def _ aud _ str ) ;
if ( columns = = null | | columns . size ( ) = = 0 ) { there is always a null column in the wildcard column query . has null column = true ; use a specialized scan for wildcard column tracker . this . columns = new scan wildcard column tracker ( scan info . get min versions ( ) , max versions , oldest unexpired ts ) ; } else { whether there is null column in the explicit column query has null column = ( columns . first ( ) . length = = 0 ) ; we can share the explicit column tracker , diff is we reset between rows , not between storefiles . this . columns = new explicit column tracker ( columns , scan info . get min versions ( ) , max versions , oldest unexpired ts ) ; }
patterns [ 8 ] = pattern . compile ( ^ ( [ lnmtsd ] [ ' â ] ) ( . * ) , pattern . case _ insensitive | pattern . unicode _ case ) ;
scan ( project _ sample _ path ) ;
assert equals ( fs meta block read cache hit cnt , all _ cf _ metrics . get block metric name ( block category . meta , false , block metric type . cache _ hit ) ) ;
list < string > tbl names = cached store . get tables ( db name , * ) ;
m _ num non zero = 0 ; } else {
collections . sort ( packages , new comparator < package info > ( ) { @ override public int compare ( package info o1 , package info o2 ) { return package library utils . type of library ( session _ , o1 . get library ( ) ) . compare to ( package library utils . type of library ( session _ , o2 . get library ( ) ) ) ; } } ) ;
if ( ( method target . get remaining buffer ( ) . ends with ( - - ) | | method target . get remaining buffer ( ) . ends with ( - ) ) & & . equals ( last option value ) ) { suggest option key ( shell context , translated , results , method target , options , unspecified , last option value , already specified ) ; candidates . add all ( results ) ; return 0 ; }
return new required field response ( true ) ;
return create instance ( obj cclass . get by type ( cls ) , handle , after marshaled flags , false ) ; }
then \ n + end \ n ; final knowledge builder knowledge builder = knowledge builder factory . new knowledge builder ( ) ; knowledge builder . add ( new byte array resource ( drl . get bytes ( ) ) , resource type . drl ) ; assert true ( knowledge builder . has errors ( ) ) ; }
for ( string node : zkutil . list children and watch for new children ( zk controller . get watcher ( ) , zk controller . get abort znode ( ) ) ) { string abort node = zkutil . join znode ( zk controller . get abort znode ( ) , node ) ; abort ( abort node ) ; }
eh cache . evict ( personnel dto . class , personnel dto . class + _ + person1 . get person id ( ) ) ; assert equals ( 1 , eh cache . size ( ) ) ; cache = cache provider . get cache ( cache name ) ; assert . assert not null ( cache ) ;
map < string , list < string > > syns = new hash map < > ( ) ; syns . put ( happy , arrays . as list ( glad , cheerful , joyful ) ) ; assert jput ( endpoint , jsonutil . to json ( syns ) , response header status = = 0 ) ; assert jq ( endpoint , synonym mappings managed map happy = = [ ' cheerful ' , ' glad ' , ' joyful ' ] ) ;
int pos = 3 ; while ( + + i buf [ pos ] = = 0 ) { - - pos ; } f ( salt , iteration count , i buf , out bytes , out pos ) ;
counter counter = metrics . counter ( ns , name ) ; counter . inc ( ) ; counter . inc ( 5 l ) ; counter . dec ( ) ; counter . dec ( 5 l ) ; }
this . store . add ( new key value ( row , family , qf1 , 1 , ( byte [ ] ) null ) , null ) ; this . store . add ( new key value ( row , family , qf2 , 1 , ( byte [ ] ) null ) , null ) ; this . store . add ( new key value ( row , family , qf3 , 1 , ( byte [ ] ) null ) , null ) ; this . store . add ( new key value ( row , family , qf4 , 1 , ( byte [ ] ) null ) , null ) ; this . store . add ( new key value ( row , family , qf5 , 1 , ( byte [ ] ) null ) , null ) ; this . store . add ( new key value ( row , family , qf6 , 1 , ( byte [ ] ) null ) , null ) ;
for ( int j = 0 ; j < k + 1 ; j + + ) { sph [ all host indexes . get ( j ) ] = sph [ all host indexes . get ( j ) ] + 1 ; } partitions . add ( new partition description ( k ) ) ; }
final int num keys = 10 ; final int max records per key = 500 ; map < string , set < integer > > all data = smoke test driver . generate ( kafka , num keys , max records per key ) ; smoke test driver . verify ( kafka , all data , max records per key ) ; break ; case process :
byte [ ] data = message . get body ( byte [ ] . class ) ;
if ( description . get ejb home view ( ) = null & & description . get ejb remote view ( ) = = null ) { final string remote class name = this . infer remote interface from home ( description . get ejb home view ( ) . get view class name ( ) , module , deployment reflection index , description ) ; description . add ejb object view ( remote class name ) ; }
if ( property file refs . contains ( arg ) ) { property file refs . add ( arg ) ; } return ; }
notification interval = 5 * 60 * 1000 ;
int range end range = m ranges . get ( end index ) ;
log . debug ( find ( 2 ) node 1 ) ;
. to ( mock : rollback )
table . set ( 0 , entry ) ; dummy value reference < object , object > other value ref = dummy value reference . create ( value ) ; entry . set value reference ( other value ref ) ; assert false ( segment . remove loading value ( key , hash , value ref ) ) ; entry . set value reference ( value ref ) ; assert true ( segment . remove loading value ( key , hash , value ref ) ) ; }
sub cluster policy configuration configuration = null ; try { configuration = federation facade . get policy configuration ( queue ) ; } catch ( yarn exception e ) { string err msg = there is no policy configured for the queue : + queue + , falling back to defaults . ; log . warn ( err msg , e ) ; }
admin . snapshot ( snapshot name , string _ table _ name , snapshot description . type . flush ) ;
if ( granule id . contains ( granule _ separator ) ) { throw new illegal argument exception ( not a valid granule id : + granule id ) ; } string [ ] splitted = granule id . split ( granule _ separator ) ; if ( splitted . length = 2 ) { throw new illegal argument exception ( not a valid granule id : + granule id ) ; } return ff . id ( collections . singleton ( ff . feature id ( splitted [ 1 ] ) ) ) ;
file test source directory = test utils . create temp dir ( ) ;
recycle bin . scrap active views ( ) ; if ( sel = null ) { position selector ( invalid _ position , sel ) ; m selected top = sel . get top ( ) ; } else if ( m touch mode > touch _ mode _ down & & m touch mode < touch _ mode _ scroll ) { view child = get child at ( m motion position - m first position ) ; if ( child = null ) position selector ( m motion position , child ) ; } else { m selected top = 0 ; m selector rect . set empty ( ) ; }
m _ velocities [ index b ] . w = w b ;
flink stream env . set stream time characteristic ( time characteristic . event time ) ;
wm . retract ( cheese handles [ 3 ] ) ;
slice _ from ( ac ) ; break ; case 15 :
final string [ ] lines = string utils . split ( cmd . trim ( ) , \ n , 2 ) ; if ( lines . length < 2 ) { return process help ( interpreter result . code . error , size cmd must be followed by a search ) ; } final string [ ] size line = string utils . split ( lines [ 0 ] , , 2 ) ; if ( size line . length = 2 ) { return process help ( interpreter result . code . error , right format is : size < value > ) ; } current result size = integer . parse int ( size line [ 1 ] ) ; items = string utils . split ( lines [ 1 ] . trim ( ) , , 3 ) ;
hide serialization progress ( ) ;
if ( provider = = null ) { throw new illegal argument exception ( missing provider ) ; } return get instance rsa ( provider ) ; }
key value kv ;
cache key < k > cache key = new cache key < > ( - 1 , key ) ; cache value < k , v > value = synch get ( cache key ) ; if ( value = = null ) { return null ; } if ( time to live millis < 0 ) { return value . value ( ) ; } if ( value . expired ( time to live millis ) ) { there was a value , which has now expired remove ( key ) ; return null ; } else { return value . value ( ) ; }
options . get header map ( ) . set mapping files ( ) ; add source file ( package unit . mapping ; public class test { } , unit mapping test . java ) ; load header mappings ( ) ; string translation = translate source file ( import unit . mapping . test ; + public class my test extends test { my test ( ) { } } , my test , my test . h ) ; assert translation ( translation , include \ unit mapping test . h \ ) ;
flushed seq id = earliest unflushed sequence id for the region . long value ( ) = = hconstants . no _ seqnum? flush op seq id : earliest unflushed sequence id for the region . long value ( ) - 1 ; } else {
if ( p = = data . size ( ) & & mark depth = = 0 ) { if so , it ' s an opportunity to start filling at index 0 again clear ( ) ; size goes to 0 , but retains memory } return o ;
input stream . close ( ) ; }
schedule traversals ( ) ;
if ( connecting = null & & connecting . is done ( ) & & connecting . is cancelled ( ) ) { if ( child = null ) child . set parent ( connecting ) ; return ; } request . logi ( reconnecting socket . io ) ;
publish with wizard ( input ) ;
int alt36 = 2 ; int la36 _ 0 = input . la ( 1 ) ; if ( ( la36 _ 0 = = over ) ) { alt36 = 1 ; }
if ( u = null & & u . is have extension ( ) ) {
instructions . add ( constant expression . null ) ;
int scanner3 = handler . scanner open with stop ( table aname , row aname , row bname , get column list ( true , false ) , null ) ; close scanner ( scanner3 , handler ) ;
st . set start rule ( calendar . november , 1 , 1 ) ; st . set end rule ( calendar . december , 1 , 1 ) ; assert true ( start rule improperly set , st . use daylight time ( ) ) ; assert true ( start rule improperly set , st . in daylight time ( ( new gregorian calendar ( 1998 , calendar . november , 13 ) . get time ( ) ) ) ) ; assert true ( start rule improperly set , ( st . in daylight time ( new gregorian calendar ( 1998 , calendar . october , 13 ) . get time ( ) ) ) ) ; try { st . set start rule ( 13 , 20 , 0 ) ; fail ( illegal argument exception is not thrown . ) ; } catch ( illegal argument exception iae ) { expected } try { st . set start rule ( 1 , 32 , 0 ) ; fail ( illegal argument exception is not thrown . ) ; } catch ( illegal argument exception iae ) { expected } try { st . set start rule ( 1 , 30 , 10 ) ; fail ( illegal argument exception is not thrown . ) ; } catch ( illegal argument exception iae ) { expected }
query string = delete from person cassandra p where p . person id in : id list and p . age = 25 ; kundera query = get query object ( query string , emf ) ;
for ( int i = 0 ; i < server _ count ; i + + ) { mt [ i ] = new main thread ( i , client ports [ i ] , quorum cfg section , false ) ; mt [ i ] . start ( ) ; }
include body = include body ;
command . edit options ( options parser ) ;
set write buffer low water mark0 ( get write buffer high water mark ( ) > > > 1 ) ;
if ( this . s width > 0 & & this . s height > 0 & & ( this . s width = bitmap . get width ( ) | | this . s height = bitmap . get height ( ) ) ) { reset ( false ) ; } if ( this . bitmap = null & & this . bitmap is cached ) { this . bitmap . recycle ( ) ; } if ( this . bitmap = null & & this . bitmap is cached & & on image event listener = null ) { on image event listener . on preview released ( ) ; }
for ( int i = 0 ; i < m _ num inputs ; i + + ) { m _ input list [ i ] . restore weights ( ) ; }
long license duration = offline license helper . get license duration remaining sec ( offline license key set id ) . first ;
this . client thread pool . shutdown now ( ) ; }
assert true ( dummy node labels manager . get cluster node label names ( ) . is empty ( ) ) ;
assert true ( scopes . is circular proxy ( g ) & & scopes . is circular proxy ( h ) ) ;
fragmentation hack . reorder indices ( fragment manager ) ;
flat opc xml creator worker = new flat opc xml creator ( word mlpackage ) ;
this . extracted temp libraries = extract contained libraries ( jar file url ) ;
jai jai def = jai . get default instance ( ) ; if ( ( jai def . get operation registry ( ) instanceof concurrent operation registry | | jai def . get operation registry ( ) instanceof it . geosolutions . jaiext . concurrent operation registry ) ) { jai def . set operation registry ( concurrent operation registry . initialize registry ( ) ) ; }
for ( resource request request : requests ) { scheduler request key scheduler key = scheduler request key . create ( request ) ; if ( dedup requests . contains key ( scheduler key ) ) { dedup requests . put ( scheduler key , new hash map < > ( ) ) ; } dedup requests . get ( scheduler key ) . put ( request . get resource name ( ) , request ) ; }
assert true ( any like normalize ( _ ar , bar ) ) ;
jsonobject dd2 = new jsonobject ( { key : ddval2 } ) ; dut . put ( dd , dd2 ) ; while ( true ) { cache = dut . point in time cache ( ) ; if ( cache . get ( cache04 dd ) . get ( key ) . equals ( ddval2 ) ) { break ; } } assert equals ( items accounted for . , 4 , cache . size ( ) ) ; assert equals ( ddval2 , cache . get ( cache04 dd ) . get ( key ) ) ; dut . shutdown ( ) ;
if ( file . is directory ( ) & & path . ends with ( ) ) { non - nls - 1 file = new file ( path + ) ; non - nls - 1 }
interpreter result result = t . interpret ( help , null ) ; assert equals ( result . message ( ) . get ( 0 ) . get type ( ) , interpreter result . type . text ) ;
cr = client . call procedure ( @ ad hoc , delete from p1 where id in ( 200 , 201 , 202 ) ; ) ; assert equals ( client response . success , cr . get status ( ) ) ; }
assert true ( iterator . has next ( ) ) ; verify ( cursor , times ( 1 ) ) . next ( ) ; iterator . next ( ) ; verify ( cursor , times ( 1 ) ) . next ( ) ; assert true ( iterator . has next ( ) ) ; verify ( cursor , times ( 2 ) ) . next ( ) ;
get all ( cache , as list ( 45 ) ) ;
assert pixel ( image , 200 , 200 , new color ( 170 , 170 , 170 ) ) ; } finally {
return merge origins ( remaining ) ; }
assert that ( actions . can be shared ( actions [ 0 ] , factory . generate ( make enum set initialized to ( attribute class , count ) ) ) ) . is true ( ) ; for ( int i = 0 ; i < actions . length ; i + + ) { assert that ( actions . can be shared ( actions [ i ] , factory . generate ( make enum set initialized to ( attribute class , i ) ) ) ) . is true ( ) ; for ( int j = i + 1 ; j < actions . length ; j + + ) { assert with message ( i + and + j ) . that ( actions . can be shared ( actions [ i ] , actions [ j ] ) ) . is false ( ) ; } }
client . connect ( 1234 , localhost , server . name , res - > { if ( res . succeeded ( ) ) { system . out . println ( connected ) ; net socket socket = res . result ( ) ; } else { system . out . println ( failed to connect : + res . cause ( ) . get message ( ) ) ; } } ) ;
clazz = ( ( ometadata internal ) database . get metadata ( ) ) . get immutable schema snapshot ( ) . get class ( orient vertex type . class _ name ) ;
if ( rel error > max _ tolerance ) { log . info ( \ n distribution : + dl . _ parms . _ distribution ) ; log . info ( \ n row : + r id ) ; log . info ( weight ( layer + layer + , row + row + , col + col + ) : + weight + + - + eps ) ; log . info ( loss : + loss ) ; log . info ( losses up down : + up + + down ) ; log . info ( = > finite differences gradient : + gradient ) ; log . info ( = > back - propagation gradient : + bprop gradient ) ; log . info ( = > relative error : + pretty print . format pct ( rel error ) ) ; failedcount + + ; }
for ( integer ii : restart indices list ) { final jetty solr runner jetty = jettys . get ( ii ) ; if ( jetty . is running ( ) ) { cluster . start jetty solr runner ( jetty ) ; assert true ( jetty . is running ( ) ) ; } } abstract distrib zk test base . wait for recoveries to finish ( collection name , zk state reader , true , true , 330 ) ; zk state reader . force update collection ( collection name ) ;
name = simple ;
int req pre comp len ;
concurrent hash map < string , string > worker thread pids = new concurrent hash map < > ( ) ; sync process event sync process event = new sync process event ( supervisor id , conf , local state , worker thread pids , shared context , worker report error , storm cluster state ) ; event manager imp sync sup event manager = new event manager imp ( ) ;
map < string , orid > vertex mapped id map = new hash map < string , orid > ( ) ;
xpath builder builder = xpath builder . xpath ( tokenize ( foo bar , ' _ ' ) [ 2 ] ) ;
if ( c > ascii _ max ) { throw new unsupported encoding exception ( ) ; }
int alt42 = 2 ;
assert subset ( input . columns , subset . columns ) ;
tap target . for bounds ( droid target , oh look , you can point to any part of the screen . you also can ' t cancel this one ) . cancelable ( false ) . icon ( droid ) . id ( 4 ) ) . listener ( new tap target sequence . listener ( ) {
student float wrapper student = new student float wrapper ( ) ; student . set age ( ( short ) get random value ( short . class ) ) ; student . set id ( ( float ) get random value ( float . class ) ) ; student . set name ( ( string ) get random value ( string . class ) ) ; em . persist ( student ) ; em . close ( ) ; }
context . ticker ( ) . advance ( 45 , time unit . seconds ) ;
canvas . draw rect ( right - divider size , lm . get decorated top ( child ) , right , bottom - divider size , paint ) ; } }
return this . stateful bean on other server . increment count ( ) ;
capacity scheduler cs = new capacity scheduler ( ) ; cs . set conf ( new yarn configuration ( ) ) ; cs . set rmcontext ( resource manager . get rmcontext ( ) ) ; capacity scheduler configuration conf = new capacity scheduler configuration ( ) ; setup queue configuration ( conf ) ; set max alloc mb ( conf , 10240 ) ; set max alloc vcores ( conf , 10 ) ; set max alloc mb ( conf , a1 , 4096 ) ; set max alloc vcores ( conf , a1 , 4 ) ; cs . init ( conf ) ; cs . start ( ) ; cs . reinitialize ( conf , mock context ) ; check queue capacities ( cs , a _ capacity , b _ capacity ) ; assert equals ( max allocation mb in cs , 10240 , cs . get maximum resource capability ( ) . get memory size ( ) ) ; assert equals ( max allocation vcores in cs , 10 , cs . get maximum resource capability ( ) . get virtual cores ( ) ) ;
tred2 ( ) ;
list < solr core > ret = new array list ( transient cores . values ( ) ) ; transient cores . clear ( ) ; return ret ; }
xpath builder builder = xpath builder . xpath ( tokenize ( foo bar , ' _ ' ) [ 2 ] ) . saxon ( ) ;
string meta model source = get meta model source as string ( test entity . class ) ;
if ( null = order by ) { generated tuple list . sort ( order by ) ; } return new linked list < > ( generated tuple list ) ;
invalidate ( ) ; return ; } }
if ( sock = = null ) { sock = datagram channel . open ( ) ; sock . socket ( ) . bind ( h2 o . self . _ key ) ; }
dn address = node reg . get host ( ) ; }
path path = paths . get ( file name ) ; if ( files . exists ( path ) ) { log . fine ( config file + file name + does not exist ) ; return props ; } if ( files . is regular file ( path ) ) { log . warning ( config file + file name + might be a directory . ) ; return props ; } log . log ( level . fine , reading config file { 0 } , file name ) ;
executor0 . complete ( ) ;
while ( true ) { callback callback = callbacks . poll ( 1 , time unit . seconds ) ; if ( callback = = null ) break ; callback . succeeded ( ) ; }
if ( e < = out buff . length - 1 ) { byte [ ] final out = new byte [ e ] ; system . arraycopy ( out buff , 0 , final out , 0 , e ) ; system . err . println ( having to resize array from + out buff . length + to + e ) ; return final out ; } else { system . err . println ( no need to resize array . ) ; return out buff ; }
list < web element > elems = find elements ( by . css selector ( td [ role = \ listitem \ ] ) ) ; assert . assert equals ( not enough suggestions shown , data . length , elems . size ( ) ) ; for ( web element elem : elems ) { result . add ( elem . get text ( ) ) ; }
expected rows = 2 ;
set type ( interpreter result . type . text ) ; out = get current output ( ) ; } return out ; } }
string builder value = new string builder ( 5 ) ; int decimal places read = 0 ; if ( c = = ' 0 ' | | c = = ' 1 ' ) { value . append ( ( char ) c ) ; c = input . read ( ) ; if ( c = = ' . ' ) { value . append ( ' . ' ) ; } else if ( c < ' 0 ' | | c > ' 9 ' ) { decimal places read = 3 ; } while ( true ) { c = input . read ( ) ; if ( c > = ' 0 ' & & c < = ' 9 ' ) { if ( decimal places read < 3 ) { value . append ( ( char ) c ) ; decimal places read + + ; } } else if ( c = = delimiter | | c = = 9 | | c = = 32 | | c = = - 1 ) { break ; } else { malformed . use quality of zero so it is dropped and skip until eof or the next delimiter skip until ( input , c , delimiter ) ; return 0 ; } } } else { malformed . use quality of zero so it is dropped and skip until eof or the next delimiter skip until ( input , c , delimiter ) ; return 0 ; } double result = double . parse double ( value . to string ( ) ) ;
assert . assert equals ( find text . get results ( ) , results on second page ) ; assert . assert true ( find text . check previous page button is enabled ( ) ) ; find text . open file node by double click ( path2 ) ; find text . wait expected text in find info panel ( expected text2 ) ; find text . click on next page button ( ) ;
transport ldaps = new tcp transport ( this . bind host , this . bind port , 3 , 50 ) ;
checkpoint cp1 = new checkpoint ( ) ; cp1 . set flexible ( ) ; cache entry cache entry = checkpoint provider . new cache entry ( stream id , null ) ; assert true ( cache entry . set checkpoint ( cp1 ) ) ; assert true ( checkpoint file . exists ( ) ) ; cache entry cache entry2 = checkpoint provider . new cache entry ( stream id , null ) ; checkpoint cp2 = cache entry2 . get checkpoint ( ) ; assert not null ( cp2 ) ;
beam sql table source table = sql env . find table ( source name ) ; return source table . build ioreader ( input pcollections . get pipeline ( ) ) . set coder ( calcite utils . to beam row type ( get row type ( ) ) . get record coder ( ) ) ; }
if ( commit ) { txn . rollback ( ) ; }
lr . add field ( http accept language , new log field ( log field . type _ string , lp . get next cb ( ) ) ) ;
last = reader . read ( ) ;
rmapp app2 = rm . submit app ( 1 * gb , user _ 1 , a2 ) ; mock am am2 = mock rm . launch and register am ( app2 , rm , nm2 ) ;
final float horizontal padding = 0 . 1f * bitmap rect . width ( ) ; final float vertical padding = 0 . 1f * bitmap rect . height ( ) ; edge . left . set coordinate ( bitmap rect . left + horizontal padding ) ;
gles20 . gl enable vertex attrib array ( ma position loc ) ;
byte [ ] b = this . b ;
assert null ( signature generator . create field type signature ( vars . get ( 0 ) ) ) ;
list < string > packages supporting custom tabs = new array list < > ( ) ; package manager pm = context . get package manager ( ) ; list < resolve info > resolved activity list = pm . query intent activities ( new intent ( intent . action _ view , uri . parse ( http : www . example . com ) ) , 0 ) ;
log . info ( = = = = = = add node 1 ) ;
final int required _ size = 280 ;
_ monitor thread . interrupt ( ) ;
if ( syntax = = sql syntax . prefix | | syntax = = sql syntax . binary | | syntax = = sql syntax . postfix ) { return syntax ; } else { return sql syntax . function ; } }
if ( result . is empty ( ) ) { log . error ( no data returned for key = [ + row key str + ] ) ; return false ; } if ( verify values & & verify cf and column integrity ) { return true ; as long as we have something , we are good . }
string icon file = org gephi branding desktop multilingual resources + lang . get language ( ) ;
obegin statement jjtn000 = new obegin statement ( jjtbeginstatement ) ; boolean jjtc000 = true ; jjtree . open node scope ( jjtn000 ) ; jjtn000 . jjt set first token ( get token ( 1 ) ) ; try { jj _ consume _ token ( begin ) ; switch ( ( jj _ ntk = = - 1 ) ?jj _ ntk ( ) : jj _ ntk ) { case isolation : jj _ consume _ token ( isolation ) ; jjtn000 . isolation = identifier ( ) ; break ; default : jj _ la1 [ 373 ] = jj _ gen ; ; } jjtree . close node scope ( jjtn000 , true ) ; jjtc000 = false ; jjtn000 . jjt set last token ( get token ( 0 ) ) ; { if ( true ) return jjtn000 ; } } catch ( throwable jjte000 ) { if ( jjtc000 ) { jjtree . clear node scope ( jjtn000 ) ; jjtc000 = false ; } else { jjtree . pop node ( ) ; } if ( jjte000 instanceof runtime exception ) { { if ( true ) throw ( runtime exception ) jjte000 ; } } if ( jjte000 instanceof parse exception ) { { if ( true ) throw ( parse exception ) jjte000 ; } } { if ( true ) throw ( error ) jjte000 ; } } finally { if ( jjtc000 ) { jjtree . close node scope ( jjtn000 , true ) ; jjtn000 . jjt set last token ( get token ( 0 ) ) ; } } throw new error ( missing return statement in function ) ;
json array speakers = get as array ( origin , input json keys . vendor apisource . topics . speaker ids ) ;
if ( entry . get size ( ) > = content1 . length ) { storage . remove ( entry ) ; }
solr frag list builder frag list builder = solr core . init plugins ( info . get children ( frag list builder ) , frag list builders , solr frag list builder . class , null ) ; if ( frag list builder = = null ) { frag list builder = new simple frag list builder ( ) ; solr core . init default plugin ( frag list builder , solr frag list builder . class ) ; } frag list builders . put ( , frag list builder ) ; frag list builders . put ( null , frag list builder ) ;
zip eight byte integer size = zip eight byte integer . zero ;
last id = new long [ ] { 0 , 0 , 0 , 0 , 0 } ;
assert equals ( foo . hash code ( ) , foo . hash code ( ) ) ;
memory store . get once ( 100 ) . subscribe ( test subscriber ) ; test subscriber . assert completed ( ) ;
resource manager gateway rm gateway1 = mock ( resource manager gateway . class ) ; rpc . register gateway ( address1 , rm gateway1 ) ; testing leader retrieval service test leader service = new testing leader retrieval service ( address1 , high availability services . default _ leader _ id ) ;
out1 . reset ( ) ;
holder . m text view . set text ( item . get text ( ) ) ;
answer = new delegate sync processor ( answer ) ;
for ( environment pipeline config pipeline ref config : this . pipelines ) { config repos config config repos = validation context . get config repos ( ) ; pipeline config pipeline config = validation context . get pipeline config by name ( pipeline ref config . get name ( ) ) ; if ( pipeline config = = null ) { continue ; other rule will error that we reference unknown pipeline } if ( validation context . should check config repo ( ) ) { if ( config repos . is reference allowed ( this . origin , pipeline config . get origin ( ) ) ) pipeline ref config . add error ( environment pipeline config . origin , string . format ( environment defined in % s cannot reference a pipeline in % s , this . origin , display name for ( pipeline config . get origin ( ) ) ) ) ; } }
list < cell > cells = log edit . get cells ( ) ; cell deleted cell = null ; for ( cell cell : cells ) { assume only one kv from the waledit matches . byte [ ] family = cell util . clone family ( cell ) ; byte [ ] qulifier = cell util . clone qualifier ( cell ) ; if ( arrays . equals ( family , ignored family ) & & arrays . equals ( qulifier , ignored qualifier ) ) { log . debug ( found the key value from waledit which should be ignored . ) ; deleted cell = cell ; } if ( arrays . equals ( family , changed family ) & & arrays . equals ( qulifier , changed qualifier ) ) { log . debug ( found the key value from waledit which should be changed . ) ; cell . get value array ( ) [ cell . get value offset ( ) ] + = 1 ; } } if ( null = row ) { cells . add ( new key value ( row , added family , added qualifier ) ) ; }
unsubscribe ( ) ; final observable < list < mail > > older = mail provider . search for older mails ( query , older as . get date ( ) , query limit ) ;
title = extract ( doc buf , ti , ti _ end , h2 , null ) ;
val http client = mock ( async http client . class ) ;
abstract height map heightmap = null ;
super . start ( ) ; }
c = x . next clean ( ) ;
populate ( false ) ; }
m hover color = themes . get color accent ( get context ( ) ) ; set drawable ( r . drawable . ic _ info _ launcher ) ;
rebalance plan = cluster test utils . make plan ( zz current , zz stores , zzz zone expansion , zzz stores ) ;
builder . get attribute builder ( ) . set discard ( new discard attribute checker . discard attribute value checker ( new model node ( true ) ) , ejb3 subsystem root resource definition . log _ ejb _ exceptions ) ; builder . get attribute builder ( ) . add reject check ( reject attribute checker . defined , ejb3 subsystem root resource definition . log _ ejb _ exceptions ) ; builder . get attribute builder ( ) . add reject check ( reject attribute checker . defined , ejb3 subsystem root resource definition . disable _ default _ ejb _ permissions ) ;
jingle session session = get session ( ) ;
search hit search hit = new search hit ( result ids , hit . get id ( ) , new text ( hit . get type ( ) ) , hit . get fields ( ) ) ; search hit . source ref ( hit . get source ref ( ) ) ; only returned fields ( search hit . get source as map ( ) , first table request . get returned fields ( ) , first table request . get original select ( ) . is select all ( ) ) ; result ids + + ; this . hash join comparison structure . insert into comparison hash ( comparison id , key , search hit ) ; } }
sc [ pp ] = 0x ffff ; } } else {
return 002 { + b + , + s + , + i + , + d + } ;
m audio manager . request audio focus ( af change listener , audio manager . stream _ music , audio manager . audiofocus _ gain ) ;
inode file rename file = get inode file ( test , hadoop rename ) ; inode file rename file2 = get inode file ( test , hadoop rename _ 2 ) ; assert not null ( rename file ) ;
run ( decider1 , 10 , result . success , state . normal ) ;
assert true ( route . matches ( get , blah id id2 id3 morestuff at the end ) ) ;
final resources res = get resources ( ) ;
tags len - = key value . tags _ length _ size ;
view utils lollipop . set state list animator from attrs ( this , attrs , 0 , r . style . widget _ design _ app bar layout ) ; }
assert . assert equals ( failed to drain completely after async . , 10 , vq . get at most ( k1 , 10 ) . size ( ) ) ;
assert equalish ( result1 , o result1 ) ; assert equalish ( result2 , o result2 ) ; assert equalish ( result3 , o result3 ) ; }
servlet pipeline . destroy ( ) ;
consumer configuration conf = new consumer configuration ( ) ; conf . set subscription type ( subscription type . exclusive ) ; string sub name = sub1 ; consumer consumer = pulsar client . subscribe ( topic name , sub name , conf ) ; run gc ( ) ;
string jdk = collections . min ( collection ) ; using jdk
final float translate x = child width 2 . 0f ;
instructions . add ( reil helpers . create bisz ( offset + 1 , target size , rotate mask , operand size . byte , rotate mask zero ) ) ;
origin metadata = read origin action . execute ( new close shield input stream ( tar input ) ) ; } else {
return init ( method name , conf , column family descriptor builder . new builder ( family ) . set max versions ( 4 ) . build ( ) ) ;
data set < triangle listing . result < k > > triangles = input . run ( new triangle listing < k , vv , ev > ( ) . set parallelism ( parallelism ) ) ;
sync sys meta from remote ( ) ;
if ( all . size ( ) > 1 ) { return this deployment ; } return all ; }
if ( the lexer . get lexeme type ( ) = = tspecial _ lexeme & & the lexer . get lexeme first character ( ) = = ' = ' ) { the lexer . next lexeme ( ) ; } else { throw new illegal argument exception ( ) ; }
byte [ ] b = new byte [ 3 ] ;
acceptor config . set thread model ( thread model . manual ) ; configure data gram codec factory ( mina consumer , acceptor config , configuration ) ; acceptor config . set disconnect on unbind ( true ) ;
return notes ;
column vector . type field vector column type = get column vector type from type info ( field type info ) ;
g . add child ( h ) ; assert that ( a . traverse ( ) ) . extracting ( node : : value ) . contains exactly ( ' a ' , ' b ' , ' c ' , ' d ' , ' e ' , ' f ' , ' g ' , ' h ' ) ;
final string id path = paths . config host id ( hostname ) ;
file folder type list . add ( audio _ file ) ;
sbuf . append ( message pattern . substring ( i , message pattern . length ( ) ) ) ;
map = to map ( time _ start . key , 2010 - 09 - 01 t00 : 00 : 00 z , time _ relation . key , contains ) ;
collator = create from rules ( custom , loader ) ; }
this . error number = byte helper . read unsigned short little endian ( data , index ) ;
return local confirm product instance . get pull parser ( my _ qname ) ;
switch ( state ) { case background _ enabled : paint background enabled ( g ) ; break ; case fileicon _ enabled : paintfile icon enabled ( g ) ; break ; case directoryicon _ enabled : paintdirectory icon enabled ( g ) ; break ; case upfoldericon _ enabled : paintup folder icon enabled ( g ) ; break ; case newfoldericon _ enabled : paintnew folder icon enabled ( g ) ; break ; case harddriveicon _ enabled : painthard drive icon enabled ( g ) ; break ; case floppydriveicon _ enabled : paintfloppy drive icon enabled ( g ) ; break ; case homefoldericon _ enabled : painthome folder icon enabled ( g ) ; break ; case detailsviewicon _ enabled : paintdetails view icon enabled ( g ) ; break ; case listviewicon _ enabled : paintlist view icon enabled ( g ) ; break ; }
if ( text = null & & message content view . get text ( ) . length ( ) = = 0 ) { message content view . set characters ( text ) ; } string type = intent . get type ( ) ;
query query = query by identifier ( request . get id ( ) ) ;
assert that ( engine descriptor . get descendants ( ) . size ( ) ) . described as ( too few test descriptors in classpath ) . is greater than ( 150 ) ; list < unique id > unique ids = unique ids ( ) ;
find s . close ( ) ;
encode simple attribute ( ( attribute ) property ) ;
async disk service . remove volume ( sd . get storage uuid ( ) ) ;
return success _ write _ to _ a ; }
string reloaded leader core id ;
string json = _ lc with undescore mapper . write value as string ( new other non standard names ( results , _ user , _ _ _ , user ) ) ;
mock result set = mock ( result set . class ) ; result set meta data mock result set meta data = mock ( result set meta data . class ) ; when ( mock result set meta data . get column count ( ) ) . then return ( 2 ) ; when ( mock result set meta data . get column label ( 1 ) ) . then return ( key ) ; when ( mock result set meta data . get column label ( 2 ) ) . then return ( value ) ; when ( mock result set . get meta data ( ) ) . then return ( mock result set meta data ) ; mock row = new mock row ( ) ;
assert equals ( 1 , input . read raw byte ( ) ) ;
if ( glyph . is missing ( ) ) { if ( missing glyph = null ) { if ( glyph = missing glyph ) iter . remove ( ) ; continue ; } missing glyph = glyph ; } }
job conf = new job conf ( cluster conf ) ;
capture < async method callback < integer > > callback capture1 = new capture < async method callback < integer > > ( ) ;
if ( p . has log aggregation context ( ) ) { return null ; } log aggregation context = convert from proto format ( p . get log aggregation context ( ) ) ; return log aggregation context ; }
rtype . load new version ( 003 , retrieve rename ( the type , the type + 3 ) ) ; result = run constructor ( rtype . get clazz ( ) , ) ; res = run on instance ( rtype . get clazz ( ) , result . return value , get integer ) ; assert equals ( 3 , ( ( integer ) res . return value ) . int value ( ) ) ; res = run on instance ( rtype . get clazz ( ) , result . return value , get string ) ; assert null ( res . return value ) ; }
tool tool = new import tsv ( ) ; log . debug ( running import tsv with arguments : + args array ) ; assert equals ( 0 , tool runner . run ( conf , tool , args array ) ) ;
if ( overflow entries . is empty ( ) ) { return null ; } else { return overflow entries . remove ( key ) ; }
if ( target instanceof database info accessor ) { ( ( database info accessor ) target ) . _ pinpoint _ set database info ( database info ) ; }
iterator < thread > it = client threads . iterator ( ) ; while ( it . has next ( ) ) { thread t = it . next ( ) ; if ( t . is alive ( ) = = false ) { it . remove ( ) ; } }
assert equals ( 0000000000000001 , m _ provider . get element ( 0x120 ) ) ; m _ provider . set data layout ( stack data layout . bytes ) ;
add new cache manager and wait for rehash ( 1 , preload ) ;
if ( set background ) { for ( int b = 0 ; b < dst bands ; b + + ) { data [ b ] [ pixel offset + band offsets [ b ] ] = background values [ b ] ; } }
container id container id201 = construct container id ( 201 ) ;
string file sep = system . get property ( file . separator ) ;
log . i ( tag , data restored from non - app domain + domain + , ignoring ) ;
this . streams = new push back stream [ streams . length ] ; for ( int idx = 0 ; idx < streams . length ; + + idx ) { this . streams [ idx ] = new push back stream ( streams [ idx ] ) ; } this . comp = comp ; }
string prefix = m _ prefix map . lookup prefix ( ns ) ; if ( prefix = = null ) {
query tq = legacy numeric range query . new long range ( field , precision step , lower , upper , true , true ) ; top docs t top docs = searcher . search ( tq , 1 ) ; assert equals ( returned count of range query must be equal to inclusive range length , upper - lower + 1 , t top docs . total hits ) ;
set intent ( new intent ( ) ) ;
hook all ( xactivity thread . get instances ( ) , null , m secret , false ) ;
final list < expr node desc > top select exprs = new array list < > ( ) ;
app log aggregator . finish log aggregation ( ) ; app log aggregator . run ( ) ;
for ( execution entity execution entity : execution entities ) { if ( event activity ids . contains ( execution entity . get activity id ( ) ) & & execution . get current flow element ( ) instanceof intermediate catch event ) { intermediate catch event intermediate catch event = ( intermediate catch event ) execution . get current flow element ( ) ; if ( intermediate catch event . get behavior ( ) instanceof intermediate catch event activity behavior ) { ( ( intermediate catch event activity behavior ) intermediate catch event . get behavior ( ) ) . event cancelled by event gateway ( execution entity ) ; event activity ids . remove ( execution entity . get activity id ( ) ) ; we only need to delete one execution at the event . } } }
string builder str balance param = new string builder ( ) ; str balance param . append ( balance parameter : num regions = ) . append ( num regions ) . append ( , num servers = ) . append ( num servers ) . append ( , max = ) . append ( max ) . append ( , min = ) . append ( min ) ; log . debug ( str balance param . to string ( ) ) ;
for ( int i = 0 ; i < length ; i + + ) { set ( i , s . read double ( ) ) ; }
head slot . release slot ( ) ; tail slot . release slot ( ) ; assert equals ( 2 , shared slot . get number leaves ( ) ) ;
assert equals ( - 1 , multi . get index ( john , doe ) ) ;
output stream output stream = data entry writer . get output stream ( actual data entry ) ; if ( output stream = null ) {
host name = ( ( m uri . get host ( ) = null ) ? m uri . get host ( ) : m host name ) ;
chrono . get epoch day ( proleptic year , month of year , day of month ) ; this . chrono = chrono ; this . proleptic year = proleptic year ; this . month of year = month of year ; this . day of month = day of month ; }
string content type = exchange helper . get content type ( exchange ) ; if ( content type = null ) { is xml = content type . to lower case ( locale . english ) . contains ( xml ) ; is json = content type . to lower case ( locale . english ) . contains ( json ) ; }
input stream is = kraken trade json test . class . get resource as stream ( trading example - openorders - data . json ) ;
if ( el = = a ) { i . a + + ; break ; }
long touched = val . _ last accessed time ;
if ( outer query translator = null ) { return outer query translator . get columns using projection ( subcriteria , property name ) ; } else { throw he ; }
t item = get item ( position ) ;
if ( fields = null ) { builder . field ( common fields . fields . get preferred name ( ) , fields ) ; } if ( missing = null ) { builder . field ( common fields . missing . get preferred name ( ) , missing ) ; } if ( format = null ) { builder . field ( common fields . format . get preferred name ( ) , format ) ; } if ( value type = null ) { builder . field ( common fields . value _ type . get preferred name ( ) , value type . get preferred name ( ) ) ; } do xcontent body ( builder , params ) ; builder . end object ( ) ; return builder ;
secret mgr = new nmtoken secret manager in nm ( state store ) ; secret mgr . recover ( ) ; secret mgr . set node id ( node id ) ; assert equals ( current key , secret mgr . get current key ( ) ) ; assert false ( secret mgr . is app attempt nmtoken key present ( attempt1 ) ) ; assert true ( secret mgr . is app attempt nmtoken key present ( attempt2 ) ) ; assert not null ( secret mgr . retrieve password ( attempt token1 ) ) ; assert not null ( secret mgr . retrieve password ( attempt token2 ) ) ;
soft reference < byte [ ] > ref buffer = thread local buffer . get ( ) ;
body builder . append formal line ( string . format ( return % s . status ( % s ) , get name of java type ( spring java type . response _ entity ) , exception details . get annotation ( spring java type . response _ status ) . get attribute ( value ) . get value ( ) ) ) ; body builder . indent ( ) ; body builder . append formal line ( string . format ( . body ( % s . singleton map ( \ message \ , error message ) ) ; , get name of java type ( new java type ( collections . class ) ) ) ) ; body builder . indent remove ( ) ; method metadata builder method builder = new method metadata builder ( get id ( ) , modifier . public , method name , spring java type . response _ entity , parameter types , parameter names , body builder ) ; method builder . set throws types ( throw types ) ;
if ( key = = null ) { throw new null pointer exception ( key = = null ) ; }
} @ override public void on failure ( throwable ex ) { future . set exception ( ex ) ; } } ) ; return future ; }
application id app id = new application id . builder ( ) . tenant ( tenant a ) . application name ( foo ) . build ( ) ; assert that ( controller . get handler ( ) . get super model ( ) . application models ( ) . get ( tenant a ) . get ( app id ) . get generation ( ) , is ( 4l ) ) ; gen = counter . increment ( ) ; controller . reload config ( tenant a , create app ( tenant a , bar , 2l , 3 ) ) ; assert that ( controller . get handler ( ) . get generation ( ) , is ( gen ) ) ; }
if ( r _ check _ vowel _ harmony ( ) ) { return false ; }
missed char = c2 ;
score = jedis . zincrby ( bfoo , 2d , ba , zincr by params . z incr by params ( ) . xx ( ) ) ; assert null ( score ) ; jedis . zadd ( bfoo , 2d , ba ) ;
int total data to read = ( value [ 7 ] & 0xff ) | ( ( value [ 8 ] & 0xff ) < < 8 ) ; total data to read * = ( data type = = mi band service . mode _ regular _ data _ len _ minute ) ? get bytes per minute of activity data ( ) : 1 ;
final sqlidentifier detector . word region word region = this . context informer . get word region ( ) ;
fold ( ( ( a & & ( ( function ( ) { } ) ( ) ) ) ) | | foo ( ) , a | | ( function ( ) { } ) ( ) | | foo ( ) ) ;
file position result res = pos setter . locate file position ( 100 , finder ) ; assert . assert equals ( res . get status ( ) , file position result . status . error , result status for scn : + 100 + , result : + res ) ;
add doc ( writer , jonathon smythe , 1 ) ;
get media player ( ) . reset ( ) ;
if ( len > 0 ) write ( buffer , last ) ; else if ( last ) write ( buffer util . empty _ buffer , true ) ; if ( last ) closed ( ) ;
string s = val . to formal string ( ) ;
public void test hyphen datacenters ( ) throws throwable { iendpoint snitch snitch = database descriptor . get endpoint snitch ( ) ;
get account manager ( ) . remove account ( account , null , null ) ; }
final set < string > deleted on client uids = new hash set < string > ( collection . deleted bookmark uids ( ) ) ;
version of offline node = set offline in zoo keeper ( state , hijack , region already in transition exception ) ;
named ( ) ; consume annotation parameters ( ) ; consume erratic trailing comma ( ) ; continue ; } else { break ; } if ( terms . is empty ( ) & & terms . peek ( ) . is ( . ) ) { if ( terms . size ( ) > = 2 ) {
golab0 : while ( true ) { lab1 : do { if ( ( in _ grouping ( g _ v , 97 , 252 ) ) ) { break lab1 ; } break golab0 ; } while ( false ) ; if ( cursor > = limit ) { return false ; } cursor + + ; }
among _ var = find _ among _ b ( a _ 5 , 8 ) ; if ( among _ var = = 0 ) { break lab2 ; }
for ( field packet field : fields ) { field . packet id = + + packet id ; buffer = field . write ( buffer , c ) ; }
context . stat manager ( ) . add rate data ( transport . bid fail banlisted , msg . get lifetime ( ) ) ; fail ( context , msg ) ; return ; }
for ( j = i - 1 ; j > = 0 ; j - - ) if ( chunks [ j ] . ends with ( - pp ) ) phrase = tokens [ j ] + + phrase ; else break ;
assert equals ( max split size , policy . get size to check ( 1000 ) ) ;
if ( i = = ( capacity - 1 ) ) { throw new error ( object identifier : invalid static initialization ; + small oid pool capacity ) ; } list [ i ] = oid ; }
enabled color = get resources ( ) . get color ( r . color . text _ _ primary _ dark ) ;
params = new modifiable solr params ( ) ; params . add ( q , * : * ) ; params . add ( fq , { collapse field = + group + hint + } ) ; params . add ( group , true ) ; params . add ( group . field , id ) ; assert q ( req ( params ) , * [ count ( doc ) = 2 ] ) ;
data set < tuple5 < long , string , string , string , integer > > orders = env . read csv file ( args [ 1 ] ) . field delimiter ( | ) . line delimiter ( \ n ) . include fields ( 101011001 ) . types ( long . class , string . class , string . class , string . class , integer . class ) . name ( orders ) ;
sc . set parameters ( state , domain . state . active ) ; return _ domain dao . search ( sc , search filter ) ;
x0 = dataset . get xvalue ( series , item - 1 ) ; y0 = double . is na n ( y1 ) ? y1 : dataset . get yvalue ( series , item - 1 ) ; x = x0 ; y = double . is na n ( y0 ) ? get range base ( ) : y0 ; trans x0 = domain axis . value to java2 d ( x , data area , plot . get domain axis edge ( ) ) ; trans y0 = range axis . value to java2 d ( y , data area , plot . get range axis edge ( ) ) ;
plan . chain ( new match first step ( context , node , profiling enabled ) ) ; } else {
int candidate diff = integer . max _ value ;
v previous value = values [ index ] ;
return basic header value parser . parse elements ( this . value , null ) ;
( ( jmenu ) menu ) . insert ( new menu item , index ) ;
return search ( clazz , query , constants . invalid , constants . invalid , true ) ;
while ( string . char at ( index - 1 ) < = ' ' ) { index - - ; }
return accumulate ;
_ cpu ticks = new array list < long [ ] > ( ) ;
return origin = = null | | origin . is local ( ) ; }
request = http request . create test request ( search?query = word & query profile = test , method . get ) ;
deferred object [ ] null args = { new deferred java object ( null ) } ; output = ( date writable ) udf . evaluate ( null args ) ; assert null ( to _ date ( ) with null string , output ) ; }
assert equals ( null , multi . get type ( - 1 ) ) ; assert equals ( null , multi . get type ( 5 ) ) ; }
new file ( new path ( dir4 ) . to uri ( ) . get path ( ) ) . set read only ( ) ; validate temp dir creation ( dir3 ) ; validate temp dir creation ( dir3 ) ; } finally {
db helper . close ( ) ;
string auth string = \ u0000 + test jaas config . username + \ u0000 + test jaas config . password ; byte buffer auth buf = byte buffer . wrap ( auth string . get bytes ( utf - 8 ) ) ; if ( enable sasl authenticate header ) { sasl authenticate request request = new sasl authenticate request . builder ( auth buf ) . build ( ) ; send kafka request receive response ( node , api keys . sasl _ authenticate , request ) ; } else { selector . send ( new network send ( node , auth buf ) ) ; wait for response ( ) ; }
if ( parts = null ) { for ( int i = 0 ; i < parts . length ; i + + ) { mmspart part = parts [ i ] ; if ( part = null ) { try { pdu part part pdu = new pdu part ( ) ; part pdu . set name ( part . name . get bytes ( ) ) ; part pdu . set content type ( part . mime type . get bytes ( ) ) ; if ( part . mime type . starts with ( text ) ) { part pdu . set charset ( character sets . utf _ 8 ) ; } part pdu . set data ( part . data ) ; pdu body . add part ( part pdu ) ; } catch ( exception e ) { } } } } byte array output stream out = new byte array output stream ( ) ;
assert false ( progress . has pending ( ) ) ;
this . ast = new module node ( this ) ;
int index = 0 ; int [ ] hash indices = new int [ inputs . size ( ) ] ; for ( string relative path : input paths ) { immutable list < source path > paths = sorted universe . get ( relative path ) ; preconditions . check state ( paths . is empty ( ) ) ; hash indices [ index + + ] = add hash ( relative path , hash source path group ( file hash cache , resolver , paths ) ) ; } entries . add ( new pair < > ( key , hash indices ) ) ; }
try { file util . delete file ( non existing file ) ; fail ( should have failed ) ; } catch ( no such file exception e ) { then }
m service data = parcel . obtain ( ) ; m service data . write interface token ( android . app . iactivity manager ) ; m service data . write strong binder ( null ) ;
e = new saxexception ( ( exception ) null ) ; assert null ( e . get message ( ) ) ; assert null ( e . get exception ( ) ) ; }
final int itype _ cat = 5 ;
} } return state handles ; }
toast . make text ( video view demo . this , please edit video view demo activity , and set path + variable to your media file url path , toast . length _ long ) . show ( ) ; return ; } else {
document doc = jsoup . parse ( < a abs : href = ' odd ' > one < a > ) ; element el = doc . select ( a ) . first ( ) ; assert true ( el . has attr ( abs : href ) ) ; assert equals ( odd , el . attr ( abs : href ) ) ; }
new htable ( test _ util . get configuration ( ) , hconstants . meta _ table _ name ) . close ( ) ;
assert decimal function ( decimal ' 1 ' % decimal ' 3 ' , decimal ( 1 ) ) ;
deferred document impl owner document = ( deferred document impl ) this . owner document ;
if ( f last = = slash ) { if ( f token length - get last length ( f last ) > 0 ) { return pre fix ( ccode , single _ line _ comment , none , 2 ) ; } else { pre fix ( ccode , single _ line _ comment , none , 2 ) ; f token offset + = f token length ; f token length = f prefix length ; break ; } } else { f token length + + ; f last = slash ; break ; } case ' * ' :
if ( glox package . get diagram layout part ( ) . get jaxb element ( ) . get samp data ( ) = = null ) { log . error ( sample data missing ) ; return ; }
final file yaml config file = new file ( conf dir file , flink _ conf _ filename ) ; if ( yaml config file . exists ( ) ) { throw new illegal configuration exception ( the flink config file ' + yaml config file + ' ( + conf dir file . get absolute path ( ) + ) does not exist . ) ; }
split log worker coordination . mark corrupted ( root dir , logfile . get path ( ) . get name ( ) , fs ) ;
target buffer = allocate buffer ( get buffer size ( ) ) ;
string [ ] domain values = model . get domain values ( model . get response idx ( ) ) ;
return ( new size + 1 ) & 0x7ffffffe ;
counting search test task . num searches = 0 ;
for ( int i = 0 ; i < 5 ; i + + ) { if ( controller state reference [ hand index ] . r axis [ i ] = null ) { last controller state [ hand index ] . r axis [ i ] . x = controller state reference [ hand index ] . r axis [ i ] . x ; last controller state [ hand index ] . r axis [ i ] . y = controller state reference [ hand index ] . r axis [ i ] . y ; } } }
if ( auto answer op set = null ) { auto answer op set . auto answer ( call ) ; } }
assert equals ( slice length , single page or null . length ) ;
is initializer = methodref constant . get name ( clazz ) . equals ( class constants . method _ name _ init ) ;
node minute first digit = new node ( k0 , k1 , k2 , k3 , k4 , k5 ) ;
text sentence break ( line start , index ) ;
if ( old capacity = = maximum _ capacity ) { threshold = integer . max _ value ; return ; }
{ long u = fours ^ fours a ; eights = ( fours & fours a ) | ( u & fours b ) ; fours = u ^ fours b ; } tot8 + = pop ( eights ) ; }
while ( properties iterator . has next ( ) ) { string key = ( string ) properties iterator . next ( ) ; string pre processed command = pre processed commands . get property ( key ) ; string [ ] command parts = pre processed command . split ( ; ) ; string command name = command parts [ 0 ] ; all _ api _ commands . put ( key , command name ) ; short cmd permissions = 1 ; if ( command parts . length > 1 & & command parts [ 1 ] = null ) { cmd permissions = short . parse short ( command parts [ 1 ] ) ; } if ( ( cmd permissions & domain _ admin _ command ) = 0 ) { domain _ admin _ api _ commands . put ( key , command name ) ; } if ( ( cmd permissions & user _ command ) = 0 ) { regular _ user _ api _ commands . put ( key , command name ) ; } }
list < cookie > cookies = m http client . get cookie store ( ) . get cookies ( ) ;
split future = send split request ( splits client , batch . get next token ( ) ) ;
if ( is locally on hold ( ) ) post hold dir = post hold dir . and ( media direction . sendonly ) ;
mc _ color _ temperature . by reference pct current color temperature = new mc _ color _ temperature . by reference ( ) ; dxva2 . instance . get monitor color temperature ( h physical monitor , pct current color temperature ) ; }
estimator [ ] [ ] distributions = new estimator [ n atts ] [ ] ; for ( int i node = 0 ; i node < n atts - 1 ; i node + + ) { distributions [ i node ] = m _ distributions [ i node ] ; } distributions [ n atts - 1 ] = new estimator [ 1 ] ; distributions [ n atts - 1 ] [ 0 ] = new discrete estimator bayes ( n cardinality , 0 . 5 ) ; m _ distributions = distributions ;
string [ ] js = new string [ ] { line _ joiner . join ( * * @ constructor * , function bar ( ) { } , * * @ suppress { extra require } * , goog . require ( ' bar ' ) ; ) , var bar = new bar ( ) ; } ; string warning = missing require : ' bar ' ;
if ( exchange . get pattern ( ) . is out capable ( ) ) { copy the header of in message to the out message exchange . get out ( ) . copy from ( exchange . get in ( ) ) ; exchange . get out ( ) . set body ( response ) ; }
assert same ( state . forward , l1 . get level state ( warning ) ) ;
gpio = gpio factory . get instance ( ) ; }
log . info ( draining connection . . . ) ; client . drain ( ) ; }
this . qualified = stripped . make qualified ( fs . get uri ( ) , fs . get working directory ( ) ) ;
m adapter = new application adapter ( new array list < app info > ( ) , r . layout . row _ application , main activity . this ) ; m recycler view . set adapter ( m adapter ) ; m swipe refresh layout = ( swipe refresh layout ) find view by id ( r . id . swipe _ container ) ; m swipe refresh layout . set color scheme colors ( get resources ( ) . get color ( r . color . theme _ accent ) ) ; m swipe refresh layout . set refreshing ( true ) ; m swipe refresh layout . set on refresh listener ( new swipe refresh layout . on refresh listener ( ) { @ override public void on refresh ( ) { new initialize applications task ( ) . execute ( ) ; } } ) ;
final odistributed server manager . db _ status db status = d manager . get database status ( curr . get key ( ) , get database name ( ) ) ; missing response node statuses . put ( curr . get key ( ) , db status ) ;
layer info tile layer = new geo server tile layer ( layer info , defaults , grid set broker ) ; assert not null ( layer info tile layer . get layer info ( ) ) ; assert null ( layer info tile layer . get layer group info ( ) ) ; layer group info tile layer = new geo server tile layer ( layer group , defaults , grid set broker ) ;
boolean result = task . trigger checkpoint ( new checkpoint meta data ( 41 l , system . current time millis ( ) ) , checkpoint options . for checkpoint ( ) ) ; assert false ( task triggered checkpoint though not ready , result ) ;
for ( string name : new string [ ] { score , val _ ss } ) { schema field sf = h . get core ( ) . get latest schema ( ) . get field or null ( name ) ; assert not null ( test depends on a ( dynamic ) field mtching ' + name + ' , schema was changed out from under us , sf ) ; assert true ( test depends on a multivalued dynamic field matching ' + name + ' , schema was changed out from under us , sf . multi valued ( ) ) ; }
e = begin _ method ( constants . acc _ public , new _ instance , invocation _ target _ exception _ array ) ;
max position = current position ;
patch listener patch listener = new sample patch listener ( app like . get application ( ) ) ;
db . users ( ) . insert permission on group ( group2 , administer ) ;
dummy key key = new dummy key ( ) ;
class < ? > clazz = context . get import handler ( ) . resolve class ( class name ) ; if ( clazz = = null ) { clazz = class . for name ( class name , true , thread . current thread ( ) . get context class loader ( ) ) ; }
string type = task . get ( keys . task _ type ) ; assert true ( unknown task type \ + type + \ is seen in + history file for task + tid , ( type . equals ( map ) | | type . equals ( reduce ) | | type . equals ( setup ) | | type . equals ( cleanup ) ) ) ; if ( type . equals ( map ) ) { string splits = task . get ( keys . splits ) ;
if ( valid list value types . contains ( element type mirror ) & & utils . is realm model ( element type mirror ) ) { final string builder message builder = new string builder ( element type of realm list must be a class implementing ' realm model ' or one of the ) ; final string separator = , ; for ( type mirror type : valid list value types ) { message builder . append ( ' \ ' ' ) . append ( type . to string ( ) ) . append ( ' \ ' ' ) . append ( separator ) ; } message builder . set length ( message builder . length ( ) - separator . length ( ) ) ; message builder . append ( ' . ' ) ; utils . error ( message builder . to string ( ) , field ) ; return false ; } return true ;
perform update ( repeated postprocessor runner , true ) ; m in order . verify no more interactions ( ) ; perform cancel after finished ( ) ;
return value type . number ; }
repository service . create deployment ( ) . add classpath resource ( org activiti engine test api tenant tenancy test . test multi tenancy signals . bpmn20 . xml ) . deploy ( ) ;
m _ last chunk = source . m _ last chunk - 1 ; m _ first free = source . m _ chunk size ;
request = create request ( foo bar ) ; response = new mock http servlet response ( ) ; chain = new mock filter chain ( ) ; header value = client digest string ( tmp , geo server user . root _ username , geoserver1 , request . get method ( ) ) ;
repository service2 . delete deployment ( deployment id , true ) ;
update rjtfailover counters ( ) ; state fetcher . restore state ( this ) ; clear job history cache ( ) ; }
public static int log2 ( long x , rounding mode mode ) { check positive ( x , x ) ;
channels [ channel _ idx ] . pattern _ loop _ row = current _ row + 1 ;
workers . await and throw on error ( runtime exception . class ) ; }
get capabilities . get parameter ( ) . add ( ecore util . copy ( param ) ) ;
array list < searcher > searchers = new array list < > ( ) ;
toast . make text ( context , missing an emoji is everything installed correctly? , toast . length _ short ) . show ( ) ;
return get simple form ( new jabber account registration ( ) , is create account ) ; }
system . arraycopy ( current entity . ch , current entity . position , current entity . ch , length , current entity . count - current entity . position ) ;
break ; case explicit : copy . explicit storage = this . explicit storage . clone ( ) ; break ; case sparse : copy . sparse probabilistic storage = this . sparse probabilistic storage . clone ( ) ; break ; case full : copy . probabilistic storage = this . probabilistic storage . clone ( ) ; break ; default : throw new runtime exception ( unsupported hll type + type ) ;
logger . warn ( could not find valid record definition for { } , parameter ) ; return data ;
account id = properties . get property ( customer account id ) ; create s3 bucket = boolean . value of ( properties . get property ( create s3 bucket ) ) ; s3 region name = properties . get property ( s3 region name ) ; s3 bucket name = properties . get property ( s3 bucket name ) . trim ( ) ; s3 bucket arn = get bucket arn ( s3 bucket name ) ; s3 object prefix = properties . get property ( s3 object prefix ) . trim ( ) ; string size in mbs property = properties . get property ( destination size in mbs ) ; s3 destination size in mbs = string utils . is null or empty ( size in mbs property ) ? null : integer . parse int ( size in mbs property . trim ( ) ) ;
directory . set writable ( true , false ) ;
int new items count = items . size ( ) ; int previous items count = m items . size ( ) ; int items before this adapter = get fast adapter ( ) . get pre item count by order ( get order ( ) ) ;
bundle = new i18 nbundle ( ) ;
shared preferences prefs = context . get shared preferences ( pref _ restriction + . + uid , context . mode _ world _ readable ) ;
dictionary based group key generator . generate keys for block ( _ transform block , _ single value group key buffer ) ;
if ( ( exemplar . f block & xsconstants . derivation _ substitution ) = 0 ) return null ;
bean [ ] b = { new bean ( ) } ;
write file ( file sys , file2 , replication ) ; check file ( file sys , file2 , replication ) ;
m _ pos = m _ root = dom source . get node ( ) ;
filter post processor fpp = new filter post processor ( asset manager ) ;
button . set text ( ) ;
length + = args . length ; return set length property ( cx , this obj , length ) ; }
int start = 0 ; if ( samples . size ( ) = = 0 ) { sample item new item = new sample item ( buffer [ 0 ] , 1 , 0 ) ; samples . add ( new item ) ; start + + ; } list iterator < sample item > it = samples . list iterator ( ) ;
remove breakpoint ( ref ) ; assert true ( f wait . get message ( ) , f wait . is ok ( ) ) ;
model = new tiny yolo ( 1 , 123 , 1 ) ; num labels doesn ' t matter since we ' re getting pretrained imagenet
initialize temp dir ( servlet context , deployment info ) ;
if ( is spi class ( name ) ) { return spi class loader . get resources ( name ) ; }
object representative = null ;
m points . remove ( 0 ) ;
assert get servers eventually matches on all cores ( new counts matcher ( 4 , 1 , 3 , 0 ) ) ; cluster . get core member by id ( 0 ) . shutdown ( ) ;
view helper . set alpha ( m overlay view , scroll utils . get float ( ( float ) scroll y flexible range , 0 , 1 ) ) ;
if ( separation < settings . epsilon ) { manifold . point count = 1 ; manifold . type = manifold type . face _ a ; before inline : manifold . local normal . set ( normals [ normal index ] ) ; manifold . local point . set ( v1 ) . add local ( v2 ) . mul local ( . 5f ) ; manifold . points [ 0 ] . local point . set ( circle . m _ p ) ; after inline : final vec2 normal = normals [ normal index ] ; manifold . local normal . x = normal . x ; manifold . local normal . y = normal . y ; manifold . local point . x = ( v1 . x + v2 . x ) * . 5f ; manifold . local point . y = ( v1 . y + v2 . y ) * . 5f ; final manifold point mpoint = manifold . points [ 0 ] ; mpoint . local point . x = circlep . x ; mpoint . local point . y = circlep . y ; mpoint . id . zero ( ) ; end inline return ; }
assert equals ( set binding . get dependencies ( ) . to string ( ) , 2 , set binding . get dependencies ( ) . size ( ) ) ;
app . wait for state ( map task1 , task state . succeeded ) ; app . wait for state ( map task2 , task state . succeeded ) ; app . wait for state ( map task3 , task state . running ) ;
query = half float point . new range query ( _ field , 10 f , 20 f ) ; result = analyze ( query , collections . empty map ( ) ) ; assert false ( result . verified ) ; ranges = new array list < > ( result . extractions ) ; assert that ( ranges . size ( ) , equal to ( 1 ) ) ; assert null ( ranges . get ( 0 ) . term ) ; assert equals ( _ field , ranges . get ( 0 ) . range . field name ) ; assert dimension ( ranges . get ( 0 ) . range . lower point , bytes - > half float point . encode dimension ( 10 f , bytes , 0 ) ) ; assert dimension ( ranges . get ( 0 ) . range . upper point , bytes - > half float point . encode dimension ( 20 f , bytes , 0 ) ) ;
return ( index + 1 ) & mask ;
this ( ) ; m _ char to string = new hash map ( ) ; resource bundle entities = null ;
assert equals ( , config . get property ( drools . rule base update handler ) ) ;
list < variable element > fields = element filter . fields in ( enum class . get enclosed elements ( ) ) ;
if ( f document handler = null ) { f document handler . text decl ( version , encoding , augs ) ; } }
@ suppress warnings ( unchecked ) simple ordered map < simple ordered map < object > > ranges from shard = ( simple ordered map < simple ordered map < object > > ) facet _ counts . get ( facet _ ranges ) ;
return apple sdk paths builder . build ( ) ;
canvas . draw line ( right + m corner offset , bottom + m corner extension , right + m corner offset , bottom - m corner length , m corner paint ) ;
int lowest vertex index = 0 ; float lowest vertex y = p vertices [ p vertex offset y ] ; final int last vertex offset = p vertex count * p vertex stride ;
oos . write object ( method . class . is instance ( get java member ( ) ) ? method : field ) ; }
final list < jsonobject > users = user query service . get nice users ( 6 ) ; data model . put ( user . users , ( object ) users ) ; final string from name = lang props service . get ( symphony en label ) + + lang props service . get ( weekly email from name label , latkes . get locale ( ) ) ;
if ( spare nodes policy . has spare ( flavor spare count ) ) return collections . empty set ( ) ;
m pager adapter . unregister data set observer ( m pager adapter observer ) ;
content values values = new content values ( ) ;
string st = log . get stack trace string ( new runtime exception ( ) ) ;
execution = runtime service . create execution query ( ) . variable value not equals ( integer var , 12345 ) . single result ( ) ;
ensure governor is annotated ( new annotation metadata builder ( spring java type . configuration ) ) ;
if ( chain = = null ) { just try to help catch what might be a programming error . log . warn ( starting up with no attached block chain . did you forget to pass one to the constructor? ) ; }
leader elector . create root if not exist ( m _ zk , leader elector . election dir for partition ( volt zk . leaders _ initiators , 2 ) ) ;
app . wait for state ( map task1 , task state . succeeded ) ; app . wait for state ( map task2 , task state . succeeded ) ; app . wait for state ( map task3 , task state . running ) ;
assert that ( collection2 shard1 nodes , core matchers . is ( get election nodes ( test _ collection _ 2 , shard1 , state reader . get zk client ( ) ) ) ) ;
expect throws ( illegal argument exception . class , ( ) - > { w . update document ( new term ( field , foo ) , new document ( ) ) ; } ) ; w . close ( ) ;
value3 = cache . get or compute ( thirdd entity , third loader , third reader , term bytes ) ; assert equals ( hit count + 1 , request cache stats . stats ( ) . get hit count ( ) ) ; assert equals ( baz , value3 . stream input ( ) . read string ( ) ) ; ioutils . close ( reader , second reader , third reader , writer , dir , cache ) ;
quota . set memory ( quota . get memory ( ) * arguments . rate multiplier ) ; starting stats . msg rate in = quota . get msg rate in ( ) ; starting stats . msg rate out = quota . get msg rate out ( ) ; starting stats . msg throughput in = quota . get bandwidth in ( ) ; starting stats . msg throughput out = quota . get bandwidth out ( ) ; final bundle data bundle data = new bundle data ( 10 , 1000 , starting stats ) ;
action . accept ( channel ) ; }
for ( int i = x - tile width + offset ; i < = width ; i + = tile width ) { context . get painter ( ) . paint progress bar foreground ( context , g , i , y , tile width , height , p bar . get orientation ( ) ) ; }
list < integer > lengths = entity manager . create query ( select length ( p . name ) + from person p , integer . class ) . get result list ( ) ;
result = get url ( http : localhost : + get port ( ) + test bug49922 target ) ;
if ( t = = null ) throw new null pointer exception ( ) ; util . < runtime exception > sneaky throw0 ( t ) ; return null ;
list < integer > start positions = new array list < > ( ) ;
load balancer simulator . run wait ( degrader load balancer strategy config . default _ update _ interval _ ms ) ;
name example label . set foreground ( color . gray ) ;
results = order function run ( client , fname , result values . length columncount ) ;
assert that ( reflections . get method usage ( usage . c1 . class . get declared method ( method ) ) , are ( usage . c2 . class . get declared method ( method ) ) ) ; assert that ( reflections . get method usage ( usage . c1 . class . get declared method ( method , string . class ) ) , are ( usage . c2 . class . get declared method ( method ) ) ) ;
verify ( rm gateway , timeout ( verification timeout ) . at least ( 1 ) ) . register task executor ( eq ( task manager . get address ( ) ) , eq ( tm resource id ) , eq ( slot report1 ) , any ( time . class ) ) ; verify ( heartbeat manager , timeout ( verification timeout ) ) . monitor target ( any ( resource id . class ) , any ( heartbeat target . class ) ) ;
generate tree ( n nodes ) ;
this . p area . add point ( ( int ) trans y2 , ( int ) trans x1 ) ;
dst = new byte [ src . length ] ; }
fail ( we could not find a jetty to kill for replica : + replica . get core url ( ) ) ; }
vocab word list rdd . count ( ) ;
for ( int i = 0 ; i < num values - 1 ; i + + ) { long timestamp delta = prev delta + buffer . read vlong ( ) ; long timestamp = prev timestamp + timestamp delta ; timestamps . add ( timestamp ) ; prev timestamp = timestamp ; prev delta = timestamp delta ; } return timestamps ;
for ( int i = 0 ; i < num tasks ; i + + ) { thread pool . run task ( create task ( i ) ) ; }
s . flush ( ) ;
list < list < path > > file groups = new linked list < list < path > > ( ) ; long [ ] size groups = new long [ ngroups ] ; int hi = files . size ( ) - 1 ; int lo = 0 ; list < path > group ;
if ( before . get unique name ( ) . equals ( after . get unique name ( ) ) ) { not sure this is best behavior , ponder as progress is made return null ; } volt xmldiff result = new volt xmldiff ( before . get unique name ( ) ) ;
header . set button expand visible ( true ) ;
if ( new port < 0 | | new port > 0x ffff ) { toast . make text ( get activity ( ) , get string ( r . string . account _ proxy _ invalid _ port _ range ) , toast . length _ long ) . show ( ) ; return false ; }
execute ( select * from t order by x limit ? , new object [ ] { limit } ) ;
assert equals ( oidc config . get authorization endpoint ( ) , oidclogin protocol service . auth url ( uri builder . from uri ( oauth client . auth _ server _ root ) ) . build ( test ) . to string ( ) ) ; assert equals ( oidc config . get token endpoint ( ) , oauth . get access token url ( ) ) ; assert equals ( oidc config . get userinfo endpoint ( ) , oidclogin protocol service . user info url ( uri builder . from uri ( oauth client . auth _ server _ root ) ) . build ( test ) . to string ( ) ) ; assert equals ( oidc config . get jwks uri ( ) , oauth . get certs url ( test ) ) ; string registration uri = uri builder . from uri ( oauth client . auth _ server _ root ) . path ( realms resource . class ) . path ( realms resource . class , get clients service ) . path ( client registration service . class , provider ) . build ( test , oidcclient registration provider factory . id ) . to string ( ) ;
input stream input stream = conn . get input stream ( ) ;
writable buffer buffer = allocator ( ) . allocate ( 1024 * 1025 ) ;
for ( int i = 0 ; i < nb _ rows _ in _ batch 2 ; i + + ) { string row key = row + i ; delete del = new delete ( row key . get bytes ( ) ) ; list . add ( del ) ; } ht1 source . delete ( list ) ; for ( int i = 0 ; i < nb _ rows _ in _ batch ; i + + ) { string row key = row + i ; delete del = new delete ( row key . get bytes ( ) ) ; list . add ( del ) ; } ht2 source . delete ( list ) ;
assert equals ( temp file . length ( ) , 0 ) ; write read file ( temp file ) ;
template . send body ( seda : login , ok ) ; template . send body ( seda : user , donald duck ) ;
for ( map < server name , list < hregion info > > map : result . values ( ) ) { for ( map . entry < server name , hserver load > svr entry : online svrs . entry set ( ) ) { if ( map . contains key ( svr entry . get key ( ) ) ) { map . put ( svr entry . get key ( ) , new array list < hregion info > ( ) ) ; } } } return result ; }
jsonarray a2 = new jsonarray ( my enum . values ( ) ) ; assert equals ( [ null , null , null ] , a2 . to string ( ) ) ; }
if ( ( f preloaded element to selection . get ( element ) ) . boolean value ( ) ) continue ;
stats . clear ( ) ;
field = cc . get field ( enum values ) ;
sc time millis = p . get sctimestamp millis ( ) ;
if ( sample model = = null ) { throw new illegal argument exception ( sample model = = null ) ; }
execute response type response = ( execute response type ) value ;
gradients = new int [ normalized intervals . length ] [ ] ;
for ( object name name : new hash set < object name > ( _ registered names ) ) { unregister ( name ) ; }
if ( m underline height > 0 ) { m rect paint . set color ( m underline color ) ; if ( m underline gravity = = gravity . bottom ) { canvas . draw rect ( padding left , height - m underline height , m tabs container . get width ( ) + padding left , height , m rect paint ) ; } else { canvas . draw rect ( padding left , 0 , m tabs container . get width ( ) + padding left , m underline height , m rect paint ) ; } }
if ( contact processed listener = null ) contact processed listener . dispose ( ) ; contact processed listener = null ; super . dispose ( ) ; }
return decode text frame ( type , buffer ) ; }
jar out . put next entry ( new jar entry ( dalvik exec test my resource ) ) ; jar out . write ( this resource contains some text . . get bytes ( ) ) ;
assert no pure calls ( source ) ; }
main thread = thread . current thread ( ) ; settings directory = misc utilities . construct path ( system . get property ( user . home ) , . jedit ) ;
assert equals ( 1 , get file system ( ) . list status ( new path ( ) ) . length ) ; base = assert web responses equals ( base , 1 ) ; assert no errors ( ) ; }
p . set height inches ( 1 ) ;
final execution environment env = execution environment . get execution environment ( ) ; graph < long , long , long > graph = graph . from collection ( test graph utils . get long long vertices ( ) , test graph utils . get long long edges ( ) , env ) ;
list < object > values = ( list < object > ) field binding . get from builder ( builder ) ;
command . queue ( ) ;
configuration chooser . set selected file ( file . get absolute file ( ) ) ; file chooser . set current directory ( file . get absolute file ( ) . get parent file ( ) ) ; try { parse the configuration file . configuration parser parser = new configuration parser ( file , system . get properties ( ) ) ; configuration configuration = new configuration ( ) ; try { parser . parse ( configuration ) ; let the gui reflect the configuration . set pro guard configuration ( configuration ) ; } catch ( parse exception ex ) { joption pane . show message dialog ( get content pane ( ) , msg ( cant parse configuration file , file . get path ( ) ) , msg ( warning ) , joption pane . error _ message ) ; } finally { parser . close ( ) ; } } catch ( ioexception ex ) { joption pane . show message dialog ( get content pane ( ) , msg ( cant open configuration file , file . get path ( ) ) , msg ( warning ) , joption pane . error _ message ) ; }
test all types . parse from ( is ) ;
try { note : ' map ' values must be of type integer , but the values in the map returned by create map ( ) are of type java . util . list . thus 1 cannot be cast to a list . map . put ( foo , 1 ) ; fail ( should have thrown a class cast exception ) ; } catch ( class cast exception e ) { * expected * }
set < synthetic bean < ? > > extra beans = new hash set < > ( ) ;
spark work . add ( map work ) ; return map work ;
scale config parameter ( source conf , dest conf , mrconfig . reducememory _ mb , mrjob config . reduce _ memory _ mb , mrjob config . default _ reduce _ memory _ mb ) ;
int argument index offset = this . parameter types . length - num arguments left to bind ; for ( int i = argument index offset ; i < this . argument names . length ; i + + ) { this . argument bindings . put ( this . argument names [ i ] , i ) ; }
value state descriptor < integer > desc = new value state descriptor < > ( any , int serializer . instance ) ; desc . set queryable ( vanilla ) ; value state < integer > state = backend . get partitioned state ( void namespace . instance , void namespace serializer . instance , desc ) ;
if ( ( url . path _ = null ) & & ( ( url . path _ . length ( ) > 0 ) & & ( ' ' = = url . path _ . char at ( 0 ) ) ) ) { url . path _ = remove leading slash points ( url . path _ ) ; return url ; }
quads . add ( new switch test quad ( \ mary smi th \ , , error switch argument not specified . ) ) ;
restore db task . set pending ( get context ( ) , false ) ;
return math . hypot ( m intercept touch down . x - ev . get x ( ) , m intercept touch down . y - ev . get y ( ) ) > view configuration . get ( get context ( ) ) . get scaled touch slop ( ) ;
jmenu item j menu item visualization graph = new jmenu item ( ) ;
set ( f , f , f ) ;
show loading progress ( is updating ) ; } else if ( is updating & & is post adapter empty ( ) ) {
new runnable ( ) { public void run ( ) { } } . run ( ) ; return null ;
thread . sleep ( 150 ) ; execute service . submit ( reader1 ) ; execute service . shutdown ( ) ; try { execute service . await termination ( 10 , time unit . seconds ) ; } catch ( interrupted exception e ) { logger . error ( error waiting for executor service shutdown , e ) ; }
bean definition builder simple url authentication failure handler = bean definition builder . root bean definition ( simple url authentication failure handler . class ) ; string authentication failed url = element . get attribute ( authentication - failed - url ) ; if ( string utils . has text ( authentication failed url ) ) { simple url authentication failure handler . add constructor arg value ( authentication failed url ) ; }
int bubbled to = heap . bubble up alternating levels ( vacated , to trickle ) ;
float seen ratio = ( ( float ) compressed file size ) data size ; assert equals ( compression emulation util . standardize compression ratio ( ratio ) , compression emulation util . standardize compression ratio ( seen ratio ) , 1 . 0 d ) ; }
if ( pubkey . length = 65 ) return false ; } else if ( pubkey [ 0 ] = = 0x02 | | pubkey [ 0 ] = = 0x03 ) {
raminput stream in = new raminput stream ( testcase , f ) ; assert equals ( input length must match , n , in . length ( ) ) ;
try { exec . await termination ( 5 , time unit . minutes ) ; } catch ( interrupted exception e ) { logger . info ( exec . await termination ( 1 , time unit . hours ) occurs error : + e ) ; }
doc . add ( new text field ( field , wizard oz the the the the the the , field . store . no ) ) ; w . add document ( doc ) ; index reader r = w . get reader ( ) ; w . close ( ) ;
if ( i > = param ends [ curr param name idx ] ) { curr param name idx + + ; } string param name = param names . get ( curr param name idx ) ;
easy mock . expect ( runner . get pending task payloads ( ) ) . and return ( lists . < task > new array list ( ) ) ;
set < contact > contacts = customer . get contacts ( ) ; if ( contacts = null ) { for ( iterator it = contacts . iterator ( ) ; it . has next ( ) ; ) { ( ( contact ) it . next ( ) ) . get name ( ) ; } } return customer ;
final int d = math . min ( w , h ) ;
insert ( views . analysis delaying in minutes ) ; insert ( views . status ) ; insert ( sonar . issuesdensity . weight ) ;
em . get transaction ( ) . begin ( ) ; ce = em . find ( child entity . class , id1 ) ; ce . set data ( y ) ; ce . set num val ( 2l ) ; em . get transaction ( ) . commit ( ) ; }
boolean exception = false ;
flink = new testing cluster ( config ) ;
validate history ( full history ) ; validate history ( part history ) ;
height = measure spec . get size ( height measure spec ) ;
rule key input change = new default rule key factory ( 0 , hash cache , path resolver , rule finder ) . build ( archive . from ( target , project filesystem , rule finder , default _ archiver , immutable list . of ( ) , default _ ranlib , immutable list . of ( ) , archive contents . normal , default _ output , immutable list . of ( fake source path . of ( different ) ) , * cacheable * true ) ) ;
matrix [ x ] [ y ] = 1 ;
error msg err = new error msg ( error msg . jaxp _ unsupported _ feature , name ) ; throw new transformer configuration exception ( err . to string ( ) ) ; }
preconditions . check not null ( metadata spec . get type ( ) , type in metadata update spec must not be null ) ; injector . get instance ( properties . class ) . set property ( druid . metadata . storage . type , metadata spec . get type ( ) ) ; config = hadoop druid indexer config . from spec ( hadoop ingestion spec . update segment list if datasource path spec is used ( config . get schema ( ) , hadoop druid indexer config . json _ mapper , new metadata store based used segment lister ( injector . get instance ( indexer metadata storage coordinator . class ) ) ) ) ; list < jobby > jobs = lists . new array list ( ) ;
string [ ] args = new string [ ] { repair } ;
configuration parameter = this . config parameters . get ( index ) ; if ( configuration parameter = = null ) { configuration parameter = new configuration parameter ( index , 0 , 1 ) ; config parameters . put ( index , configuration parameter ) ; } configuration parameter . set read only ( read only ) ;
qsi . map tsi . reset task vars ( ) ;
string pom roo addon suite = project operations . get path resolver ( ) . get identifier ( path . root . get module path id ( ) , folder + pom . xml ) ; validate . is true ( file manager . exists ( pom roo addon suite ) , folder + pom . xml not found ) ; input stream input stream = file manager . get input stream ( pom roo addon suite ) ;
assert u ( adoc ( id , 1 , nopositionstext , this is a test this is only a test , text , just another test ) ) ;
string buffer req5 = new string buffer ( ) ; req5 . append ( get tests http 1 . 1 \ n ) ; req5 . append ( host : bad . eclipse . org \ n ) ; bad virtual host req5 . append ( connection : close \ n ) ; req5 . append ( \ n ) ; http tester . response response = http . request ( req5 ) ;
archival manager . purge checkpoints ( name node file . image _ rollback ) ;
object key ; if ( object instanceof activation ) { key = class object type . match _ object type . get class type ( ) ; } else if ( object instanceof fact ) { key = ( ( fact ) object ) . get fact template ( ) . get name ( ) ; } else { key = object . get class ( ) ; } object type conf object type conf = this . type conf map . get ( key ) ;
if ( m on group click listener = null ) { if ( m on group click listener . on group click ( this , v , pos metadata . position . group pos , id ) ) { pos metadata . recycle ( ) ; return true ; } } if ( pos metadata . is expanded ( ) ) {
vector < string > result = new vector < string > ( ) ; while ( filelist . has next ( ) ) { result . add ( filelist . next ( ) ) ; } collections . sort ( result , new comparator < string > ( ) { public int compare ( string o1 , string o2 ) { if ( reverse order ) return o2 . compare to ( o1 ) ; else return o1 . compare to ( o2 ) ; } } ) ;
channel manager . close channel ( channel ) ; return ; }
if ( derived = = schema grammar . f any type ) return derived = = base ;
set padding ( padding , padding , padding , padding ) ; }
assert that ( original , not ( null value ( ) ) ) ; transform evaluator < object > evaluator = do fn lifecycle manager removing transform evaluator . wrapping ( underlying , lifecycle manager ) ; try { evaluator . finish bundle ( ) ; } catch ( exception e ) { assert that ( lifecycle manager . get ( ) , matchers . not ( matchers . < do fn < ? , ? > > the instance ( original ) ) ) ; return ; }
body builder . append formal line ( return new % s ( \ % s edit \ ) ; , get name of java type ( spring java type . model _ and _ view ) , views path ) ; body builder . indent remove ( ) ;
node outcrop character = exposure color . get next sibling ( ) ; assert equals ( gsml : outcrop character , outcrop character . get node name ( ) ) ; assert xpath evaluates to ( x , gsml : geologic unit [ @ gml : id = ' gu . 25699 ' ] gsml : outcrop character gsml : cgi _ term value gsml : value , doc ) ;
assert equals ( num entries , iterables . size ( retriever ) ) ; }
tags . add ( new array backed tag ( tag type . ttl _ tag _ type , bytes . to bytes ( t ) ) ) ;
indexed collection indexed collection = ( indexed collection ) collection binding ;
path = c . get string ( c . get column index or throw ( images . media . data ) ) ;
execute command ( schema ls - l + label2 . name ( ) , : + label2 . name ( ) , index state . online . name ( ) , : + label1 . name ( ) ) ;
{ config . set start timestamp millis ( system . current time millis ( ) ) ; config . set end timestamp millis ( system . current time millis ( ) + 1 ) ; try { checker . check stream enabled and time range ok ( ) ; fail ( ) ; } catch ( otsstream reader exception ex ) { assert true ( ex . get message ( ) . starts with ( to avoid timing error between different machines ) ) ; } } { config . set start timestamp millis ( system . current time millis ( ) - 3 * 3600 * 1000 + otsstream reader constants . before _ offset _ time _ millis - 1 ) ; config . set end timestamp millis ( system . current time millis ( ) - 1 * 3600 * 1000 ) ; try { checker . check stream enabled and time range ok ( ) ; fail ( ) ; } catch ( otsstream reader exception ex ) { assert true ( ex . get message ( ) . starts with ( as expiration time is ) ) ; } }
assert . assert equals ( session failed to replicate after container 1 was shutdown . , 8 , state . remaining guesses ) ;
public void test transition _ stream response catchup to snapshot ( ) throws exception { bootstrap pull thread bs puller = create bootstrap pull thread ( false , false , false , false , false , null , 12000 , 1 , true , 50 l , 100 l , source1 , source2 ) ; checkpoint cp = _ ckpt handler two sources . create initial bootstrap checkpoint ( null , 0 l ) ;
int key size = 0 ;
try { sc . read ( buf , 0 , 2 ) ; fail ( should throw null pointer exception ) ; } catch ( null pointer exception expected ) { }
if ( e . get message ( ) . starts with ( invalid global max attempts configuration ) ) throw e ; }
delete . set config set name ( config set bogus ) ;
c1 . mark delete ( p2 ) ; try { as mark - delete is at position : p2 it should read entry : p3 assert equals ( 1 , c1 . replay entries ( positions ) . size ( ) ) ; } catch ( managed ledger exception e ) { fail ( should have not failed ) ; }
zkutil . set data ( this . watcher , node path , zkutil . position to byte array ( last sequence id ) ) ; } }
m preview request builder . set ( capture request . control _ af _ trigger , camera metadata . control _ af _ trigger _ cancel ) ;
m loader handler . remove messages ( msg _ load _ image ) ;
if ( res patch info . delete res . contains ( name ) & & res patch info . mod res . contains ( name ) & & res patch info . large mod res . contains ( name ) & & name . equals ( share constants . res _ manifest ) ) { tinker zip util . extract tinker entry ( old apk , zip entry , out ) ; total entry count + + ; }
return ( kerberos . equals ignore case ( conf . get ( hbase . security . authentication ) ) & & conf . get ( hbase . zookeeper . client . keytab . file ) = null ) ;
job . set num reduce tasks ( 0 ) ; date start time = new date ( ) ;
sers . put ( java . sql . timestamp . class , date serializer . instance ) ;
string name = netty config . server _ thread _ group _ name + ( + config . get server port ( ) + ) ; epoll event loop group epoll group = new epoll event loop group ( config . get server num threads ( ) , get named thread factory ( name ) ) ;
direction = media direction . sendrecv ; } get peer ( ) . get call ( ) . set channel direction ( channel . get id ( ) , media type , direction ) ; }
vector contacts = get targets ( msg ) ; if ( contacts . is empty ( ) ) { print log ( no target found , message discarded , log level . high ) ; if ( msg . is ack ( ) ) sip _ provider . send message ( message factory . create response ( msg , 404 , sip responses . reason of ( 404 ) , null ) ) ; return ; } print log ( message will be redirect to all user ' s contacts , log level . medium ) ;
social user . remove federated identity ( facebook1 ) ;
for ( color control box c box : color boxes . get ( current tab ) ) { int line start char = text area . get line start offset ( c box . get line ( ) ) ; int x = text area . offset to x ( c box . get line ( ) , c box . get char index ( ) - line start char ) ; int y = text area . line to y ( c box . get line ( ) ) + fm . get descent ( ) ; c box . set pos ( x , y + 1 ) ; c box . draw ( g2d ) ; }
try { if ( match ( ifc , metadata reader factory ) ) { return true ; } } catch ( ioexception ex ) { logger . debug ( could not read interface [ + ifc + ] for type - filtered class [ + metadata . get class name ( ) + ] ) ; }
send doc ( 2 ) ; thread . sleep ( sleep ms before heal partition ) ;
terms enum left intersection = left terms . intersect ( automaton , null ) ; terms enum right intersection = right terms . intersect ( automaton , null ) ; assert terms enum ( left intersection , right intersection , rarely ( ) , both have freqs , both have positions ) ; } } }
if ( cursor < i _ p1 ) { return false ; }
query query = em . create query ( select a from actor a where a . id = : id and a . name = : name ) ; query . set parameter ( id , 1 ) ; query . set parameter ( name , tom cruise ) ; list < actor > actors = query . get result list ( ) ; return actors ;
tuple ds . min by ( - 1 ) ; }
final long block size = conf . get long ( dfsconfig keys . dfs _ block _ size _ key , dfsconfig keys . dfs _ block _ size _ default ) ;
return ( int ) open bit set . union count ( this . bits , ( ( bit doc set ) other ) . bits ) ;
context set = true ;
bkc . get ( ) . open ledger ( lh . get id ( ) , book keeper . digest type . crc32 , dl conf . get bkdigest pw ( ) . get bytes ( utf _ 8 ) ) ;
j menu item visualization roc . set accelerator ( key stroke . get key stroke ( key event . vk _ r , key event . ctrl _ mask ) ) ;
il . append ( method gen . load handler ( ) ) ;
assert false ( waiting jobs list contains failed job , mgr . get waiting jobs ( default ) . contains ( job ) ) ;
int dest low = low ;
if ( clip _ on = null ) clip _ on . replay ( ) ; if ( listener = null ) listener . on ua call ringing ( this ) ; }
popup = new modal window ( popup ) ;
assert true ( arrays . equals ( b , ret ) ) ;
throw new illegal state exception ( protocol layering without a tunnel not supported . ) ;
if ( width < = 0 | | height < = 0 ) return ;
assert false ( node util . is object result ( get node ( a ( ) ) ) ) ; assert false ( node util . is object result ( get node ( ' ' . a ) ) ) ; assert false ( node util . is object result ( get node ( a . b ) ) ) ; assert false ( node util . is object result ( get node ( a . b ( ) ) ) ) ; assert false ( node util . is object result ( get node ( a ( ) . b ( ) ) ) ) ; assert false ( node util . is object result ( get node ( a ? true : { } ) ) ) ;
regexp . delete ( regexp . length ( ) - 1 , regexp . length ( ) ) ;
match = resolve ambiguous method ( candidates . key set ( ) , param types ) ;
do distrib pivots ( rb , shard num , facet _ counts ) ;
for ( int i = 0 ; i < = 10 ; i + + ) { print line ( out , xmargin - 3 , xmargin + 2 , ymargin + ( i * height ) 10 , ymargin + ( i * height ) 10 , black ) ; print text ( out , xmargin - 10 , ymargin + 4 + ( i * height ) 10 , string . value of ( 100 - i * 10 ) , end ) ; } if ( is map ) { print color codes for copy , sort , reduce print rect ( out , 14 , 14 , xmargin + w + 4 , ymargin + 20 , colors [ 0 ] ) ; print text ( out , xmargin + w + 24 , ymargin + 30 , copy , start ) ; print rect ( out , 14 , 14 , xmargin + w + 4 , ymargin + 50 , colors [ 1 ] ) ; print text ( out , xmargin + w + 24 , ymargin + 60 , sort , start ) ; print rect ( out , 14 , 14 , xmargin + w + 4 , ymargin + 80 , colors [ 2 ] ) ; print text ( out , xmargin + w + 24 , ymargin + 90 , reduce , start ) ; }
if ( total path . is component ( ) ) { print critical path ( critical path excluding scheduling delays , critical path stats . get optimal path ( ) ) ; } }
try { check traverse ( pc , iip , dir op ) ; } catch ( parent not directory exception pnde ) { if ( is create ) { throw new access control exception ( pnde . get message ( ) ) ; } throw pnde ; } return iip ;
wfsinfo info = get geo server ( ) . get service ( wfsinfo . class ) ; gmlinfo gml info = info . get gml ( ) . get ( wfsinfo . version . v _ 20 ) ; gml info . set mime type to force ( null ) ; get geo server ( ) . save ( info ) ;
log position . position = header . get log pos ( ) ; return event ;
append file ( source , default _ file _ size * 2 ) ; } }
try { time retention strategy time retention strategy = new time retention strategy ( retention time unit , retention time value ) ; _ table deletion strategy . put ( offline table name , time retention strategy ) ; logger . info ( updated deletion strategy for table : { } using retention time : { } { } . , offline table name , retention time value , retention time unit ) ; } catch ( exception e ) { logger . error ( caught exception while building deletion strategy with retention time : { } { ] , remove deletion strategy for table : { } . , retention time value , retention time unit , offline table name ) ; _ table deletion strategy . remove ( offline table name ) ; }
xint namespace type attr = ( xint ) attr values [ xsattribute checker . attidx _ namespace ] ;
family max versions = cf vs max versions . get ( prev fam ) ;
conf . set class ( client backoff policy . backoff _ policy _ class , exponential client backoff policy . class , client backoff policy . class ) ;
int sleep ms = m base sleep time ms * ( thread local random . current ( ) . next int ( 1 < < count , 1 < < ( count + 1 ) ) ) ; return math . min ( abs ( sleep ms , m max sleep ms ) , m max sleep ms ) ; }
redirect path = redirect path + ; + session config . get session uri param name ( request . get context ( ) ) + = + request . get requested session id ( ) ;
this . assert timer state ( ) ; return this . persistent ;
a \ t ( 1 , 1 , n1 , , 40 , false ) ; + app1 in a b \ t ( 1 , 1 , n1 , , 59 , false ) ; + app2 in b c \ t ( 1 , 1 , n1 , , 1 , false ) ; ; app3 in c build env ( labels config , nodes config , queues config , apps config ) ; policy . edit schedule ( ) ;
amazon kinesis client client = new amazon kinesis client ( awsutil . get credentials provider ( config props ) , aws client config ) ; client . set region ( region . get region ( regions . from name ( config props . get property ( awsconfig constants . aws _ region ) ) ) ) ;
instructions . add ( reil helpers . create xor ( base offset + + , first operand size , abs1 . first ( ) , second operand size , abs2 . first ( ) , first operand size , xored signs ) ) ;
policy . on init ( commits ) ;
for ( int i = 0 ; i < n instances ; i + + ) { if ( i = i min1 & & n cluster id [ i ] . size ( ) = 0 ) { int i1 = math . min ( i min1 , i ) ; int i2 = math . max ( i min1 , i ) ; double f distance = get distance ( f distance0 , n cluster id [ i1 ] , n cluster id [ i2 ] ) ; if ( m _ b debug ) { f cluster distance [ i1 ] [ i2 ] = f distance ; f cluster distance [ i2 ] [ i1 ] = f distance ; } queue . add ( new tuple ( f distance , i1 , i2 , n cluster id [ i1 ] . size ( ) , n cluster id [ i2 ] . size ( ) ) ) ; } } n clusters - - ;
transaction . commit allowing state loss ( ) ; support invalidate options menu ( ) ; } else if ( intent . get action ( ) = null ) {
out = exchange . get context ( ) . get type converter ( ) . convert to ( string . class , body ) ;
int gray = ( r * 6966 + g * 23436 + b * 2366 ) > > 15 ;
new list . add ( get current song index ( ) , get current song index ( ) ) ;
assert equals ( tz1 , mapper . writer ( ) . get config ( ) . get time zone ( ) ) ; assert equals ( tz1 , mapper . reader ( ) . get config ( ) . get time zone ( ) ) ; simple date format f = new simple date format ( yyyy - mm - dd hh : mm : ss ) ;
path output = workspace . build and return output ( : genrule - one ) ; string original output = new string ( files . read all bytes ( output ) , utf _ 8 ) ; output = workspace . build and return output ( : genrule - two ) ; string updated output = new string ( files . read all bytes ( output ) , utf _ 8 ) ; assert not equals ( original output , updated output ) ;
pivot x = width 2 ;
if ( state . frame loader . get frame count ( ) = = 1 ) { invalidate self ( ) ; } else if ( is running ) { is running = true ; state . frame loader . subscribe ( this ) ; invalidate self ( ) ; } }
long domain id = data center . get domain id ( ) ; if ( domain id = null ) { domain domain = api dbutils . find domain by id ( domain id ) ; zone response . set domain id ( domain . get id ( ) ) ; zone response . set domain name ( domain . get name ( ) ) ; } zone response . set type ( data center . get network type ( ) . to string ( ) ) ; zone response . set allocation state ( data center . get allocation state ( ) . to string ( ) ) ; zone response . set zone token ( data center . get zone token ( ) ) ; zone response . set dhcp provider ( data center . get dhcp provider ( ) ) ; zone response . set object name ( zone ) ; return zone response ;
system . arraycopy ( incoming public key , 0 , handshake bytes , server dhoffset , key _ length ) ;
if ( ( return type . equals ( xpath constants . string ) ) | | ( return type . equals ( xpath constants . number ) ) | | ( return type . equals ( xpath constants . boolean ) ) | | ( return type . equals ( xpath constants . node ) ) | | ( return type . equals ( xpath constants . nodeset ) ) ) { return true ; } return false ;
conf . set long ( memory size util . offheap _ memstore _ size _ key , ( 1l * 1024l ) ) ;
geo server tile layer info info = tile layer info util . load or create ( mock layer group info , gwcconfig . get old defaults ( ) ) ; geo server tile layer tile layer = mock ( geo server tile layer . class ) ; when ( tile layer . get info ( ) ) . then return ( info ) ; when ( mock mediator . has tile layer ( same ( mock layer group info ) ) ) . then return ( true ) ;
for ( hash map < string , byte iterator > result : result vector ) { assert equals ( assert that this row has the correct number of fields , field set . size ( ) , result . size ( ) ) ; for ( string field : field set ) { assert equals ( assert this field is correct in this row , key map . get ( key _ prefix + test index ) . get ( field ) . to string ( ) , result . get ( field ) . to string ( ) ) ; } test index + + ; }
thread . sleep ( 1000 ) ; rule . run on ui thread ( new runnable ( ) { @ override public void run ( ) { mvi lifecycle backstack activity . press back button ( ) ; } } ) ; thread . sleep ( 1000 ) ; assert . assert equals ( 2 , second fragment presenter . detach view invokations ) ;
int [ ] number = collection . stream ( ) . map to int ( ( s ) - > integer . parse int ( s . substring ( 1 ) ) ) . to array ( ) ;
while ( marker index < tokens . length ) { markers [ marker index ] = other ; marker index + + ; } return markers ;
get cluster nodes request request = get cluster nodes request . new instance ( enum set . all of ( node state . class ) ) ; list < node report > node reports = client . get cluster nodes ( request ) . get node reports ( ) ; assert . assert equals ( 1 , node reports . size ( ) ) ; rm . stop ( ) ;
return new string ( base64encoded ) ;
chain = new service login filter chain ( * * ) ; chain . get http methods ( ) . add ( httpmethod . get ) ; chain . get http methods ( ) . add ( httpmethod . post ) ; matcher = proxy . matcher for chain ( chain ) ; assert true ( matcher . matches ( create request ( httpmethod . get , wms ) ) ) ; assert true ( matcher . matches ( create request ( httpmethod . post , wms ) ) ) ; assert true ( matcher . matches ( create request ( httpmethod . put , wms ) ) ) ;
checksum opt opt = checksum opt . process checksum opt ( default checksum opt , user opt ) ; data checksum data checksum = data checksum . new data checksum ( opt . get checksum type ( ) , opt . get bytes per checksum ( ) ) ; if ( data checksum = = null ) { throw new hadoop illegal argument exception ( invalid checksum type : user opt = + user opt + , default = + default checksum opt + , effective = null ) ; }
semantic pipeline sem = new semantic pipeline ( g ) ; sem . process ( ) ; string language = g . get option string ( language ) ;
metrics map cache metrics = ( metrics map ) h . get core ( ) . get core metric manager ( ) . get registry ( ) . get metrics ( ) . get ( cache . searcher . per seg spatial field cache _ + field name ) ;
mock task . handle ( new task tattempt event ( task attempts . get ( 1 ) . get attempt id ( ) , task event type . t _ attempt _ failed ) ) ; assert task scheduled state ( ) ;
path metadata meta = ms . get ( new path ( p1 ) ) ;
gavin = ( human ) s . get ( human . class , gavin . get id ( ) ) ; gavin . set address ( atlanta , ga ) ; gav = ( being ) s . create query ( from being b where b . location like ' % ga % ' ) . unique result ( ) ; assert equals ( gav . get location ( ) , gavin . get address ( ) ) ; s . delete ( gavin ) ; s . delete ( x23y4 ) ; assert true ( s . create query ( from being ) . list ( ) . is empty ( ) ) ; t . commit ( ) ; s . close ( ) ; }
odatabase document tx graph ;
return remove duplicate site urls ( grouped pw pairs ) ;
in . read utf ( ) ;
if ( response . is committed ( ) ) { return true ; }
log . info ( system metrics publisher with the timeline service v2 is + configured ) ;
assert equals ( 1 f + 1 f , exec ( float x = 1 ; float y = 1 ; return x + y ; ) ) ;
latch . await ( ) ;
if ( m running ) { m handler . remove messages ( flip _ msg ) ; message msg = m handler . obtain message ( flip _ msg ) ; m handler . send message delayed ( msg , m flip interval ) ; }
request attributes . request completed ( ) ;
fields . add ( new field schema ( pa rt1 , serde constants . string _ type _ name , ) ) ; fields . add ( new field schema ( part0 , serde constants . string _ type _ name , ) ) ; return fields ; }
htu . start mini zkcluster ( ) ; mini hbase cluster hbm = htu . start mini hbase cluster ( 1 , 4 ) ; hbm . wait for active and ready master ( ) ; admin = htu . get hbase admin ( ) ;
for ( plugin plugin : local plugins ) { if ( plugin instanceof reload event processor plugin ) { ( ( reload event processor plugin ) plugin ) . reload event ( reloadable type . get name ( ) , reloadable type . get clazz ( ) , versionsuffix ) ; } }
if ( msg = null ) { messages . add ( msg ) ; + + num retrieved ; } else { batch done = true ; break ; }
assert . assert equals ( a = \ xx ; x \ , cookie filter . filter ( a = \ xx ; x \ , null ) ) ; }
internal processor . process ( exchange , target , result processor ) ; } catch ( throwable e ) {
min zoom = zoom bounds . get min zoom ( ) ; max zoom = zoom bounds . get max zoom ( ) ; return zoom bounds ;
extra params . put ( kerberos delegation token authenticator . delegation _ param , d token . encode to url string ( ) ) ; } else {
set float ( this . m context . get string ( res id ) , value ) ;
disable swipe drag capabilities ( ) ;
if ( m _ first free < m _ chunk size ) simplified test single - character - fits chunk = m _ array [ m _ last chunk ] ; else { extend array? int i = m _ array . length ; if ( m _ last chunk + 1 = = i ) { char [ ] [ ] newarray = new char [ i + 16 ] [ ] ; system . arraycopy ( m _ array , 0 , newarray , 0 , i ) ; m _ array = newarray ; } advance one chunk chunk = m _ array [ + + m _ last chunk ] ; if ( chunk = = null ) { hierarchical encapsulation if ( m _ last chunk = = 1 < < m _ rebundle bits & & m _ chunk bits < m _ max chunk bits ) { should do all the work of both encapsulating existing data and establishing new sizes offsets m _ inner fsb = new fast string buffer ( this ) ; } add a chunk . chunk = m _ array [ m _ last chunk ] = new char [ m _ chunk size ] ; } m _ first free = 0 ; }
consul service discovery factory factory = new consul service discovery factory ( ) ; try { introspection support . get properties ( entry . get value ( ) , parameters , null , false ) ; introspection support . set properties ( camel context , camel context . get type converter ( ) , factory , parameters ) ; bean factory . register singleton ( entry . get key ( ) , factory . new instance ( camel context ) ) ; } catch ( exception e ) { throw new bean creation exception ( entry . get key ( ) , e . get message ( ) , e ) ; }
string second uploading image html = uploading image html old apis . replace all ( 54 , 65 ) . replace all ( image . jpg , image2 . jpg ) ; string original content = some text \ n + uploading image html old apis + \ n more text + second uploading image html ; string modified content = editor fragment . replace media file with url ( original content , media file ) ; assert equals ( some text \ n + expected tag + \ n more text + second uploading image html , modified content ) ;
map < t , set < t > > root to tree elements = new hash map < > ( ) ; for ( map . entry < t , entry < t > > entry : map . entry set ( ) ) { t node = entry . get key ( ) ; t root = find internal ( node ) ; root to tree elements . compute if absent ( root , unused - > new hash set < > ( ) ) ; root to tree elements . get ( root ) . add ( node ) ; } return root to tree elements . values ( ) ;
edge e = l . successors ; while ( e = null ) {
site settings table . save settings ( m settings ) ; }
aggregating log handler . stop ( ) ; non aggregating log handler with mock executor log handler = new non aggregating log handler with mock executor ( null , null , null ) ; log handler . init ( new configuration ( ) ) ;
exchange . get out ( ) . set header ( constants . property _ bank , bank name ) ; exchange . get out ( ) . set header ( constants . property _ ssn , ssn ) ; exchange . get out ( ) . set header ( constants . property _ rate , rate ) ; }
on valid signature ( request , response , chain ) ;
log . w ( tag , unsupport media type ) ; } }
fake http servlet response response = send command ( post , null , new json object ( ) ) ; assert equals ( 500 , response . get status ( ) ) ; json object json response = new json parser ( ) . parse ( response . get body ( ) ) . get as json object ( ) ; assert equals ( error codes . unknown _ command , json response . get ( status ) . get as int ( ) ) ; json object value = json response . get ( value ) . get as json object ( ) ; assert true ( value . get ( message ) . get as string ( ) . starts with ( post ) ) ; }
int nk = 0 ; for ( int i = 0 ; i < k ; i + + ) { if ( size [ i ] > 0 ) { nk + + ; } } int [ ] count = new int [ nk ] ;
release plan plan2 = ( release plan ) session . get attribute ( plan ) ;
exchange helper . prepare out to in ( exchange ) ; } callback . done ( done sync ) ; } } ) ; return sync ; }
from ( direct : start )
suggested fix . builder fix = suggested fix . builder ( ) ; string replacement ; string optional cast = ; string optional suffix = ; switch ( double and float status ) { case primitive _ double _ into _ float :
client ( 0 ) . get cache ( ) . put ( dummy , a primitive value cannot be queried ) ; }
mock rm rm2 = new mock rm ( conf , mem store ) ;
async build library task task = new async build library task ( m context , this ) ; task . set on build library progress update ( welcome activity . m building library progress fragment ) ; task . set on build library progress update ( this ) ; task . execute on executor ( async task . thread _ pool _ executor ) ; return start _ sticky ; }
write doc list ( name , ( doc list ) val , return fields , null ) ;
is in replication scope = true ;
rpc exception le = null ; last exception .
if ( anchor delim = - 1 & & ( anchor start = = 0 | | md . substring ( anchor start - 1 ) . starts with ( md _ link _ open ) ) ) { int anchor end = md . index of ( md _ link _ brckt , anchor delim ) ; string title = md . substring ( anchor start + 1 , anchor delim ) ; string link = md . substring ( anchor delim + 2 , anchor end ) ; string anchor = build anchor ( title , link ) ; string md left = md . substring ( 0 , anchor start ) ; string md right = md . substring ( anchor end + 1 ) ; md = md left + anchor + md right ; } else { more anchors = false ; }
assert equals ( handler1 , systray service . get active popup message handler ( ) ) ;
map < string , call argument > names = new linked hash map < > ( ) ;
final item benvolio = new item ( jid create . entity bare from ( benvolio @ example . net ) , benvolio ) ;
override target = new hash map < > ( ) ;
assert that ( layer . get default style ( ) . get name ( ) , equal to ( raster ) ) ; buffered image image = get as image ( wms reflect?layers = test : test123 & format = image png & width = 200 , image png ) ;
intent intent = get intent ( ) ; string message = strings . null to empty ( intent . get string extra ( key _ extra _ message ) ) ;
final model node address = create address ( thread - pool , test - pool ) ;
edit add = menu ;
return source . num expected rows ( ) ;
try { if ( region . close ( abort ) = = null ) {
object rv = null ; method type method type = method type . get ( method ) ;
vertices [ x1 ] + = position . x - tx ;
read cursor . next ( pointer ( left child ) ) ; node . key at ( read cursor , from , 0 ) ; long from inclusive = from . long value ( ) + 2 ; long to exclusive = from . long value ( ) ;
component model hardcoded mapper model = ldaptest utils . get subcomponent by name ( app realm , ldap model , hardcoded role ) ; app realm . remove component ( hardcoded mapper model ) ; } finally {
double d = ( profile . get peak tunnel1m throughput kbps ( ) * 1024d ) + profile . get speed bonus ( ) ;
return default _ storage ; }
orid index rid = ( ( oidentifiable ) values ) . get identity ( ) ; if ( index rid . equals ( doc id ) ) {
assert false ( row result2 . columns . contains key ( column aname ) ) ;
return class manager ;
xdr _ int ( 128 ) ; metric _ id = metadata _ msg
offsets [ k ] = this . serializer . position ;
return new error response pdu ( req , error status , sub . get error index ( ) + 1 ) ; } }
throw new java . lang . runtime exception ( not implemented . ) ; }
result = ssl host configs . get ( sni host name ) ;
if ( launch simulator with udid ( ios simulator path , simulator udid ) ) { return optional . empty ( ) ; } optional < long > boot millis waited = wait for simulator to boot ( timeout millis , simulator udid ) ;
case boolean : row . put ( field name , ( boolean ) value ) ; break ; case binary : if ( value = null ) row . put ( field name , property accessor helper . get bytes ( value ) ) ; break ; case array :
task stack builder . create ( activity )
super . write ( ' . ' ) ;
for ( string deleted session handle : deleted sessions from disk ) { session notifier thread notifier thread = handle to notifier ( deleted session handle ) ; if ( notifier thread . sessions to ctx . get ( deleted session handle ) = null ) { notifier thread . deleted sessions . put ( deleted session handle , notifier thread ) ; } deleted sessions from disk . remove ( deleted session handle ) ; }
list < network external load balancer vo > networks = _ network lbdao . list by load balancer device id ( lb device id ) ;
avg bucket pipeline aggregation builder builder2 = new avg bucket pipeline aggregation builder ( name , global > metric ) ;
final string new file name = new file ;
file manifest file = new file ( dir , jar file . manifest _ name ) ;
set < string > groupset = new hash set < string > ( groups ) ;
if ( field . does indexing ( ) ) { exact match settings for field ( field ) ; } }
string current field value = shell context . get parameters ( ) . get ( fields ) ; string [ ] fields = string utils . split ( current field value , , ) ;
this . project caches to reset . add ( project ) ; break ;
if ( features = = null ) { let ' s retain the ordering as well features = new linked hash map ( ) ; } features . put ( name , value ? boolean . true : boolean . false ) ; }
string host2 = neo4j . org ; boolean host2 known = true ; try { inet address . get by name ( host2 ) ; } catch ( unknown host exception e ) { host2 known = false ; }
for ( asserting input argument : arguments ) { argument . calls = 0 ; } actual value = scalar . evaluate ( ( input [ ] ) arguments ) ;
if ( meta contact . get default contact ( operation set file transfer . class ) = = null ) this . send file item . set enabled ( false ) ; if ( contact phone util . is call enabled ( ) ) { this . call item . set enabled ( false ) ; }
response = overlord resource . get complete tasks ( req ) ; assert . assert equals ( 2 , ( ( ( list ) response . get entity ( ) ) . size ( ) ) ) ; task master . stop ( ) ; assert . assert false ( task master . is leader ( ) ) ; easy mock . verify ( task lockbox , task action client factory ) ; }
scan = new scan ( ) ; result = get single scan result ( ht , scan ) ; assert single result ( result , rows [ 2 ] , family , qualifier , value ) ; scan = new scan ( rows [ 0 ] , rows [ 3 ] ) ; result = get single scan result ( ht , scan ) ; assert single result ( result , rows [ 2 ] , family , qualifier , value ) ; scan = new scan ( rows [ 2 ] , rows [ 3 ] ) ; result = get single scan result ( ht , scan ) ; assert single result ( result , rows [ 2 ] , family , qualifier , value ) ;
gh long > > > = 4 ;
if ( super . connect input ( i , n ) ) { return false ; }
val1 . set value ( 30 ) . set exists ( true ) ; val2 . set exists ( false ) ; func . get double ( ) ; assert false ( func . exists ( ) ) ;
row filter filter = row filter . new builder ( ) . set row key regex filter ( byte string . copy from utf8 ( . * 17 . * ) ) . build ( ) ; bigtable source source = new bigtable source ( service factory , table , filter , byte key range . all _ keys , null * size * ) ; list < bigtable source > splits = source . split ( num rows * bytes per row num splits , null ) ;
context app context = instrumentation registry . get target context ( ) ; assert equals ( com . luseen . spacenavigationview , app context . get package name ( ) ) ; }
cfg rep = auth mgmt resource . get authenticator config ( cfg id ) ; assert config ( cfg rep , cfg id , foo , idp create user if unique authenticator factory . require _ password _ update _ after _ registration , true ) ;
if ( get configuration ( ) . get boolean ( hbase . regionserver . workers , true ) ) { start services ( ) ; }
string default cluster props = { \ + zk state reader . legacy _ cloud + \ : \ true \ } ;
runtime aggregator registry aggregator registry = new runtime aggregator registry ( config . get iteration aggregators ( get user code class loader ( ) ) ) ; iteration aggregator broker . instance ( ) . hand in ( broker key , aggregator registry ) ; data input view superstep result = null ;
string text = term . text ( ) ; int num edits = fuzzy query . float to edits ( minimum similarity , text . code point count ( 0 , text . length ( ) ) ) ; return new fuzzy query ( term , num edits , prefix length ) ; }
lib imobile device . idevice _ set _ debug _ level ( level > 0 ? 1 : 0 ) ;
iterator < host > it = host list . iterator ( ) ; host src host = it . next ( ) ;
if ( pg a = = primitive grouping . date _ group ) { integer ai = type info utils . date types . get ( pc a ) ; integer bi = type info utils . date types . get ( pc b ) ; return ( ai > bi ) ? a : b ; }
assert equals ( loaded view added node changed graph type added node changed graph type added edge , m _ listener . event list ) ;
fail ( expected a rejection ) ;
for ( injection target target : mdr . get injection targets ( ) ) { sb . append ( < injection - target > \ n ) ; append element ( sb , indent6 , injection - target - class , target . get target class ( ) ) ; append element ( sb , indent6 , injection - target - name , target . get target name ( ) ) ; sb . append ( < injection - target > \ n ) ; }
m is visible = false ;
assert equals ( apos to quotes ( { ' value ' : null } ) , mapper . write value as string ( new case changing string wrapper ( null ) ) ) ;
final element configuration = xml utils . get configuration ( get class ( ) ) ;
l . input stack top = b . info = = edge . exception ? 1 : start + b . info ;
handle sdts in instance ( ) ; return sanity check ( ) ;
assert . assert false ( mock . predicate ( 4 l , 0 l , 0 l , 3 l , 0 l , 2 l ) ) ;
gregorian calendar calendar = new gregorian calendar ( time zone . get time zone ( utc ) ) ;
if ( element . get kind ( ) . is class ( ) & & is private ( element ) ) { return null ; } return super . visit class ( node , a void ) ; }
all . add ( new locale deserializer ( ) ) ;
if ( ascending tail map . size ( ) < descending tail map . size ( ) ) { add intersections ( endpoints , ascending tail map , descending tail map ) ; } else { add intersections ( endpoints , descending tail map , ascending tail map ) ; }
msg printer . print status msg ( creating stemmer . . . ) ;
private static set wrapper < integer > nest ( integer elem , set wrapper < integer > . . . nested ) { nested set builder < integer > builder = nested set builder . stable order ( ) ; builder . add ( elem ) ; for ( set wrapper < integer > wrap : nested ) { builder . add transitive ( wrap . set ) ; } return new set wrapper < > ( builder . build ( ) ) ;
if ( result . get tbl pr ( ) = = null ) { log . error ( null tbl pr . fixme ) ; } return result ; }
test types ( * * @ constructor * function foo ( ) { } \ n + * * @ constructor * function bar ( ) { } \ n + var bar instance = new bar ; \ n + var baz = * * @ type { foo } * ( bar instance ) ; \ n , invalid cast - must be a subtype or supertype \ n + from : bar \ n + to : foo ) ;
if ( render progress < = start _ trim _ duration _ offset ) { float start trim progress = ( render progress ) start _ trim _ duration _ offset ; m start degrees = m origin start degrees + max _ swipe _ degrees * material _ interpolator . get interpolation ( start trim progress ) ; float m swipe degrees = m end degrees - m start degrees ; float level swipe degrees progress = math . abs ( m swipe degrees ) max _ swipe _ degrees ; float level1 increment = decelerate _ interpolator . get interpolation ( level swipe degrees progress ) - linear _ interpolator . get interpolation ( level swipe degrees progress ) ; float level3 increment = accelerate _ interpolator . get interpolation ( level swipe degrees progress ) - linear _ interpolator . get interpolation ( level swipe degrees progress ) ; m level swipe degrees [ 0 ] = - m swipe degrees * level _ sweep _ angle _ offsets [ 0 ] * ( 1 . 0f + level1 increment ) ; m level swipe degrees [ 1 ] = - m swipe degrees * level _ sweep _ angle _ offsets [ 1 ] * 1 . 0f ; m level swipe degrees [ 2 ] = - m swipe degrees * level _ sweep _ angle _ offsets [ 2 ] * ( 1 . 0f + level3 increment ) ; }
assert that ( name value pairs [ i ] . to string ( ) , is ( equal to ( expected string representations [ i ] ) ) ) ;
if ( level = = level _ negate _ read ) { acl . insert ace ( acl . get entries ( ) . size ( ) , permission , sid , false ) ; not granting } else { acl . insert ace ( acl . get entries ( ) . size ( ) , permission , sid , true ) ; granting }
color = color . red ;
assert not null ( rel2 ) ; assert true ( rel2 . is source followed by target ( ) ) ; assert true ( rel2 . is source following target ( ) ) ; assert true ( rel2 . is target following source ( ) ) ; assert true ( rel2 . is target followed by source ( ) ) ;
final string json = new string ( jsonserialiser . serialise ( aggregator , true ) ) ;
for ( int i = 0 ; i < 100 ; i + + ) { assert equals ( 2 + i , name registry . get id or allocate for ( a b c + i ) ) ; }
base uri . absolutize ( get user dir ( ) ) ;
final hmaster master = this . util . get hbase cluster ( ) . get master ( ) ;
synchronized ( thread _ count ) { if ( connection = null ) { connection . close ( ) ; connection = null ; } }
child many to many no cascade child1 = new child many to many no cascade ( ) ;
component verifier extension . result result = verifier . verify ( component verifier extension . scope . connectivity , collections . empty map ( ) ) ;
final integer null integer = null ;
assert query ordered ( select orderkey , custkey , orderstatus from orders order by nullif ( orderkey , 3 ) asc nulls first , custkey asc ) ;
if ( client connection manager = null ) { log . info ( shutting down client connection manager : + client connection manager ) ; client connection manager . shutdown ( ) ; client connection manager = null ; } super . do stop ( ) ;
return new remote exception ( e ) ;
if ( char array buffer = null ) { out . print multi ln ( char array buffer . to string ( ) ) ; }
config mother . add pipeline with group ( config , group1 , pipeline1 , stage1 a , job1 a1 , job1 a2 ) ; config mother . add admin user for pipeline group ( config , groupadmin1 , group1 ) ; pipeline configs group = config . find group ( group1 ) ;
request to sign = apikey = + encoded api key + & command = reboot virtual machine & id = + _ windows vm id . get ( ) ; request to sign = request to sign . to lower case ( ) ; signature = sign request ( request to sign , _ secret key . get ( ) ) ; encoded signature = urlencoder . encode ( signature , utf - 8 ) ; url = developer server + ?command = reboot virtual machine & id = + _ windows vm id . get ( ) + & apikey = + encoded api key + & signature = + encoded signature ;
file manifest file to package = get manifest file ( ) ;
final invocation handler stub method handler = new stub invocation handler impl ( pm , class data , stub ) ;
short result = load short ( position ) ;
conf . set boolean ( yarn configuration . nm _ docker _ allow _ privileged _ containers , true ) ;
if ( process tree . is alive ( pid ) ) { return ; }
sasl rpc client . init crypto cipher ( crypto cipher meta , this . rpc client . conf ) ;
if ( canonical name . equals ( parameter class . get qualified name ( ) ) ) { return false ; } list < psi type > psi types = new array list < psi type > ( pct . resolve generics ( ) . get substitutor ( ) . get substitution map ( ) . values ( ) ) ;
body def body def = new body def ( ) ; ground body = world . create body ( body def ) ;
warn ( cat _ ugi , failed to reset ugi - and so could not try to relogin ) ; log . debug ( failed to reset ugi : { } , e , e ) ; } } else {
final checksum partial crc = data checksum . new data checksum ( disk checksum . get checksum type ( ) , disk checksum . get bytes per checksum ( ) ) ;
list < date time field spec > date time field specs = schema . get date time field specs ( ) ;
verify stmt fails ( client , drop function add2bigint , cannot drop user defined function \ add2bigint \ . the statement proc . sql0 depends on it ) ; catalog error = catalog matches compiler function set ( client ) ; assert equals ( , catalog error ) ;
try { sub cluster deregister request request = sub cluster deregister request . new instance ( sub cluster id null , state lost ) ; federation membership state store input validator . validate ( request ) ; assert . fail ( ) ; } catch ( federation state store invalid input exception e ) { log . info ( e . get message ( ) ) ; assert . assert true ( e . get message ( ) . starts with ( missing sub cluster id information . ) ) ; }
if ( complex structural tests enabled ) { iterable < object > simple structs = transform ( insert null every ( 5 , write values ) , rc file tester : : to hive struct ) ; test round trip type ( new row type ( immutable list . of ( create row type ( type ) ) , optional . of ( immutable list . of ( field ) ) ) , transform ( simple structs , collections : : singleton list ) , skip formats set ) ; }
config c1 = build config ( false , 25701 ) ; config c2 = build config ( false , 25704 ) ; config c3 = build config ( false , 25703 ) ; list < string > cluster one members = arrays . as list ( 127 . 0 . 0 . 1 : 25701 ) ;
map . put ( a , 100 ) ; map . put ( b , null ) ; map . put ( c , hello ) ; map < string , string > result = roundtrip ( map ) ;
formatter . set property data source ( new object property ( null , expected class ) ) ; formatter . get value ( ) ; calls format
if ( bestid < 0 ) { bestid = i ; } break ; } } }
final object row = convert to row ( schema , record ) ; return ( row ) row ;
m _ wrapped _ handler _ not _ initialized = false ; }
assert . assert equals ( _ ewyk , _ executions . poll ( ) ) ;
object mapper . configure ( deserialization feature . fail _ on _ unknown _ properties , false ) ; plan tree tree = object mapper . read value ( t , plan tree . class ) ;
database database = db . get database ( db name ) ; assert ( database = null ) ; map < string , string > db params = database . get parameters ( ) ; if ( db params = null ) { for ( hive conf . conf vars var : hive conf . db vars ) { string new value = db params . get ( var . varname ) ; if ( new value = null ) { log . info ( changing { } from { } to { } , var . varname , conf . get var ( var ) , new value ) ; conf . set var ( var , new value ) ; } } } return 0 ;
for ( int i = relation span . end ( ) ; i < math . min ( sentence . length ( ) , relation span . end ( ) ) ; + + i ) { if ( title . equals ( sentence . ner tag ( i ) ) ) { indicator ( feats , title _ after , t ) ; } if ( top _ employee _ triggers . contains ( sentence . word ( i ) . to lower case ( ) ) ) { indicator ( feats , top _ employee _ trigger _ after , t ) ; } }
log . d ( tag , skip copying , the so lib is exist and not change : + zip entry name ) ; continue ;
string element type owner = rs . get string ( 1 ) ;
assert equals ( sub stats . msg rate out , sub stats . consumers . get ( 0 ) . msg rate out ) ; assert equals ( sub stats . msg throughput out , sub stats . consumers . get ( 0 ) . msg throughput out ) ; assert equals ( stats . msg rate out , sub stats . consumers . get ( 0 ) . msg rate out ) ; assert equals ( stats . msg throughput out , sub stats . consumers . get ( 0 ) . msg throughput out ) ; assert not null ( sub stats . consumers . get ( 0 ) . client version ) ; message msg ;
mock rm rm2 = new mock rm ( conf , rm1 . get rmstate store ( ) ) ;
if ( local cancel conversion task response = = null ) { throw new org . apache . axis2 . databinding . adbexception ( property cannot be null ) ; }
if ( inserted child . get node type ( ) = = node . text _ node ) { child node prev = inserted child . previous sibling ( ) ; child node next = inserted child . next sibling ; if an adjacent sibling of the new child is a text node , flag this node as unnormalized . if ( ( prev = null & & prev . get node type ( ) = = node . text _ node ) | | ( next = null & & next . get node type ( ) = = node . text _ node ) ) { is normalized ( false ) ; } } else { if the new child is not normalized , then this node is inherently not normalized . if ( inserted child . is normalized ( ) ) { is normalized ( false ) ; } }
notes1 = notebook . get all notes ( user1 ) ;
return item collection state . of string ( state ) ;
view root view = inflate ( get context ( ) , r . layout . chip _ view , this ) ;
public void size _ non matching predicate ( ) { set < queryable entry > entries = generate entries ( 100000 ) ; and result set result set = new and result set ( entries , null , as list ( new false predicate ( ) ) ) ; int size = result set . size ( ) ;
block = get block at ( block . get start offset ( ) , false , true ) ;
_ helix resource manager . delete offline table ( table _ name ) ;
integer async thread pool size = client properties . get value ( runtime properties , client properties . async _ threadpool _ size , integer . class ) ;
orecord serializer json . instance . from string ( i source , this , null , i options , false ) ; add new parameter to accommodate new api ,
for ( file status snapshot : snapshots ) { path info = new path ( snapshot . get path ( ) , snapshot description utils . snapshotinfo _ file ) ; if the snapshot is bad if ( fs . exists ( info ) ) { log . error ( snapshot information for + snapshot . get path ( ) + doesn ' t exist ) ; continue ; } fsdata input stream in = null ; try { in = fs . open ( info ) ; snapshot description desc = snapshot description . parse from ( in ) ; snapshot descs . add ( desc ) ; } catch ( ioexception e ) { log . warn ( found a corrupted snapshot + snapshot . get path ( ) , e ) ; } finally { if ( in = null ) { in . close ( ) ; } } } return snapshot descs ;
secret key factory factory = secret key factory . get instance ( pbkdf2 with hmac sha1 ) ;
client ( ) . prepare search ( test1 ) . get ( ) ; assert acked ( client ( ) . admin ( ) . cluster ( ) . prepare update settings ( ) . set transient settings ( collections . singleton map ( transport search action . shard _ count _ limit _ setting . get key ( ) , num primaries1 - 1 ) ) ) ;
integer absent key = this . size ( ) + 1 ;
jaxb string object pair [ ] value = ( jaxb string object pair [ ] ) o ; map < object , object > r = new hash map < object , object > ( ) ; for ( jaxb string object pair p : value ) { if ( p . get value ( ) instanceof jaxb list wrapper ) { r . put ( p . get key ( ) , new array list ( arrays . as list ( ( ( jaxb list wrapper ) p . get value ( ) ) . get elements ( ) ) ) ) ; } else { r . put ( p . get key ( ) , p . get value ( ) ) ; } } return r ;
operands . add ( beam sql primitive . of ( sql type name . integer , 2 ) ) ; operands . add ( beam sql primitive . of ( sql type name . integer , 4 ) ) ; operands . add ( beam sql primitive . of ( sql type name . integer , 5 ) ) ; assert . assert false ( new beam sql round expression ( operands ) . accept ( ) ) ; }
infinite . subscribe on ( schedulers . computation ( ) ) . on backpressure drop ( ) . take ( 500 ) . subscribe ( ts ) ;
connector . set accepting ( true ) ; uri = handler . exchange . exchange ( delayed connection ) ; assert that ( uri , is ( four ) ) ; response = http tester . parse response ( in2 ) ; assert that ( response . get status ( ) , is ( 200 ) ) ; assert that ( response . get content ( ) , is ( delayed connection ) ) ; } } } }
if ( account id . get account property boolean ( protocol provider factory . default _ encryption , true ) & & account id . is encryption protocol enabled ( srtp control type . dtls _ srtp ) ) { supported features . add ( urn _ xmpp _ jingle _ dtls _ srtp ) ; } }
add operation ( default _ shutdown _ operation , true ) ;
if ( ( block & xsconstants . derivation _ restriction ) = 0 | | ( derived . get base type ( ) . get final ( ) & xsconstants . derivation _ restriction ) = 0 ) { return false ; }
http server server = grizzly http server factory . create http server ( base uri , new my application ( ) , false ) ; runtime . get runtime ( ) . add shutdown hook ( new thread ( server : : shutdown now ) ) ; server . start ( ) ;
throw new illegal state exception ( unsupported cause : + cause ) ;
if ( x + _ x + chunks . width > clip rect . x & & x + _ x < clip rect . x + clip rect . width ) { useful for debugging purposes if ( debug . chunk _ paint _ debug ) { gfx . draw ( new rectangle2 d . float ( x + _ x , y - 10 , chunks . width , 10 ) ) ; } if ( chunks . is accessible ( ) & & chunks . glyphs = null ) { gfx . set font ( chunks . style . get font ( ) ) ; gfx . set color ( chunks . style . get foreground color ( ) ) ; if ( glyph vector ) chunks . draw glyphs ( gfx , x + _ x , y ) ; else if ( chunks . str = null ) { gfx . draw string ( chunks . str , x + _ x , y ) ; } } } _ x + = chunks . width ;
return new filter ( ) { final fixed bit set bs = bits ; @ override public doc id set get doc id set ( final leaf reader context context , final bits accept docs ) { leaf reader reader = context . reader ( ) ;
list < member is available > events = new linked list < > ( ) ; events . add ( role for id ( master , 1 ) ) ; events . add ( role for id ( slave , 2 ) ) ; events . add ( role for id ( slave , 3 ) ) ;
double val = math . pow ( 2 , exponent ) ;
holder . m text view . set text ( item . get text ( ) ) ;
for ( byte codes byte code : byte codes ) { try { classes . add ( class node ( byte code . bytes ( ) ) ) ; } catch ( exception e ) { failures . add ( new failure ( e , e . to string ( ) ) ) ; } }
long max size = long . min _ value ; the size of the biggest small table for ( int pos = 0 ; pos < map join tables . length ; pos + + ) { if ( pos = = desc . get pos big table ( ) ) { continue ; } long small table size = desc . get parent data sizes ( ) . get ( pos ) ; total size + = small table size ; if ( max size < small table size ) { max size = small table size ; biggest = pos ; } } table memory sizes = divide hybrid hash table memory ( map join tables , desc , total size , total map join memory ) ;
final class < ? > return type = creation method . get return type ( ) ;
list < expr node desc > both filters = lists . new array list ( ) ; both filters . add ( node ) ; both filters . add ( node2 ) ; expr node generic func desc both = new expr node generic func desc ( type info factory . string type info , new generic udfopand ( ) , both filters ) ; accumulo range generator range generator = new accumulo range generator ( conf , handler , row id mapping , rid ) ;
do { this listing = namenode . get partial listing ( src , this listing . get last name ( ) ) ; if ( this listing = = null ) { return null ; the directory is deleted } partial listing = this listing . get partial listing ( ) ; for ( hdfs file status file status : partial listing ) { listing . add ( hdfs file status . to file status ( file status , src ) ) ; } } while ( this listing . has more ( ) ) ; return listing . to array ( new file status [ listing . size ( ) ] ) ;
list < java symbol name > parameter names = new array list < java symbol name > ( ) ; parameter names . add ( new java symbol name ( ids ) ) ; method metadata existing method = get governor method ( method name , annotated java type . convert from annotated java types ( parameter types ) ) ; if ( existing method = null ) { return existing method ; }
return throw bad utf8 ( v2 , at + 2 ) ;
calendar . set ( 2009 , 8 , 26 ) ; date date = calendar . get time ( ) ;
sorted set < string > result = sets . new tree set ( new task name comparator ( ) ) ;
realm results < all java types > still none = none . where ( ) . greater than ( all java types . field _ long , test _ size ) . find all ( ) ; assert equals ( 0 , still none . size ( ) ) ; }
assert . assert array equals ( new int [ ] { 11 } , dummy . get batch sizes ( ) ) ; dummy = new dummy exponentially decaying batch work ( 11 , 3 , 1 ) ;
if ( sync ) { try { client . call procedure with timeout ( timeout _ long , proc name , params ) ; } catch ( exception ex ) { fail ( proc name + is supposed to succeed but failed with message : + ex . get message ( ) ) ; } } else { client . call procedure with timeout ( m _ callback , timeout _ long , proc name , params ) ; client . drain ( ) ; check callback success ( ) ; }
if ( modifier . is native ( method . get modifiers ( ) ) ) { continue ; }
column metadata column metadata = _ segment metadata . get column metadata for ( column name ) ; if ( ( column metadata . has dictionary ( ) ) | | ( column metadata . get min value ( ) = null ) ) { return ; } pinot data buffer dictionary buffer = _ segment writer . get index for ( column name , column index type . dictionary ) ;
expression right expression = expression . get right expression ( ) ; right expression . visit ( acg ) ; operand stack . box ( ) ; compare to method . call ( controller . get method visitor ( ) ) ;
list results = criteria . list ( ) ;
assert equals ( a & quot ; b & lt ; c & gt ; d & amp ; , html escaper ( ) . escape ( a \ b < c > d & ) ) ;
task task = task service . create task query ( ) . task assignee like ( % \ \ % % ) . single result ( ) ;
cancel query ( query runner , first dashboard query ) ; wait for query state ( query runner , first dashboard query , failed ) ; wait for query state ( query runner , second dashboard query , running ) ; }
if ( report . get application id ( ) . equals ( app1 . get application id ( ) ) ) { assert . assert false ( report . get host ( ) . equals ( n a ) ) ; assert . assert true ( report . get rpc port ( ) = - 1 ) ; } }
first exception = child . get exception ( ) ; first child key = child . get root cause of exception ( ) ; } root causes builder . add transitive ( child . root causes ) ; cycle builder . add all ( cycle info . prepare cycles ( current value , child . cycles ) ) ; is transitively transient | = child . is transitively transient ( ) ; is catastrophic | = child . is catastrophic ( ) ; }
try { web hdfs . rename snapshot ( foo , null , s2 ) ; fail ( expected illegal argument exception ) ; } catch ( remote exception e ) { assert . assert equals ( required param oldsnapshotname for + op : renamesnapshot is null or empty , e . get localized message ( ) ) ; }
user group information . set configuration ( hadoop configuration ) ;
if ( this . web application context instanceof configurable application context ) { configurable application context cac = ( configurable application context ) this . web application context ; if ( cac . is active ( ) ) {
texture grass = asset manager . load texture ( textures terrain splat grass . jpg ) ; grass . set wrap ( wrap mode . repeat ) ; mat terrain . set texture ( diffuse map , grass ) ; mat terrain . set float ( diffuse map _ 0 _ scale , grass scale ) ;
assert true ( inet addresses . for string ( 1 . 2 . 3 . 4 ) = inet addresses . get coerced ipv4 address ( inet addresses . for string ( 2002 : 0102 : 0304 : : 1 ) ) ) ;
u - > { logins . remove ( u . get login ( ) ) ; bulk indexer . add ( new index request ( u , organization uuids by login ) ) ; } ) ;
. include ( this . get class ( ) . get name ( ) + . * )
for ( int i = 0 ; i < 3 ; i + + ) { must use boolean rather than boolean reference value to prevent auto - unboxing ambiguity assert equals ( boolean . true , params . get bool ( true - + i ) ) ; assert equals ( boolean . false , params . get bool ( false - + i ) ) ; }
file parent dir = new file ( tmp path ) ;
session . get transaction ( ) . begin ( ) ; session . get transaction ( ) . commit ( ) ; }
assert true ( mtl . get meta region location ( watcher ) . equals ( hsa ) ) ;
if ( has output property ( query , metadata _ property _ name , false ) ) { filter filter = ff . equal ( ff . property ( id ) , ff . property ( metadata . mid ) , true ) ; final string metadata table = get metadata table ( ) ; join join = new join ( metadata table , filter ) ; join . set alias ( metadata ) ; join . set type ( type . outer ) ; result . get joins ( ) . add ( join ) ; }
assert equals ( client ctx . with ( rpc constants . rpc _ server _ method , tag value . create ( method . get full method name ( ) ) ) , stats _ context _ key . get ( server context ) ) ;
new policy qualifier info ( encoding ) ;
uri1 = uri . create ( https : server ) ;
bootstrap . set pipeline factory ( new storm client pipeline factory ( this , storm conf ) ) ; reconnect ( ) ; }
assert that ( changes . changes for property ( 2 , to ) , is diff sets ( as set ( 1 l ) , as set ( 2 l ) ) ) ; assert that ( changes . changes for property ( 3 , from ) , is diff sets ( as set ( 1 l ) , null ) ) ; assert that ( changes . changes for property ( 2 , from ) , is diff sets ( null , as set ( 1 l ) ) ) ; }
options . writing base ( false ) . minimum transaction id ( 1 ) . maximum transaction id ( 1 ) . record id column ( 5 ) ;
channel . transfer to ( 0 , channel . size ( ) , out ) ;
int bottom limit = get height ( ) - get padding bottom ( ) ;
final string msg = get message ( ) ;
if ( new bottom - rect . top > m max crop height ) { new bottom = rect . top + m max crop height ; } if ( bounds . bottom - new bottom < snap margin ) { new bottom = bounds . bottom ; }
handlers . add ( this , depth , varbind ) ; }
fis = new file input stream ( file of write only file channel ) ;
assert true ( ( ts instanceof method interceptor ) ) ; assert true ( ( ts instanceof introduction interceptor ) ) ; assert true ( ts . get time stamp ( ) = = t ) ; ( ( itester ) ts ) . foo ( ) ; ( ( itest bean ) ts ) . get age ( ) ;
response = _ connector . get response ( get ctx index . html http 1 . 0 \ r \ n \ r \ n ) ; assert true ( response . starts with ( http 1 . 1 403 forbidden ) ) ; }
if ( contextual search field trial . is peek promo enabled ( ) ) return false ; return is peek promo condition satisfied ( ) ;
env . get config ( ) . set global job parameters ( params ) ;
range query range = next range ( dimensions ) ; int rv = random ( ) . next int ( 4 ) ; query query ; range . query type query type ; if ( rv = = 0 ) { query type = range . query type . intersects ; query = new intersects query ( query range ) ; } else if ( rv = = 1 ) { query type = range . query type . contains ; query = new contains query ( query range ) ; } else if ( rv = = 2 ) { query type = range . query type . within ; query = new within query ( query range ) ; } else { query type = range . query type . crosses ; query = new crosses query ( query range ) ; } if ( verbose ) { system . out . println ( query = + query ) ; }
this . is closed = true ; }
builder . scheme ( scheme ) ;
if ( table name . contains ( ) ) { table name = table name . substring ( 0 , table name . index of ( ) ) ; } int ind2 = table name . index of ( ' . ' ) ; if ( ind2 > 0 ) { table name = table name . substring ( ind2 + 1 ) ; } return table name ;
system . arraycopy ( state , 0 , new state , 0 , n ) ;
execute command ( schema , label . name ( ) , property ) ;
send listener listener = new send listener ( stream , ( ) - > transport service . on response sent ( request id , action , response , final options ) , message . length ( ) ) ;
if ( media item . is playable ( ) ) { state = state _ playable ; if ( media idhelper . is media item playing ( context , media item ) ) { state = get state from controller ( context ) ; } } return state ; }
if ( t = = null ) throw new null pointer exception ( ) ; util . < runtime exception > sneaky throw0 ( t ) ; return null ;
try { our basic pass - through vertex shader final string vert = util . read file ( util . get resource as stream ( res shadertut lesson6 . vert ) ) ; our fragment shader , which does the blur in one direction at a time final string frag = util . read file ( util . get resource as stream ( res shadertut lesson6 . frag ) ) ; create our shader program shader program . set strict mode ( false ) ; shader = new shader program ( vert , frag , sprite batch . attributes ) ; good idea to log any warnings if they exist if ( shader . get log ( ) . length ( ) = 0 ) system . out . println ( shader . get log ( ) ) ; always a good idea to set up default uniforms . . . shader . use ( ) ; our normal map shader . set uniformi ( u _ normals , 1 ) ; gl _ texture1 light ambient colors shader . set uniformf ( light color , light _ color ) ; shader . set uniformf ( ambient color , ambient _ color ) ; shader . set uniformf ( falloff , falloff ) ; batch = new sprite batch ( shader ) ; } catch ( exception e ) { simple exception handling . . . e . print stack trace ( ) ; system . exit ( 0 ) ; }
exchange . get out ( ) . set body ( bye world ) ; } } ) . to ( mock : result ) ; } } ; }
assert that ( print pruned ( final int x = 1 + 0 ; ) ) . is equal to ( final int x = 1 + 0 ) ; assert that ( print pruned ( final int x = 1 - 0 ; ) ) . is equal to ( final int x = 1 - 0 ) ;
if ( previous = = null ) { dispatch aggregate events ( attr . owner node , attr , null , mutation event . addition ) ; } else { dispatch aggregate events ( attr . owner node , attr , previous . get node value ( ) , mutation event . modification ) ; }
assert equals ( user2 didn ' t receive the grant voice notification , can speak , answer [ 0 ] ) ;
add days and check for completion ( 7 , next event . invoice , next event . payment , next event . invoice _ payment ) ;
assert . assert equals ( unexpected invocation count : + count22 , 1 , count22 . values ( ) . stream ( ) . map to int ( c - > c ) . sum ( ) ) ;
policy . set queues ( mock csqueues ( new string [ ] { a , b } , new int [ ] { 1 , 2 } , new float [ ] { 1 . 1f , 1 . 2f } , ) ) ; verify order ( policy , , new string [ ] { b , a } ) ;
collection < uri > edits dirs = cluster . get name edits dirs ( 0 ) ;
if ( ( eq _ s _ b ( 1 , a ) ) ) { return false ; }
this . set endpoint ( https : cloudsearch . us - east - 1 . amazonaws . com ) ;
for ( int i = 0 ; i < server _ count ; i + + ) { assert . assert true ( waiting for server + i + being up , client base . wait for server up ( 127 . 0 . 0 . 1 : + client ports [ i ] , connection _ timeout ) ) ; } countdown watcher watch = new countdown watcher ( ) ;
assert equals ( equality key . stated , fh1 . get equality key ( ) . get status ( ) ) ; assert equals ( equality key . stated , fh2 . get equality key ( ) . get status ( ) ) ;
assert . assert true ( kill bill client . get overdue state for account ( account json . get account id ( ) , request options ) . get is clear state ( ) ) ;
file system . set faulty delete ( new file ( cache dir , k1 . 0 . tmp ) , false ) ; file system . set faulty delete ( cache dir , false ) ; disk lru cache . snapshot snapshot = cache . get ( k1 ) ; assert null ( snapshot ) ; }
result . clear ( ) ; clazz . get method ( log method , string . class ) . invoke ( logger , msg ) ; assert result ( level , null , null , msg ) ;
final integer i obj = glyph to unicode ( i ) ; final integer u2 = glyph to unicode ( j ) ; if ( i obj = = null ) {
dirty state _ . mark dirty ( false ) ; } } , true ) ; } else if ( response . is modified ( ) ) {
ss . exit ( main : check _ availability ) ; for ( check check : checks ) { try { check . f . get ( 10 , time unit . seconds ) ; } catch ( timeout exception e ) { fail ( check . description + timed out ) ; } catch ( exception e ) { log . error ( check . description , e ) ; fail ( check . description + + e . get message ( ) ) ; } }
if ( new caret = ( advance ? run limit : run start ) ) { return new caret ; } } } }
if ( start > end ) { return ; } if ( start < 0 ) { start = 0 ; }
assert equals ( 5 * 1024 , lfs . lookaside cache . get cache size ( ) ) ;
int lv = 12345 ; storage info si = new storage info ( lv , 10 , 0 ) ; storage directory sd = new nnstorage ( si ) . new storage directory ( edits dir ) ; format ( sd ) ; uri edits uri = util . string as uri ( sd . get root ( ) . get absolute path ( ) ) ;
ex to throw = ex ;
assert that ( target pattern . normalize ( a g b . . ) ) . is equal to ( a g ) ;
config c1 = build config ( false , 25701 ) ; config c2 = build config ( false , 25704 ) ; config c3 = build config ( false , 25703 ) ; list < string > cluster one members = arrays . as list ( 127 . 0 . 0 . 1 : 25701 ) ;
property model < integer > read timeout model = new property model < integer > ( model , read timeout ) ;
type [ ] actual type arguments = parameterized type . get actual type arguments ( ) ;
if ( pressed & & ( ( view ) get parent ( ) ) . is pressed ( ) ) { return ; } super . set pressed ( pressed ) ; }
app id = bundle . get symbolic name ( ) ; org . openhab . binding . freebox
filter f = filter ;
if ( incoming messages . size ( ) > 0 ) { log . error ( the incoming message queue should never be greater than 0 when queue size is 0 ) ; incoming messages . clear ( ) ; } message message ;
mikktspace tangent generator . generate ( g ) ; create debug tangents ( g2 ) ;
bytes read = fis . read ( 0 , buffer , 0 , buffer . length ) ; assert that ( bytes read , is ( orig len ) ) ; fis . close ( ) ; byte [ ] data from snapshot = dfstest util . read file buffer ( hdfs , file1snap1 ) ;
final float scale factor fixed = 1f + ( scale factor - 1f ) * zoom _ factor ;
crypto = null ;
invalid import test ( @ my _ repo some skylark : file . bzl , skylark imports . invalid _ label _ prefix ) ;
paint border ( ctx , g , x , y , w , h , null ) ;
multi config suite builder builder = new multi config suite builder ( test index count suite . class ) ;
run test ( ht , 5 ) ; }
container . volatile int - = 1 ;
end = - 1 ; fill buf ( ) ;
return web resource factory . new resource ( api class , target ) ;
check dirty tracking ( bob , rate , oca ) ; clear dirty tracking ( bob ) ;
try { execute command ( schema sample ) ; fail ( this should fail ) ; } catch ( shell exception e ) { assert that ( e . get message ( ) , contains string ( invalid usage of sample ) ) ; }
suffixes = get suffix ids ( conf , namenode addr , dfsconfig keys . dfs _ namenode _ service _ rpc _ address _ key , dfsconfig keys . dfs _ namenode _ rpc _ address _ key ) ; }
for ( video stream video stream : ret list ) hash map . put ( video stream . resolution , video stream ) ;
int my count = 0 ;
vectorized row batch vrb2 = vectorized row group gen util . get vectorized row batch ( 7 , 2 , seed ) ; vrb2 . selected in use = true ; vrb2 . selected [ 0 ] = 1 ; vrb2 . selected [ 1 ] = 2 ; vrb2 . selected [ 2 ] = 4 ; vrb2 . size = 3 ; lcv0 = ( long column vector ) vrb2 . cols [ 0 ] ; lcv0 . vector [ 0 ] = 5 ;
jobconf . set long ( mrjob config . task _ timeout , 0 ) ;
set element ( dom . create div ( ) ) ; dom . set style attribute ( get element ( ) , position , relative ) ; set style name ( gwt - progress bar - shell ) ;
array initializer array init = new array initializer ( array type ) ;
if ( dom parser . get error handler ( ) = f init error handler ) { dom parser . set error handler ( f init error handler ) ; }
conf . set int ( hbase . client . retries . number , 1 ) ;
list < user model > users = session . users ( ) . search for user ( jbrown4 , app realm ) ;
if ( context . is stopping ( ) ) { return ; } admin . modify table ( builder . build ( ) ) ; }
assert equals ( ship strategy type . partition _ hash , workset reducer . get input ( ) . get ship strategy ( ) ) ; assert equals ( list0 , workset reducer . get keys ( 0 ) ) ;
use shell = true ; }
if ( table = = null ) { return ; }
big decimal amount = new big decimal ( 0 . 01 ) ;
return volt execute sql ( ) ;
for ( int i = 0 ; i < pattern . length ( ) ; i + + ) { occurrence . put ( pattern . char at ( i ) , i ) ; } }
pattern . compile ( a * . + ) ; }
searcher = new index searcher ( reader ) ; searcher . set similarity ( new classic similarity ( ) ) ; scorer searcher = new scorer index searcher ( reader ) ; scorer searcher . set similarity ( new counting similarity ( ) ) ; }
remote echo remote business interface = ( remote echo ) ctx . lookup ( java _ global _ namespace _ prefix + module _ name + + ejb name + + remote echo . class . get name ( ) ) ; assert . assert not null ( null object returned for remote business interface lookup in java : global namespace , remote business interface ) ;
if ( is equal or null ( this . get header ( ) , other . get header ( ) ) ) { result . add ( new base column field diff impl ( field _ header , this . get header ( ) , other . get header ( ) ) ) ; } return result ; }
service urls . add ( routerurl ) ; registry directory . notify ( service urls ) ; list routers = registry directory . get routers ( ) ; assert . assert equals ( 1 + 1 , routers . size ( ) ) ; service urls . clear ( ) ;
final long block size = 512 ; string [ ] racks = { rack1 , rack1 } ;
if ( view _ . get input editor display ( ) . is focused ( ) & & ( view _ . get input editor display ( ) . get text ( ) . length ( ) > 0 ) ) { process command entry ( ) ; }
student mongo string student max = new student mongo string ( ) ;
if ( ( plan . get local address ( ) = null ) & & plan . get local address ( ) . equals ( fact . get local address ( ) ) ) return unreachable ; return complete ;
test round trip type ( type , write values , skip formats set ) ;
student hbase big integer student max = new student hbase big integer ( ) ; student max . set age ( ( short ) get max value ( short . class ) ) ; student max . set id ( ( big integer ) get max value ( big integer . class ) ) ; student max . set name ( ( string ) get max value ( string . class ) ) ; em . persist ( student max ) ;
if ( using writer ) { return ; } try { get coyote response ( ) . set character encoding ( charset ) ; } catch ( unsupported encoding exception e ) { log . warn ( sm . get string ( coyote response . encoding . invalid , charset ) , e ) ; return ; }
if ( html name = null & & ( html name . equals ignore case ( a ) | | html name . equals ignore case ( td ) ) ) { state . empty = false ; _ printer . print text ( ' > ' ) ; }
log . w ( tag , camera rejected even safe - mode parameters no configuration ) ; } } }
done . count down ( ) ;
for ( connection factory old : to _ remove ) { remove bean ( old ) ; if ( log . is debug enabled ( ) ) log . debug ( { } removed { } , this , old ) ; }
abstract element [ ] non home elements = this . document dao . find elements ( non home dir ) ; assert that ( non home elements ) . has size ( should be filtered ? 11 : 12 ) ; cannot
this . start ( container _ 1 ) ;
long expected rows = 6 ; long expected keys = cols per row ; scan s = new scan ( ) ; s . set filter ( new page filter ( expected rows ) ) ; verify scan ( s , expected rows , expected keys ) ; s . set filter ( new page filter ( expected rows ) ) ; verify scan full ( s , expected kvs ) ;
if ( action init called . await ( 3 , time unit . seconds ) ) { fail ( two trigger action instances should have been created by now ) ; }
long size = 0 ;
if ( to append ) { promote tmp to target ( target path , target , target fs ) ; } return bytes read ; } finally {
open tracing context . tracer factory = new my tracer factory ( ) ;
metric registry info info = metrics coprocessor . create registry info for region coprocessor ( custom region endpoint . class . get name ( ) ) ; optional < metric registry > registry = metric registries . global ( ) . get ( info ) ;
if ( sg . get global type decl ( name , ) = = null ) { sg . add global type decl ( ( xstype definition ) component , ) ; } } break ; case xsconstants . attribute _ declaration : if ( ( ( xsattribute decl ) component ) . get scope ( ) = = xsattribute decl . scope _ global ) { if ( sg . get global attribute decl ( name ) = = null ) { sg . add global attribute decl ( ( xsattribute decl ) component ) ; }
int blank row = external to internal row ( bottom margin - 1 ) ; lines [ blank row ] = null ; color [ blank row ] = new style row ( style , m columns ) ; line wrap [ blank row ] = false ; return ;
node include node = new node . include directive ( attrs , start , parent ) ;
add directive ( render , new render directive ( ) ) ;
if ( sqw = = null ) { error handler . error ( no syslog host is set for syslog appedender named \ + this . name + \ . ) ; return ; } if ( layout header checked ) { if ( layout = null & & layout . get header ( ) = null ) { send layout message ( layout . get header ( ) ) ; } layout header checked = true ; }
sys locale = config . locale ; }
code . mark ( interceptor null case ) ;
write map ( ( ( map serializable ) val ) . to map ( new named list ( ) . as shallow map ( ) ) ) ; return true ;
vim plugin . get history ( ) . add entry ( history group . command , cmd ) ;
function expression expr e = new function expression ( ) ; expr e . set attributes ( another function name , yes another why not , 44 ) ; not 42 , not that it much matters arguments = new array list < abstract expression > ( ) ; arguments . add ( expr a ) ; arguments . add ( constant ) ; arguments . add ( other constant ) ; expr e . set args ( arguments ) ; list < abstract expression > result ;
try { ccr = prepare application catalog diff ( invocation name , operation bytes , operation string , adhoc ddlstmts , replay hash override , is promotion , use adhoc ddl , get hostname ( ) , get username ( ) ) ; } catch ( exception e ) { volt zk . remove action blocker ( zk , volt zk . catalog update in progress , host log ) ; err msg = unexpected error during preparing catalog diffs : + e . get message ( ) ; return make quick response ( client response . graceful _ failure , err msg ) ; } if ( ccr . error msg = null ) { volt zk . remove action blocker ( zk , volt zk . catalog update in progress , host log ) ; return make quick response ( client response . graceful _ failure , ccr . error msg ) ; }
mod tinfo = new table info ( table name ) ;
test proc with valid json ( { \ null \ : - 1 } , client , select set field proc , null , - 1 , 5 ) ; test proc with valid json ( { \ null \ : - 1 } , client , @ ad hoc , select set _ field ( doc , ' null ' , ' - 1 ' ) from js1 where id = 5 ) ;
logger . log ( level . info , - creating symlink at + reconstructed file at final location + ( target : + reconstructed file version . get link target ( ) + ) . . . ) ;
channel sender sender = managed channel . get channel sender ( ) ;
try { ds1 . join ( ds2 ) . where ( 0 ) . equal to ( 0 ) . project first ( 0 , 3 ) ; } catch ( exception e ) { assert . fail ( ) ; } }
admin client . store mgmt ops . verify or add store ( new store def , process _ name , service ) ;
try ( transaction transaction = new transaction ( ) ) { connector metadata metadata = transaction . get metadata ( ) ; metadata . drop view ( new session ( ) , temporary create view ) ; fail ( drop non - existing should fail ) ; } catch ( view not found exception e ) { assert equals ( e . get view name ( ) , temporary create view ) ; }
list < ? extends message > messages = obsolete outbox . get messages ( null , false ) ; if ( messages . size ( ) > 0 ) { . . . and move them to the drafts folder ( we don ' t want to surprise the user by sending potentially very old messages ) local folder drafts = new local folder ( local store , account . get drafts folder name ( ) ) ; obsolete outbox . move messages ( messages , drafts ) ; }
request = client . new request ( urls [ 0 ] + ?action = check ) ; response1 = request . send ( ) ; assert equals ( http servlet response . sc _ ok , response1 . get status ( ) ) ; assert equals ( 1 , m1 . get sessions created ( ) ) ;
stream execution environment explicit = get simple job ( ) ; explicit . enable checkpointing ( 1000 l , checkpointing mode . at _ least _ once ) ; for ( job vertex vertex : explicit . get stream graph ( ) . get job graph ( ) . get vertices ( ) ) { assert equals ( checkpointing mode . at _ least _ once , new stream config ( vertex . get configuration ( ) ) . get checkpoint mode ( ) ) ; } }
m is visible = false ;
fsdata input stream in = fs . open ( test _ path ) ; list < located block > located blocks = dfstest util . get all blocks ( in ) ; in . close ( ) ; assert equals ( 1 , located blocks . size ( ) ) ; assert equals ( 3 , located blocks . get ( 0 ) . get locations ( ) . length ) ; data node dn = cluster . get data node ( located blocks . get ( 0 ) . get locations ( ) [ 0 ] . get ipc port ( ) ) ; dn . shutdown ( ) ;
int cnt42 = 0 ;
application attempt id app attempt id1 = create app attempt id ( 1 , 1 ) ;
return move selected ( id ) ; } else if ( id = = keyboard . key id . delete ) {
url url = new url ( http : + conf . get ( dfs . http . address ) + dfshealth . jsp ) ;
release message latest = release message service . find latest release message for messages ( watched keys ) ;
if ( exact denom > 0 ) { fract = simple fraction . build fraction exact denominator ( dec part , exact denom ) ; } else { fract = simple fraction . build fraction max denominator ( dec part , max denom ) ; }
f . channel ( ) . close future ( ) . sync ( ) ;
string error code = parse error code ( node ) ; if ( error code = = null | | error code . equals ( unsupported protocol ) ) return null ; unsupported protocol exception e = ( unsupported protocol exception ) super . unmarshall ( node ) ;
if ( frags . size ( ) > num fragments & & preserve multi ) { frags = frags . sub list ( 0 , num fragments ) ; } return get response for fragments ( frags , req ) ;
immutable set . builder < integer > present columns = immutable set . builder ( ) ; immutable map . builder < integer , type > present columns and types = immutable map . builder ( ) ; orc type root = types . get ( 0 ) ; for ( map . entry < integer , type > entry : included columns . entry set ( ) ) { an old file can have less columns since columns can be added after the file was written if ( entry . get key ( ) < root . get field count ( ) ) { present columns . add ( entry . get key ( ) ) ; present columns and types . put ( entry . get key ( ) , entry . get value ( ) ) ; } } this . present columns = present columns . build ( ) ; this . max block bytes = require non null ( max block size , max block size is null ) . to bytes ( ) ;
indarray x div norm3 = nd4j . create uninitialized ( x . shape ( ) , x . ordering ( ) ) ;
if ( type = = string . class ) { return object helper . cast ( type , value ) ; } class < ? > key = type ; property editor editor = lookup editor ( key ) ; if ( editor = null ) {
final type rt = method declaration . get type ( ) ;
string error code = parse error code ( node ) ; if ( error code = = null | | error code . equals ( reserved cache nodes offering not found ) ) return null ; reserved cache nodes offering not found exception e = ( reserved cache nodes offering not found exception ) super . unmarshall ( node ) ; return e ; }
try { await . result ( read future , duration . from milliseconds ( 1000 ) ) ; fail ( should fail reading next when there is a corrupted log segment ) ; } catch ( timeout exception te ) { expected }
m empty list text . set text ( r . string . physical _ web _ empty _ list ) ; int tint color = context compat . get color ( this , r . color . light _ grey ) ; m scanning image view . set image resource ( r . drawable . physical _ web _ logo ) ; m scanning image view . set color filter ( tint color , porter duff . mode . src _ in ) ;
unfocused sel bg = uimanager . get color ( nb . explorer . unfocused sel bg ) ; noi18 n
build target target = build target factory . new instance ( : rule ) . with flavors ( cxx platform utils . default _ platform . get flavor ( ) , cxx description enhancer . shared _ flavor , linker map mode . no _ linker _ map . get flavor ( ) ) ;
stream1 = zip file . get input stream ( entry1 ) ;
warmup complete . set ( true ) ;
pro guard pro guard = new pro guard ( configuration ) ;
switch ( item . get item id ( ) ) { case android . r . id . home : on back pressed ( ) ; return true ; default : return super . on options item selected ( item ) ; }
assert null ( injector . get existing binding ( key . get ( bar . class ) ) ) ;
m _ width = math . sqrt ( variance ) * math . pow ( m _ sum of weights , m _ exponent ) ; if ( m _ width < = m _ min width ) { m _ width = m _ min width ; }
} } catch ( xml pull parser exception ex ) {
client . new request ( localhost , 8080 ) . send ( listener ) ;
m is paste shown = true ; ( ( main activity ) get activity ( ) ) . show folder fragment action items ( current dir , get activity ( ) . get menu inflater ( ) , ( ( main activity ) get activity ( ) ) . get menu ( ) , true ) ; }
task tracker manager . finish task ( tt1 , attempt _ test _ 0002 _ m _ 000003 _ 0 , j2 ) ; }
declared function type super method type = declared function type . meet ( method types ) ; declared function type local method type = local prop def . method type ; boolean gets type from parent = gets type info from parent method ( local prop def ) ; if ( super method type = = null ) {
add application dev properties file ( get pom from module name ( application ) , top level package ) ;
request items = new linked hash map < string , list < write request > > ( ) ; }
pad = ;
if ( ( c > = ' a ' & & c < = ' z ' ) ) c - = 0x20 ;
assert null ( sc . get layer group by name ( container tree b . prefixed name ( ) ) ) ; assert null ( sc . get layer by name ( landmarks layer . prefixed name ( ) ) ) ;
repl factor = 2 ;
toast . make text ( this , got click : + item . to string ( ) , toast . length _ short ) . show ( ) ; return true ;
query info display = live query infos . remove ( op key ) ; if ( display = = null ) { log . debug ( unexpected display object value of null for operation { } , op key ) ; } else if ( historical query infos = null ) {
for ( option option : get options ( ) ) { buf . append ( option . to xml ( ) ) ; }
cache = new cache < > ( 10 * 1024 * 1024 , 3600 , 100 * 1024 * 1024 ) ; 10 mb assert true ( cache . put ( five , five mb ) ) ; assert true ( cache . put ( two , two mb ) ) ; assert true ( cache . put ( four , four mb ) ) ; assert equals ( cache . get ( two ) , two mb ) ; assert null ( cache . get ( five ) ) ; assert equals ( cache . get ( four ) , four mb ) ; }
transaction util . do in hibernate ( this : : session factory , session - > { book . set name ( book3 updated ) ; session . update ( book ) ; } ) ;
string writer sw = new string writer ( 100 ) ;
sdf = new simple date format ( yyyy - mm - dd , date format symbols . get instance ( ) , ulocale . canada ) ;
string workload file = props . get string ( benchmark . key _ value _ file ) ; this . kv provider = new string key value provider ( workload file ) ; system . out . println ( finished loading + workload file ) ;
if ( in = = null ) { return ( null ) ; }
assert function ( decimal ' 37 ' < decimal ' 37 ' , boolean , false ) ;
m down degrees = get degrees from coords ( event x , event y , force legal , is inner circle ) ;
byte [ ] l _ ascii = new byte [ raw . length < < 3 ] ;
byte i1 [ ] = var2 . clone ( ) ; verify invocation ( i1 , channel , error _ code ) ; byte i2 [ ] = var2 . clone ( ) ; update short bytes ( i2 , short . max _ value ) ; verify invocation ( i2 , channel , error _ code ) ; update short bytes ( i2 , ( short ) - 1 ) ; verify invocation ( i2 , channel , error _ code ) ;
boolean headers shown = saved instance state . get boolean ( extra _ headers ) ; if ( headers shown ) { hide all headers ( ) ; } else if ( this . headers shown ) { show all headers with reset ( true ) ; } this . headers shown = headers shown ; if ( saved instance state . get boolean ( extra _ sticky ) & & are headers sticky ( ) ) { set sticky headers ( true ) ; }
return rm context . get node label manager ( ) . get nodes without alabel ( ) ;
throw new illegal state exception ( unsupported http method + method . to string ( ) ) ; }
cat . body weight . add ( kitten . body weight ) ; cat . body weight . subtract ( kitten . body weight ) ; cat . body weight . multiply ( kitten . body weight ) ; cat . body weight . divide ( kitten . body weight ) ; }
return generate at mean ( pre out distribution params ) ; }
assert true ( namenode is not in safe mode , cluster . get name node ( ) . is in safe mode ( ) ) ;
for ( int index = 0 ; index < get child count ( ) ; index + + ) { if ( get child at ( index ) = = m circle view ) { m circle view index = index ; break ; } } }
rw . write lock ( ) . lock ( ) ;
manager . add compaction results ( al ( ) , al ( create file ( 2 , 0 , open _ key , open _ key ) ) ) ;
{ fake llrealtime segment data manager segment data manager = create fake segment manager ( ) ; segment data manager . _ state . set ( segment data manager , llrealtime segment data manager . state . initial _ consuming ) ; assert . assert false ( segment data manager . invoke end criteria reached ( ) ) ; _ time now + = max time for segment close ms + 1 ;
log . info ( starting one - to - many collection loader filter tests . ) ; test data test data = new test data ( ) ; test data . prepare ( ) ; session session = open session ( ) ;
return describe match ( for loop tree ) ;
if ( update indicator position ) { m tab strip . set indicator position from tab position ( position , position offset ) ; }
final byte color map [ ] [ ] = new byte [ 3 ] [ mapsize < 256?mapsize + 1 : mapsize ] ;
assert . assert false ( op . satisfied by ( row , static row , false ) ) ; row = build row ( build cell ( age , int32 type . instance . decompose ( 6 ) , system . current time millis ( ) ) ) ;
conditional expression new expression = ast . new conditional expression ( ) ; new expression . set expression ( get inversed expression ( rewrite , expression . get expression ( ) ) ) ; new expression . set then expression ( ( expression ) rewrite . create copy target ( expression . get else expression ( ) ) ) ; new expression . set else expression ( ( expression ) rewrite . create copy target ( expression . get then expression ( ) ) ) ;
final edge edge = new edge . builder ( ) . group ( test groups . edge ) . source ( 1 ) . dest ( 2 ) . directed ( true ) . property ( accumulo property names . column _ qualifier , 1 ) . property ( accumulo property names . column _ qualifier _ 2 , 1 ) . property ( accumulo property names . prop _ 1 , 0 ) . property ( accumulo property names . prop _ 2 , 0 ) . property ( accumulo property names . prop _ 3 , 0 ) . property ( accumulo property names . prop _ 4 , 0 ) . property ( accumulo property names . count , 1 ) . build ( ) ; final edge edge2 = new edge . builder ( ) . group ( test groups . edge ) . source ( 1 ) . dest ( 2 ) . directed ( true ) . property ( accumulo property names . column _ qualifier , 1 ) . property ( accumulo property names . column _ qualifier _ 2 , 1 ) . property ( accumulo property names . prop _ 1 , 0 ) . property ( accumulo property names . prop _ 2 , 0 ) . property ( accumulo property names . prop _ 3 , 0 ) . property ( accumulo property names . prop _ 4 , 0 ) . property ( accumulo property names . count , 1 ) . build ( ) ;
int byte length = dec writable . big integer bytes internal scratch ( ) ; lazy binary utils . write vint ( byte stream , byte length ) ;
& & ( string at ( ( m _ current - 3 ) , 5 , halgh , ) & & ( ( m _ current + 1 ) = = m _ last ) ) ) { metaph add exact approx ( g , k ) ; m _ current + = 2 ; return true ;
hold append . count down ( ) ; put finished . await ( ) ; flush finished . await ( ) ;
return default _ boundary _ bytes ; } else {
break ; case tckind . _ tk _ short :
tester . assert members of ( container cluster0 , container0 ) ; tester . assert members of ( container cluster1 , container1 ) ; tester . assert members of ( content cluster0 , content0 ) ; tester . assert members of ( content cluster1 , content1 ) ; return new system state ( all hosts , container0 , container1 , content0 , content1 ) ;
type = modify type by annotation ( config , bean desc . get class info ( ) , type , null ) ; java type key type = type . get key type ( ) ; java type content type = type . get content type ( ) ;
assert equals ( 0 , management service . create job query ( ) . count ( ) ) ;
mbscustom page data page data = get page data ( current page id ) ; mbscustom page data current data = null ; iterator iterator = page set . iterator ( ) ;
firebase messaging . get instance ( ) . subscribe to topic ( news ) ;
for ( header header : http response . get all headers ( ) ) { if ( http headers . transfer _ encoding . equals ( header . get name ( ) ) ) { response . add header ( header . get name ( ) , header . get value ( ) ) ; } }
list . add ( first normal cue ) ;
add set end point interceptor ( target , redis . clients . jedis . jedis shard info ) ;
throw new mbean info retrieval exception ( unable to obtain mbean info for bean [ + this . object name + ] : it is likely that this bean was unregistered during the proxy creation process , ex ) ; }
client message client message = client add partition listener codec . encode request ( ) ; client invocation invocation = new client invocation ( client , client message , null , owner connection ) ; invocation . set event handler ( this ) ; invocation . invoke urgent ( ) . get ( ) ; }
packet . add extension ( multiple addresses ) ;
final date time result = ( ( default subscription internal api ) subscription internal api ) . get effective date for new bcd ( new bcd , effective date , internal call context ) ; assert . assert equals ( result , internal call context . to utcdate time ( new local date ( 2012 - 06 - 03 ) ) ) ;
need context patch = false ;
submit application request app request = records . new record ( submit application request . class ) ; app request . set application submission context ( app context ) ;
return buffer len = = 1 & & index = = 0 ; }
indarray outputs = get outputs ( outputs archive , true ) [ 0 ] ; compare multiclass auc ( predictions , outputs , predictions keras , predictions dl4j , 10 , eps ) ; }
super . on create ( saved instance state ) ; set content view ( r . layout . activity _ third ) ; find view by id ( r . id . button1 ) . set on click listener ( new on click listener ( ) { @ override public void on click ( view v ) { intent intent = new intent ( ) ; intent . set class ( third activity . this , second activity . class ) ; intent . put extra ( time , system . current time millis ( ) ) ; start activity ( intent ) ; } } ) ; log . d ( tag , on create ) ;
assert equals ( 0 , bb . position ( ) ) ;
assert true ( v3 > v1 ) ; } finally {
final lock new lock = new lock ( ts , uuid , next lock id . get and increment ( ) , null ) ;
container launcher . expected core pool size = 12 ; for ( int i = 1 ; i < = 4 ; i + + ) { container launcher . handle ( new container launcher event ( task attempt id , container id , host1 + i + : 1234 , null , container launcher . event type . container _ remote _ launch ) ) ; } wait for events ( container launcher , 12 ) ; assert . assert equals ( 12 , thread pool . get pool size ( ) ) ; assert . assert null ( container launcher . found errors ) ;
create layer group ( workspace a , layer _ group _ a , clone vector layer into workspace ( workspace a , name space a , mock data . buildings . get local part ( ) ) , clone vector layer into workspace ( workspace a , name space a , mock data . bridges . get local part ( ) ) ) ; create layer group ( workspace b , layer _ group _ b , clone vector layer into workspace ( workspace b , name space b , mock data . buildings . get local part ( ) ) , clone vector layer into workspace ( workspace b , name space b , mock data . bridges . get local part ( ) ) ) ; global layer group = create layer group ( layer _ group _ c , catalog . get layer by name ( mock data . buildings . get local part ( ) ) , catalog . get layer by name ( mock data . bridges . get local part ( ) ) ) ;
var string = procedure = insert & parameters = [ 6 , \ aaa \ ] ;
int static values offset = class def . get static values offset ( ) ; if ( static values offset = 0 ) { read array ( new encoded value reader ( dex . open ( static values offset ) ) ) ; }
assert no locks ( ) ; tm ( tx executor ) . resume ( tx2 ) ;
procedure proc = queue . poll ( ) ; assert equals ( 100 , proc . get proc id ( ) ) ; assert equals ( false , queue . wait table exclusive lock ( proc , table name ) ) ; queue . wake table exclusive lock ( proc , table name ) ; }
for ( server name sn : favored nodes ) { string rack = get rack of server ( sn ) ; racks . add ( rack ) ; set < server name > servers in rack = rack to fnmapping . get ( rack ) ; if ( servers in rack = = null ) { servers in rack = sets . new hash set ( ) ; rack to fnmapping . put ( rack , servers in rack ) ; } servers in rack . add ( sn ) ; }
assert true ( map . size ( ) = = 4 ) ;
save caret = new jcheck box ( j edit . get property ( options . general . save caret ) ) ;
geoserver info . set xml external entities enabled ( null ) ; get geo server ( ) . save ( geoserver info ) ;
field < ? > compare with = field ( field ) ; if ( compare with = null ) { int size = fields . length ; [ 4540 ] match by identity first for ( int i = 0 ; i < size ; i + + ) if ( fields [ i ] = = compare with ) return i ; for ( int i = 0 ; i < size ; i + + ) if ( fields [ i ] . equals ( compare with ) ) return i ; }
invalid = true ;
if ( buffer [ content offset ] > 7 ) { throw new asn1 exception ( asn . 1 bitstring : wrong content at [ + content offset + ] . a number of unused bits must be in range 0 to 7 ) ; } if ( length = = 1 & & buffer [ content offset ] = 0 ) { throw new asn1 exception ( asn . 1 bitstring : wrong content at [ + content offset + ] . for empty string unused bits must be 0 ) ; }
assert true ( checkpoint file . delete ( ) ) ; assert true ( checkpoint file . exists ( ) ) ; if ( cp backup file . rename to ( checkpoint file ) ) log . error ( file rename failed : + cp backup file . get absolute path ( ) + - - > + checkpoint file . get absolute path ( ) ) ; assert true ( checkpoint file . exists ( ) ) ;
string repl db name = db name + _ dupe ; tuple bootstrap dump = bootstrap load and verify ( db name , repl db name ) ; run ( insert into table + db name + . unptned values ( ' + unptn _ data [ 1 ] + ' ) , driver ) ;
on post bind view ( this , view holder . item view ) ;
while ( bs live shadow > = 8 ) { out shadow . write ( bs buff shadow > > 24 ) ; write 8 - bit bs buff shadow < < = 8 ; bs live shadow - = 8 ; }
source builder . fetch source ( false ) ;
client . set storage policy ( bar dir , one _ ssd ) ;
logger . debugf ( service account user for client ' % s ' not found or default protocol mapper for service account not found . creating now , client . get client id ( ) ) ; new client manager ( new realm manager ( session ) ) . enable service account ( client ) ; client user = session . users ( ) . get service account ( client ) ; }
remove view at ( 1 ) ; } } , 300 ) ; return bitmap ; }
invoke isolated ( false ) ;
spatial args args = new spatial args ( operation , query shape ) ;
file file = new file ( user guide dir , summary . md ) ;
m content = new spannable string builder ( < b > some text ) ; m watcher . on text changed ( m content , 0 , 0 , 3 ) ; m watcher . after text changed ( m content ) ; assert equals ( 0 , m span range . get opening tag loc ( ) ) ; assert equals ( 3 , m span range . get closing tag loc ( ) ) ;
assert . assert equals ( size of grid should match to size of hyper space , hyper space size1 , grid . get model count ( ) + grid . get failure count ( ) ) ;
component dto dto = component testing . new private project dto ( db tester . organizations ( ) . insert ( ) , main . get uuid ( ) ) . set db key ( main . get key ( ) ) ;
test util . stall _ till _ cloudsize ( args . cluster size ) ;
m am pm ycenter = layout ycenter - m am pm circle radius 2 + circle radius ;
log . info ( setting up app master command ) ;
if ( rhs . get acl ( ) = = null ) { return false ; }
register ore ( log wood , new item stack ( blocks . log , 1 , wildcard _ value ) ) ; register ore ( log wood , new item stack ( blocks . log2 , 1 , wildcard _ value ) ) ; register ore ( plank wood , new item stack ( blocks . planks , 1 , wildcard _ value ) ) ; register ore ( slab wood , new item stack ( blocks . wooden _ slab , 1 , wildcard _ value ) ) ; register ore ( stair wood , blocks . oak _ stairs ) ; register ore ( stair wood , blocks . spruce _ stairs ) ; register ore ( stair wood , blocks . birch _ stairs ) ; register ore ( stair wood , blocks . jungle _ stairs ) ; register ore ( stair wood , blocks . acacia _ stairs ) ; register ore ( stair wood , blocks . dark _ oak _ stairs ) ; register ore ( stick wood , items . stick ) ; register ore ( tree sapling , new item stack ( blocks . sapling , 1 , wildcard _ value ) ) ;
if ( var index < 0 ) kit . code bug ( ) ; if ( number var flags = = null ) { int size = fnode . get param and var count ( ) - fnode . get param count ( ) ; number var flags = new boolean [ size ] ; } number var flags [ var index ] = true ; }
styles . get style ( ) . remove ( live styles . get ( s . get style id ( ) ) ) ; }
output . write utf ( clinit _ name ) ;
int border _ left = math . max ( 0 , win _ left ) ;
get cbvec ( cbvec , mem , mem _ idx , index [ index _ idx + 0 ] , l mem , veclen ) ;
zkassign . delete node ( server . get zoo keeper ( ) , hri . get encoded name ( ) , event type . rs _ zk _ region _ opened ) ; }
else if ( ctx . family ( ) = = postgres ) ctx . visit ( name ) ; when emulating , just repeat the window specification else ctx . visit ( window ) ;
if ( truncated ) { socket . close ( ) ; }
map < string , object > properties = new hash map < string , object > ( get ssl socket connector properties ( ) ) ;
if ( dbg ) { log ( nitz : not setting time , clock has rolled + backwards since nitz time was received , + nitz ) ; } return ; } if ( millis since nitz received > integer . max _ value ) {
throw new parsing exception ( in stream is empty ) ;
geometry arrow x = new geometry ( arrow x , new arrow ( new vector3f ( arrow size , 0 , 0 ) ) ) ;
return environment . media _ mounted . equals ( environment . get external storage state ( ) ) | | is external storage removable ( ) ? context . get application context ( ) . get external cache dir ( ) : context . get cache dir ( ) ;
if ( namespace = namespace . for uri ( reader . get namespace uri ( ) ) ) throw unexpected element ( reader ) ; final element element = element . for name ( reader . get local name ( ) ) ;
fading edge scroll view scroll view = ( fading edge scroll view ) inflater . inflate ( r . layout . autofill _ editor _ base , container , false ) ;
tester . notify job completion ( job type . component , application , true ) ;
m _ client . call procedure ( @ ad hoc , alter table foo drop constraint blerg ; ) ; indexes = m _ client . call procedure ( @ statistics , index , 0 ) . get results ( ) [ 0 ] ; do { indexes = m _ client . call procedure ( @ statistics , index , 0 ) . get results ( ) [ 0 ] ; tables = m _ client . call procedure ( @ statistics , table , 0 ) . get results ( ) [ 0 ] ; } while ( indexes . get row count ( ) = 0 ) ;
ast in node = n = = null ? ast factory . create ( in , in ) : ast factory . create ( not _ in , not in ) ; ast factory . make astroot ( current ast , in node ) ; ast in list node = ast factory . create ( in _ list , in list ) ; in node . add child ( in list node ) ; ast elements node = ast factory . create ( elements , elements ) ; in list node . add child ( elements node ) ; elements node . add child ( p ) ; }
application _ 0 . schedule ( ) ; task _ 0 _ 0 check application resource usage ( 3 * gb , application _ 0 ) ; check node resource usage ( 1 * gb , nm _ 0 ) ; check node resource usage ( 3 * gb , nm _ 1 ) ; }
given ( sent text message . get jmscorrelation id ( ) ) . will return ( correlation _ id ) ;
set motion event splitting enabled ( true ) ; }
show publish ui ( ) . set global value ( new ui prefs . show publish ui ( ) . get global value ( ) ) ;
service loader . service definition definition2 = new service loader . service definition ( spi portable hook . class . get name ( ) , spi portable hook . class . get class loader ( ) ) ; set < service loader . service definition > definitions = set of ( definition1 , definition2 ) ;
assert . assert null ( json map . get ( invalid _ root ) ) ; assert . assert null ( json map . get ( invalid _ path ) ) ; }
if ( replacement delimiter escape = null ) { encoded file name = encoded file name . replace all ( replacement delimiter escape , delimiter _ escape ) ; } return encoded file name ; }
result . add ( interfaces [ counter ] ) ;
ascii hex decode field . set text ( ) ;
wait for breakpoint event ( 2 ) ; assert true ( breakpoint event problem : expected + 2 + breakpoint event ( s ) , received + f breakpoint event count , f breakpoint event count = = 2 ) ; assert true ( breakpoint event problem : expected + 1 + breakpoint _ added event ( s ) , received + get breakpoint event count ( bp _ added ) , get breakpoint event count ( bp _ added ) = = 1 ) ; assert true ( breakpoint event problem : expected + 1 + breakpoint _ updated event ( s ) , received + get breakpoint event count ( bp _ updated ) , get breakpoint event count ( bp _ updated ) = = 1 ) ; clear event counters ( ) ;
assert repair contains deletions ( msg2 , null , one _ two , with exclusive end if ( three _ four , timestamp2 > = timestamp1 ) , five _ six ) ;
if ( op . is error ( ) ) { throw op . ex ; }
if ( socket = null ) { socket . close ( ) ; }
case function : name = node util . get name ( n ) ; fn = n ; target = n ; if ( n . get parent ( ) . is assign ( ) & & n . get parent ( ) . get jsdoc info ( ) . is ng inject ( ) ) {
running job r job = utils for tests . run job kill ( job , in dir , out dir ) ;
for ( int i = 0 ; i < row key . length - 1 ; + + i ) { if ( row key [ i ] = = byte array escape utils . delimiter ) { if ( num delims > = 3 ) { throw new accumulo element conversion exception ( too many delimiters found in row key - found more than the expected 3 . ) ; } positions of delimiters [ num delims + + ] = i ; } }
zoned date time expiration time = creation time . plus ( this . max time to live in seconds , chrono unit . seconds ) ; if ( current system time . is after ( expiration time ) ) { logger . debug ( access token is expired because the time since creation is greater than max time to live in seconds ) ; return true ; }
postgre sqledge saver . write edges ( provider , edges ) ; postgre sqlhelpers . end transaction ( connection ) ;
if ( ( exemplar . f block & xsconstants . derivation _ substitution ) = 0 ) return null ;
uiapplication . shared application ( ) . set idle timer disabled ( config . prevent screen dimming ) ; gdx . app . debug ( iosapplication , i os version : + uidevice . current device ( ) . system version ( ) ) ;
synchronized ( service mutex ) { check active ( ) ; return register instance ( create hazelcast instance ( config ) ) ; }
rm . get my fifo scheduler ( ) . force resource limit ( resource . new instance ( 2048 , 0 ) ) ;
final string legacy string = boolean . to string ( legacy ) ; final string legacy anti = boolean . to string ( legacy ) ; collection admin request . set cluster property ( zk state reader . legacy _ cloud , legacy string ) . process ( cluster . get solr client ( ) ) ; cluster properties props = new cluster properties ( zk client ( ) ) ; assert equals ( value of legacy cloud cluster prop unexpected , legacy string , props . get cluster property ( zk state reader . legacy _ cloud , legacy anti ) ) ;
mock . set result minimum wait time ( 500 ) ;
buffer buffer = new buffer ( ) ; buffer . write ( byte string . decode hex ( f4908080 ) ) ; assert equals ( buffer . replacement _ character , buffer . read utf8 code point ( ) ) ; assert true ( buffer . exhausted ( ) ) ; }
if ( local register image = = null ) { throw new org . apache . axis2 . databinding . adbexception ( property cannot be null ) ; }
for ( int i = 0 ; i < num dirs ; i + + ) { file subdir = new file ( dir , new abstract id ( ) . to string ( ) ) ; assert true ( subdir . mkdir ( ) ) ; generate random dirs ( subdir , num files , num dirs , depth - 1 ) ; }
request context request context = new request context ( ) ; rest request decoded = decode ( encoded , request context ) ; assert . assert equals ( decoded . get uri ( ) . to string ( ) , http : localhost : 7279?args = xyz ) ; assert . assert equals ( decoded . get method ( ) , put ) ; assert . assert equals ( decoded . get entity ( ) , request . get entity ( ) ) ; assert . assert true ( encoded . get header ( content - type ) . starts with ( multipart mixed ) ) ; assert . assert true ( ( boolean ) request context . get local attr ( r2 constants . is _ query _ tunneled ) ) ; assert . assert equals ( decoded . get header ( content - length ) , integer . to string ( request . get entity ( ) . length ( ) ) ) ; }
items . add ( new storage object ( ) . set bucket ( testbucket ) . set name ( testdirectory ) ) ;
trigger haptic pulse ( controller count , 1 . 0f ) ; controller count + + ;
size minimum size = new size ( 300 , 300 ) ; size size = dom metrics . adjusted element size ( preferred size _ , minimum size , 0 , pad 100 ) ; client margin frame _ . set size ( size . width + px , size . height + px ) ; if ( desktop . is desktop ( ) ) desktop . get frame ( ) . set shiny dialog url ( url _ ) ;
sw active security . set checked ( security . is password set ( ) ) ;
weave by type ( to definition . class ) . after ( ) . transform ( constant ( bye world ) ) ; } } ) ;
else if ( offset [ 0 ] = = paragraph . length ( ) ) { offset [ 0 ] = paragraph buffer . length ( ) - line delimiter . length ( ) ; }
( ( annotated method ) _ setter ) . call on with ( instance , prop name , value ) ;
automaton a2 = operations . remove dead states ( operations . determinize ( union terms ( terms ) , integer . max _ value ) ) ;
clazz common class = find common class ( this referenced class , other referenced class , false ) ; if ( common class . get name ( ) . equals ( class constants . name _ java _ lang _ object ) ) {
for ( simple registration listener listener : listeners ) { if ( listener . was called ( ) ) { assert . assert equals ( listener . layer , expected listener layer ) ; assert . assert equals ( listener . app context , expected listener app context ) ; assert . assert true ( listener . was correctly called ( ) ) ; } else { assert . assert false ( ( listener . layer . equals ( expected listener layer ) & & listener . app context . equals ( expected listener app context ) ) ) ; } }
exclusion exclusion = null ; if ( node . exclusion = null ) { exclusion = node . exclusion ; } leak node parent = new leak node ( null , holder , null , null , null ) ; enqueue ( exclusion , parent , child , < java local > , local ) ; } else {
string group address string = string utils . substring before ( binding config , : ) ;
long cloud client docs = cloud client . query ( new solr query ( * : * ) ) . get results ( ) . get num found ( ) ;
input = input . replace ( _ _ entity _ import _ _ , entity . get fully qualified type name ( ) ) ;
} } catch ( exception e ) {
codec reconfiguration state = reconfiguration _ state _ write _ pending ; } return true ; }
final int screen width = image utils . get screen width ( this ) ;
verify file contents ( get file system ( ) , path2 , file1x ) ;
string normalized name ;
return pattern2 ; }
float no h = vo h + 1 e - 5f ;
final usage invoice item ii3 = new usage invoice item ( invoice id , account id , bundle id , subscription id , plan name , phase name , usage . get name ( ) , start date . minus days ( 1 ) , end date , big decimal . ten , currency ) ;
case ' * ' : case ' ? ' : value _ pattern = true ; in _ index + + ; break ; case ' = ' : case ' : ' : case ' ' : case ' \ n ' : final string ichar = ( ( c1 = = ' \ n ' ) ? \ \ n : + c1 ) ; throw new malformed object name exception ( invalid character ' + ichar + ' in value part of property ) ; default :
thread t = thread . current thread ( ) ;
if ( this . init done ) { return ; }
return 1 ;
this . z = calculate z ( ) ;
prepared get cursor . as rx observable ( ) ; fail because exception was not thrown ( stor ioexception . class ) ; } catch ( stor ioexception expected ) {
zero index = normalize index ( 0 , inactive counts . get normalizing index offset ( ) , inactive counts . length ( ) ) ;
bind many to many second pass ( this . collection , persistent classes , key columns , inverse columns , element columns , is embedded , coll type , ignore not found , unique , cascade delete enabled , association table binder , property , property holder , building context ) ; return false ;
modifiable solr params token param = new modifiable solr params ( ) ; token param . set ( delegation , invalid token ) ; try { do solr request ( ss , get admin request ( token param ) , error code . forbidden . code ) ; assert . fail ( expected exception ) ; } catch ( illegal argument exception ex ) { } } finally {
_ xml list = lib . new xmllist ( ) ;
do return ( cache ) . when ( ns service ) . get ownership cache ( ) ; ns obj . handle unload request ( pulsar ) ; thread . sleep ( 1000 ) ;
validate . not null ( compilation unit . get imports ( ) , compilation unit imports should be non - null when producing type ' % s ' , cid . get name ( ) ) ;
master procedure testing utility . test recovery and double execution ( proc exec , proc id ) ;
immutable map . builder < integer , group > groups builder = new immutable map . builder < > ( ) ; for ( map . entry < integer , list < node > > group : nodes . stream ( ) . collect ( collectors . grouping by ( node : : group ) ) . entry set ( ) ) groups builder . put ( group . get key ( ) , new group ( group . get key ( ) , group . get value ( ) ) ) ; this . groups = groups builder . build ( ) ;
assert . assert true ( get urlcode ( index ) = = 200 ) ;
web request req = new get method web request ( context _ url + services hello ) ;
assert equals ( 3 , groups . length ) ; assert equals ( main , groups [ 0 ] . get name ( ) ) ; assert equals ( ruleflow - group , groups [ 1 ] . get name ( ) ) ; assert equals ( agenda - group , groups [ 2 ] . get name ( ) ) ; }
mock . expected bodies received ( ac ) ; template . send body and header ( direct : start , hello , slip , direct : a , + get ftp url ( ) + , direct : c ) ; assert mock endpoints satisfied ( ) ; }
consumer props . put ( common client configs . client _ id _ config , client id + - restore - consumer ) ; return consumer props ;
verify deleted eventually ( server , job id1 , key1a ) ; verify deleted eventually ( server , job id1 , key1b ) ; verify deleted eventually ( server , job id2 , key2a ) ; verify deleted eventually ( server , job id2 , key2b ) ;
get ez manager ( ) . pause reencrypt for testing ( ) ; dfs admin . reencrypt encryption zone ( zone , reencrypt action . start ) ; wait for queued zones ( 1 ) ;
testing util . wait for no rebalance ( cache ( 2 ) , cache ( 3 ) ) ; assert equals ( rebalancing status . complete . to string ( ) , stm2 . get rebalancing status ( ) ) ; assert null ( stm2 . get cache topology ( ) . get pending ch ( ) ) ; ch = stm2 . get cache topology ( ) . get current ch ( ) ; assert equals ( arrays . as list ( address ( 2 ) , address ( 3 ) ) , ch . get members ( ) ) ; for ( int i = 0 ; i < ch . get num segments ( ) ; i + + ) { assert equals ( ch . get num owners ( ) , ch . locate owners for segment ( i ) . size ( ) ) ; } }
conf copy . set boolean ( ipc . server . tcpnodelay , true ) ;
assert that ( received values ) . contains exactly ( fast key , leaf key ) ;
client . set timeline collector info ( collector info . new instance ( collector manager . get rest server bind address ( ) ) ) ; client . init ( conf ) ; client . start ( ) ; cluster entity cluster = new cluster entity ( ) ; cluster . set id ( yarn configuration . default _ rm _ cluster _ id ) ; flow run entity flow = new flow run entity ( ) ; flow . set user ( user group information . get current user ( ) . get short user name ( ) ) ; flow . set name ( test _ flow _ name ) ; flow . set version ( test _ flow _ version ) ; flow . set run id ( 1 l ) ; flow . set parent ( cluster . get type ( ) , cluster . get id ( ) ) ; application entity app = new application entity ( ) ; app . set id ( app id . to string ( ) ) ; flow . add child ( app . get type ( ) , app . get id ( ) ) ; application attempt id attempt id = application attempt id . new instance ( app id , 1 ) ; application attempt entity app attempt = new application attempt entity ( ) ; app attempt . set id ( attempt id . to string ( ) ) ; container id container id = container id . new container id ( attempt id , 1 ) ;
annotated member accessor = merge . getter ; accessor . fix access ( ctxt . is enabled ( mapper feature . override _ public _ access _ modifiers ) ) ; if ( ( prop instanceof setterless property ) ) { prop = merging settable bean property . construct ( prop , accessor ) ; } }
synchronized ( this ) { class < ? > clazz = find loaded class ( name ) ; if ( clazz = = null ) { clazz = load and define class ( name ) ; } return clazz ; } }
if ( is read only ( elem ) ) { form field . set field flags ( pdf form field . ff _ read _ only ) ; }
alg = ai . get algorithm ( ) ;
p left index + + ;
{ version , the file version , either 87a or 89a } , { logical screen descriptor , the logical screen descriptor , except for the global color table } , { global color table , the global color table } , { color table entry , a global color table entry } ,
q = em . create query ( delete from blog post ) ; int delete count = q . execute update ( ) ; assert . assert equals ( 2 , delete count ) ; em . clear ( ) ;
statistics . on handled failure ( exchange . get exception ( ) ) ;
resource manager . reset resource usage ( ) ;
. add ( new phrase query ( body , alpha bravo ) , boolean clause . occur . must )
field m initial application = activity thread . get declared field ( m initial application ) ;
file system . set faulty delete ( new file ( cache dir , a . 0 ) , false ) ;
if ( effect _ tick > 0 | | fast _ volume _ slides ) { set _ volume ( volume + up - down ) ; }
assert translation ( translation , java util arrays _ as list with nsobject array _ ( array ) ; ) ;
return ( argv = null ) & & stream job . reduce _ none . equals ( argv ) ; }
new exchange . remove property ( exchange . aggregated _ size ) ;
assert true ( cluster . get name node ( ) . is in safe mode ( ) ) ; url url = new url ( http : + conf . get ( dfs . http . address ) + dfshealth . jsp ) ;
logger . debug ( download success + local tmp file . get absolute path ( ) ) ; return null ;
admin = test _ util . get admin ( ) ;
int port = 50051 ; server = server builder . for port ( port ) . add service ( new hello camel impl ( ) ) . build ( ) . start ( ) ; log . info ( server started . i ' m listening on + port ) ; runtime . get runtime ( ) . add shutdown hook ( new thread ( ) { @ override public void run ( ) { hello camel server . this . stop ( ) ; } } ) ; }
final string new catalog url = configuration . get path to catalog for test ( export - ddl - addedtable . jar ) ;
if ( activity data . get theme ref ( ) = null ) { theme ref = activity data . get theme ref ( ) ; } else { theme ref = android manifest . get theme ref ( ) ; } if ( theme ref = null ) { activity info . theme = runtime environment . application . get resources ( ) . get identifier ( theme ref . replace ( @ , ) , style , package name ) ; } }
fixture . meta cl service . create meta contact ( mcl slick fixture . mock provider , parent meta group , new contact id ) ;
assert rows net ( execute net with paging ( select a , b , d , count ( b ) , max ( d ) from % s where b = 1 and c in ( 1 , 2 ) group by a , b per partition limit 2 limit 5 allow filtering , page size ) , row ( 1 , 1 , 2 , 2 l , 2 ) , row ( 2 , 1 , 3 , 2 l , 3 ) , row ( 4 , 1 , 5 , 2 l , 5 ) ) ; assert rows net ( execute net with paging ( select a , b , d , count ( b ) , max ( d ) from % s where b in ( 1 , 2 ) and c in ( 1 , 2 ) group by a , b per partition limit 1 limit 2 allow filtering , page size ) , row ( 1 , 1 , 2 , 2 l , 2 ) , row ( 2 , 1 , 3 , 2 l , 3 ) ) ;
if ( evicted = null ) { entries . add ( new thread cache . dirty entry ( evicted . key , evicted . entry . value , evicted . entry ) ) ; dirty keys . remove ( evicted . key ) ; } for ( bytes key : dirty keys ) { final lrunode node = get internal ( key ) ; if ( node = = null ) { throw new illegal state exception ( key = + key + found in dirty key set , but entry is null ) ; } entries . add ( new thread cache . dirty entry ( key , node . entry . value , node . entry ) ) ; node . entry . mark clean ( ) ; if ( node . entry . value = = null ) { deleted . add ( node . key ) ; } }
three args ( a + + , b + + , b + + ) ;
if ( debug ) log . v ( tag , abort input : no handler for view ) ; return ;
byte buf [ ] = new byte [ 32 ] ;
field node min value = null , max value = null , values = null ; if ( is aic ) { class node enum ref = enum class . get plain node reference ( ) ; create values field values = new field node ( values , private _ fs | opcodes . acc _ synthetic , enum ref . make array ( ) , enum class , null ) ; values . set synthetic ( true ) ; add methods ( enum class , values ) ; check for abstract methods ( enum class ) ; create min _ value and max _ value fields min value = new field node ( min _ value , public _ fs , enum ref , enum class , null ) ; max value = new field node ( max _ value , public _ fs , enum ref , enum class , null ) ; }
status . set status ( writing region info on filesystem ) ;
for ( completable future < completed checkpoint > savepoint future : savepoint futures ) { assert true ( savepoint future . is done ( ) ) ; }
int index = class name . last index of ( ' . ' ) ; if ( index > = 0 ) { string package name = class name . substring ( 0 , index ) ; if ( get package ( package name ) = = null ) { define package ( package name , , , , , , , null ) ; } }
assert equals ( 0 , context . get inflight repository ( ) . size ( ) ) ; template . send body and header ( direct : start , hello world , foo , log : foo ) ;
if ( t ime _ present ) { iiometadata node t ime _ node = new iiometadata node ( t ime ) ; t ime _ node . set attribute ( year , integer . to string ( t ime _ year ) ) ; t ime _ node . set attribute ( month , integer . to string ( t ime _ month ) ) ; t ime _ node . set attribute ( day , integer . to string ( t ime _ day ) ) ; t ime _ node . set attribute ( hour , integer . to string ( t ime _ hour ) ) ; t ime _ node . set attribute ( minute , integer . to string ( t ime _ minute ) ) ; t ime _ node . set attribute ( second , integer . to string ( t ime _ second ) ) ; root . append child ( t ime _ node ) ; }
assert true ( storage types [ 0 ] = = storage type . disk ) ; datanode info source = locations [ 0 ] ;
default config . set property ( bkdl _ bookkeeper _ ack _ quorum _ size , bkdl _ bookkeeper _ ack _ quorum _ size _ default + 2 ) ;
return new fuzzy completion weight ( this , automaton , refs ) ;
string socket binding = props . get property ( constants . orb _ socket _ binding ) ; builder . add dependency ( socket binding . jboss _ binding _ name . append ( socket binding ) , socket binding . class , orb service . get iiopsocket binding injector ( ) ) ; string ssl socket binding = props . get property ( constants . orb _ ssl _ socket _ binding ) ;
for ( int i = length - 1 ; i > = 0 ; i - - ) { output . write byte ( ( int ) ( value > > ( i * 8 ) ) ) ; } }
while ( access queue . remove ( e ) ) { } }
subscription base internal api . update bcd ( base entitlement . get id ( ) , 15 , new local date ( 2016 , 5 , 31 ) , internal call context ) ;
message msg = obtain message ( event _ set _ network _ automatic _ complete , nsm ) ; if ( local _ debug ) log . d ( log _ tag , wrapping and sending message to connect automatically ) ; m cm . set network selection mode automatic ( msg ) ; }
int total child width = 0 ; for ( int i = 0 ; i < count ; i + + ) { final view child = get child at ( i ) ; if ( child . get visibility ( ) = gone ) { measure the child . measure child with margins ( child , width measure spec , 0 , height measure spec , 0 ) ; total child width + = child . get measured width ( ) ; } } if ( total child width > content width ) { set orientation ( linear layout . vertical ) ; } else { m space . set visibility ( view . visible ) ; }
java complex test class p = database . new instance ( java complex test class . class ) ; p . set name ( chuck ) ; p . get map object ( ) . put ( father , mike ) ; p . get map object ( ) . put ( mother , julia ) ; p . get map object ( ) . put ( number , 10 ) ; p . get map object ( ) . put ( date , cal . get time ( ) ) ; for ( string reference relativ : relatives . key set ( ) ) { assert . assert equals ( relatives . get ( reference relativ ) , p . get map object ( ) . get ( reference relativ ) ) ; } p . get map object ( ) . key set ( ) . size ( ) ; database . save ( p ) ; orid rid = database . get identity ( p ) ; database . close ( ) ;
final int replica global checkpoint = random int between ( math . to int exact ( sequence numbers . no _ ops _ performed ) , math . to int exact ( primary shard . get global checkpoint ( ) ) ) ;
result scanner scanner = null ; try { scanner = _ h table . get scanner ( s ) ; int num results = 0 ; for ( result rr = scanner . next ( ) ; rr = null ; rr = scanner . next ( ) ) { get row key string key = bytes . to string ( rr . get row ( ) ) ; if ( _ debug ) { system . out . println ( got scan result for key : + key ) ; } hash map < string , byte iterator > row result = new hash map < string , byte iterator > ( ) ; for ( key value kv : rr . raw ( ) ) { row result . put ( bytes . to string ( kv . get qualifier ( ) ) , new byte array byte iterator ( kv . get value ( ) ) ) ; } add row result to result vector result . add ( row result ) ; num results + + ; if ( num results > = recordcount ) if hit recordcount , bail out { break ; } } done with row } catch ( ioexception e ) { if ( _ debug ) { system . out . println ( error in getting parsing scan result : + e ) ; } return server error ; } finally { scanner . close ( ) ; try { h table pool . put table ( _ h table ) ; } catch ( ioexception e ) { } } return ok ;
log . tracef ( unblocking the write command on node + address ( 1 ) ) ; before cache0 barrier . await ( 10 , time unit . seconds ) ;
list < rex node > exps = parent . get child exps ( ) ;
check url ( simple servlet page . html , version1 ) ;
if ( fragmented frames count = 0 & & frame opcode = opcode _ cont & & frame opcode = opcode _ ping ) { protocol violation ( channel , received non - continuation data frame while inside fragmented message ) ; return null ; }
thread factory query workers factory = new thread factory builder ( ) . set daemon ( false ) . set priority ( thread . norm _ priority ) . set name format ( pqw - % d ) . build ( ) ; query workers = more executors . listening decorator ( executors . new fixed thread pool ( num query worker threads , query workers factory ) ) ; }
compute engine . release startup ( ) ; while ( ce server . get status ( ) = = monitored . status . down ) { wait for is ready to change to true , otherwise test will fail with timeout } assert that ( ce server . get status ( ) ) . is equal to ( monitored . status . operational ) ; }
int n part ; int rem f ; int t freq ; int a freq ; n part = n groups ;
try { int val = ( ( my hmaster ) test _ util . get hbase cluster ( ) . get master ( 0 ) ) . echo ( 42 ) ; assert equals ( 42 , val ) ; } catch ( class cast exception e ) { fail ( could not cast master to our class ) ; }
int children = cloud client . get zk state reader ( ) . get zk client ( ) . get children ( live _ nodes , null , true ) . size ( ) ; chaos monkey . stop ( not leader . jetty ) ; time out time out = new time out ( 60 , time unit . seconds ) ; while ( time out . has timed out ( ) ) { if ( children > cloud client . get zk state reader ( ) . get zk client ( ) . get children ( live _ nodes , null , true ) . size ( ) ) { break ; } thread . sleep ( 500 ) ; } assert true ( children > cloud client . get zk state reader ( ) . get zk client ( ) . get children ( live _ nodes , null , true ) . size ( ) ) ; int cversion = get overseer cversion ( ) ;
try { request = client . new request ( new uri ( ) ) ; listener = new future response listener ( request , length 10 ) ; request . send ( listener ) ; listener . get ( 5 , time unit . seconds ) ; assert . fail ( ) ; } catch ( execution exception x ) { assert . assert that ( x . get message ( ) , matchers . contains string ( buffering capacity exceeded ) ) ; }
default : buf [ pos + + ] = ( byte ) c ; break ; } }
if ( id = = r . id . action _ settings ) { return true ; } return super . on options item selected ( item ) ; }
transaction . set dialog ( ( sipdialog ) st . get dialog ( ) , dialog id ) ;
list < hregion info > assigned regions on live rs = new array list < hregion info > ( ) ;
m title frame layout . set layout params ( new linear layout . layout params ( match _ parent , match _ parent ) ) ;
return super . calculate dx to make visible ( view , snap preference ) ;
parameters . set limit ( get connector ( ) . get max parameter count ( ) ) ;
effect = ( effect & bits ) | ( bits & effect ) ;
for ( iterator < remote roster entry > it = roster exchange . get roster entries ( ) ; it . has next ( ) ; ) { remote roster entry remote roster entry = it . next ( ) ; get connection ( 1 ) . get roster ( ) . create entry ( remote roster entry . get user ( ) , remote roster entry . get name ( ) , remote roster entry . get group array names ( ) ) ; }
string max leaf records string = _ segment metadata properties configuration . get string ( metadata keys . star tree . star _ tree _ max _ leaf _ records ) ; if ( max leaf records string = null ) { _ star tree metadata . set max leaf records ( long . value of ( max leaf records string ) ) ; }
begin tx ( ) ; relationships [ 0 ] . get end node ( ) . delete ( ) ; relationships [ 0 ] . delete ( ) ; finish tx ( ) ; relationship [ ] other relationships = create relationship chain ( 1 ) ;
int index of start = line . index of ( cms _ abort _ preclean ) ;
lru cache stats . hit ( true , true , block type . data ) ;
final channel buffer buffer = channel buffers . wrapped buffer ( frame , data ) ; channels . write ( ctx , e . get future ( ) , buffer , e . get remote address ( ) ) ; } return ; }
this . zkc = zkc ;
if ( refinfo = null & & refinfo . running ( ) ) { ref . lock . read lock ( ) . unlock ( ) ; if ( refinfo = info ) not us , ensure is doomed { block and bail ( refinfo ) ; } } else ensures . add ( ref ) ;
string jmx value = access jmxvalue ( ) ; if ( jmx value = null ) { return jmx value . equals ( value ) ; } return false ;
weather db helper db helper = new weather db helper ( m context ) ; sqlite database db = db helper . get writable database ( ) ; content values test values = test utilities . create north pole location values ( ) ;
string ip = resolve hostname ( connection . get host ( ) ) ; if ( container = null ) { for ( device device : container . get devices ( ) ) { if ( contains ( device . get provides ( ) , server ) ) { for ( connection device connection : device . get connections ( ) ) { boolean uri set = ( connection . get uri ( ) = null ) ; boolean port equal = string . value of ( connection . get port ( ) ) . equals ( device connection . get port ( ) ) ; boolean host equal = ip . equals ( device connection . get address ( ) ) ; if ( uri set & & port equal & & host equal ) { connection . set uri ( device connection . get uri ( ) ) ; connection . set api level ( plex api level . get api level ( device . get product version ( ) ) ) ; logger . debug ( server found , version { } , api level { } , device . get product version ( ) , connection . get api level ( ) ) ; } } } } }
long reservation start = timestamp . value of ( 2050 - 12 - 03 10 : 37 : 37 ) . get time ( ) ; long reservation end = timestamp . value of ( 2050 - 12 - 03 10 : 47 : 37 ) . get time ( ) ; long search start = timestamp . value of ( 2050 - 12 - 03 10 : 10 : 37 ) . get time ( ) ; long search end = timestamp . value of ( 2050 - 12 - 03 10 : 20 : 37 ) . get time ( ) ;
logger . error ( can ' t handle command { } on channel { } , command , channel uid ) ;
configuration . set ( fs . + llap cache aware fs . scheme + . impl , llap cache aware fs . class . get canonical name ( ) ) ; path = llap cache aware fs . register file ( cache , path , file key , chunk index , configuration ) ; this . cache fs path = path ; return path ; }
user group information real user ugi = user group information . create remote user ( real _ user _ name ) ; user group information proxy user ugi = user group information . create proxy user for testing ( proxy _ user _ name , real user ugi , group _ names ) ;
final object [ ] assembled props = type helper . assemble ( disassembled state , persister . get property types ( ) , session , instance ) ;
public map < byte array , list < versioned < byte [ ] > > > get all ( composite voldemort request < byte array , byte [ ] > request ) throws voldemort exception { map < byte array , list < versioned < byte [ ] > > > result = null ;
if ( word . ends with ( yz ) ) { word = word . substring ( 0 , last pos - 1 ) . concat ( ys ) ; } return word ;
if ( error code = = 257 | | ( error code > = 259 & & error code < = 263 ) ) { throw new sqlgrammar exception ( message , sql exception , sql ) ; }
assert equals ( new hash set < > ( arrays . as list ( connector _ name , connector2 _ name ) ) , new hash set < > ( connectors ) ) ; power mock . verify all ( ) ; }
err cnt = 0 ;
saved instance state . put string array list ( bundle _ expanded _ selections + prefix , selections ) ; }
entity metadata entity metadata = kundera metadata manager . get entity metadata ( kundera metadata , entity clazz ) ; metamodel impl metamodel = ( metamodel impl ) kundera metadata manager . get metamodel ( kundera metadata , entity metadata . get persistence unit ( ) ) ; entity type entity type = metamodel . entity ( entity metadata . get entity clazz ( ) ) ; table schema table = table api . get table ( entity metadata . get table name ( ) ) ;
for ( int i = 0 ; i < block size ; i + + ) { out [ out off + i ] = ( byte ) ( ofb out v [ i ] ^ in [ in off + i ] ) ; }
xml = < wfs : get feature with lock + service = \ wfs \ + version = \ 2 . 0 . 0 \ + expiry = \ 100 \ + xmlns : cdf = \ http : www . opengis . net cite data \ + xmlns : fes = ' + fes . namespace + ' + xmlns : wfs = ' + wfs . namespace + ' > + < wfs : query type names = \ cdf : locks \ > + < fes : filter > + < fes : resource id rid = \ + fid + \ > + < fes : filter > + < wfs : query > + < wfs : get feature with lock > ; dom = post as dom ( wfs , xml ) ;
flow scope informed true = interpreter . get preciser scope knowing condition outcome ( condition , blind , true ) ;
result . order ( byte order . native order ( ) ) ; return result ;
read only transaction id store tx id store = new read only transaction id store ( page cache , store dir ) ;
assert equals ( category newer . get name ( ) , item1 merged . get category ( ) . get name ( ) ) ; assert same ( item1 merged , item1 merged . get category ( ) . get example item ( ) ) ; tx . commit ( ) ; s . close ( ) ; s = open session ( ) ; tx = s . begin transaction ( ) ; item1 = ( item ) s . get ( item . class , item1 . get id ( ) ) ; assert equals ( category newer . get name ( ) , item1 . get category ( ) . get name ( ) ) ; assert same ( item1 , item1 . get category ( ) . get example item ( ) ) ;
for ( int x = 0 ; x < num _ files ; x + + ) { path p = new path ( dir , foo + x ) ; data output stream out = fs . create ( p ) ; out . write bytes ( blahblah ) ; } system . err . println ( created + num _ files + files ) ;
map < byte buffer , map < string , t > > serialized to original = new hash map < > ( partitions . size ( ) ) ;
third digit = new node ( k0 , k1 , k2 , k3 , k4 , k5 , k6 , k7 , k8 , k9 ) ;
if ( get activity ( ) = null ) { find target fragment if ( ) ; } fragment fragment = m target fragment ;
int num = in . read short ( ) ;
get window ( ) . set status bar color ( get resources ( ) . get color ( r . color . color primary dark _ light ) ) ;
if ( cell spacing > 0 ) { html css helper . append style ( table root , property . compose css ( table _ border _ model , separate ) ) ; table root . set attribute ( cellspacing , ww seems only to store cell spacing 2 but displays and applies cell spacing * 2 convert to pixels ( cell spacing * 2 ) ) ; } else { html css helper . append style ( table root , property . compose css ( table _ border _ model , collapse ) ) ; }
return describe match ( method tree ) ; }
assert equals ( 2 , builder . get payload ( ) . get repeated int32 count ( ) ) ; assert equals ( 1000 , builder . get payload ( ) . get repeated int32 ( 0 ) ) ; assert equals ( 4321 , builder . get payload ( ) . get repeated int32 ( 1 ) ) ;
assert false ( cut . equals ( new array list < string > ( ) ) ) ;
int first result = first _ a - first _ b ;
if ( ( this . capacity > > 1 ) > = bytes written ) { this . capacity = this . capacity > > 1 ; } }
string encoded token = under test . refresh ( token , 20 * 60 ) ; claims result = under test . decode ( encoded token ) . get ( ) ;
code index - - ; code index = text compaction ( codewords , code index , result ) ; break ; } if ( code index < codewords . length ) { code = codewords [ code index + + ] ; } else { throw format exception . get format instance ( ) ; } }
return ( sqlexception converter ) converter class . new instance ( ) ; }
for ( int index = 0 ; index < zone strings . length ; index + + ) { if ( id . equals ( zone strings [ index ] [ 0 ] ) ) { last zone index = index ; return index ; } } return - 1 ;
first digit . add child ( minute first digit ) ;
queue init = new concurrent linked queue < > ( ) ; accumulator < queue , t > accumulator = new accumulator < queue , t > ( ) { @ override public queue accumulate ( queue accumulated , t in ) { if ( in = = null ) { throw new ise ( cannot have null result ) ; } accumulated . offer ( in ) ; return accumulated ; } } ; return new pair < > ( init , accumulator ) ;
list < catch statement > catches = new array list < catch statement > ( ) ; for ( ; node = null & & is type ( literal _ catch , node ) ; node = node . get next sibling ( ) ) { final list < catch statement > catch statements = catch statement ( node ) ; catches . add all ( catch statements ) ; } if ( is type ( literal _ finally , node ) ) { finally statement = statement ( node ) ; node = node . get next sibling ( ) ; }
lazy message lite . builder merged = lazy message lite . new builder ( ) . merge from ( outer ) ;
final string full har path str = make archive ( input path , glob ) ;
player . rotate ( 0 , 2 * tpf , 0 ) ;
issue dto issue1 = issue testing . new issue ( rule , project , file ) ;
template . send body ( uri , message 10 ) ; assert equals ( 10 , edpc . get queue size ( ) ) ; assert equals ( 10 , edpc . get queue capacity ( ) ) ; service helper . stop service ( consumer ) ;
verify non null or empty ( json , tag definition json body should be specified ) ; verify non null or empty ( json . get name ( ) , tag definition name needs to be set , json . get description ( ) , tag definition description needs to be set ) ; final tag definition created tag def = tag user api . create tag definition ( json . get name ( ) , json . get description ( ) , context . create context ( created by , reason , comment , request ) ) ; return uri builder . build response ( uri info , tag definition resource . class , get tag definition , created tag def . get id ( ) , request ) ; }
index3 = index builder . create ( ) . tmp dir ( temporary folder . new folder ( ) ) . schema ( new incremental index schema . builder ( ) . with metrics ( new count aggregator factory ( cnt ) , new double sum aggregator factory ( c1 , c1 ) , new hyper uniques aggregator factory ( uniques , c2 ) ) . with rollup ( false ) . build ( ) ) . rows ( input rows with dimensions ( immutable list . of ( c2 ) ) ) . build mmapped index ( ) ;
assert equals ( 1 , find iterable . into ( new array list < document > ( ) ) . size ( ) ) ;
on subscriber removed ( subscriber ) ;
configuration cfg = new configuration ( ) ; cfg . add resource ( org hibernate test onetoone formula person . hbm . xml ) ; session factory implementor sf = ( session factory implementor ) cfg . build session factory ( ) ; try { do compare ( sf , ( outer join loadable ) sf . get class metadata ( org . hibernate . test . onetoone . formula . person . class ) ) ; } finally { sf . close ( ) ; }
if ( write _ pos _ beg < read _ pos _ end ) { write _ pos _ beg = read _ pos _ end ; write _ pos = write _ pos _ beg ; }
type = dependency type . get type ( dependency ) ;
handle fault ( exchange ) ; } finally {
string class name = type paths . get ( type paths . size ( ) - 1 ) ;
if ( to type . is primitive ( ) ) { if ( from type = = void . type | | from type = = null | | from type . is primitive ( ) ) { both primitives , do primitive cast return primitive . cast primitive ( to type , from type , ( primitive ) from value , check only , operation ) ; } else { if ( primitive . is wrapper type ( from type ) ) { wrapper to primitive convert value to primitive and check cast it . object r = check only ? valid _ cast : class unboxed from type = primitive . unbox type ( from type ) ; primitive prim from value ; if ( check only ) prim from value = null ; must be null in check only else prim from value = ( primitive ) primitive . wrap ( from value , unboxed from type ) ; return primitive . cast primitive ( to type , unboxed from type , prim from value , check only , operation ) ; } else { cannot cast from arbitrary object to primitive if ( check only ) return invalid _ cast ; else throw cast error ( to type , from type , operation ) ; } } }
if ( m fetched months [ 2 ] < 1 | | m fetched months [ 2 ] = next month | | m refresh events ) { if ( contains value ( last fetched month , next month ) & & is in edit mode ( ) ) { list < week view event > events = m month change listener . on month change ( next month = = 1 ? day . get ( calendar . year ) + 1 : day . get ( calendar . year ) , next month ) ; sort events ( events ) ; for ( week view event event : events ) { m event rects . add ( new event rect ( event , null ) ) ; } } m fetched months [ 2 ] = next month ; }
mouse handler pane . set on mouse pressed ( me - > { if ( me . is consumed ( ) ) { me . consume ( ) ; track . fire event ( me ) ; } } ) ;
ret val = ( const value + const value2 = = x ) ;
final graph database service db = new test graph database factory ( ) . set file system ( fs . get ( ) ) . new impermanent database builder ( store directory . graph db dir ( ) ) . set config ( graph database settings . allow _ upgrade , true ) . new graph database ( ) ;
view . render ( model , request , response ) ; assert equals ( url , response . get included url ( ) ) ; model . for each ( ( k , v ) - > verify ( request ) . set attribute ( k , v ) ) ; }
if ( name . index of ( ' , ' ) < 0 ) return implied mask ( 1 < < name index ( name . trim ( ) ) ) ; int mask = 0 ;
window function = input . get execution environment ( ) . clean ( window function ) ; aggregate function = input . get execution environment ( ) . clean ( aggregate function ) ; final string call location = utils . get call location name ( ) ;
throw new org . apache . axis2 . databinding . adbexception ( customer gateway id cannot be null ) ;
assert that ( actual ) . is true ( ) ;
throw new runtime exception ( ioe ) ; }
xml = < get feature version = ' 1 . 1 . 0 ' xmlns : gml = \ http : www . opengis . net gml \ > + < query type name = \ + system test data . buildings . get local part ( ) + \ > + < property name > address < property name > + < filter > + < property is like wild card = \ * \ single char = \ . \ escape char = \ \ \ \ match case = \ true \ > + < property name > address < property name > + < literal > * main street < literal > + < property is like > + < filter > + < query > + < get feature > ; doc = post as dom ( wfs , xml ) ; assert equals ( wfs : feature collection , doc . get document element ( ) . get node name ( ) ) ; feature members = doc . get elements by tag name ( cite : buildings ) ;
b = stream . read ( ) ; if ( b < 0 ) { if ( shift = = 0 ) { throw new eofexception ( ) ; } else { throw new ioexception ( varint not terminated ) ; } } long bits = b & 0x7 f ; if ( shift > = 64 | | ( shift = = 63 & & bits > 1 ) ) {
for ( object e : q . to array ( ) ) { if ( e instanceof runnable scheduled future ) { runnable scheduled future < ? > t = ( runnable scheduled future < ? > ) e ; if ( ( t . is periodic ( ) ? keep periodic : keep delayed ) | | t . is cancelled ( ) ) { also remove if already cancelled if ( q . remove ( t ) ) t . cancel ( false ) ; } } }
case project _ key _ update :
direction . set ( cam . get direction ( ) ) . normalize local ( ) ;
if ( at pos = - 1 ) { pos = at pos + 1 ; }
mi = new memory index ( true , true ) ; mi . add field ( field , some terms be here , analyzer ) ; test util . check reader ( mi . create searcher ( ) . get index reader ( ) ) ; mi = new memory index ( true , false ) ;
filename + = . tif ;
button . set text ( ) ;
return ( xmlname ) name value ;
filter row filter = get row filter ( args ) ; if ( row filter = null ) { log . info ( setting row filter for counter . ) ; s . set filter ( row filter ) ; } return s ; }
bytes = new byte [ format [ width ] * format [ height ] * 4 ] ;
long main one time = long . value of ( check url ( main - one , false ) ) ; long main two time = long . value of ( check url ( main - two , false ) ) ; long main three time = long . value of ( check url ( main - three , false ) ) ; long other two time = long . value of ( check url ( other - two , false ) ) ; long test one time = long . value of ( check url ( test - one , false ) ) ; assert . assert true ( main one time < other two time ) ;
type = ( base dmntype impl ) resolve type ref ( dmn model , node , item def , item def , item def . get type ref ( ) ) ;
data packet extension data = new data packet extension ( session id , 0 , bbbb = ccc ) ; assert null ( data . get decoded data ( ) ) ;
for ( suspendable classifier sc : loader ) { st = sc . is suspendable ( db , source name , source debug info , is interface , class name , super class name , interfaces , method name , method desc , method signature , method exceptions ) ; if ( st = null ) return st ; }
symbol aliases all source aliases = sources match . get aliases ( ) ;
if ( test listener = null ) { test listener . reset ( ) ; }
hdfs . delete ( bar , true ) ;
if ( spring . system should advance ( ) ) { spring . advance ( delta time 1000 . 0 ) ; } else { m active springs . remove ( spring ) ; } }
if ( pending confirm . size ( ) > 0 ) { pending confirm . clear ( ) ; }
get request header group ( ) . remove header ( cookie line header ) ;
int n = t . size ( ) ; if ( n = = 0 ) return ; if ( n > = threshold ) { n = ( int ) ( n load factor + 1 ) ; if ( n > maximum _ capacity ) n = maximum _ capacity ; int capacity = table . length ; while ( capacity < n ) capacity < < = 1 ; resize ( capacity ) ; } for ( iterator i = t . entry set ( ) . iterator ( ) ; i . has next ( ) ; ) { int entry < value > e = ( int entry < value > ) i . next ( ) ; put ( e . key , e . value ) ; }
for ( int i = 0 ; i < 7 ; i + + ) { ocache entry cache entry = read buffer . load for read ( file id , i , false , write buffer , 1 , true ) ; read buffer . release from read ( cache entry , write buffer ) ; } assert . assert equals ( am . size ( ) , 0 ) ; assert . assert equals ( a1out . size ( ) , 2 ) ; pages 1 - 2 assert . assert equals ( a1in . size ( ) , 4 ) ; pages 3 - 6
if ( right screen < screen count - 1 ) { right screen + + ; } else if ( left screen > 0 ) { left screen - - ; }
if ( resources available ) { start containers from queue ( queued opportunistic containers ) ; } }
person bmm p1 = ( person bmm ) dao . find person by id column ( person bmm . class , bimanytomany _ 1 ) ; assert person1 ( p1 ) ;
if ( date util . current gmttime ( ) . get time ( ) - vm . get proxy assign time ( ) . get time ( ) < _ proxy session timeout value ) { return true ; } return false ;
return context . get type converter ( ) . convert to ( type , this , out ) ; }
string plain text = null ;
l = from language tag ( en - latn - posix - p2003 , use builder ) ; assert equals ( en , l . get language ( ) ) ; assert equals ( latn , l . get script ( ) ) ; assert equals ( , l . get country ( ) ) ; assert equals ( posix _ p2003 , l . get variant ( ) ) ;
assert true ( namesystem . has retry cache ( ) ) ;
assert true ( arrays . equals ( test _ data , target ) ) ;
ensure writable bytes ( length ) ; slice . set bytes ( buffer position , source , source index , length ) ; buffer position + = length ; }
chat id = message . get chat id ( ) ;
manager . add account explicitly ( dummy , , null ) ; types to delete . add ( account . type ) ; } catch ( security exception e ) {
reader r = new string reader ( text ) ;
enterprise archive ear = shrink wrap . create ( enterprise archive . class , ear _ name ) ;
estimated [ i ] = ( long ) math . ceil ( ( double ) math . max ( 0 l , sstable reader . get total bytes ( sstables ) - ( long ) ( max bytes for level ( i , max sstable size in bytes ) * 1 . 001 ) ) ( double ) max sstable size in bytes ) ;
mockito . reset ( http servlet request ) ; string value = safe request parameter converter . convert ( access event ) ; assert that ( value ) . is equal to ( alice ) ; }
num1 = 0 ;
strip anonymity ( ) ; } super . put ( key , value ) ; } }
monitors . new monitor ( my monitor . class ) ;
return get by id ( id , clazz ) ;
string separator = ( args . length < 1 | | args [ 0 ] = = undefined . instance ) ? , : script runtime . to string ( args [ 0 ] ) ; if ( this obj instanceof native array ) { native array na = ( native array ) this obj ; if ( na . dense only ) { string buffer sb = new string buffer ( ) ; for ( int i = 0 ; i < length ; i + + ) { if ( i = 0 ) { sb . append ( separator ) ; } if ( i < na . dense . length ) { object temp = na . dense [ i ] ; if ( temp = null & & temp = undefined . instance & & temp = scriptable . not _ found ) { sb . append ( script runtime . to string ( temp ) ) ; } } } return sb . to string ( ) ; } } if ( length = = 0 ) { return ; }
int stripe start = stripe length * stripe index ;
account setup check settings . action check settings ( this , m account , check direction . incoming ) ;
verify ( subtask state2 _ 1 , never ( ) ) . discard state ( ) ;
scheduler . schedule block report ( dn conf . initial block report delay ms ) ; }
if ( bytes [ 0 ] = = ( byte ) 0xff ) { eliminate redundant 0xff for ( int j = 0 ; j < 3 ; j + + ) { if ( ( bytes [ j ] = = ( byte ) 0xff ) & & ( ( bytes [ j + 1 ] & 0x80 ) = = 0x80 ) ) start + + ; else break ; } } else if ( bytes [ 0 ] = = 0x00 ) { eliminate redundant 0x00 for ( int j = 0 ; j < 3 ; j + + ) { if ( ( bytes [ j ] = = 0x00 ) & & ( ( bytes [ j + 1 ] & 0x80 ) = = 0 ) ) start + + ; else break ; } } put length ( 4 - start ) ;
conf . set int ( yarn configuration . federation _ cache _ time _ to _ live _ secs , 0 ) ; return conf ; }
native crypto . ssl _ use _ certificate ( s , get server certificates ( ) ) ; try { native crypto . ssl _ check _ private _ key ( s ) ; fail ( ) ; } catch ( sslexception expected ) { } native crypto . ssl _ use _ private key ( s , get server private key ( ) . get pkey context ( ) ) ; native crypto . ssl _ check _ private _ key ( s ) ; native crypto . ssl _ free ( s ) ; native crypto . ssl _ ctx _ free ( c ) ;
names = _ secondary . find enum values ( enum type , enum values , names ) ;
url url = new url ( http : 127 . 0 . 0 . 1 : + shuffle handler . get config ( ) . get ( shuffle handler . shuffle _ port _ config _ key ) + map output?job = job _ 12345 _ 1 & reduce = 1 & map = attempt _ 12345 _ 1 _ m _ 1 _ 0 ) ; http urlconnection conn = ( http urlconnection ) url . open connection ( ) ; conn . set request property ( shuffle header . http _ header _ name , shuffle header . default _ http _ header _ name ) ; conn . set request property ( shuffle header . http _ header _ version , shuffle header . default _ http _ header _ version ) ; conn . connect ( ) ; data input stream input = new data input stream ( conn . get input stream ( ) ) ; assert . assert equals ( http urlconnection . http _ ok , conn . get response code ( ) ) ; assert . assert equals ( close , conn . get header field ( http header . connection . as string ( ) ) ) ; shuffle header header = new shuffle header ( ) ; header . read fields ( input ) ; input . close ( ) ; shuffle handler . stop ( ) ;
return get confirmed connections ( ) . stream ( ) . map ( e - > e . get peers node address optional ( ) . get ( ) ) . collect ( collectors . to set ( ) ) ; }
byte [ ] bytes1 = new byte [ 11 ] ; system . arraycopy ( bytes , 0 , bytes1 , 0 , bytes1 . length ) ; byte [ ] bytes2 = new byte [ bytes . length - bytes1 . length ] ; system . arraycopy ( bytes , bytes1 . length , bytes2 , 0 , bytes2 . length ) ; gzipcontent decoder decoder = new gzipcontent decoder ( pool , 2048 ) ;
compute relocating shards + + ;
seekpos = new jtext field ( + configuration . get thumbnail seek pos ( ) ) ;
byte [ ] buffer = new byte [ 1024 ] ; byte array output stream bytes = new byte array output stream ( ) ; while ( true ) { int byte count = in . read ( buffer ) ; if ( byte count = = - 1 ) { return bytes . to byte array ( ) ; } bytes . write ( buffer , 0 , byte count ) ; } }
float area = area utils . calc screen area ( bv , last distance , cam . get width ( ) ) ; float tris to draw = area * tris per pixel ; level = num levels - 1 ; for ( int i = num levels ; - - i > = 0 ; ) { if ( tris to draw - num tris [ i ] < 0 ) { break ; } level = i ; } last level = level ;
remove chunk ( chunk ) ;
queue . finish application attempt ( app _ 0 , a ) ;
if ( fs . delete ( temp table dir , true ) & & fs . exists ( temp table dir ) ) { throw new ioexception ( couldn ' t delete + temp table dir ) ; }
palette drawer controls = new palette drawer ( tools , dbeaver icons . get image descriptor ( uiicon . configuration ) ) ; palette root . add ( controls ) ;
fire call peer change event ( call peer change event . call _ peer _ image _ change , old image , image ) ;
output . write uint32 no tag ( 0 ) ;
my parser = new pipelined msg parser ( this , hispipe , this . sip stack . get max message size ( ) ) ;
m impl = new expandable item indicator impl anim ( ) ; } else {
query = new term range query ( field , new bytes ref ( value ) , new bytes ref ( value ) , true , true ) ; highlight and assert match ( searcher , highlighter , query , field , value ) ;
if ( is sub query ( ) & & order expression node . get type ( ) = = ident & & select expressions by result variable . contains key ( order expression node . get text ( ) ) ) { return true ; } return false ; }
object v = top level . get ( java adapter , top level ) ; if ( v = not _ found ) { function f = ( function ) v ; object [ ] adapter args = { this , args [ 0 ] } ; return f . construct ( cx , top level , adapter args ) ; } } catch ( exception ex ) {
if ( m appended & & m generate mini drawer ) { if we should create a mini drawer we have to do this now m mini drawer = new mini drawer ( ) . with drawer ( result ) . with account header ( m account header ) ; }
string error code = parse error code ( node ) ; if ( error code = = null | | error code . equals ( invalid snsdestination ) ) return null ; invalid snsdestination exception e = ( invalid snsdestination exception ) super . unmarshall ( node ) ;
do return ( 4 . 0 . 0 ) . when ( fake ns info ) . get software version ( ) ; do return ( 3 . 0 . 0 ) . when ( mock dn conf ) . get minimum name node version ( ) ; assert equals ( 4 . 0 . 0 , actor . retrieve namespace info ( ) . get software version ( ) ) ;
current thread . set context class loader ( host class loader ) ;
query = new rule query ( ) . set is template ( true ) ; results = under test . search ( query , new search options ( ) ) ; assert that ( results . get ids ( ) ) . contains only ( rule is template . get key ( ) ) ;
ovf create descriptor params ovf desc params = new ovf create descriptor params ( ) ; ovf desc params . set ovf files ( ovf files ) ; ovf create descriptor result ovf create descriptor result = _ context . get service ( ) . create descriptor ( mor ovf , get mor ( ) , ovf desc params ) ; string ovf path = export dir + file . separator + export name + . ovf ; file names . add ( ovf path ) ; file writer out = new file writer ( ovf path ) ; out . write ( ovf create descriptor result . get ovf descriptor ( ) ) ; out . close ( ) ;
iterable < sstable reader > obsolete = filter out ( concat uniq ( staged . update , logged . update ) , originals ) ; logger . trace ( obsoleting { } , obsolete ) ; accumulate = prepare for obsoletion ( obsolete , log , obsoletions = new array list < > ( ) , accumulate ) ;
canonical request . append ( input . query line ( ) . substring ( 1 ) ) ; canonical request . append ( ' \ n ' ) ;
final boolean do pause = thread idx < big merge count - max thread count ; double new mbper sec ; if ( do pause ) { new mbper sec = 0 . 0 ; } else if ( merge . max num segments = - 1 ) { new mbper sec = force merge mbper sec ; } else if ( do auto iothrottle = = false ) { new mbper sec = double . positive _ infinity ; } else if ( merge . estimated merge bytes < min _ big _ merge _ mb * 1024 * 1024 ) {
try { no need to defer the template as it can adjust to the endpoint at runtime start service ( answer , context , bean , null ) ; } catch ( exception e ) { throw object helper . wrap runtime camel exception ( e ) ; } return answer ; }
results = entity manager . create query ( query text ) . get result list ( ) ;
if ( current message = = null ) { throw new illegal state exception ( received + http chunk . class . get simple name ( ) + without + http message . class . get simple name ( ) ) ; } http chunk chunk = ( http chunk ) msg ; if ( too long frame found ) { if ( chunk . is last ( ) ) { this . current message = null ; } return ; }
instructions . add ( reil helpers . create bsh ( base offset + + , qw , tmp var2 , dw , string . value of ( - 31 l ) , bt , tmp var3 ) ) ; instructions . add ( reil helpers . create and ( base offset + + , bt , tmp var3 , bt , string . value of ( 1 l ) , bt , n ) ) ;
string query = select s from student character s ; query q = em . create query ( query ) ; list < student character > students = q . get result list ( ) ; assert . assert not null ( students ) ; assert . assert equals ( 3 , students . size ( ) ) ; int count = 0 ; for ( student character student : students ) { if ( student . get id ( ) . equals ( get max value ( character . class ) ) ) { assert . assert equals ( get max value ( short . class ) , student . get age ( ) ) ; assert . assert equals ( kuldeep , student . get name ( ) ) ; count + + ; } else if ( student . get id ( ) . equals ( get min value ( character . class ) ) ) { assert . assert equals ( get min value ( short . class ) , student . get age ( ) ) ; assert . assert equals ( get min value ( string . class ) , student . get name ( ) ) ; count + + ; } else { assert . assert equals ( get random value ( character . class ) , student . get id ( ) ) ; assert . assert equals ( get random value ( short . class ) , student . get age ( ) ) ; assert . assert equals ( get random value ( string . class ) , student . get name ( ) ) ; count + + ; } } assert . assert equals ( 3 , count ) ; em . close ( ) ;
tr = transformation . get more data ( op code , fin , rsv , message buffer binary ) ;
current window = merge ( ( interval window ) current window , ( interval window ) next window ) ; }
keep alive manager . on transport idle ( ) ;
req = new mock http servlet request ( get , pathmatching aa . html ) ;
verify hints ( directory , descriptor ) ;
assert equals ( initial local + 1 , get number of local connections ( provider , hot rod server mbean ) ) ;
if ( is security domain known ) { view configuration . add view interceptor ( view method , new immediate interceptor factory ( roles allowed interceptor . deny _ all ) , interceptor order . view . ejb _ security _ authorization _ interceptor ) ; } else { final interceptor authorization interceptor = new authorization interceptor ( ejbmethod security attribute . deny all ( ) , view class name , view method , context id ) ; view configuration . add view interceptor ( view method , new immediate interceptor factory ( authorization interceptor ) , interceptor order . view . ejb _ security _ authorization _ interceptor ) ; }
if ( default value = = null ) { we don ' t have a default value out . write boolean ( false ) ; } else { we have a default value out . write boolean ( true ) ; byte [ ] serialized default value ; try ( byte array output stream baos = new byte array output stream ( ) ; data output view stream wrapper out view = new data output view stream wrapper ( baos ) ) { type serializer < t > duplicate serializer = serializer . duplicate ( ) ; duplicate serializer . serialize ( default value , out view ) ; out view . flush ( ) ; serialized default value = baos . to byte array ( ) ; } catch ( exception e ) { throw new ioexception ( unable to serialize default value of type + default value . get class ( ) . get simple name ( ) + . , e ) ; } out . write int ( serialized default value . length ) ; out . write ( serialized default value ) ; }
this . length = utils . get int be ( b , offset _ pos , offset _ length - 1 ) ;
buffer pool thread local . clear ( ) ;
if ( archive descriptor factory = = null ) { return new standard scanner ( ) ; } else { return new standard scanner ( archive descriptor factory ) ; }
boolean has mask arrays = data . has mask arrays ( ) ;
if ( left = = right ) return true ; if ( left = = null | | right = = null ) return false ; if ( left . length = right . length ) return false ; if ( left . length = = 0 ) return true ;
return new ebus telegram ( buffer ) ; } catch ( index out of bounds exception e ) {
if ( used & & has field ) { throw new solr exception ( solr exception . error code . bad _ request , error : + get id ( doc , schema ) + unknown field ' + name + ' ) ; } }
kill bill client . create account tag ( input . get account id ( ) , auto pay off id , request options ) ;
assert equals ( int property , props . get jsonobject ( i ) . get ( name ) ) ; assert equals ( new integer ( 0 ) , props . get jsonobject ( i ) . get ( min occurs ) ) ; assert equals ( new integer ( 1 ) , props . get jsonobject ( i ) . get ( max occurs ) ) ; assert equals ( true , props . get jsonobject ( i ) . get ( nillable ) ) ; assert equals ( xsd : int , props . get jsonobject ( i ) . get ( type ) ) ; assert equals ( int , props . get jsonobject ( i ) . get ( local type ) ) ; + + i ;
final file file = temporary folder . new file ( file . java ) ; final file text file text = new file text ( file , new array list < > ( ) ) ; tree walker . process filtered ( file , file text ) ;
return ( t ) find view by id ( id ) ;
log . e ( provided comple awb , invoke tools at public bundle tools . get current time ( ) > + tools . get current time ( ) ) ;
if ( next > = ' a ' & & next < = ' z ' ) { decoded char = ( char ) ( next - 64 ) ; } else { throw format exception . get format instance ( ) ; } break ;
component synthesized component = synthesize annotation ( attributes , component . class , web controller . class ) ; assert not null ( synthesized component ) ;
d = get as dom ( root ( ) + service = wps & request = describeprocess & identifier = jts : intersection ) ;
if ( offset + 8 > end ) { throw new eofexception ( ) ; } current double = double . long bits to double ( lazy binary utils . byte array to long ( bytes , offset ) ) ; offset + = 8 ; break ; case binary :
hasher hasher = hash _ fn . new hasher ( ) ; hasher . put float ( 0x01000101f ) . put float ( 0f ) ; assert equals ( 0x49f9d18ee8ae1b28 l , hasher . hash ( ) . as long ( ) ) ; hasher = hash _ fn . new hasher ( ) ; hasher . put double ( 0x0000000001000101d ) ; assert equals ( 0x388ee898bad75cbf l , hasher . hash ( ) . as long ( ) ) ; }
case 0x38 : case 0x39 : case 0x3a : case 0x3b : case 0x3c : case 0x3d : case 0x3e : case 0x3f : return ( ( tag - bc _ long _ short _ zero ) < < 16 ) + 256 * read ( ) + read ( ) ; case ' l ' :
float fit span = math . min ( total ascent align , total descent ( 1 . 0f - align ) ) ;
conf = view file system test setup . create config ( ) ;
if ( m enable depth test ) { gles20 . gl disable ( gles20 . gl _ depth _ test ) ; } else { gles20 . gl enable ( gles20 . gl _ depth _ test ) ; gles20 . gl depth func ( gles20 . gl _ less ) ; } gles20 . gl depth mask ( m enable depth mask ) ;
final list < integer > expected null rows = new array list < > ( ) ; for ( int i = 0 ; i < index . get num rows ( ) ; i + + ) { final list < string > row = get row ( dictionary column , i ) ; if ( row . is empty ( ) | | row . stream ( ) . any match ( strings : : is null or empty ) ) { expected null rows . add ( i ) ; } } assert . assert equals ( subset list . to string ( ) , expected null rows . size ( ) > 0 , bitmap index . has nulls ( ) ) ; if ( expected null rows . size ( ) > 0 ) { assert . assert equals ( subset list . to string ( ) , 0 , bitmap index . get index ( null ) ) ; final immutable bitmap null bitmap = bitmap index . get bitmap ( bitmap index . get index ( null ) ) ; final list < integer > actual null rows = new array list < > ( ) ; final int iterator iterator = null bitmap . iterator ( ) ; while ( iterator . has next ( ) ) { actual null rows . add ( iterator . next ( ) ) ; } assert . assert equals ( subset list . to string ( ) , expected null rows , actual null rows ) ; } else { assert . assert equals ( - 1 , bitmap index . get index ( null ) ) ; }
assert equals ( factory . get class ( ) , executors . default thread factory ( ) . get class ( ) ) ; }
final string diagram version = diagram elem . get attribute ( attr _ version ) ; if ( common utils . is empty ( diagram version ) ) { throw new dbexception ( diagram version not found ) ; } if ( diagram version . equals ( string . value of ( erd _ version _ 1 ) ) ) { throw new dbexception ( unsupported diagram version : + diagram version ) ; } list < table load info > table infos = new array list < > ( ) ;
indexer . delete relationship index ( m , graph db , relationship ) ;
coreference ( document , sieve ) ; }
address cache . get advanced cache ( ) . with flags ( flag . skip _ cache _ load , flag . guaranteed _ delivery ) . put ( cluster address , address ) ;
boolean actions enabled = m patio thumbnails . size ( ) < m max pictures ;
key cache size in mb = ( conf . key _ cache _ size _ in _ mb = = null ) ? math . min ( math . max ( 1 , ( int ) ( runtime . get runtime ( ) . total memory ( ) * 0 . 05 1024 1024 ) ) , 100 ) : conf . key _ cache _ size _ in _ mb ; if ( key cache size in mb < 0 ) throw new number format exception ( ) ; to escape duplicating error message }
assert that ( integer . value of ( bulk item response . get id ( ) ) , both ( greater than ( 0 ) ) . and ( less than or equal to ( test docs ) ) ) ;
return ; } system . out . println ( info : update file exists [ + update _ file + ] - installing ) ;
check property ( test schema class , string list map , otype . embeddedmap , otype . embeddedlist ) ;
if ( object = null ) { tracked . put ( item , object ) ; modified ( ) ; * increment modification count * notify all ( ) ; * notify any waiters * } } else { became untracked = true ; } } }
object column _ added = param . get object extra ( column _ added ) ; boolean added = ( column _ added = = null ? false : ( boolean ) param . get object extra ( column _ added ) ) ; list < string > list column = new array list < string > ( ) ; list column . add all ( arrays . as list ( cursor . get column names ( ) ) ) ; if ( added ) list column . remove ( list column . size ( ) - 1 ) ;
data . delete ( parent ) ;
llap task scheduler service . task info task info = new llap task scheduler service . task info ( locality delay conf1 , clock , new object ( ) , new object ( ) , mock ( priority . class ) , mock ( resource . class ) , null , null , clock . get time ( ) , null ) ; assert false ( task info . should force locality ( ) ) ;
all match keys = conf . get val by regex ( hosts reg ex ) ; for ( entry < string , string > entry : all match keys . entry set ( ) ) { proxy hosts . put ( entry . get key ( ) , new machine list ( entry . get value ( ) ) ) ; } }
try ( resource a = new resource ( ref a ) ; resource b = new resource ( ref b ) ; ) { assert that ( a . equals ( b ) , a . equals ( b ) , is ( true ) ) ; }
read info header = null ;
int pos = random . next int ( 1 , a . remaining ( ) - 5 ) ; a . position ( pos ) ; b . limit ( bytes . length - 1 ) . position ( pos ) ; assert . assert true ( byte buffer util . starts with ( a , b ) ) ;
double this double , other double ;
new blank key checker ( ) { @ override public void call ( ) throws exception { memcached client . replace ( , 0 , 1 ) ; } } . check ( ) ;
int replacement handle = write object internal ( repl obj , false , false , compute stream replacement ) ;
log . w ( tag , camera rejected even safe - mode parameters no configuration ) ;
_ map = new concurrent hash map < k , v > ( initial entries , 0 . 8f , 4 ) ; _ max entries = max entries ; }
application id application1 = tester . make application id ( ) ; tester . prepare ( application1 , cluster spec . request ( cluster spec . type . content , cluster spec . id . from ( my content ) , version . from string ( 6 . 100 ) ) , 6 , 2 , flavor . canonical name ( ) ) ; fail ( two groups have been allocated to the same parent host ) ; }
return deployment ;
resource test capability3 = resource . new instance ( 4000 , 4 ) ;
object mapper mapper = neural net configuration . mapper ( ) ; annotated class ac = annotated class . construct ( layer . class , mapper . get serialization config ( ) . get annotation introspector ( ) , null ) ; collection < named type > types = mapper . get subtype resolver ( ) . collect and resolve subtypes ( ac , mapper . get serialization config ( ) , mapper . get serialization config ( ) . get annotation introspector ( ) ) ;
for ( int ii = 0 ; ii < argument ois . size ( ) ; + + ii ) { args [ ii ] = argument ois . get ( ii ) ; } generic udafparameter info param info = new simple generic udafparameter info ( args , is windowing , is distinct , is all columns ) ;
assert points ( points , sub . class , private at and public g , private gand public at , at first then g , g first then at ) ; points = injection point . for instance methods and fields ( sub sub . class ) ;
base url = null ; } }
string ls = s . trim ( ) . to lower case ( locale . root ) ; if ( ls = s ) { for ( int i = 0 ; i < list . length ; + + i ) { if ( ls . equals ( list [ i ] ) ) return i ; } } return - 1 ;
case ' \ u093 c ' : len = delete ( s , i , len ) ; i - - ; break ; case ' \ u0929 ' : s [ i ] = ' \ u0928 ' ; break ; case ' \ u0931 ' : s [ i ] = ' \ u0930 ' ; break ; case ' \ u0934 ' : s [ i ] = ' \ u0933 ' ; break ; case ' \ u0958 ' : s [ i ] = ' \ u0915 ' ; break ; case ' \ u0959 ' : s [ i ] = ' \ u0916 ' ; break ; case ' \ u095 a ' :
assert null ( internal subchannel . obtain active transport ( ) ) ;
if ( is label float ) animate floating label ( get skinnable ( ) . get text ( ) . is empty ( ) ) ;
enable dependants ( get extension ( row ) , ( boolean ) value ) ;
for ( int i = 0 ; i < threads . length ; i + + ) { threads [ i ] . start ( ) ; }
callback . done ( done ) ; } } } ) ; } finally {
int next byte = bytes . get unsigned byte ( cur offset + + ) ; if ( next byte = byte ops . dup ) break ;
if ( bw = null ) { bw . close ( ) ; } } }
configuration conf2 = new configuration ( conf ) ;
{ global properties gp = new global properties ( ) ; gp . set hash partitioned ( new field list ( 0 ) ) ; local properties lp = local properties . for grouping ( new field list ( 2 , 1 ) ) ; requested global properties rgp = new requested global properties ( ) ; rgp . set hash partitioned ( new field list ( 3 ) ) ; requested local properties rlp = new requested local properties ( ) ; rlp . set grouped fields ( new field list ( 77 , 69 ) ) ; to join1 . set ship strategy ( ship strategy type . partition _ hash , new field list ( 88 ) , data exchange mode . pipelined ) ; to join2 . set ship strategy ( ship strategy type . forward , data exchange mode . pipelined ) ; to after join . set required global props ( rgp ) ; to after join . set required local props ( rlp ) ; feedback properties meet requirements report report = after join . check partial solution properties met ( target , gp , lp ) ; assert equals ( not _ met , report ) ; }
cr = client . call procedure ( decode , 4 ) ;
bmp . recycle ( ) ;
error page error handler error mapper = new error page error handler ( ) ; error mapper . add error page ( 500 , errorpage ) ; app . set error handler ( error mapper ) ; try { server . start ( ) ; string host = connector . get host ( ) ; if ( host = = null ) { host = localhost ; } int port = connector . get local port ( ) ; uri server uri = new uri ( http , null , host , port , test , null , null ) ; make call to test handler http urlconnection connection = ( http urlconnection ) server uri . to url ( ) . open connection ( ) ; try { connection . set allow user interaction ( false ) ; log response status code int status code = connection . get response code ( ) ; log . info ( response status code : { } , status code ) ; if ( status code = = 200 ) { collect response message and log it string content = get response content ( connection ) ; log . info ( response content : { } , content ) ; } } finally { connection . disconnect ( ) ; } assert request log ( capture log ) ; } finally { server . stop ( ) ; }
hr = proxy . query interface ( new refiid ( iid _ idispatch ) , interface pointer ) ;
json generator . write end object ( ) ;
new file ( new path ( dir4 ) . to uri ( ) . get path ( ) ) . set read only ( ) ;
execution environment env = execution environment . get execution environment ( ) ; data set < tuple2 < long , long > > data = env . from elements ( 33 l , 44 l ) . map ( new map function < long , tuple2 < long , long > > ( ) { @ override public tuple2 < long , long > map ( long value ) { return new tuple2 < long , long > ( value , value ) ; } } ) ;
this . dispatcher . get event handler ( ) . handle ( new application localization event ( localization event type . destroy _ application _ resources , this ) ) ;
string [ ] args = new string [ ] { export table name , fq _ output _ dir , 1000 } ;
stor ioexception expected = ( stor ioexception ) test subscriber . get on error events ( ) . get ( 0 ) ; illegal state exception cause = ( illegal state exception ) expected . get cause ( ) ;
system . err . println ( no property found for constant : + constant ) ; return 0 ; }
if ( m vectors [ 1 ] > 1 ) m vectors [ 1 ] = 1f ; if ( m vectors [ 1 ] < - 1 ) m vectors [ 1 ] = - 1f ; if ( m vectors [ 2 ] > 1 ) m vectors [ 2 ] = 1f ;
vargs . add ( app master main class ) ;
if ( stick . get axis count ( ) = = 0 ) { logger . log ( level . fine , not a joystick : { 0 } , c ) ; continue ; } joystick index . put ( c , stick ) ;
val1 . set value ( 21 . 56 ) . set exists ( true ) ; val2 . set value ( 21 . 56 ) . set exists ( true ) ; assert equals ( true , func . get boolean ( ) ) ; assert true ( func . exists ( ) ) ; val1 . set value ( 21 . 56 ) . set exists ( true ) ; val2 . set value ( 21 . 57 ) . set exists ( true ) ; assert equals ( false , func . get boolean ( ) ) ; assert true ( func . exists ( ) ) ; }
assert . assert equals ( 1 , node status updater . get container statuses ( ) . size ( ) ) ; assert . assert equals ( 1 , node status updater . get container statuses ( ) . size ( ) ) ; }
{ error msg . jaxp _ no _ result _ err , result - objekt som \ u00 f6verf \ u00 f6rdes till ' ' { 0 } ' ' \ u00 e4r ogiltigt . } ,
try { handler . warn ( xslterror resources . wg _ found _ curlybrace , null ) ; found \ } \ but no attribute template open ) ; } catch ( org . xml . sax . saxexception se ) { throw new transformer exception ( se ) ; } buffer . append ( } ) ;
float xs = old quaternion . get x ( ) * s ;
else to set . set bit ( f position ) ;
data input stream data input stream = new data input stream ( input stream ) ;
return is handler in chain ( ( ( handler wrapper ) current ) . get handler ( ) , handler ) ;
if ( v instanceof map ) return create document from map ( embedded type , ( map < string , object > ) v ) ; else if ( omulti value . is multi value ( v ) ) { final list set = new array list ( ) ; for ( object o : omulti value . get multi value iterable ( v ) ) { if ( o instanceof map ) { final odocument doc = create document from map ( embedded type , ( map < string , object > ) o ) ; set . add ( doc ) ; } else if ( o instanceof oidentifiable ) set . add ( ( ( oidentifiable ) o ) . get record ( ) ) ; else set . add ( o ) ; } v = set ; } break ;
complex [ ] a = fft ( x ) ;
when skip send to endpoint definition new when = new when skip send to endpoint definition ( ) ; new when . set expression ( when . get expression ( ) ) ; new when . set id ( when . get id ( ) ) ; new when . set inherit error handler ( when . is inherit error handler ( ) ) ; new when . set parent ( when . get parent ( ) ) ; new when . set other attributes ( when . get other attributes ( ) ) ; new when . set description ( when . get description ( ) ) ;
module path = t . get input ( ) . get path ( ) . resolve module as path ( import name ) ;
try { index service = indices service . create index ( index , empty list ( ) ) ; indices to close . add ( index . get index ( ) ) ; } catch ( ioexception e ) { throw new elasticsearch exception ( failed to create temporary index for parsing the alias , e ) ; } index service . mapper service ( ) . merge ( index , mapper service . merge reason . mapping _ recovery , false ) ; } indices . put ( action . get index ( ) , index service ) ; }
list < serializable > gpu res1 = arrays . as list ( 1 , 2 , 4 ) ;
a file store . usable space = 100000 ; b file store . usable space = 10000 ; map < path , integer > data path to shard count = new hash map < > ( ) ;
if ( exp = null & & client exceptions util . is meta clearing exception ( exp ) ) { assert null ( conn . get cached location ( table _ name , row ) ) ; } else if ( success ) { assert not null ( conn . get cached location ( table _ name , row ) ) ; }
if ( tab to cache = = null ) return ;
set < class < ? extends idisconf update pipeline > > i disconf update pipeline = reflections . get sub types of ( idisconf update pipeline . class ) ; if ( i disconf update pipeline = null & & i disconf update pipeline . size ( ) = 0 ) { scan model . seti disconf update pipeline ( ( class < idisconf update pipeline > ) i disconf update pipeline . to array ( ) [ 0 ] ) ; } return scan model ;
if ( stream . read ( ) = 1 ) { throw new ioexception ( no asf ) ; non - nls - 1 }
support . fire property change ( container , old engine , this . engine ) ;
if ( aspect . is default package ( ) ) { top of file . append ( package ) . append ( aspect . get package ( ) . get fully qualified package name ( ) ) . append ( ; ) . append ( get new line ( ) ) ; top of file . append ( get new line ( ) ) ; }
assert false ( exception utils . is jvm fatal error ( new linkage error ( ) ) ) ;
permission cm = new cumulative permission ( ) . set ( base permission . read ) . set ( base permission . administration ) ;
append tokens ( where , tokens ) ; }
common hidden stream token monitored = la ( 1 ) ;
if ( packages = null & & packages . length > 0 & & class name . contains ( . ) & & class _ cache . contains key ( class name ) ) { for ( string pkg : packages ) { try { return _ for name ( pkg + . + class name ) ; } catch ( class not found exception e2 ) { } } try { return _ for name ( java . lang . + class name ) ; } catch ( class not found exception e2 ) { } } try { return _ for name ( class name ) ; } catch ( class not found exception e ) { inner class int i = class name . last index of ( ' . ' ) ; if ( i > 0 & & i < class name . length ( ) - 1 ) { try { return _ for name ( class name . substring ( 0 , i ) + + class name . substring ( i + 1 ) ) ; } catch ( class not found exception e2 ) { } } throw new illegal state exception ( e . get message ( ) , e ) ; }
odatabase document tx graph ;
return router web service . get cluster metrics info ( ) ; }
enum hash bi map < currency , string > empty bimap = enum hash bi map . create ( currency . class ) ; enum hash bi map < currency , string > bimap3 = enum hash bi map . create ( empty bimap ) ; assert equals ( bimap3 , empty bimap ) ; }
return dtmfilter . show _ text | dtmfilter . show _ cdata _ section ;
final float second bar top right = lerp ( 2 * bar width + bar dist , bar width + bar dist , m progress ) ;
socket . configure blocking ( false ) ; socket sock = socket . socket ( ) ; socket properties . set properties ( sock ) ; nio channel channel = nio channels . pop ( ) ;
assert equals ( 0 , map . size ( ) ) ;
response = client ( ) . admin ( ) . indices ( ) . prepare get field mappings ( indexa ) . set types ( type a ) . set fields ( field1 , obj . subfield ) . get ( ) ;
ark = ark . add packages ( true , org . apache . camel . itest . springboot ) ; ark = add springboot package ( ark , org . apache . camel . itest . springboot ) ;
if ( reserved keys . is empty ( ) & & config keys . is empty ( ) ) return s ;
logger . debug ( failed to send stomp error to client , ex ) ;
br = initialize stream ( ) ; }
return instance _ size + data stream . get retained bytes ( ) + present stream . get retained bytes ( ) ;
container impl container1 = create mock container ( user , 1 ) ;
response = get response ( ) ; assert equals ( response . get class ( ) , command success . class ) ; assert equals ( ( ( command success ) response ) . get request id ( ) , 2 ) ;
assert that ( create collection ( hash set . class , 0 ) , is ( instance of ( hash set . class ) ) ) ;
while ( left < hi ) { int middle = ( left + hi ) > > > 1 ; float middle value = a [ middle ] ; if ( middle value < 0 . 0f ) { left = middle + 1 ; } else { hi = middle ; } }
if ( activity index = = invalid _ activity _ index ) { activity index = m activity list . get ( 0 ) . m activity index ; activity entry new entry = new activity entry ( activity index , webapp id ) ; m activity list . set ( 0 , new entry ) ; } mark activity used ( activity index , webapp id ) ; return activity index ; }
file plugin home directory = new file ( plugin contents directory , runtime home directory . get name ( ) ) ; directory scanner directory scanner = runtime . get directory scanner ( get project ( ) ) ;
assert true ( this . driver . get current url ( ) . starts with ( http : localhost : 8081 test - app ) ) ; broker server rule . stop session ( session , true ) ; session = broker server rule . start session ( ) ;
set current page ( get current page ( ) - 1 ) ;
min frequencies . peek ( ) . clear ( ) ; }
boolean is perfect square = ( floor * floor = = x ) ;
request record pending head = pending requests . poll ( ) ; if ( pending head = = null ) { idle connections . add ( connection ) ;
start pointers [ block count ] = fields index in . read vlong ( ) ; avg chunk sizes [ block count ] = fields index in . read vlong ( ) ; final int bits per start pointer = fields index in . read vint ( ) ; if ( bits per start pointer > 64 ) { throw new corrupt index exception ( corrupted bits per start pointer : + bits per start pointer , fields index in ) ; } start pointers deltas [ block count ] = packed ints . get reader no header ( fields index in , packed ints . format . packed , packed ints version , num chunks , bits per start pointer ) ; + + block count ;
json = { \ n + \ type \ : \ string dictionary \ , \ n + \ byte order \ : \ little _ endian \ , \ n + \ bitmap serde factory \ : { \ type \ : \ roaring \ } \ n + } ; serde = ( dictionary encoded column part serde ) mapper . read value ( mapper . write value as string ( mapper . read value ( json , column part serde . class ) ) , column part serde . class ) ;
uri ret uri2 = urihelper . relativize uri ( uri1 , uri1 ) ;
final list < duration > duration list = new array list < duration > ( ) ; duration list . add ( trial phase . get duration ( ) ) ; final date time start discount phase = test subscription helper . add duration ( subscription . get start date ( ) , duration list ) ; final duration ctd = test util . get duration month ( 1 ) ; final date time new charged through date = test subscription helper . add duration ( start discount phase , ctd ) ; subscription internal api . set charged through date ( subscription . get id ( ) , new charged through date , internal call context ) ; subscription = ( default subscription base ) subscription internal api . get subscription from id ( subscription . get id ( ) , internal call context ) ;
string out = workspace . run buck command ( run , binary . to string ( ) ) . get stdout ( ) ;
consumer consumer2 = new consumer ( sub , sub type . exclusive , topic . get name ( ) , 2 * consumer id * , 0 , cons2 * consumer name * , 50000 , server cnx , myrole - 1 ) ;
tags . add all ( kv creator . get visibility expression resolver ( ) . create visibility exp tags ( cell visibility . get expression ( ) ) ) ;
attributes . remove ( file attribute . acl ) ; path src = new path ( tmp src2 ) ;
execute ( delete from % s where a = ? and b = ? and c = ? , 2 , - 1 , 0 ) ;
cache builder . maximum size ( size ) ;
block [ ] stat blocks = stat . get blocks ( ) ;
path p = new path ( test atime update ) ; dfstest util . create file ( cluster . get file system ( ) , p , 0 , ( short ) 1 , 0 l ) ; fs . set times ( p , - 1 l , 123456 l ) ;
s = new scanner ( 123 456 ) ; assert equals ( 38 , s . next long ( 5 ) ) ; try { s . next long ( 5 ) ; fail ( ) ; } catch ( input mismatch exception expected ) { }
tabular data table = ( tabular data ) open mbean . get attribute ( statistics table ) ; assert true ( table . is empty ( ) ) ; stats . method called ( foo , 100 , true ) ;
string out = e . get in ( ) . get body ( string . class ) ; assert equals ( should be empty , , out ) ; }
ticker . advance ( 1 , milliseconds ) ; assert that ( key set ) . contains exactly ( 4 , 5 , 6 , 7 , 8 , 9 , 0 , 1 , 2 ) ;
card card = new custom card ( get activity ( ) ) ;
map < string , properties > eips = context . find eips ( ) ; tabular data answer = new tabular data support ( camel open mbean types . list eips tabular type ( ) ) ;
for ( final compiler input input : input list ) { future list . add ( executor service . submit ( new runnable ( ) { @ override public void run ( ) { input . get ast root ( compiler ) ; } } ) ) ; } pool executor . shutdown ( ) ;
assert that ( spi . get register ( 0 ) . get value ( ) , is ( equal to ( 11 ) ) ) ; assert that ( spi . get register ( 1 ) . get value ( ) , is ( equal to ( 10 ) ) ) ; }
cluster . start data nodes ( conf , 2 , true , null , null ) ; cluster . wait active ( ) ;
set < string > names = get names ( rows ) ; map < string , string > answer = new linked hash map < > ( ) ;
case 4 : m temp rect . offset ( touch x - m previous touch x , touch y - m previous touch y ) ; if ( m temp rect . left > get left ( ) & & m temp rect . top > get top ( ) & & m temp rect . right < get right ( ) & & m temp rect . bottom < get bottom ( ) ) { m crop view rect . set ( m temp rect ) ; update grid points ( ) ; post invalidate ( ) ; } return ;
spans = nested span near query . create weight ( searcher , false , 1f ) . get spans ( searcher . get index reader ( ) . leaves ( ) . get ( 0 ) , span weight . postings . payloads ) ; assert true ( spans is null and it shouldn ' t be , spans = null ) ; check spans ( spans , 2 , new int [ ] { 3 , 3 } ) ; close index reader . close ( ) ; directory . close ( ) ; }
general dataset < string , score phrase measures > newdataset = new dataset < > ( ) ;
if ( danmaku . underline color = 0 ) { paint line paint = displayer config . get underline paint ( danmaku ) ; float bottom = _ top + danmaku . paint height - displayer config . underline _ height ; canvas . draw line ( _ left , bottom , _ left + danmaku . paint width , bottom , line paint ) ; }
assert equals ( hello world , exchange . get in ( ) . get body ( ) ) ; assert equals ( 123 , exchange . get in ( ) . get header ( bar ) ) ; assert null ( exchange . get in ( ) . get header ( foo ) ) ; }
result = shift int bits ( bits & float . mantissa _ mask , digits ) ; } else {
float point distance from line = point . subtract ( p ) . length ( ) , maximum distance = 0 ; if ( cosinus < 0 ) { checking if the distance from p to point is inside the half sphere defined by head envelope compute the distance from the line to the half sphere border maximum distance = bone head radius ; } else if ( head to point length < bone length ) { compute the maximum available distance if ( bone tail radius > bone head radius ) { compute the distance from head to p float head to pdistance = p . subtract ( head ) . length ( ) ; from tangens function we have float x = head to pdistance * ( ( bone tail radius - bone head radius ) bone length ) ; maximum distance = x + bone head radius ; } else if ( bone tail radius < bone head radius ) { compute the distance from head to p float tail to pdistance = p . subtract ( tail ) . length ( ) ; from tangens function we have float x = tail to pdistance * ( ( bone head radius - bone tail radius ) bone length ) ; maximum distance = x + bone tail radius ; } else { maximum distance = bone tail radius ; } } else { checking if the distance from p to point is inside the half sphere defined by tail envelope maximum distance = bone tail radius ; } return point distance from line < = maximum distance + distance ;
executor service executor service = executors . new fixed thread pool ( producer _ count ) ;
meta table accessor . add location ( put , sn , sn . get startcode ( ) , - 1 , 2 ) ; meta . put ( put ) ;
int ii = i & 255 ;
menu . find item ( r . id . audio _ stuff ) . set icon ( get toolbar icon ( google material . icon . gmd _ audiotrack ) ) ; menu . find item ( r . id . video _ stuff ) . set icon ( get toolbar icon ( google material . icon . gmd _ switch _ video ) ) ; menu . find item ( r . id . text _ stuff ) . set icon ( get toolbar icon ( google material . icon . gmd _ subtitles ) ) ; menu . find item ( r . id . action _ share ) . set icon ( get toolbar icon ( google material . icon . gmd _ share ) ) ; menu . find item ( r . id . rotate _ layout ) . set icon ( get toolbar icon ( google material . icon . gmd _ screen _ lock _ rotation ) ) ; return true ; }
if ( labels . is empty ( ) ) { continue ; }
if ( initially running ) { update the topology since the state should have changed from running to paused updated topology = deactivate topology ( state manager , updated topology , proposed packing plan ) ; }
double lower val = get range ( ) . get lower bound ( ) ;
capabilities . put ( webdriver . remote . sessionid , session . get session id ( ) . to string ( ) ) ; return describe session ( capabilities ) ;
m role container . set background ( null ) ;
l + + ;
interaction vec = new interaction wrapped vec ( fr . any vec ( ) . group ( ) . add vec ( ) , fr . any vec ( ) . _ row layout , null , null , false , true , false , fr . vec ( 0 ) . _ key , fr . vec ( 4 ) . _ key ) ;
verify run ( select * from + db name + _ dupe . virtual _ view2 , empty , driver mirror ) ;
for ( int i = 0 ; i < 503 ; i + + ) { endpoint e = context . get endpoint ( seda : queue : + i ) ; template . send body ( e , hello ) ; }
int min length = flags . get padding presence field length ( ) + int _ field _ length ;
boolean fail = false ;
while ( old cursor < old item count ) { abstract map . simple entry < integer , t > old indexed item = adjusted old indexed items [ old cursor + + ] ; int deleted index = old indexed item . get key ( ) ; int deleted offset = get item offset or index ( deleted index , old indexed item . get value ( ) ) ; this . patch operation list . add ( new patch operation < t > ( patch operation . op _ del , deleted index ) ) ; mark deleted index or offset ( this . old to patched index map , deleted index , deleted offset ) ; }
equal key series count + + ;
int x = scaled x + ( int ) ( w * a x ) ;
list < application state > app states = get paragraph ( paragraph id ) . get all application states ( ) ;
assert matches ( die â sÃ¤tze zum testen . , 1 ) ;
check blocking states dao ( cancelled base entitlement , cancelled add on entitlement , base effective cancellation or change date , true ) ;
int equals = arg . index of ( ' = ' ) ;
if ( ensure visible & & mode = = notebook queue unit . exec _ mode _ batch ) ensure visible = false ; outputs _ . get ( chunk id ) . get output widget ( ) . show chunk output ( event . get output ( ) , mode , notebook queue unit . exec _ scope _ partial , queue _ . is chunk executing ( chunk id ) , ensure visible ) ;
modifier . rollback ( ) ;
map < string , string > config params = new hash map < string , string > ( ) ; url util . parse query parameters ( uri . get query ( ) , false , config params ) ; host details . put all ( config params ) ; transaction txn = transaction . current txn ( ) ;
_ y = _ dh . get my public value bytes ( ) ; byte xy [ ] = new byte [ _ x . length + _ y . length ] ; system . arraycopy ( _ x , 0 , xy , 0 , _ x . length ) ; system . arraycopy ( _ y , 0 , xy , _ x . length , _ y . length ) ; hash hxy = _ context . sha ( ) . calculate hash ( xy ) ; _ ts b = ( _ context . clock ( ) . now ( ) + 500 ) 1000l ; our ( bob ' s ) timestamp in seconds byte to encrypt [ ] = new byte [ hxy . get data ( ) . length + ( 4 + 12 ) ] ; system . arraycopy ( hxy . get data ( ) , 0 , to encrypt , 0 , hxy . get data ( ) . length ) ; byte ts b [ ] = data helper . to long ( 4 , _ ts b ) ; system . arraycopy ( ts b , 0 , to encrypt , hxy . get data ( ) . length , ts b . length ) ;
injector . get instance ( persist service . class ) . start ( ) ; injector . get instance ( unit of work . class ) . begin ( ) ; }
compact equals ( sf create ( too big , too big ) * empty * ) ;
if ( peer . length ( ) > = 520 & & peer lc . ends with ( . i2p ) ) { try { destination d = new destination ( ) ; d . from base64 ( peer . substring ( 0 , peer . length ( ) - 4 ) ) ; return d . calculate hash ( ) ; } catch ( data format exception dfe ) { } }
assert equals ( 2 , regions . length ) ;
assert that ( jobs , not ( has key ( job id3 ) ) ) ;
{ match ( ' * ' ) ; if ( state . failed ) return ; } state . type = _ type ;
table table = referenced entity . get table ( ) ; if ( id col itr . has next ( ) ) { log . debug ( no column in the identifier ) ; } while ( id col itr . has next ( ) ) { boolean match = false ;
store . remove oldest checkpoint ( ) ;
mat source color = imread ( args [ 0 ] ) ; mat source grey = new mat ( source color . size ( ) , cv _ 8 uc1 ) ; cvt color ( source color , source grey , color _ bgr2 gray ) ;
camera resolution = new point ( ( screen resolution . x > > 3 ) < < 3 , ( screen resolution . y > > 3 ) < < 3 ) ;
num fitted columns = 1 ;
fs permission perms = fsutils . get file permissions ( fs , conf , hconstants . data _ file _ umask _ key ) ;
final document document = document _ builder . new document ( ) ; final configuration mock configuration = mock ( configuration . class ) ; when ( mock configuration . get configuration ( ) ) . then return ( xml utils . string to element ( execution _ configuration _ xml ) ) ; final execution execution = new execution ( id , phase , mock configuration , goals ) ;
file system fs = file system . get ( ( new path ( tbl . get sd ( ) . get location ( ) ) ) . to uri ( ) , hive conf ) ; assert false ( old table location still exists , fs . exists ( new path ( tbl . get sd ( ) . get location ( ) ) ) ) ; assert true ( data did not move to new location , fs . exists ( new path ( tbl3 . get sd ( ) . get location ( ) ) ) ) ; if ( is thrift client ) { assert equals ( alter table didn ' t move data correct location , tbl3 . get sd ( ) . get location ( ) , tbl2 . get sd ( ) . get location ( ) ) ; }
stmt . execute ( set hive . support . concurrency = false ) ;
spark . messages ( ) . post ( msg ) ; logger . debug ( successfully sent message ' { } ' , msg . get markdown ( ) ) ; return true ;
for ( int k = 0 ; k < thisread ; k + + ) { assert true ( file is corrupted at or after byte + ( file size - bytes to read ) , b [ k ] = = compb [ k ] ) ; }
m list = new wrapper view list ( context ) ;
final named operation detail extended named operation = new named operation detail . builder ( ) . operation name ( op name ) . description ( standard operation ) . operation chain ( { \ operations \ : [ { \ class \ : \ uk . gov . gchq . gaffer . operation . impl . get . get all elements \ } , { \ class \ : \ uk . gov . gchq . gaffer . operation . impl . limit \ , \ result limit \ : \ { param1 } \ } ] } ) . parameters ( param detail map ) . build ( ) ; given ( cache . get named operation ( op name , user ) ) . will return ( extended named operation ) ;
assert equals ( test . get ( - 2080272129 ) , test ) ;
if ( first = null & & ( first . is this call ( ) ) ) return ; list < statement > statements = new array list < statement > ( ) ;
} else if ( new field name . equals ( field name ) ) {
return m current page index ; }
wait for database status ( 0 , europe - 1 , get database name ( ) , odistributed server manager . db _ status . not _ available , 30000 ) ; assert database status equals ( 0 , europe - 1 , get database name ( ) , odistributed server manager . db _ status . not _ available ) ; assert database status equals ( 0 , europe - 0 , get database name ( ) , odistributed server manager . db _ status . online ) ; wait for database status ( 1 , europe - 0 , get database name ( ) , odistributed server manager . db _ status . not _ available , 90000 ) ;
collection util . intro sort ( snippets , ( o1 , o2 ) - > double . compare ( o2 . get score ( ) , o1 . get score ( ) ) ) ;
if ( cd = null ) { solr cores . remove core descriptor ( cd ) ; cores locator . delete ( this , cd ) ; } return ;
result point [ ] points = result . get result points ( ) ;
if ( this . paused ) paused condition . signal all ( ) ; pause lock . unlock ( ) ; }
verify draw ( 2 , char _ height 2 ) ; ticker column . set target char ( ' ' ) ; set progress ( 0 . 5f ) ;
if ( period in minutes < = 0 ) { throw new hadoop illegal argument exception ( non - positive period value : + period in minutes + . the cleaner period must be greater than or equal to zero . ) ; } return period in minutes ;
ready . count down ( ) ;
this . offset = base offset = = - 1 ? base offset : base offset + relative offset ;
test cli frontend = new custom yarn test cli ( directory path . get absolute path ( ) , final application status . succeeded ) ; run options options = cli frontend parser . parse run command ( new string [ ] { - yid , test _ yarn _ application _ id . to string ( ) } ) ;
throw index ( n ) ; }
result = client . call procedure ( select , allow _ nulls , 0 ) . get results ( ) ;
double max = ( ( ( ( double ) nanos _ per _ call ) nanos _ in _ one _ sec ) * iter count ) * tolerance ;
field . set accessible ( true ) ; for ( final map . entry < class < ? > , object > entry : map . entry set ( ) ) { if ( entry . get key ( ) . is assignable from ( field . get type ( ) ) ) { object value = entry . get value ( ) ; treat factories as a special case for lazy load of the objects . if ( value instanceof injector object factory ) { value = ( ( injector object factory < ? > ) value ) . create ( ) ; inject ( value ) ; } field . set ( object , value ) ; accept = true ; break ; } } return accept ;
target node < ? , ? > target node = target graph . get ( dep target ) ; elements . add ( library factory . get library ( target node ) . or else ( null ) ) ; } return elements . stream ( ) ; } ) . filter ( objects : : non null ) . collect ( more collectors . to immutable set ( ) ) ; }
return replication partitions list ;
hm obj = get hm obj ( query ( params ( base params , facet params . facet _ heatmap _ geom , buffer ( point ( 110 40 ) , 7 ) , facet params . facet _ heatmap _ level , 7 ) ) ) ;
if ( account manager . size ( ) > 0 ) { current account = account manager . get ( 0 ) ; notify account data changed ( ) ; }
assert not null ( json request . class . get constructor ( string . class , string . class , response . listener . class , response . error listener . class ) ) ; assert not null ( json request . class . get constructor ( int . class , string . class , string . class , response . listener . class , response . error listener . class ) ) ; assert not null ( json array request . class . get constructor ( string . class , response . listener . class , response . error listener . class ) ) ;
operator state handles snapshot = harness . snapshot ( 0 l , 0 l ) ; harness . close ( ) ; harness = get cep test harness ( false ) ;
when ( method . get modifiers ( ) ) . then return ( 1025 ) ; assert true ( method filters . modifier ( modifier . public ) . accept ( method ) ) ; assert true ( method filters . modifier ( modifier . abstract ) . accept ( method ) ) ; assert false ( method filters . modifier ( modifier . final ) . accept ( method ) ) ;
break ; } total count + = count ; } while ( total count < = 10 ) ;
if ( throwable instanceof error ) { throw ( error ) throwable ; } return result ; }
if ( filter = null & & filter . accept ( data ) ) return ;
assert equals ( 1 , realm . where ( null types . class ) . greater than or equal to ( null types . field _ date _ null , new date ( 10000 ) ) . count ( ) ) ;
assert equals ( - 1 . 3 , searcher . doc ( td . score docs [ 0 ] . doc ) . get ( value ) ) ; assert equals ( 4 . 2 , searcher . doc ( td . score docs [ 1 ] . doc ) . get ( value ) ) ; assert equals ( 30 . 1 , searcher . doc ( td . score docs [ 2 ] . doc ) . get ( value ) ) ; ir . close ( ) ; dir . close ( ) ; }
synchronized ( _ lock ) { pool stats . lifecycle stats < histogram > lifecycle stats = _ lifecycle . get stats ( ) ; pool stats < histogram > stats = new async pool stats < histogram > ( _ total created , _ total destroyed , _ total create errors , _ total destroy errors , _ total bad destroyed , _ total timed out , _ checked out , _ max size , _ min size , _ pool size , _ sample max checked out , _ sample max pool size , _ idle . size ( ) , new latency metric < histogram > ( _ wait time ) , lifecycle stats ) ; _ sample max checked out = _ checked out ; _ sample max pool size = _ pool size ; return stats ; }
int i longer = longer . get first nonzero digit ( ) ;
mime message = new mime message ( sender . get session ( ) ) ;
this . extend access info = extend access info ; }
return new pair < > ( display options = 0 ? boolean . true : boolean . false , options ) ;
delete async trace ( trace ) ; return ;
init listeners ( ) ;
verify ( installation id , times ( 1 ) ) . clear ( ) ;
hregion info splita = new hregion info ( td . get table name ( ) , bytes . to bytes ( aaa ) , bytes . to bytes ( ccc ) ) ; thread . sleep ( 1001 ) ;
minutes per world unit = ( map width in longitude * 60 ) ( double ) world width ;
assert . assert true ( failed to set last modified for [ + xml + ] , xml . set last modified ( system . current time millis ( ) - 2 * host config . file _ modification _ resolution _ ms ) ) ; return xml ;
state table . release snapshot ( snapshot2 ) ;
new node . set x ( ( float ) ( ( 0 . 01 + math . random ( ) ) * 1000 ) - 500 ) ; new node . set y ( ( float ) ( ( 0 . 01 + math . random ( ) ) * 1000 ) - 500 ) ; return new node ; }
new content . insert ( find _ insertion _ point _ start _ of _ string , find _ insertion _ point _ head _ content ) ;
matrix trans client . start ( ) ;
if ( quanta < 1 | | quanta > 16 ) return 0 ; if ( do place ) { fluid util . destroy block on fluid placement ( world , pos ) ; world . set block state ( pos , get default state ( ) . with property ( level , quanta - 1 ) , 11 ) ; }
write end tag ( true ) ;
return ( ( _ filter = = null ) | | filtering parser delegate . class . is instance ( p ) ) ? p : new filtering parser delegate ( p , _ filter , false , multi value ) ;
col6d = nd4j . create ( new int [ ] { in depth , mini batch , out h , out w , kernel [ 0 ] , kernel [ 1 ] } , ' c ' ) ; col6d permuted = col6d . permute ( 1 , 0 , 4 , 5 , 2 , 3 ) ; indarray epsilon temp = epsilon . permute ( 1 , 0 , 2 , 3 ) ;
sb . append ( ( char ) 0x00 ) ;
perms . put ( e . get key ( ) , list ) ;
int node id = make node identity ( node handle ) ;
read value before key ( 0 ) ;
logger . error ( there is no inputstream to return for + name ) ;
if ( get data type ( ) . equals ( type ) ) return ( field < z > ) this ; else return new cast < z > ( this , type ) ;
this . m textures to be unloaded . remove ( p texture ) ;
float item width ;
assert true ( is cached ( customer . id , customer . class , bought items ) ) ;
timestamp set . add ( instant1 . plus ( duration . of days ( 300 l ) ) . plus millis ( 1 l ) ) ;
json assert . assert equals ( string . format ( { % n + \ class \ : \ uk . gov . gchq . gaffer . sketches . datasketches . sampling . binaryoperator . reservoir items union aggregator \ % n + } ) , json ) ;
for ( discovery discovery : internal cluster ( ) . get instances ( discovery . class ) ) { if ( discovery instanceof zen discovery ) { final zen discovery zen discovery = ( zen discovery ) discovery ; assert busy ( ( ) - > { final cluster state [ ] states = zen discovery . pending cluster states ( ) ; assert that ( zen discovery . cluster state ( ) . nodes ( ) . get local node ( ) . get name ( ) + still having pending states : \ n + stream . of ( states ) . map ( cluster state : : to string ) . collect ( collectors . joining ( \ n ) ) , states , empty array ( ) ) ; } ) ; } }
writer . write start element ( nodesizefactor ) ; writer . write characters ( string . value of ( node size factor ) ) ; writer . write end element ( ) ; writer . write start element ( edgesizefactor ) ;
encoder . flow controller ( ) . write pending bytes ( ) ;
int ret = super . execute ( driver context ) ;
q = new query ( ?query = test _ collapse & collapsefield = amid & summary = placeholder ) ; r = do search ( collapse , q , 0 , 10 , chained ) ; assert equals ( 4 , r . get hit count ( ) ) ;
coordinator . wakeup on join complete = false ; consumer client . poll ( 0 ) ; coordinator . ensure active group ( ) ; assert equals ( 1 , coordinator . on join prepare invokes ) ;
while ( ( hi > lo0 ) & & ( ( ( elem template element ) v . element at ( hi ) ) . compare to ( mid node ) > 0 ) ) { - - hi ; }
result = ( source estimation ) response . get entity ( ) ;
arrays . fill ( buffer , cur , cur + signature . signature _ bytes , ( byte ) 0x0 ) ; cur + = signature . signature _ bytes ; }
if ( modifier . is public ( class to construct . get modifiers ( ) ) ) { constructor . set accessible ( true ) ; }
throw new org . apache . axis2 . databinding . adbexception ( message cannot be null ) ; } else {
cli . get options ( ) . stream ( ) . for each ( option : : ensure validity ) ; cli . get arguments ( ) . stream ( ) . for each ( argument : : ensure validity ) ;
add strict max pool . get ( ejb3 subsystem model . max _ pool _ size ) . set ( max pool size ) ;
this . prefix = prefix . to string ( ) ;
file system fs = split . get path ( ) . get file system ( job ) ;
m controller . set hierarchy ( null ) ;
orc data source = new testing orc data source ( new file orc data source ( temp file . get file ( ) , new data size ( 1 , unit . megabyte ) , new data size ( 1 , unit . megabyte ) , new data size ( 1 , unit . megabyte ) , true ) ) ;
if ( args . listener = null ) { if ( dbg ) log . d ( log _ tag , notifying listener : + args . listener . to string ( ) + image : + args . uri + completed ) ; args . listener . on image load complete ( msg . what , args . cookie , args . view , image present ) ; } break ; default : } }
if ( src inode file . get parent ( ) = target parent ) { throw new hadoop illegal argument exception ( source file + src + is not in the same directory with the target + target iip . get path ( ) ) ; }
metrics . add all ( output row signature . get row order ( ) ) ; }
class reader cr = new class reader ( bytes ) ; false returner ca = new false returner ( is compilable ) ; cr . accept ( ca , 0 ) ; byte [ ] newbytes = ca . get bytes ( ) ; return newbytes ; } }
wait for connections received ( 2 ) ;
final int header second byte = variable run length & 0xff ;
form tester form = tester . new form tester ( form ) ; form . set value ( settings : enabled , true ) ; form . submit ( ) ;
return new offset ; }
try { utx . rollback ( ) ; assert . fail ( should have gone to catch block1 ) ; } catch ( illegal state exception isex ) { assert . assert equals ( cannot locate a transaction for rollback . , isex . get message ( ) ) ; } utx . begin ( ) ;
string uri string = uri . decode ( args [ 1 ] ) ;
router protocol . truncate ( router file , 0 , testclient ) ;
testing client . testing ( ) . remove user session ( test , session id ) ; oauth . do login ( test - user @ localhost , password ) ; string session id4 = events . expect login ( ) . assert event ( ) . get session id ( ) ; assert not equals ( session id , session id4 ) ; events . clear ( ) ; }
project . get logger ( ) . info ( task is not found + vod . get full name ( ) + mtl task context ) ;
m icon menu view . mark stale children ( ) ;
assert equals ( new integer ( 0 ) , bean util . pojo . get simple property ( fb , fooint ) ) ;
p2 . add comment ( null , null ) ; em . persist ( p2 ) ; assert . fail ( should have gone into catch block . ) ; }
int num keys = ( int ) ( ( float ) starter keys . length * 10 . 33 f ) ; list < byte [ ] > keys = new array list < byte [ ] > ( ) ;
unmarshaller u = jc . create unmarshaller ( ) ; jaxb validation event handler event handler = new org . docx4j . jaxb . jaxb validation event handler ( ) ;
assignor . on assignment ( assignment ) ;
queue metrics user metrics = queue metrics . get user metrics ( app . get user ( ) ) ;
string query string = query . get query ( ) ; matcher matcher = targets pattern . matcher ( query string ) ; string builder builder = new string builder ( ) ; int last end = 0 ; while ( matcher . find ( ) ) { builder . append ( query string . substring ( last end , matcher . start ( ) ) ) ; build target target = build target parser . instance . parse ( matcher . group ( ) , pattern , cell path resolver ) ; optional < build target > translated = translator . translate ( cell path resolver , pattern , target ) ; builder . append ( translated . or else ( target ) . get fully qualified name ( ) ) ; last end = matcher . end ( ) ; } builder . append ( query string . substring ( last end , query string . length ( ) ) ) ; string new query = builder . to string ( ) ; return query string . equals ( new query ) ? optional . empty ( ) : optional . of ( query . of ( new query ) ) ;
derived suites . add ( create submap suite ( parent builder , bound . no _ bound , bound . inclusive ) ) ;
int update flags = force ? iresource . force : iresource . none ; update flags | = keep history ? iresource . keep _ history : iresource . none ; set contents ( source . get contents ( ) , update flags , monitor ) ; }
reader reader = resources . get resource as reader ( org apache ibatis submitted stringlist mybatis - config . xml ) ;
system . arraycopy ( array , index , array , index + 1 , size - index ) ;
acc accumulator = value bytes = = null ? agg function . create accumulator ( ) : value serializer . deserialize ( new data input view stream wrapper ( new byte array input stream with pos ( value bytes ) ) ) ;
m playback kickstarter = new playback kickstarter ( this . get application context ( ) ) ;
managed type = new default mapped super class < x > ( clazz , persistence type . mapped _ superclass , ( abstract identifiable type ) get type ( clazz . get superclass ( ) , is id class ) ) ; on declared fields ( clazz , managed type ) ; mapped super class types . put ( clazz , ( mapped superclass type < ? > ) managed type ) ;
multimap < integer , integer > host to partitions = get host to partition map ( ) ; for ( collection < integer > subgroup : sorted hosts ) { set < integer > partitions = new hash set < integer > ( ) ; for ( integer hid : subgroup ) { partitions . add all ( host to partitions . get ( hid ) ) ; } host log . info ( computing partitions to replace . qualified partitions : + partitions ) ; sort the partitions by replicas number for ( integer pid : partitions ) { reps per part . put ( pid , get replica count for partition ( pid ) ) ; } compute replacement partitions ( reps per part , kfactor , sites per host , partitions to replace ) ; if ( partitions to replace . size ( ) = = sites per host ) { host log . info ( iv2 sites will replicate the following partitions : + partitions to replace ) ; break ; } } return partitions to replace ;
( ( text view ) find view by id ( r . id . emoji _ easter _ egg ) ) . set text color ( get sub text color ( ) ) ;
new ast = analyze create view ( new ast , get qb ( ) , null ) ; if ( new ast = = null ) { log . error ( analyze create table failed to initialize materialized view after cbo ; + new ast is + get ast ( ) . dump ( ) ) ; throw new semantic exception ( analyze create table failed to initialize materialized view after cbo ) ; } return new ast ;
do return ( false ) . when ( mock dir3 ) . add child ( ( inode ) mockito . is null ( ) , any boolean ( ) , mockito . any int ( ) ) ; mockito . when ( mock dir3 . add child ( ( inode ) mockito . is not null ( ) , any boolean ( ) , mockito . any int ( ) ) ) . then return ( false ) . then call real method ( ) ; inode directory root = fsdir . get inode4 write ( ) . as directory ( ) ; root . replace child ( dir3 , mock dir3 , fsdir . get inode map ( ) ) ; foo3 node . set parent ( mock dir3 ) ; try { hdfs . rename ( foo , foo3 , rename . overwrite ) ; fail ( the rename from + foo + to + foo3 + should fail ) ; } catch ( ioexception e ) { generic test utils . assert exception contains ( rename from + foo + to + foo3 + failed . , e ) ; }
exchange . set property ( jpa constants . entity _ manager , em ) ;
slice _ del ( ) ; break ; } } while ( false ) ; cursor = limit - v _ 2 ;
db tester . get db client ( ) . rule dao ( ) . update ( db tester . get session ( ) , rule . set status ( rule status . ready ) . set updated at ( 2000000000000 l ) ) ; under test . commit and index ( db tester . get session ( ) , rule . get key ( ) ) ; assert that ( es tester . count documents ( rule index definition . index _ type _ rule ) ) . is equal to ( 1 ) ;
scopes = new tree map < > ( bytes . bytes _ comparator ) ; scopes . put ( b , hconstants . replication _ scope _ global ) ; user entry = create entry ( scopes , a , b ) ; assert equals ( user entry b , filter . filter ( user entry ) ) ; scopes . put ( a , hconstants . replication _ scope _ local ) ; assert equals ( user entry b , filter . filter ( user entry ) ) ; }
presenter . detach view ( false ) ;
final continuing http servlet request continuing request = new continuing http servlet request ( guice filter . get request ( key . get ( http servlet request . class ) ) ) ; for ( map . entry < key < ? > , object > entry : seed map . entry set ( ) ) { object value = validate and canonicalize value ( entry . get key ( ) , entry . get value ( ) ) ; continuing request . set attribute ( entry . get key ( ) . to string ( ) , value ) ; } return new request scoper ( ) { @ override public closeable scope open ( ) { check scoping state ( null = = guice filter . local context . get ( ) , cannot continue request in the same thread as a http request ) ; return new guice filter . context ( continuing request , continuing request , null ) . open ( ) ; } } ;
action get . to xcontent ( json builder ( ) , to xcontent . empty _ params ) ; }
throw new invalid base64 character exception ( string . format ( bad base64 input character decimal % d in array position % d , ( ( int ) source [ i ] ) & 0x ff , i ) ) ;
node defsite = null ;
file restore1 = new file ( temporary , restore1 ) ; store . restore shard ( uuid1 , restore1 ) ; assert equals ( read all bytes ( file1 . to path ( ) ) , read all bytes ( restore1 . to path ( ) ) ) ;
this . contact email = address electronic mail address ; }
post delayed ( this , item _ caption _ cycle _ delay ) ;
if ( readable = tt ) is = stream utils . limited input stream ( is , len ) ; try { return decode body ( channel , is , header ) ; } finally { if ( is . available ( ) > 0 ) { try { if ( logger . is warn enabled ( ) ) { logger . warn ( skip input stream + is . available ( ) ) ; } stream utils . skip unused stream ( is ) ; } catch ( ioexception e ) { logger . warn ( e . get message ( ) , e ) ; } } }
parser . on file system change ( watchman overflow event . of ( filesystem . get root path ( ) , ) ) ;
set . add ( i ) ;
query = select s from student s where s . is exceptional = ?1 ;
ch . write and flush ( unpooled . buffer ( ) . writer index ( 16 ) ) ;
returned stream = stream ; return ; } }
if ( worklist . contains ( b ) ) { worklist . add ( b ) ; } }
matcher = create matcher ( minutes _ regex _ pattern , replace source ) ;
modifiable solr params p = new modifiable solr params ( ) ; p . add ( collection , rules coll ) ; p . add ( action , modifycollection ) ; p . add ( rule , cores : < 5 ) ; p . add ( rule , node : * , replica : 1 ) ; p . add ( rule , freedisk : > + min gb2 ) ; p . add ( auto add replicas , true ) ; cluster . get solr client ( ) . request ( new generic solr request ( post , collections _ handler _ path , p ) ) ; doc collection rules collection = get collection state ( rules coll ) ;
tester . bad ( . * no semicolon found . * , new ddlitem ( partition table books on column cash ) ) ;
dismiss = ( velocity x < 0 ) = = ( m final delta < 0 ) ;
if ( get flag ( flags , java element labels . d _ post _ qualified ) ) { int offset = f buffer . length ( ) ; ijava element openable = ( ijava element ) declaration . get openable ( ) ; if ( openable = null ) { f buffer . append ( java element labels . concat _ string ) ; append element label ( openable , java element labels . cf _ qualified | java element labels . cu _ qualified | ( flags & qualifier _ flags ) ) ; } if ( get flag ( flags , java element labels . colorize ) ) { f buffer . set style ( offset , f buffer . length ( ) - offset , qualifier _ style ) ; } }
t . get message ( ) ; }
xdr _ int ( 128 ) ; metric _ id = metadata _ msg
try { revoke from namespace using access control client ( test _ util , system user connection , user name , test _ table . get namespace as string ( ) , permission . action . read , permission . action . write ) ; } catch ( throwable e ) { log . error ( error during call of access control client . revoke , e ) ; }
set status ( response context , http status . s _ 400 _ bad _ request ) ; response data . get headers ( ) . clear ( ) ; return completable future . completed future ( null ) ; } } ) . when ( _ filter ) . on error ( any ( throwable . class ) , eq ( _ filter request context ) , any ( filter response context . class ) ) ; do answer ( new answer < object > ( )
v - = 1 < < ( bpv - 1 ) ;
final int given types [ ] = { connectivity manager . type _ wifi , connectivity manager . type _ mobile , connectivity . unknown _ type } ;
if ( full ackcount > absolute _ max _ acks | | partial ackcount > absolute _ max _ acks ) throw new illegal argument exception ( too many acks full partial + full ackcount + ' ' + partial ackcount ) ;
tab view = inflater . inflate ( m tab view layout id , m tab strip , false ) ;
{ http : user @ host : 8080 path info ; param?query fragment , http , host , 8080 , path info ; param , param , query , fragment } , { xxxxx : user @ host : 8080 path info ; param?query fragment , xxxxx , host , 8080 , path info ; param , param , query , fragment } ,
for ( int i = 0 ; i < children . size ( ) - 1 ; i + + ) { byte [ ] payload = callbacks . poll ( ) . get data ( ) ; final host info info = host info . from bytes ( payload ) ; host info map . put ( parse host id ( children . get ( i ) ) , info ) ; } return host info map ; }
case 0 : return new class < ? > [ ] { default . class } ;
assert that ( data types . object . compare value to ( test map , empty map ) , is ( 1 ) ) ; assert that ( data types . object . compare value to ( empty map , test map ) , is ( - 1 ) ) ;
maven bundle ( ) . group id ( javax . ws . rs ) . artifact id ( javax . ws . rs - api ) . version as in project ( ) ,
set error ( error , view compat . is laid out ( this ) & & is enabled ( ) & & ( m error view = = null | | text utils . equals ( m error view . get text ( ) , error ) ) ) ;
output . write ( foobar . get bytes ( ) ) ;
int number of entries = 2 * thread local random . current ( ) . next int ( write _ delete _ batch _ min _ entries 2 , write _ delete _ batch _ max _ entries 2 + 1 ) ; assert is empty ( ) ; assert null ( should not be present in the store , cl . load ( 0 ) ) ; list < marshalled entry < ? , ? > > entries = int stream . range ( 0 , number of entries ) . boxed ( ) . map ( i - > marshalled entry ( i . to string ( ) , val + i , null ) ) . collect ( collectors . to list ( ) ) ; cl . write batch ( entries ) ;
check translation ( foo @ acme . com , foo @ acme . com ) ; check translation ( root joe @ foo . com , root joe @ foo . com ) ; }
helper3 _ fail ( three hunk class , two hunk , true , false , true , renaming name suggestor . strategy _ suffix ) ;
snackbar utils . dismiss transient bottom bar and wait until fully dismissed ( snackbar ) ;
fs . set erasure coding policy ( child dir , ec32 policy . get name ( ) ) ;
rm . get my fifo scheduler ( ) . force resource limit ( resource . new instance ( 2048 , 0 ) ) ;
f entity manager . start document entity ( input source ) ;
int count = - 1 ;
string json = object mapper . write value as string ( bad outer ) ;
assert xpath evaluates to ( string . value of ( legend _ width ) , legend url path ws + @ width , dom ) ; assert xpath evaluates to ( string . value of ( legend _ height ) , legend url path ws + @ height , dom ) ; assert xpath evaluates to ( legend _ format , legend url path ws + wms : format , dom ) ; assert xpath evaluates to ( base + styles gs + image _ url , legend url path ws + wms : online resource @ xlink : href , dom ) ; }
mgmt . build edge index ( connect , weight asc , direction . out , time ) ;
ngroups + + ; } }
} } pat idx end - - ; str idx end - - ; }
assert path exists ( renamed destination file does not exist , renamed ) ; assert path does not exist ( source file found after rename during append : \ n + listing , target ) ;
ts merge = gen map red utils . create temporary table scan operator ( fs input . get compilation op context ( ) , input rs ) ;
assert not null ( child builder . to string ( ) ) ; assert not null ( parent builder . to string ( ) ) ; }
m _ normalization method = determine normalization ( model ) ; set up regression tables ( model , function type ) ;
final atomic boolean out = new atomic boolean ( false ) ;
long count = n ;
return decision ; }
restart nm ( max _ tries ) ; check num of local dirs ( ) ; verify ( del service , times ( 1 ) ) . delete ( arg that ( new file deletion matcher ( del service , null , new path ( resource localization service . nm _ private _ dir + _ del _ ) , null ) ) ) ;
parser = new parsable byte array ( bytes , 4 ) ;
return jre enum ( + class name + , + short name + ) ; } return class name + ' _ ' + short name ; }
configuration conf = util . get configuration ( ) ; conf . set int ( hconstants . hbase _ client _ operation _ timeout , 5000 ) ; conf . set strings ( coprocessor host . region _ coprocessor _ conf _ key , org . apache . hadoop . hbase . coprocessor . column aggregation endpoint . class . get name ( ) , protobuf coprocessor service . class . get name ( ) ) ; conf . set strings ( coprocessor host . master _ coprocessor _ conf _ key , protobuf coprocessor service . class . get name ( ) ) ; util . start mini cluster ( 2 ) ; admin admin = util . get admin ( ) ; htable descriptor desc = new htable descriptor ( test _ table ) ; desc . add family ( new hcolumn descriptor ( test _ family ) ) ; admin . create table ( desc , new byte [ ] [ ] { rows [ row seperator1 ] , rows [ row seperator2 ] } ) ; util . wait until all regions assigned ( test _ table ) ; table table = util . get connection ( ) . get table ( test _ table ) ;
this . idp link email page . assert current ( ) ; assert . assert that ( this . idp link email page . get message ( ) , is ( an email with instructions to link + object util . capitalize ( get provider id ( ) ) + account pedroigor with your + app _ realm _ id + account has been sent to you . ) ) ; assert . assert equals ( 1 , green mail . get received messages ( ) . length ) ;
assert equals ( value , non owner . get advanced cache ( ) . with flags ( flag . force _ write _ lock ) . get ( key ) ) ;
coordinator . sent messages . clear ( ) ; assert . assert equals ( consistent session . state . repairing , coordinator . get state ( ) ) ; coordinator . handle finalize promise ( participant1 , true ) ; assert . assert equals ( consistent session . state . repairing , coordinator . get state ( ) ) ; assert . assert false ( coordinator . fail called ) ; coordinator . handle finalize promise ( participant2 , false ) ;
string res = valid . to string ( ) ; env . from elements ( valid ) ; list < string > result = new linked list < > ( ) ; result . add ( res ) ; expected result = true ; compare result as text ( result , expected result ) ;
default : _ _ last was cr = false ; out . write ( ch ) ; return ; } }
rec1 = create record ( rec1fields ) ; rec2 = create record ( rec2fields ) ; rec2 . update binary represenation ( ) ; rec1 . union fields ( rec2 ) ; check unioned record ( rec1 , rec1fields , rec2fields ) ;
compute timeline duration ( ) ;
connection . start ( ) ; return true ;
if ( logger . is debug enabled ( ) ) logger . debug ( returning < + socket address + > as next address for + account ) ; socket address = lookups [ lookup index ] ; lookup index + + ; return true ; }
if ( mid1 . x = = mid2 . x ) { return new point ( mid1 . x , mid1 . y + ydir * size 2 . 0 ) ; } double slope = ( mid1 . y - mid2 . y ) ( mid1 . x - mid2 . x ) ; double x1 = 0 ;
while ( running ) { synchronized ( wait lock ) { wait lock . wait ( 100 l ) ; } }
sentinel config sentinel config = new sentinel config ( ( sentinel config . builder ) vespa model . get config ( new sentinel config . builder ( ) , localhost config id ) ) ; boolean found = false ; for ( sentinel config . service service : sentinel config . service ( ) ) { if ( logd . equals ( service . name ( ) ) ) { found = true ; } } assert true ( found ) ;
servlet = add servlet ( ctx , jsp , org . apache . jasper . servlet . jsp servlet ) ; servlet . add init parameter ( fork , false ) ; servlet . set load on startup ( 3 ) ; servlet . set overridable ( true ) ;
perform vertical swipe up gesture ( r . id . coordinator _ layout , center x , original appbar bottom , toolbar height ) ;
int parts skipped = ipv6 _ part _ count - ( parts hi + parts lo ) ;
assert true ( check same origin ( mydomain1 . com , - 1 , http : mydomain1 . com ) ) ; assert true ( check same origin ( mydomain1 . com , - 1 , http : mydomain1 . com : 80 ) ) ; assert true ( check same origin ( mydomain1 . com , - 1 , http : mydomain1 . com path ) ) ; assert true ( check same origin ( mydomain1 . com , - 1 , http : mydomain1 . com : 80 path ) ) ; assert false ( check same origin ( mydomain2 . com , - 1 , http : mydomain1 . com ) ) ; assert false ( check same origin ( mydomain2 . com , - 1 , http : mydomain1 . com : 80 ) ) ; assert false ( check same origin ( mydomain2 . com , - 1 , http : mydomain1 . com path ) ) ; assert false ( check same origin ( mydomain2 . com , - 1 , http : mydomain1 . com : 80 path ) ) ;
do return ( true ) . when ( load manager2 ) . is centralized ( ) ;
while ( _ cache . size ( ) > 0 & & ( _ cached files . get ( ) > _ max cached files | | _ cached size . get ( ) > _ max cache size ) ) { scan the entire cache and generate an ordered list by last accessed time . sorted set < cached http content > sorted = new tree set < cached http content > ( new comparator < cached http content > ( ) { public int compare ( cached http content c1 , cached http content c2 ) { if ( c1 . _ last accessed < c2 . _ last accessed ) return - 1 ; if ( c1 . _ last accessed > c2 . _ last accessed ) return 1 ; if ( c1 . _ content length value < c2 . _ content length value ) return - 1 ; return c1 . _ key . compare to ( c2 . _ key ) ; } } ) ; for ( cached http content content : _ cache . values ( ) ) sorted . add ( content ) ; invalidate least recently used first for ( cached http content content : sorted ) { if ( _ cached files . get ( ) < = _ max cached files & & _ cached size . get ( ) < = _ max cache size ) break ; if ( content = = _ cache . remove ( content . get key ( ) ) ) content . invalidate ( ) ; } }
heap for index ( insert index ) . bubble up ( insert index , element ) ; return size < = maximum size | | poll last ( ) = element ; }
queried relation t1 = ( ( queried relation ) mss . sources ( ) . get ( t3 . t1 ) ) ;
assert equals ( active session id , tester . tenant ( ) . get application repo ( ) . get session id for application ( tester . application id ( ) ) ) ; assert equals ( 3 , tester . tenant ( ) . get local session repo ( ) . list sessions ( ) . size ( ) ) ; clock . advance ( duration . of hours ( 1 ) ) ; longer than session lifetime
assert equals ( null , context . get parameter as ( key , long . class ) ) ; assert equals ( new integer ( 100 ) , context . get parameter as ( key1 , integer . class ) ) ;
if ( m _ version check ) { open urlasync ( ) ; } try { if we need to prompt the user for a password , do so . password = cliconfig . read password if needed ( user , password , enter password : ) ; } catch ( ioexception ex ) { print usage ( unable to read password : + ex ) ; }
fsutils . delete ( fs , file , true ) ;
query = select s from student s where s . joining date and time = ?1 ;
if ( config . alt = 1 ) { continue ; } semantic context updated context = config . semantic context . eval precedence ( parser , _ outer context ) ;
local import volume tracker = true ; } else {
m empty text view = ( text view ) m root view . find view by id ( r . id . empty _ view _ text ) ; m empty text view . set typeface ( typeface helper . get typeface ( m context , roboto - light ) ) ; m empty text view . set paint flags ( m empty text view . get paint flags ( ) | paint . anti _ alias _ flag | paint . subpixel _ text _ flag ) ;
final bolt state machine machine = new machine with transaction ( ready ) ;
ensure works ( injector , foo bar . class , bar . class , foo . class ) ;
web server buck event listener . parse started ( any object ( parse event . started . class ) ) ;
rectangle . offset ( child . get left ( ) - child . get scroll x ( ) , child . get top ( ) - child . get scroll y ( ) ) ;
reg . on register ( ) ; reg . start ( ) ; reg . shutdown ( ) ; reg . deregister ( ) ; reg . start ( ) ; assert . fail ( ) ; } catch ( illegal state exception e ) { } } finally
bulk processor . add ( new index request ( ) ) ;
repository factory . initialize ignored names ( runtime environment . get instance ( ) ) ;
if ( last output stream status . is active ( ) & & channel statuses [ channel index ] . stream status . is active ( ) ) { long watermark millis = watermark . get timestamp ( ) ; if the input watermark ' s value is less than the last received watermark for its input channel , ignore it also . if ( watermark millis > channel statuses [ channel index ] . watermark ) { channel statuses [ channel index ] . watermark = watermark millis ; previously unaligned input channels are now aligned if its watermark has caught up if ( channel statuses [ channel index ] . is watermark aligned & & watermark millis > = last output watermark ) { channel statuses [ channel index ] . is watermark aligned = true ; } now , attempt to find a new min watermark across all aligned channels find and output new min watermark across aligned channels ( ) ; } }
assert q ( function queries does not work correctly on tdate fields , req ( q , _ val _ : \ sum ( tdate , 1 . 0 ) \ ) , * [ @ num found = ' 11 ' ] , date [ @ name = ' tdate ' ] [ . = ' + largest date + ' ] ) ;
try { assert true ( latch . await ( 10000 , time unit . milliseconds ) ) ; } catch ( interrupted exception ex ) { fail ( interrupted ex ) ; } system . out . println ( req log : + hystrix request log . get current request ( ) . get executed commands as string ( ) ) ; assert equals ( 0 l , stream . get latest ( ) . get error count ( ) ) ; assert equals ( 0 l , stream . get latest ( ) . get total requests ( ) ) ; }
throw wrapper . exception unavailable ( ) ;
test seek1 ( block _ size + half _ chunk _ size ) ;
final string sei metadata key = sei metadata . create identifier ( sei type details ) ; register dependency ( sei metadata key , metadata identification string ) ; return new sei impl metadata ( metadata identification string , aspect name , governor physical type metadata , get project operations ( ) . get top level package ( ) , endpoint , sei type , sei metadata . get service ( ) , sei methods ) ;
expect throws ( illegal argument exception . class , ( ) - > { new parallel leaf reader ( true , new leaf reader [ 0 ] , new leaf reader [ ] { ir1 } ) ; } ) ; dir1 . close ( ) ;
invalidations . add ( cache key ) ; return get delegate ( ) . get clients ( realm ) ;
template menu item shiny item = new template menu item ( template _ shiny ) ; shiny item . add icon ( new image resource2x ( resources . shiny icon2x ( ) ) ) ; list templates _ . add item ( shiny item ) ;
throwable exception = get execution exception ( ) ;
owsconfiguration configuration = new owsconfiguration ( ) ; encoder encoder = new encoder ( configuration , configuration . schema ( ) ) ; encoder . set indenting ( true ) ; encoder . set indent size ( 2 ) ; encoder . set line width ( 60 ) ; encoder . set omit xmldeclaration ( request . is soap ( ) ) ; string schema location = build schema url ( base url ( request . get http request ( ) ) , ows 1 . 0 . 0 ows exception report . xsd ) ;
if ( file size > 4096 & & multi _ threaded _ digest . get ( ) ) { we ' ll have to read file content in order to calculate the digest . in that case it would be beneficial to serialize those calculations since there is a high probability that md5 will be requested for multiple output files simultaneously . exception is made for small ( < = 4 k ) files since they will not likely to introduce significant delays ( at worst they will result in two extra disk seeks by interrupting other reads ) . digest = get digest in exclusive mode ( path ) ; } else { digest = get digest internal ( path ) ; } preconditions . check not null ( digest , we should have gotten a digest for % s at this point but we still don ' t have one , path ) ;
) { object key = keys . next element ( ) ; string key str = ( string ) key ; try { if ( key str . starts with ( error ) ) { errors = true ; } log msg ( key str + = + subhash . get ( key str ) ) ; } catch ( exception e ) { errors = true ; log msg ( reading - + key + = threw : + e . to string ( ) ) ; } }
em . get transaction ( ) . begin ( ) ; ing1 = em . find ( list join column bidirectional ref ing entity . class , ing1 . get id ( ) ) ; ing2 = em . find ( list join column bidirectional ref ing entity . class , ing2 . get id ( ) ) ; ed1 = em . find ( list join column bidirectional ref ed entity . class , ed1 . get id ( ) ) ;
return math . max ( buffers . size ( ) , 0 ) ;
if ( replace value = null ) { rep val = replace value ; } big decimal b = number utils . to big decimal ( n ) ; big decimal c = number utils . to big decimal ( rep val ) ;
} } } ) ; clp . set behavior ( behavior ) ;
new latency value selector ( true ) , false , false ) ) ; return group infos ; }
rest request request = mux rest request ( immutable map . of ( 2 , r2 ) ) ; count down latch latch = new count down latch ( 1 ) ;
int [ ] bi = new int [ input . length ] ; int found = 0 ; for ( int k = 0 ; k < input . length ; k + + ) { for ( int i = 0 ; i < words ; i + + ) { if ( input [ k ] . equals ( vectors reader . get word ( i ) ) ) { bi [ k ] = i ; system . out . printf ( \ n word : % s position in vocabulary : % d \ n , input [ k ] , bi [ k ] ) ; found + + ; } } if ( found = = k ) { system . out . printf ( % s : out of dictionary word \ n , input [ k ] ) ; } } if ( found < input . length ) { continue ; } float [ ] vec = new float [ size ] ;
creator . thaw ( ) ; creator . freeze ( ) ; global scope = creator . create scope ( root , null ) ; assert true ( global scope . is declared ( a , false ) ) ; assert true ( global scope . is declared ( b , false ) ) ; assert true ( global scope . is declared ( x , false ) ) ; assert true ( global scope . is declared ( y , false ) ) ; assert false ( global scope . is declared ( nonexistant , false ) ) ; compiler . report change to change scope ( script1 ) ; invalidate the original scope .
assert that ( 42 l ) . is equal to ( some static method ( ) ) ;
if ( this . callable = null ) { this . callable . set close ( ) ; callable . with retries ( ) ; this . callable = null ; }
new list . add ( new inet socket address ( inet address . get by address ( new byte [ ] { 10 , 10 , 10 , i } ) , 1234 + i ) ) ;
string client rack = rack2 ;
instructions . add ( reil helpers . create or ( base offset + + , dw , diff11 , dw , diff12 , dw , diff1 ) ) ; instructions . add ( reil helpers . create or ( base offset + + , dw , diff21 , dw , diff22 , dw , diff2 ) ) ; instructions . add ( reil helpers . create or ( base offset + + , dw , diff31 , dw , diff32 , dw , diff3 ) ) ; instructions . add ( reil helpers . create or ( base offset + + , dw , diff41 , dw , diff42 , dw , diff4 ) ) ;
final float title translate = top offset - model . m icon . get height ( ) * title _ margin _ scale _ fraction ;
assert size eventually ( 0 , map , 240 ) ;
upload . set size max ( max request size ) ;
if ( int len = = 0 ) { quotient . int len = quotient . offset = 0 ; return 0 ; } if ( v < 0 ) v = - v ; int d = ( int ) ( v > > > 32 ) ; quotient . clear ( ) ;
this . start prefix mapping ( prefix from raw name , ns , false ) ;
update window sizing ( ) ; int child left ;
log . debug ( fs state after snapshot : ) ; util . get hbase cluster ( ) . get master ( ) . get master file system ( ) . log file system state ( log ) ; snapshot testing utils . confirm snapshot valid ( util , protobuf util . create hbase protos snapshot desc ( snapshots . get ( 0 ) ) , table _ name , test _ fam ) ;
try { validate if avro schema ( key ser def ) ; } catch ( exception e ) { logger . error ( validating key schema failed for store : + store definition . get name ( ) ) ; throw new voldemort exception ( error validating key schema for store : + store definition . get name ( ) + + e . get message ( ) , e ) ; }
if ( pinfo . partition key = = null ) { all matched = false ; }
long k = this . active . key size ( ) ;
signature signature = signature . get instance ( sha1with + m key . get algorithm ( ) ) ; signature . init sign ( m key ) ; m output jar . put next entry ( new jar entry ( meta - inf cert . sf ) ) ; signature output stream out = new signature output stream ( m output jar , signature ) ; write signature file ( out ) ;
m _ package lookup info = new tree map < string , list < object > > ( ) ;
m refs . put ( ref , ref . add value event listener ( new data ref listener ( new index ) ) ) ; }
process instance sub process instance = runtime service . create process instance query ( ) . super process instance id ( pi . get id ( ) ) . single result ( ) ;
return ( char sequence . class . is assignable from ( in value type ) | | string writer . class . is assignable from ( in value type ) ) ;
string postfix = atts . get value ( postfix ) ;
boolean has access acl = false ;
test counter ( get enum counters ( groups , counters ) ) ;
test base utils . compare result collections ( results1 , results2 , new comparator < string > ( ) { @ override public int compare ( string o1 , string o2 ) { return o2 = = null ? o1 = = null ? 0 : 1 : o1 . compare to ( o2 ) ; } } ) ;
verify ( shutdown future ) . cancel ( is a ( boolean . class ) ) ;
if ( outstanding requests . is empty ( ) ) last request = null ;
btn donate iap = ( button ) find view by id ( r . id . button _ donate _ play _ store ) ; btn donate iap . set text ( string . format ( % s % dâ¬ , get string ( r . string . donate ) . to upper case ( ) , progress ) ) ; bar . set on seek bar change listener ( new seek bar . on seek bar change listener ( ) { @ override public void on progress changed ( seek bar seek bar , int i , boolean b ) { if ( i = = 0 ) progress = 2 ; else progress = ( i + 1 ) * 2 ; btn donate iap . set text ( string . format ( % s % dâ¬ , get string ( r . string . donate ) . to upper case ( ) , progress ) ) ; } @ override public void on start tracking touch ( seek bar seek bar ) { } @ override public void on stop tracking touch ( seek bar seek bar ) { } } ) ;
map < string , string > partition spec _ 2 = new hash map < string , string > ( ) ;
b3 = base64 alphabet [ marker0 ] ; decoded data [ encoded index ] = ( byte ) ( b1 < < 2 | b2 > > 4 ) ;
distributed log manager read dlm = factory . create distributed log manager with shared clients ( stream name ) ; async log reader reader = read dlm . get async log reader ( dlsn . initial dlsn ) ;
sb . append ( \ t failed container : ) ; } sb . append ( ) . append ( instance . get container id ( ) ) ; sb . append ( , logs at : ) . append ( instance . get log url ( ) ) ; } }
all in channels = collections . singleton list ( input ) . iterator ( ) ;
emitter . on next ( results ) ; } } , back _ pressure _ strategy ) ; }
try { deferred . group in order ( deferreds ) . add callback ( new queries cb ( ) ) . add errback ( new queries eb ( ) ) . join ( ) ; } catch ( exception e ) { e . print stack trace ( ) ; }
get mock endpoint ( mock : foo ) . expected bodies received ( hello world ) ; template . send body ( seda : foo , hello world ) ;
throw new bad ldap grammar exception ( unexpected end of value + unterminated ' \ \ ' ) ;
collection < long > double string = get value list ( response , countn , field facets , string _ sd , double , false ) ;
dos . write int ( - 1 ) ;
fill table ( ) ; jscroll pane scroll pane = new jscroll pane ( servers table ) ; east panel . add ( scroll pane , border layout . center ) ;
size = math . min ( ( h - 4 ) 3 , ( w - 4 ) 3 ) ;
return new minimal iterable < e > ( arrays . as list ( elements ) . iterator ( ) ) ;
long limit = long . max _ value + ( ( a ^ b ) > > > ( long . size - 1 ) ) ; if ( leading zeros < long . size | ( a < 0 & b = = long . min _ value ) ) { overflow return limit ; } long result = a * b ; if ( a = = 0 | | result a = = b ) { return result ; } return limit ;
on disk index segment = index . schedule segment flush ( false ) . call ( ) ; index . segments . add ( futures . immediate future ( segment ) ) ; segments . add ( segment . get index path ( ) ) ; } catch ( exception | fserror e ) { e . print stack trace ( ) ; assert . fail ( ) ; } }
return ( rec ) current record ;
log . info ( task - id isn ' t equal whb = + whb . get task ids ( ) + , local assignment = + local assignment . get task ids ( ) ) ;
bind ( document position map . class ) . to ( document position map impl . class ) ;
vm . inputs . discovery pager adapter set primary page ( null , 1 ) ;
events . create ( cross activities demo activity . event _ show _ image ) . param ( false ) . post ( ) ; } else if ( image . get drawable ( ) = null ) {
content holder . get children ( ) . add ( create content pane ( time , is24 hour view ) ) ; calendar place holder . get children ( ) . add ( content holder ) ; rectangle clip = new rectangle ( ) ;
try ( page cursor linked = cursor . open linked cursor ( 1 ) ) { assert false ( cursor . should retry ( ) ) ; }
if ( diff > 0 ) { first check if the difference can be represented in 31 bits if ( diff < = concise set utils . max _ literal _ length ) { ret val . add ( concise set utils . all _ ones _ literal ) ; } else { create a fill from last set bit to end index for number of 31 bit blocks minus one int end index word count = concise set utils . max literal length division ( end index ) ; ret val . add ( concise set utils . sequence _ bit | ( end index word count - words walked - 1 ) ) ; ret val . add ( concise set utils . all _ ones _ literal ) ; } }
dex transform dex transform = new dex transform ( new dex [ ] { new dex ( 0 ) , dex } , collision policy . fail ) ;
perform vertical swipe up gesture ( r . id . coordinator _ layout , center x , original appbar bottom , toolbar height ) ;
bitmap http request request = null ;
html settings html settings = new html settings ( ) ; html settings . set image dir path ( inputfilepath + _ files ) ;
lexer . match ( token types . id ) ; token protocol name = lexer . get next token ( ) ; this . lexer . spor ht ( ) ;
if ( gpath = = null ) { gpath = new general path ( ) ; gpath . move to ( curve draw x [ 0 ] , curve draw y [ 0 ] ) ; } gpath . curve to ( curve draw x [ 1 ] , curve draw y [ 1 ] , curve draw x [ 2 ] , curve draw y [ 2 ] , curve draw x [ 3 ] , curve draw y [ 3 ] ) ;
current state name = payment model dao . get last success state name ( ) ; }
object jpa = object helper . new instance ( jpa trace event message class ) ;
filepath re = filepath re . replace all ( | \ \ \ \ \ \ \ \ , [ \ \ \ \ \ \ \ \ ] ) ; }
fields . clear ( ) ;
writable comparable a value = ( writable comparable ) a . get ( key ) ; writable comparable b value = ( writable comparable ) b . get ( key ) ; assert equals ( 0 , a value . compare to ( b value ) ) ; } }
try { vespa xmlfeed reader . operation op = new vespa xmlfeed reader . operation ( ) ; parser . read ( op ) ; fail ( ) ; } catch ( exception e ) { system . out . println ( e . get message ( ) ) ; }
string child job id = type converter . from yarn ( id ) . to string ( ) ;
final byte [ ] aarr = this . key ; final byte [ ] barr = o . key ; final int len = math . min ( aarr . length , barr . length ) ; for ( int i = 0 ; i < len ; + + i ) { final int a = ( aarr [ i ] & 0xff ) ; final int b = ( barr [ i ] & 0xff ) ; if ( a = b ) { return a - b ; } } if ( aarr . length = = barr . length ) { same hash contents - compare the blob types int type compare = this . type . compare to ( o . type ) ; if ( type compare = = 0 ) { same type - compare random components return this . random . compare to ( o . random ) ; } else { return type compare ; } } else { return aarr . length - barr . length ; }
if ( collection utils . is empty ( main anomalies ) ) { return updated main anomalies by dimension ; }
assert not null ( qpm . sub map . get ( a ) ) ; query phrase map qpm2 = qpm . sub map . get ( a ) ; assert false ( qpm2 . terminal ) ; assert equals ( 1 , qpm2 . sub map . size ( ) ) ; assert not null ( qpm2 . sub map . get ( b ) ) ; query phrase map qpm3 = qpm2 . sub map . get ( b ) ; assert true ( qpm3 . terminal ) ; assert equals ( 1 f , qpm3 . boost , 0 ) ;
assert . assert equals ( size of grid should match to size of hyper space , hyper space size , grid . get model count ( ) + grid . get failure count ( ) ) ;
if ( xp = = null ) { xp = build persister ( ) ; } return xp ;
e . execute ( new runnable ( ) { @ override public void run ( ) { assert that ( thread . current thread ( ) . is interrupted ( ) ) . is false ( ) ; } } ) ;
test proc with valid json ( table _ row3 , client , id field proc , inner . veggies , good for you ) ;
string header = new string ( content , 0 , eol ) . trim ( ) ;
selection only operator = get operator for query with filter ( query ) ;
final blob library cache manager library cache manager = job manager services . library cache manager ; try { library cache manager . register job ( job graph . get job id ( ) , job graph . get user jar blob keys ( ) , job graph . get classpaths ( ) ) ; } catch ( ioexception e ) { throw new exception ( cannot set up the user code libraries : + e . get message ( ) , e ) ; } final class loader user code loader = library cache manager . get class loader ( job graph . get job id ( ) ) ;
if ( compile against abis . equals ( true ) ) { exported rule = resolver . get rule ( ( ( java library ) exported rule ) . get abi jar ( ) . get ( ) ) ; transitive exported rule = resolver . get rule ( ( ( java library ) transitive exported rule ) . get abi jar ( ) . get ( ) ) ; } assert that ( android lib rule . get build deps ( ) , matchers . all of ( matchers . has item ( exported rule ) , matchers . has item ( transitive exported rule ) ) ) ; }
int s len = str = null ? str . length ( ) : 0 ;
target cache manager = test cache manager factory . create cache manager ( hot rod cache configuration ( ) ) ;
case enumerated : return new asn1 enumerated ( bytes ) ; case generalized _ time : return new asn1 generalized time ( bytes ) ; case general _ string : return new dergeneral string ( bytes ) ; case ia5 _ string : return new deria5 string ( bytes ) ; case integer : return new asn1 integer ( bytes ) ; case null : return dernull . instance ; actual content is ignored ( enforce 0 length? ) case numeric _ string :
if ( item instanceof iexpandable ) { if ( ( ( iexpandable ) item ) . get sub items ( ) = null ) { return true ; } }
infix expression condition = ast . new infix expression ( ) ; condition . set operator ( infix expression . operator . conditional _ and ) ;
thread . sleep ( 10000l ) ;
get left post processor ( ) . add filter ( new translucent bucket filter ( ) ) ; get right post processor ( ) . add filter ( new translucent bucket filter ( ) ) ; } else { throw new illegal state exception ( the vr environment is not attached to any application . ) ; } } else {
list < key descriptor type > key descriptors = sp ssodescriptor . get key descriptor ( ) ; for ( key descriptor type key descriptor : key descriptors ) { write key descriptor ( key descriptor ) ; } list < endpoint type > slo services = sp ssodescriptor . get single logout service ( ) ;
get = new get ( row ) ; get . set row offset per column family ( 2 ) ; result = ht . get ( get ) ; verify result ( result , kv list exp , to log , testing basic set row offset ) ;
runtime exception rte = new runtime exception ( . start selector : selector . open exception ) ;
assert that ( result [ 0 ] [ 0 ] , instance of ( set . class ) ) ;
if ( is apply imports ) { dtmiterator cnl = new org . apache . xpath . node set dtm ( child , m _ xcontext . get dtmmanager ( ) ) ; m _ xcontext . push context node list ( cnl ) ; }
if ( compo embeddable type = null & & table info . get table id type ( ) . is annotation present ( embeddable . class ) ) { field [ ] fields = table info . get table id type ( ) . get declared fields ( ) ; string builder primary key builder = new string builder ( ) ; append primary key ( translator , compo embeddable type , fields , primary key builder ) ;
hash set < string > terminals = get terminated component set ( ) ;
hll . add raw ( 0x0000000000000001 l * ' j ' = 1 * ) ; assert equals ( bit vector . get register ( 1 * ' j ' * ) , 0 ) ; hll . add raw ( 0x0000000000000012 l * ' j ' = 2 * ) ; assert equals ( bit vector . get register ( 2 * ' j ' * ) , 1 ) ;
reader reader = resources . get resource as reader ( org apache ibatis submitted cursor _ nested mybatis - config . xml ) ;
batch parts . add ( olingo2 batch query request . resource path ( olingo2 app impl . metadata ) . build ( ) ) ;
snmp pdu packet pdupacket = ( snmp pdu packet ) pdu ;
rpc retrying caller factory caller factory = new rpc retrying caller factory ( conf ) ;
tload . start ( ) ; tcreate . start ( ) ; tload . join ( ) ;
} else if ( frame type = frame type . continuation ) { throw new connection exception ( sm . get string ( http2 parser . headers . wrong frame type , connection id , integer . to string ( headers current stream ) , frame type ) , http2 error . compression _ error ) ; } }
atomic reference < throwable > thrown = new atomic reference < > ( null ) ; callback = reactive streams helper . attach callback ( data , ( exchange , error ) - > { thrown . compare and set ( null , error ) ; if ( counter . decrement and get ( ) = = 0 ) { original callback . processed ( exchange , thrown . get ( ) ) ; } } ) ; }
int indx = operation . index of ( ' ' ) ;
ns list = new suballocated int vector ( math . max ( math . min ( isize + 16 , 2048 ) , 32 ) ) ; for ( int i = 0 ; i < isize ; + + i ) { ns list . add element ( inherited . element at ( i ) ) ; }
this . file read size = size ; logger . finest ( get logging filename ( ) + : + start of frame body at : + byte buffer . position ( ) + , frames sizes and padding is : + size ) ;
return columns . get next row or next column ( cell ) ;
if ( component type instanceof class ) { class < ? > component class = ( class < ? > ) component type ; if ( component class . is primitive ( ) ) { return component class ; } } return subtype of ( component type ) ; } }
write val ( null = = k ? null : k . to string ( ) , v ) ; }
if ( error code = = 257 | | ( error code > = 259 & & error code < = 263 ) ) { throw new sqlgrammar exception ( message , sql exception , sql ) ; }
msg printer . enable status msgs ( true ) ; msg printer . enable error msgs ( true ) ; if ( args . length < 2 ) { msg printer . print usage ( java ephyra trec8 to11 question _ file + pattern _ file [ log = logfile ] [ load _ log ] ) ; system . exit ( 1 ) ; }
properties . set property ( entry , ) ;
int major version = extract major version ( version ) ; int minor version = extract minor version ( version ) ; hfile . check format version ( major version ) ; throws iae if invalid
return new refactoring status ( ) ;
return linker . link all ( ) ; } }
if ( boolean utils . is true ( query . assigned ( ) ) ) { filters . put ( is _ assigned _ filter , exists query ( issue index definition . field _ issue _ assignee ) ) ; } else if ( boolean utils . is false ( query . assigned ( ) ) ) { filters . put ( is _ assigned _ filter , bool query ( ) . must not ( exists query ( issue index definition . field _ issue _ assignee ) ) ) ; }
if ( html name = = null | | ( html name . equals ignore case ( a ) & & html name . equals ignore case ( td ) ) ) state . after element = true ; state . empty = false ;
merge nodes ( node , candidate ) ;
final int bottom = child . get bottom ( ) - 1 ; canvas . draw line ( child . get left ( ) , bottom , child . get right ( ) - 2 , bottom , divider paint ) ; return ret val ; }
sqlite statement stmt = database . compile statement ( select * from `countme` where `name` = ' cessationoftime ' ) ;
list < cell scannable > cells = ( this . cell block ? new array list < cell scannable > ( count of actions ) : null ) ; long nonce group = multi action . get nonce group ( ) ;
c . set int ( hconstants . hbase _ rpc _ timeout _ key , 1500 ) ; connection connection = connection factory . create connection ( c ) ;
if ( _ log . should log ( log . debug ) ) _ log . debug ( peer supports options 0x + long . to string ( options , 16 ) + : + to string ( ) ) ;
assert equals ( failed for element : + element . to string ( ) , false , processor . test ( cells . get second ( ) ) ) ; } }
if ( tf . m _ completed . stream ( ) . any match ( b - > b ) ) { m _ snapshot err log str . append ( \ n rejected snapshot ) . append ( s . get nonce ( ) ) . append ( because it was not completed . ) ; return null ; }
weight = mgmt . get property key ( weight ) ; uid = mgmt . get property key ( uid ) ; someid = mgmt . get property key ( someid ) ; name = mgmt . get property key ( name ) ; value = mgmt . get property key ( value ) ; friend = mgmt . get edge label ( friend ) ; link = mgmt . get edge label ( link ) ; connect = mgmt . get edge label ( connect ) ; parent = mgmt . get edge label ( parent ) ; child = mgmt . get edge label ( child ) ; spouse = mgmt . get edge label ( spouse ) ; person = mgmt . get vertex label ( person ) ; tag = mgmt . get vertex label ( tag ) ;
set peeking percentage ( 0f ) ;
for ( data set < ? > data set : batch context . get dangling data sets ( ) . values ( ) ) { data set . output ( new discarding output format ( ) ) ; }
if ( beam beats ( direction , source , rect2 , rect1 ) ) { return false ; }
if ( tool window = null ) { tool window . show ( null ) ; return ; }
return get message from parent ( code , args to use , locale ) ;
for ( int i = 0 ; i < my _ message _ len ; i + + ) { dos . write ( my message [ i ] ) ; }
test _ util . move region and wait ( dest region . get region info ( ) , dest server . get server name ( ) ) ;
ordered fields and methods = order fields and methods by dependency ( unordered fields and methods , properties inventory ) ;
executing thread . set context class loader ( user code class loader ) ;
client a . create new file ( test - file ) ;
if ( adder method = null ) { class < ? > [ ] param types = adder method . get parameter types ( ) ; if ( is sanity check successful ( name , adder method , param types , complex property ) ) { return ; } invoke method with single parameter on this object ( adder method , complex property ) ; } else { add error ( could not find method [ + add + name + ] in class [ + obj class . get name ( ) + ] . ) ; } }
replace ( curs , xml ) ;
. build ( ) ; holder2 . add word ( testz ) ; assert equals ( 3 , holder2 . num words ( ) ) ; holder2 . truncate vocabulary ( ) ; assert equals ( 2 , holder2 . num words ( ) ) ; }
slice _ from ( ant ) ; return false ; case 14 :
db . get collection ( restaurants ) . update one ( new document ( name , juni ) , new document ( set , new document ( cuisine , american ( new ) ) ) . append ( current date , new document ( last modified , true ) ) ) ;
int new buffer index = m current adapter index - m side buffer ; if ( new buffer index > - 1 ) { m loaded views . add first ( make and add view ( new buffer index , false ) ) ; m current buffer index + + ; } }
params . put all ( collections . singleton map ( list , arrays . as list ( field entity , field entity2 ) ) ) ; instance = process engine . get runtime service ( ) . start process instance by key ( jpavariable process , params ) ; task = get task ( instance ) ;
md5 token server token = ( md5 token ) token ; return ( this . auth _ value = null ) & & ( server token . auth _ value = null ) & & ( this . auth _ value . equals ignore case ( server token . auth _ value ) ) ;
p left index + + ; current left structure = create run structure ( word , pl , p left index ) ; p left replacement . add ( current left structure ) ; } }
mockito . do call real method ( ) . when ( output format ) . configure accumulo output format ( conf ) ;
map < string , object > headers = new hash map < string , object > ( ) ;
key value kv = new key value ( rowkey , rowkey , rowkey , rowkey ) ;
menu item sticky item = menu . find item ( r . id . action _ sticky _ headers ) ;
data = new instances ( data ) ; data . delete with missing class ( ) ;
set version ( info . get version ( ) ) ; set path ( info . get path ( ) ) ; set type ( info . get type ( ) ) ; set package name ( info . get package name ( ) ) ; set alias ( info . get alias ( ) ) ; }
folding state descriptor < tuple2 < integer , long > , string > folding state = new folding state descriptor < > ( any , 0 , new sum fold ( ) , string serializer . instance ) ; queryable state stream < integer , string > queryable state = source . key by ( new key selector < tuple2 < integer , long > , integer > ( ) { private static final long serial version uid = - 842809958106747539 l ; @ override public integer get key ( tuple2 < integer , long > value ) throws exception { return value . f0 ; } } ) . as queryable state ( pumba , folding state ) ;
message = consumer . receive ( ) ;
map < string , object > map = get fingerprint ( ) ;
for ( string value : s ) client . query ( new solr query ( * : * ) ) ;
headers1 . put ( grpclb constants . token _ metadata _ key , lbtoken _ _ old ) ; when ( args1 . get headers ( ) ) . then return ( headers1 ) ; assert same ( b1 . result , picker . pick subchannel ( args1 ) ) ; verify ( args1 ) . get headers ( ) ; assert that ( headers1 . get all ( grpclb constants . token _ metadata _ key ) ) . contains exactly ( lbtoken0001 ) ; pick subchannel args args2 = mock ( pick subchannel args . class ) ;
int temp viewport width = viewport width ; viewport width = viewport height ; viewport height = temp viewport width ; }
if ( disconnected . get ( ) ) { add back ( head ) ; throw new end of stream exception ( channel for sessionid 0x + long . to hex string ( session id ) + is lost ) ; }
if ( my port = 80 & & my scheme . equals ( http ) | | my port = 443 & & my scheme . equals ( https ) ) { return m host . to host string ( ) ; } else { return m host . get host name ( ) ; }
log . info ( the table + table name + is in disabling state . hence recovering by moving the table + to disabled state . ) ; new disable table handler ( this . master , table name . get bytes ( ) , catalog tracker , this , true ) . process ( ) ; } }
op . get ( where , auto _ start ) . set ( false ) ;
got = version utils . random version between ( random ( ) , null , version . v _ 6 _ 0 _ 0 _ beta1 ) ;
final int header second byte = variable run length & 0xff ;
string [ ] tool args = parser . get remaining args ( ) ; return tool . run ( tool args ) ; }
message . obtain ( m remove message ) . send to target ( ) ; }
locations = new array list < datanode descriptor > ( ) ;
else if ( is icon ) { focus rect . set bounds ( icon rect ) ; } g . set color ( get focus color ( ) ) ; g . draw rect ( ( focus rect . x - 1 ) , ( focus rect . y - 1 ) , focus rect . width + 1 , focus rect . height + 1 ) ; }
if ( dest . exists ( ) ) { dest . delete ( ) ; }
if ( source classname . equals ( sink classname ) ) { server . replication source handler = ( replication source service ) new replication instance ( source classname , conf , server , wal fs , wal dir , old waldir ) ; server . replication sink handler = ( replication sink service ) server . replication source handler ; } else { server . replication source handler = ( replication source service ) new replication instance ( source classname , conf , server , wal fs , wal dir , old waldir ) ; server . replication sink handler = ( replication sink service ) new replication instance ( sink classname , conf , server , wal fs , wal dir , old waldir ) ; }
if ( hostname . length ( ) = = base32 _ hash _ length + 8 & & hostname . ends with ( . b32 . i2p ) ) return null ; list < string > urls = get urls ( ) ;
catalog mode mode = get mode ( ) ;
string user = system . getenv ( application constants . environment . user . name ( ) ) ; log . info ( setting user in submit work request to : + user ) ; container id container id = container id . new instance ( application attempt id . new instance ( app id , attempt num ) , task num ) ;
if ( this . width = width ) { this . width = width ; needs rebuild = true ; }
dl = new deep learning ( parms ) . train model ( ) . get ( ) ; model metrics auto encoder mm = ( model metrics auto encoder ) dl . _ output . _ training _ metrics ;
string s = integer . to string ( value ) ; int len = s . length ( ) ; check length before write ( len ) ; for ( int i = 0 ; i < len ; i + + ) { char c = s . char at ( i ) ; header buffer . put ( ( byte ) c ) ; } }
boolean decommissioned = dn9 . is decommissioned ( ) ;
reboot instances response ec2response = ec2 soap service impl . to reboot instances response ( service provider . get instance ( ) . get ec2 engine ( ) . reboot instances ( ec2request ) ) ; serialize response ( response , ec2response ) ; }
throw new null pointer exception ( string . format ( error message template , error message args ) ) ;
return local associate address response . get pull parser ( my _ qname ) ;
screen . blit ( small bitmap , get x ( ) + ( get width ( ) - small bitmap . get width ( ) ) 2 , get y ( ) + ( get height ( ) - small bitmap . get height ( ) ) 2 ) ; }
if ( this . style . text size = 0 ) { text . set text size ( typed value . complex _ unit _ sp , this . style . text size ) ; }
close ( true ) ; }
tableoids [ pos ] = oid ;
return new worker attempt event ( worker attempt event type . kill , attempt id ) ;
if ( q . get offset ( ) = null ) { count = math . max ( 0 , count - q . get offset ( ) ) ; } if ( q . get count ( ) = null ) { count = math . min ( count , q . get count ( ) ) ; } return count ;
if ( read short ( off + 6 ) > opcodes . v1 _ 8 ) { throw new illegal argument exception ( ) ; }
extend with node ( target ) ; } else { type element npe element = elements . get type element ( java . lang . null pointer exception ) ; extend with node with exception ( target , npe element . as type ( ) ) ; }
list < integer > server list = new array list < integer > ( interim cluster . get node ids ( ) ) ; map < string , string > config props = new hash map < string , string > ( ) ; config props . put ( admin . max . threads , 5 ) ;
success = fs . rename ( src , dst ) ;
async build library task task = new async build library task ( m context , this ) ; task . set on build library progress update ( welcome activity . m building library progress fragment ) ; task . set on build library progress update ( this ) ; task . execute on executor ( async task . thread _ pool _ executor ) ; return start _ sticky ;
long result = reader . read unsigned varint ( input , true , false , 8 ) ; assert equals ( c . result _ end _ of _ input , result ) ;
test split ( splits . get ( 0 ) ) ; test split ( splits . get ( 1 ) ) ; }
m renderer . scale * = detector . get scale factor ( ) ; m renderer . scale = math . max ( m min scale , m renderer . scale ) ; invalidate ( ) ; return true ; }
if ( get request ( ) . get coyote request ( ) . get supports relative redirects ( ) & & get context ( ) . get use relative redirects ( ) ) { location uri = location ; } else { location uri = to absolute ( location ) ; }
activity chooser model data model = activity chooser model . get ( m context , m share history file name ) ;
big integer tmp id ;
for ( final uuid tag definition id : tag definition ids ) { tag dao . delete tag ( object id , object type , tag definition id , internal call context factory . create internal call context ( object id , object type , context ) ) ; } }
expect ( mock context . get bean names for type ( extension filter . class ) ) . and return ( new string [ 0 ] ) ;
app attempt . rm context . get amrmtoken secret manager ( ) . application master finished ( app attempt . get app attempt id ( ) ) ;
local reserved instances set tracker = true ; } else {
case 15 : message type = cen basic and evolved ; object class = cen ; what _ parts = what . split ( ) ;
bus handler . push expected events ( next event . phase , next event . invoice , next event . invoice _ payment , next event . payment ) ; clock . add delta from reality ( at _ least _ one _ month _ ms ) ; assert listener status ( ) ; invoice checker . check invoice ( account id , 1 , call context , immutable list . < expected invoice item check > of ( new expected invoice item check ( new local date ( 2012 , 2 , 1 ) , null , invoice item type . fixed , big decimal . zero ) ) ) ;
supplier < backoff > s = ( ) - > new zero backoff ( * max retries = * 3 ) ; trip after ncircuit breaker cb = new trip after ncircuit breaker ( * max consecutive failures = * 2 ) ; retrier2 r = new retrier2 ( s , retry _ all , cb ) ; try { thread . current thread ( ) . interrupt ( ) ; r . execute ( ( ) - > 10 ) ; } catch ( interrupted exception expected ) { intentionally left empty . }
this . requires sorting = true ;
add docs and commit ( new doc ( instant . parse ( 2017 - 10 - 23 t00 : 00 : 00 z ) ) , new doc ( instant . parse ( 2017 - 10 - 24 t01 : 00 : 00 z ) ) , new doc ( instant . parse ( 2017 - 10 - 24 t02 : 00 : 00 z ) ) ) ; assert invariants ( ) ;
{ final hll hll = new hll ( log2m , regwidth , 128 * explicit threshold , arbitrary , unused * , 256 * sparse threshold , arbitrary * , hlltype . sparse ) ; hll . add raw ( probabilistic test util . construct hllvalue ( log2m , 0 , 1 ) ) ; final long cardinality = hll . cardinality ( ) ;
string found encrypted key method = find encrypted key method ( encrypted element ) ; if ( xmlcipher . rsa _ v1dot5 . equals ( found encrypted key method ) ) { string error message = the found key transport encryption method is not allowed ; throw new xmlencryption exception ( error message ) ; } }
test plan follower ( false ) ;
if ( broker request . get filter query ( ) = null & & filter query matched schema ( schema , broker request . get filter query ( ) , broker request . get filter sub query map ( ) ) ) { return true ; }
ensure open ( ) ; if ( b = = null ) { throw new null pointer exception ( null buffer for read ) ; } else if ( off < 0 | | len < 0 | | len > b . length - off ) { throw new index out of bounds exception ( ) ; } else if ( len = = 0 ) { return 0 ; }
document d2 = xml utils . marshalto w3 cdom document ( svg , jc svg ) ;
periods holder . set period ( null ) ;
assert same ( color . red , subaru . color provider . get ( ) ) ;
prefix = attr . get prefix ( ) ; prefix = ( prefix = = null | | prefix . length ( ) = = 0 ) ? xmlsymbols . empty _ string : f symbol table . add symbol ( prefix ) ; localpart = f symbol table . add symbol ( attr . get local name ( ) ) ; if ( prefix = = xmlsymbols . prefix _ xmlns ) { xmlns : prefix local uri = f local nsbinder . get uri ( localpart ) ; local prefix mapping value = f symbol table . add symbol ( value ) ; if ( value . length ( ) = 0 ) { if ( local uri = = null ) {
result . set display help ( detected options . has ( this . help ) ) ; result . set ansi color output disabled ( detected options . has ( this . disable ansi colors ) ) ; result . set details ( detected options . value of ( this . details ) ) ; result . set theme ( detected options . value of ( this . theme ) ) ; result . set additional classpath entries ( detected options . values of ( this . additional classpath entries ) ) ;
string query = key = value ; mdlink . set content ( metadata? + query ) ; get catalog ( ) . save ( resource ) ; doc = get as dom ( wms?service = wms & request = get capabilities & version = 1 . 3 . 0 , true ) ;
random access file old file = new random access file ( old f , rws ) ; file lock old lock = old file . get channel ( ) . try lock ( ) ; try { old file . seek ( 0 ) ; int old version = old file . read int ( ) ; if ( old version < last _ pre _ upgrade _ layout _ version ) return false ; } finally { old lock . release ( ) ; old file . close ( ) ; } return true ;
assert . assert equals ( 1 , lr . sem . available permits ( ) ) ;
color array [ color index + + ] = 0 . 5f ;
super ( a fork join worker thread ) ;
append formal line ( comment . get comment ( ) ) ; } }
jbutton cancel button = new jbutton ( msg ( cancel ) ) ;
if ( this . max rotation = 0 ) { final int rotation angle = ( int ) ( - effects amount * this . max rotation ) ; this . transformation camera . save ( ) ; this . transformation camera . rotate y ( rotation angle ) ; this . transformation camera . get matrix ( image matrix ) ; this . transformation camera . restore ( ) ; }
block blob append stream block blob stream = get block blob append stream ( append stream ) ; block blob stream . set max block size ( max block size ) ; block blob stream . set compaction block count ( compaction block count ) ; append block list ( append stream , mem stream , new int [ ] { n2 } ) ; verify block list ( block blob stream , new int [ ] { n2 } ) ; append stream . hflush ( ) ;
finally { if ( bos = null ) try { bos . close ( ) ; } catch ( exception e ) { * empty * }
method result = method . speed ;
string baseline time dimension value = response parser utils . compute time dimension value ( time bucket id , dimension value ) ; string current time dimension value = baseline time dimension value ; third eye response row baseline row = baseline response map . get ( baseline time dimension value ) ;
assert false ( parser . get errors ( ) . to string ( ) , parser . has errors ( ) ) ; pkg . add global ( results , list . class ) ;
return m footer view infos . get ( adj position - adapter count ) . data ; }
statement statement = statement in new transaction ( anonymous context . write token ( ) ) ;
identity loader < integer > loader = identity loader ( ) ; loading cache < integer , integer > cache = caffeinated guava . build ( caffeine . new builder ( ) . executor ( more executors . direct executor ( ) ) . maximum size ( 10 ) , loader ) ; cache testing . warm up ( cache , 0 , 10 ) ; set < integer > key set = cache . as map ( ) . key set ( ) ; assert that ( key set ) . contains exactly ( 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 ) ;
return persister . get owner entity persister ( ) . get identifier ( owner , session ) ; }
response = client . execute ( build post request ( url1 , state . session id , state . jsf view state , 1 ) ) ;
if ( test . equals ( method . get name ( ) ) | | accept . equals ( method . get name ( ) ) ) { return get default value ( method . get return type ( ) ) ; } throw new illegal state exception ( unexpected + method + invoked on + proxy ) ; } } ) ; } else {
ioutils . rm ( dir file ) ;
din = new data input stream ( new byte array input stream ( bos . to byte array ( ) ) ) ; msg = initial message . parse ( - 65530 l , din ) ; assert . fail ( bad protocol version accepted ) ; } catch ( initial message . initial message exception ex ) { }
assert null ( null types1 . get field long null ( ) ) ;
this . memory manager . release ( this . empty segments ) ; this . empty segments . clear ( ) ; if ( log . is debug enabled ( ) ) { log . debug ( block resettable iterator closed . ) ; }
slider panel panel = new slider panel ( activity , old screen , config ) ; panel . set id ( r . id . slidable _ panel ) ; old screen . set id ( r . id . slidable _ content ) ; panel . add view ( old screen ) ; decor view . add view ( panel , 0 ) ; return panel ; }
final socket session socket session = get socket session ( ) ; if ( socket session = = null ) { update status ( thing status . offline , thing status detail . handler _ initializing _ error , no socket session found ) ; return ; } set protocol handler ( new rio zone protocol ( config zone , handler controller , get system favorites handler ( ) , get presets protocol ( ) , socket session , new stateful handler callback ( new abstract rio handler callback ( ) { @ override public void status changed ( thing status status , thing status detail detail , string msg ) { update status ( status , detail , msg ) ; } @ override public void state changed ( string channel id , state state ) { if ( channel id . equals ( rio constants . channel _ zonename ) ) { zone name . set ( state . to string ( ) ) ; } update state ( channel id , state ) ; fire state updated ( channel id , state ) ; } @ override public void set property ( string property name , string property value ) { get thing ( ) . set property ( property name , property value ) ; } } ) ) ) ; update status ( thing status . online ) ;
string s = byte order must be specified for iso - 10646 - ucs - 2 . ;
end y1 = dot center y + ( adjusted fraction * dot radius ) ;
endpoint endpoint = exchange . get from endpoint ( ) ; if ( endpoint instanceof http common endpoint ) { answer = ( ( http common endpoint ) endpoint ) . is chunked ( ) ; } } else {
fstable descriptors . create table descriptor ( fs , tempdir , this . h table descriptor ) ; path temp table dir = new path ( tempdir , table name ) ; path table dir = new path ( file system manager . get root dir ( ) , table name ) ;
( new illegal state exception ( this + violated the reactive streams rule 2 . 13 by throwing an exception from on error . , t2 ) ) . print stack trace ( system . err ) ; } } } }
return target annotation = null & & collections . disjoint ( forbidden _ element _ types , arrays . as list ( target annotation . value ( ) ) ) ;
int this port = http client . normalize port ( scheme , uri1 . get port ( ) ) ;
composite channel buffer composite = ( composite channel buffer ) cumulation ;
super . begin function template ( node , name node ) ;
mv . visit var insn ( opcodes . aload , args off ) ;
locale . germany ,
configuration . put ( topic configuration . topic _ name _ key , topic configuration . default _ topic _ name ) ;
assert that ( is invalidated ( child ) ) . is true ( ) ;
start gravatar upload ( media utils . get real path from uri ( get activity ( ) , media uri ) ) ;
assert equals ( 1 + ( short ) 1 , exec ( int x = 1 ; short y = 1 ; return x + y ; ) ) ;
return description = = null ? null : test case map . get ( description ) ;
list < hsnapshot description > snapshots = master . get completed snapshots ( ) ;
string bootstrap time = get property from client info ( bootstrap time ) ;
tabular data table = ( tabular data ) open mbean . get attribute ( statistics table ) ; assert true ( table . is empty ( ) ) ; stats . method called ( foo , 100 , true ) ;
assert true ( source task . trigger checkpoint ( new checkpoint meta data ( 34 , 900 ) , checkpoint options . for checkpoint ( ) ) ) ; sync . trigger ( ) ;
if ( session . get note ( constants . form _ principal _ note ) = = null ) { return false ; }
tree map < datanode info , node record > map = new tree map < datanode info , node record > ( ) ;
continue ; } }
conf . set boolean ( hbase . rs . evictblocksonclose , false ) ;
byte [ ] buffer = new byte [ 2 * count ] ;
test ( line _ joiner . join ( for ( ; ; ) { const * * number * x = 3 , * * number * y = 4 ; , var f = function ( ) { return x + y ; } } ) , line _ joiner . join ( var jscomp loop 0 = { } ; , for ( ; ; jscomp loop 0 = { x : jscomp loop 0 . x , y : jscomp loop 0 . y } ) { , * * @ const @ type { number } * jscomp loop 0 . x = 3 ; , * * @ const @ type { number } * jscomp loop 0 . y = 4 ; , var f = function ( jscomp loop 0 ) { , return function ( ) { return jscomp loop 0 . x + jscomp loop 0 . y } , } ( jscomp loop 0 ) ; , } ) ) ;
value = ( in . read ( ) = = 0x01 ) ? true : false ; }
{ \ type \ : \ record \ , \ name \ : \ foo \ , \ fields \ : [ { \ name \ : \ bar \ , \ type \ : [ \ int \ , \ null \ , \ string \ ] , \ default \ : 42 } ] } , { \ type \ : \ record \ , \ name \ : \ foo \ , \ fields \ : [ { \ name \ : \ bar \ , \ type \ : [ \ int \ , \ string \ ] , \ default \ : { \ int \ : 42 } , \ optional \ : true } ] } } , {
start node = body . get first first child ( ) ; } else {
rollback ( objects ) ;
map < string , string > auto name to java name = new hash map < > ( ) ; for ( string java name : class to url map . key set ( ) ) { auto name to java name . put ( get auto name ( java name ) , java name ) ; } map < string , string > result = new hash map < > ( ) ;
reader reader = resources . get resource as reader ( org apache ibatis submitted propertiesinmapperfiles mybatis - config . xml ) ;
final count down latch wait for start = new count down latch ( 1 ) ;
peek = under test . peek ( worker _ uuid _ 2 ) ;
this . set widget ( support table ) ;
if ( m _ tracer = null ) super . fire start elem ( name ) ;
try { sub cluster deregister request request = sub cluster deregister request . new instance ( sub cluster id invalid , state lost ) ; federation membership state store input validator . validate ( request ) ; assert . fail ( ) ; } catch ( federation state store invalid input exception e ) { log . info ( e . get message ( ) ) ; assert . assert true ( e . get message ( ) . starts with ( invalid sub cluster id information . ) ) ; }
if ( addr = null ) send trap pdu ( addr , pdu ) ; else send trap pdu ( pdu ) ; }
if ( out value . resource id = 0 & & out value . type = = typed value . type _ attribute ) { color state list state list = null ; if ( android . os . build . version . sdk _ int > = android . os . build . version _ codes . m ) { state list = m context . get resources ( ) . get color state list ( out value . resource id , m context . get theme ( ) ) ; } else { state list = m context . get resources ( ) . get color state list ( out value . resource id ) ; } if ( state list = null ) return state list . get default color ( ) ; }
assert equals ( 1 l , task service . create task query ( ) . count ( ) ) ; task escalation task = task service . create task query ( ) . single result ( ) ; assert equals ( escalation task 2 , escalation task . get name ( ) ) ; task service . complete ( escalation task . get id ( ) ) ;
hash map < string , result loader map . load pair > unloaded properties = new hash map < string , result loader map . load pair > ( ) ;
try { in reader . close ( ) ; } catch ( ioexception ioe ) { log . warn ( error while closing the input stream , ioe ) ; } if ( completed . get ( ) ) err thread . interrupt ( ) ; try { err reader . close ( ) ; } catch ( ioexception ioe ) { log . warn ( error while closing the error stream , ioe ) ; }
int state = ( integer ) m bean server connection . get attribute ( new object name ( jboss : name = service - mbean - support - test ) , state ) ;
try { factory . new hazelcast instance ( config ) ; fail ( second hazelcast instance should not have started ) ; } catch ( illegal state exception e ) { expected }
final path snapshot bar = snapshot test helper . get snapshot path ( sdir2 , s2 , foo bar ) ; assert true ( hdfs . exists ( snapshot bar ) ) ;
s . delete ( fumm ) ;
assert true ( protobuf util . get store files ( server , region name , family ) . size ( ) < sf count ) ;
add detach listener ( this ) ;
assert jq ( req ( q , * : * ) , response num found = = 0 ) ;
assert true ( admin . is table available ( tablename ) ) ;
fs . create new file ( new path ( archived hfile dir , dfd - dfd ) ) ;
string request path = basepath + workspaces + ws + featuretypes + ft + . html ;
m sensor mgr = ( sensor manager ) m context . get system service ( context . sensor _ service ) ; if ( m sensor mgr = = null ) { throw new unsupported operation exception ( sensors not supported ) ; } m accelerometer = m sensor mgr . get default sensor ( sensor . type _ accelerometer ) ; if ( m sensor mgr . register listener ( this , m accelerometer , sensor manager . sensor _ delay _ ui ) ) { if not supported m sensor mgr . unregister listener ( this ) ; throw new unsupported operation exception ( accelerometer not supported ) ; } }
lazy value < context injection resolver > lazy context resolver = values . lazy ( ( value < context injection resolver > ) ( ) - > injection manager . get instance ( context injection resolver . class ) ) ; supplier < configuration > configuration = server bag : : get configuration ; provider < multivalued parameter extractor provider > param extractor = server bag : : get multivalued parameter extractor provider ;
http servlet request request mock = control . create mock ( http servlet request . class ) ; expect ( request mock . get request uri ( ) ) . and return ( index . xhtml ) . any times ( ) ;
response commit = http . post ( commit resource , query as json row ( match p = ( s ) - [ r : r ] - > ( e ) delete p return p ) ) ; assert that ( commit , contains no errors ( ) ) ; assert that ( commit , row contains deleted entities in path ( 2 , 1 ) ) ; assert that ( commit . status ( ) , equal to ( 200 ) ) ; assert that ( nodes in database ( ) , equal to ( 0 l ) ) ;
query ( ) . from ( cat , cats ) . where ( cat . kittens . size ( ) . gt ( 0 ) ) . select ( cat . name ) . fetch ( ) ; assert true ( last . res . size ( ) = = 4 ) ;
try { key spec ks = new pbekey spec ( null ) ; factory . generate secret ( ks ) ; fail ( ) ; } catch ( invalid key spec exception expected ) { }
if ( m result schema = = null | | m result schema . is set field schemas ( ) ) { throw new hive sqlexception ( error compiling query : schema and field schema + should be set when query plan has a fetch task ) ; } result schema = new table schema ( m result schema ) ; set has result set ( true ) ; } else { set has result set ( false ) ; }
num . divide ( num ) ;
new records = driver . get ( clazz ) ; assert equals ( 10 , new records . get records ( ) . size ( ) ) ; assert equals ( record of type + clazz + not updated in the store , 9 , count matching entries ( records . get records ( ) , new records . get records ( ) ) ) ; }
return ( ( c < < 6 ) | ( d & 0x3 f ) ) - 0x10000 ;
runtime context r ctx = mock ( runtime context . class ) ; window function . set runtime context ( r ctx ) ; verify ( mock ) . set runtime context ( r ctx ) ;
assert null ( find annotation declaring class for types ( candidates , non annotated interface . class ) ) ;
server name first master address = new server name ( localhost , 1 , system . current time millis ( ) ) ; server name second master address = new server name ( localhost , 2 , system . current time millis ( ) ) ;
list < media browser compat . media item > invalid = provider . get children ( invalid _ media _ id , resources ) ;
my store . add ( create cell ( qf1 , ts , seq id , old value ) , mem store sizing ) ;
bottom offset = math . min ( bottom offset , get list padding top ( ) - first top ) ; }
wait for connections received ( 2 ) ; verify ( event publisher , times ( 3 ) ) . post update ( item1 , on off type . on ) ; verify ( event publisher , times ( 3 ) ) . post update ( item2 , on off type . off ) ; verify no more interactions ( event publisher ) ; }
set < string > all groups = new hash set < > ( ) ;
int rand index = rand . next int ( j ) ; int [ ] tmp = data [ rand index ] ;
final translation result source result = helpers . translate operand ( environment , offset , source operand , true ) ;
connection . ensure connected ( new latched action listener < > ( new action listener < void > ( ) { @ override public void on response ( void a void ) { } @ override public void on failure ( exception e ) { throw new assertion error ( e ) ; } } , latch ) ) ; latch . await ( ) ;
m preview builder . set ( capture request . control _ ae _ precapture _ trigger , capture request . control _ ae _ precapture _ trigger _ start ) ;
when ( vertex . get max parallelism ( ) ) . then return ( 222 ) ; when ( vertex . is max parallelism configured ( ) ) . then return ( true ) ; try { savepoint loader . load and validate savepoint ( job id , tasks , path , ucl , false ) ; fail ( did not throw expected exception ) ; } catch ( illegal state exception expected ) { assert true ( expected . get message ( ) . contains ( max parallelism mismatch ) ) ; }
string builder extended info = new string builder ( ) ; for ( int i = 2 ; i < tokens . length - 1 ; i + + ) { if ( i > 2 ) { extended info . append ( : ) ; } extended info . append ( tokens [ i ] ) ; } string sha1 hex = tokens [ tokens . length - 1 ] ;
string prefix = f element qname . prefix = null ? f element qname . prefix : xmlsymbols . empty _ string ;
assert true ( volt db . instance ( ) . get cluster create time ( ) = = cluster1 create time ) ;
try { method method = access controller . do privileged ( new modal privileged action ( container . class , start lwmodal ) ) ; if ( method = null ) { method . invoke ( dialog , ( object [ ] ) null ) ; } } catch ( illegal access exception ex ) { } catch ( illegal argument exception ex ) { } catch ( invocation target exception ex ) { } if ( parent component instanceof jinternal frame ) { try { ( ( jinternal frame ) parent component ) . set selected ( true ) ; } catch ( java . beans . property veto exception e ) { } }
message base . set flag ( bbwi . byte buffer , message . more _ fragments _ bit ) ;
lookup source futures = immutable list . copy of ( this . lookup source futures ) ; }
for ( int i = rstart + 1 - y + len ; i < rstart + 1 ; i + + ) result [ i ] = 0 ; value = result ; int len = result len ; offset = result . length - result len ; }
d . get title ( ) ;
snapshot protos . snapshot description snapshot desc = get snapshot ( ) ; final procedure executor < master procedure env > proc exec = get master procedure executor ( ) ;
prepare all zero chunks ( block group , stripes , cell size , data blk num ) ; return stripes ;
for ( path file name : files ) { try { process file ( file name ) ; } catch ( ioexception ex ) { log . error ( error reading + file name , ex ) ; } } if ( verbose | | print key ) { system . out . println ( scanned kv count - > + count ) ; }
byte buf buffer = current buffer ; current buffer = null ; return new default http content ( buffer ) ; }
int navigation bar height = 0 ; int resource id = get resources ( ) . get identifier ( navigation _ bar _ height , dimen , android ) ; if ( resource id > 0 ) { navigation bar height = get resources ( ) . get dimension pixel size ( resource id ) ; } list view . set clip to padding ( false ) ; list view . set padding ( 0 , 0 , 0 , navigation bar height ) ; }
set measured dimension ( view width , view height ) ;
int code = google api availability . get instance ( ) . is google play services available ( get application context ( ) ) ; if ( code = connection result . success ) { dialog dlg = google api availability . get instance ( ) . get error dialog ( this , code , rc _ handle _ gms ) ; dlg . show ( ) ; } if ( m camera source = null ) { try { m preview . start ( m camera source , m graphic overlay ) ; } catch ( ioexception e ) { log . e ( tag , unable to start camera source . , e ) ; m camera source . release ( ) ; m camera source = null ; } }
if ( bound op1 . equals ( compare op . less ) | | bound op1 . equals ( compare op . less _ or _ equal ) ) { this . upper compareop = bound op1 ; this . upper bound value = bound value1 ; this . lower compare op = bound op2 ; this . lower bound value = bound value2 ; } else { this . upper compareop = bound op2 ; this . upper bound value = bound value2 ; this . lower compare op = bound op1 ; this . lower bound value = bound value1 ; }
execution context execution context = test execution context . new instance ( ) ;
simple field locator fl = new simple field locator ( ) ; new traversal util ( content list , fl ) ; report . append ( \ n \ n simple fields in + part . get part name ( ) + \ n ) ; report . append ( = = = = = = = = = = = = = \ n ) ;
no of deletes + + ; batch op . ret code details [ i ] = operation status . success ; } } else {
new htable ( test _ util . get configuration ( ) , hconstants . meta _ table _ name ) . close ( ) ;
cm . stop ( ) ;
s = session factory ( ) . open session ( ) ; s . set flush mode ( flush mode . manual ) ; s . begin transaction ( ) ; simple = ( simple ) s . get ( simple . class , long . value of ( 10 ) ) ; assert equals ( not same parent instances , check . get name ( ) , simple . get name ( ) ) ;
username = user . run as ( new privileged exception action < string > ( ) { public string run ( ) throws exception { return user . get current ( ) . get name ( ) ; } } ) ;
m paint . set style ( paint . style . fill ) ; m paint . set stroke width ( 0 ) ; m paint . set color ( text _ color ) ;
icontext context = worker data . get context ( ) ; string topology id = worker data . get topology id ( ) ;
assert true ( get a wrong exception , ex instanceof camel execution exception ) ;
data = arrays . copy of ( data , data . length + read _ granularity ) ;
this . session reference = null ;
var y = a ; ) ; test ( modules , new string [ ] {
insteon hub binding config config = insteon hub binding config . parse ( item . get name ( ) , binding config ) ;
resource utilization resource allocation to free up = resource utilization . new instance ( this . utilization tracker . get current utilization ( ) ) ;
int size = 0 ; for ( int i = min ; i < = max ; i + + ) { boolean bs contains i = bs . get ( i ) ; assert equals ( bs contains i , set . contains ( i ) ) ; if ( bs contains i ) size + + ; } assert equals ( size , set . size ( ) ) ;
fsdata output stream out = null ;
skip value ( input , protocol version . v3 ) ;
normalizer base . mode owner mode = collator utilities . to normalizer mode ( owner . get decomposition ( ) ) ;
test proc with valid json ( full _ table , client , null set field doc proc , null , id , - 1 ) ;
remove obsolete references from field value ( a . get field value ( ) , annotations still present , false ) ; } }
int max bitmap size = ( int ) math . sqrt ( math . pow ( width , 2 ) + math . pow ( height , 2 ) ) ;
if ( signum > = 0 ) { norm bit len | = ( 1 < < 31 ) ; } for ( int i = 0 ; i < 4 & & len > 0 ; i + + , len - - ) { final byte b = ( byte ) ( norm bit len > > > ( 8 * ( 3 - i ) ) ) ; target . put ( offset + + , b ) ; } }
float [ ] probs = new float [ cv _ preds [ i ] . num cols ( ) ] ;
temp sqlite file . delete ( ) ;
map < string , object > params = new hash map < > ( ) ; params . put ( fieldname , date ) ; search response r = client ( ) . prepare search ( cache _ test _ idx ) . set size ( 0 ) . add aggregation ( date range ( foo ) . field ( date ) . script ( new script ( script type . inline , mockscript , date script mocks plugin . double _ plus _ one _ month , params ) ) . add range ( new date time ( 2012 , 1 , 1 , 0 , 0 , 0 , 0 , date time zone . utc ) , new date time ( 2013 , 1 , 1 , 0 , 0 , 0 , 0 , date time zone . utc ) ) ) . get ( ) ; assert search response ( r ) ; assert that ( client ( ) . admin ( ) . indices ( ) . prepare stats ( cache _ test _ idx ) . set request cache ( true ) . get ( ) . get total ( ) . get request cache ( ) . get hit count ( ) , equal to ( 0 l ) ) ;
assert true ( sql . contains ( not oid in ( select oid from object _ property where property _ type in ( : ptype0 ) and upper ( value ) = : value0 ) ) ) ;
if ( global doc > segment values . doc id ( ) ) { segment values . advance ( global doc ) ; } if ( global doc = = segment values . doc id ( ) ) { ord = segment values . ord value ( ) ; } else { ord = - 1 ; } }
log . warn ( could not delete collection { } - ignoring , collection name ) ; }
ordinal map = null ; leaf reader leaf reader = searcher . get index reader ( ) . leaves ( ) . get ( 0 ) . reader ( ) ; sorted doc values join sorted doc values = leaf reader . get sorted doc values ( join field ) ; if ( join sorted doc values = null ) { value count = join sorted doc values . get value count ( ) ; } else { return new match no docs query ( join util . create join query : no join values ) ; } } else {
while ( operator stack . empty ( ) ) { filter stack . push ( pop arguments ( operator stack , filter stack ) ) ; } filter = filter stack . pop ( ) ; if ( filter stack . empty ( ) ) { throw new illegal argument exception ( incorrect filter string ) ; } return filter ;
assert equals ( null , multi . get type ( john : doe ) ) ;
final atomic boolean to be release after cancellation is released = new atomic boolean ( false ) ;
if ( node ordinal % 2 = = 0 ) { return settings . builder ( ) . put ( node . ingest , false ) . put ( super . node settings ( node ordinal ) ) . build ( ) ; } return super . node settings ( node ordinal ) ;
int required segments = segment ( length ) + 1 ;
switch ( ev . get action ( ) ) { case motion event . action _ down : case motion event . action _ move : m icon last touch pos . set ( ( int ) ev . get x ( ) , ( int ) ev . get y ( ) ) ; break ; } return false ; }
conf1 . set int ( hbase . regionserver . hlog . blocksize , 1024 * 20 ) ; conf1 . set int ( replication . source . size . capacity , 1024 ) ; conf1 . set long ( replication . source . sleepforretries , 100 ) ; conf1 . set int ( hbase . regionserver . maxlogs , 10 ) ; conf1 . set long ( hbase . master . logcleaner . ttl , 10 ) ; conf1 . set boolean ( hconstants . replication _ enable _ key , true ) ; conf1 . set boolean ( dfs . support . append , true ) ; conf1 . set long ( hconstants . thread _ wake _ frequency , 100 ) ; conf1 . set strings ( coprocessor host . user _ region _ coprocessor _ conf _ key , coprocessor counter . class . get name ( ) ) ; utility1 = new hbase testing utility ( conf1 ) ;
sb . append ( message ) ; delegate . text log ( level , category , sb . to string ( ) ) ; delegate . event log ( new server event impl ( level , category , delegate . get time service ( ) . instant ( ) , message , detail , context , who , scope ) ) ; }
query results < ? > results = create query ( modifiers ) . select ( var ) . fetch results ( ) ;
current sort field = f ;
return get local address ( ) ;
flows page . click new ( ) ;
if ( input . equals ( piii ) ) { return pair . of ( cpu . x86 _ 32 , os . linux ) ; } else if ( input . equals ( k8 ) ) { return pair . of ( cpu . x86 _ 64 , os . linux ) ; } else if ( input . equals ( ppc ) ) { return pair . of ( cpu . ppc , os . linux ) ; } else if ( input . equals ( arm ) ) { return pair . of ( cpu . arm , os . linux ) ; } else if ( input . equals ( s390x ) ) { return pair . of ( cpu . s390 x , os . linux ) ; }
closable iterator < pair < byte array , versioned < byte [ ] > > > iter = this . store . entries ( ) ;
final set < byte sequence > required column families = input configurator . get fetched columns ( accumulo input format . class , configuration ) . stream ( ) . map ( pair : : get first ) . map ( c - > new array byte sequence ( c . to string ( ) ) ) . collect ( collectors . to set ( ) ) ;
cache ( 0 ) . compute if absent ( key1 , compute function ) ; assert cache value ( key1 , value _ 1 ) ;
ecode = qt . execute client ( version file , fname ) ;
assert true ( this . driver . get current url ( ) . starts with ( http : localhost : 8081 test - app ) ) ; broker server rule . stop session ( session , true ) ; session = broker server rule . start session ( ) ;
application ids = null ;
try { final file checksum dstcs = dstfs . get file checksum ( dststatus . get path ( ) ) ; return true if checksum is not supported ( i . e . some of the checksums is null ) return srccs = = null | | dstcs = = null | | srccs . equals ( dstcs ) ; } catch ( file not found exception fnfe ) { return false ; }
assert false ( server . is closed ( ) ) ;
end _ flag - - ; chunked value nesting level - - ;
return decimal ( + column . default _ precision + , + column . default _ scale + ) ; }
array list < string > security groups = new array list < string > ( ) ; security groups . add ( getting started group ) ; launch specification . set security groups ( security groups ) ;
object mapper mapper = new object mapper ( ) ;
c = get random ( ' \ u db80 ' , ' \ u dbff ' ) ;
if ( looper . my looper ( ) = = m main looper ) { m callback . execute message ( msg ) ; msg . recycle ( ) ; return ; } m h . send message ( msg ) ;
vcard . set to ( ( jid ) null ) ;
if ( m override closing transition ) { finish ( ) ; } else { do finish after transition ( ) ; }
reboot engine ( false ) ; assert config property ( false ) ;
throw new error ( unexpected exception + ex ) ; } }
final clioutput response prop response = subversion api . propset ( dto factory . create dto ( property set request . class ) . with value ( user ) . with project path ( wc root . get absolute path ( ) ) . with path ( . ) . with force ( true ) . with depth ( depth . fully _ recursive ) . with name ( owner ) ) ; assert true ( prop response . get output ( ) . size ( ) > 0 ) ;
data stream < word with count > window counts = text . flat map ( new flat map function < string , word with count > ( ) { @ override public void flat map ( string value , collector < word with count > out ) { for ( string word : value . split ( \ \ s ) ) { out . collect ( new word with count ( word , 1 l ) ) ; } } } ) . key by ( word ) . time window ( time . seconds ( 5 ) ) . reduce ( new reduce function < word with count > ( ) { @ override public word with count reduce ( word with count a , word with count b ) { return new word with count ( a . word , a . count + b . count ) ; } } ) ;
result = search on interfaces ( method , annotation type , annotation name , container type , processor , visited , meta depth , clazz . get interfaces ( ) ) ;
boost queries = get boost queries ( ) ;
map type map type = map type ( integer , integer ) ; assert array hash operator ( array [ map ( array [ 1 ] , array [ 2 ] ) ] , map type , immutable list . of ( map block of ( integer , integer , immutable map . of ( 1 l , 2 l ) ) ) ) ; }
random rnd = new random ( ) ; for ( int i = 0 ; i < num _ records ; i + + ) { long val = rnd . next long ( ) ; handler . add record ( new tuple3 < long , string , byte > ( val , string . value of ( val ) , ( byte ) val ) ) ; assert true ( handler . has data ( ) ) ; } mutable object iterator < tuple3 < long , string , byte > > sorted = handler . finish write and sort keys ( sort memory ) ;
default : return new copy quoted strategy ( format field ) ; case ' d ' : return day _ of _ year _ strategy ; case ' e ' : return get locale specific strategy ( calendar . day _ of _ week , defining calendar ) ; case ' f ' : return day _ of _ week _ in _ month _ strategy ; case ' g ' : return get locale specific strategy ( calendar . era , defining calendar ) ; case ' h ' : return modulo _ hour _ of _ day _ strategy ;
boolean has default = false , has escape xml = false ; has default = ctxt . is attribute specified ( default ) ; has escape xml = ctxt . is attribute specified ( escape xml ) ;
if ( _ config . client factories = transport client factories ) { d2 client = new transport client factory aware d2 client ( d2 client , transport client factories . values ( ) ) ; } return d2 client ;
list < pull request > closed pull requests = pull request service . get pull requests ( null , closed ) ; assert not null ( closed pull requests ) ; boolean found = false ; for ( pull request pr : closed pull requests ) { if ( pr . get id ( ) = = latest pull request id ) { found = true ; break ; } } assert true ( didn ' t find pull request + latest pull request id , found ) ;
jtx transaction jtx = worker . maybe request transaction ( required ( ) , ctx _ 1 ) ; assert not null ( jtx ) ; db session session1 = session provider . get db session ( ) ; execute update ( session1 , insert into girl values ( 1 , ' sophia ' , null ) ) ;
assert true ( s1 . contains ( my reference data ) ) ; assert true ( s1 . contains ( my other reference data ) ) ; s2 . evict ( my reference data ) ; s2 . evict ( my other reference data ) ;
final int count per family = 1000 ;
final index searcher index searcher = obtain searcher ( ) ;
try { item = get item instance ( channel [ 71000 ] ) ; fail ( missing exception ) ; } catch ( binding config parse exception e ) { e . print stack trace ( ) ; }
view holder . item view . set background color ( color . parse color ( aaffffff ) ) ; image view image view = ( image view ) view holder . item view . find view by id ( r . id . stick _ img ) ; secure random img gen = new secure random ( ) ;
return get involved clusters of index ( parsed target . get target index ( ) ) ; } }
stop watch . start ( ) ; buffered input stream input stream4 = new buffered input stream ( new file input stream ( new file ( dump data writer . input1000 mb ) ) ) ; while ( input stream4 . read ( ) = - 1 ) { } long duration4 = stop watch . stop ( ) ; system . out . println ( duration4 ) ; }
intent intent = get intent ( ) ; string message = strings . null to empty ( intent . get string extra ( key _ extra _ message ) ) ;
close quietly ( raw socket ) ;
while ( y > 0 ) { x - - ; y - - ; diff = ( a . value [ a . offset + x ] & long _ mask ) - ( b . value [ b . offset + y ] & long _ mask ) - ( ( int ) - ( diff > > 32 ) ) ; a . value [ a . offset + x ] = ( int ) diff ; }
set < long > txids = new hash set < long > ( ) ;
long earliest start = find earliest time ( allocation . to interval map ( ) ) ; if ( job arrival < earliest start ) { allocation . add interval ( new reservation interval ( job arrival , earliest start ) , zero resource ) ; }
m current search view . set text ( html . from html ( filters . get search description ( this ) ) ) ; m current sort by view . set text ( filters . get order description ( this ) ) ;
path paths [ ] = new path [ ] { new path ( test root , * f ) , new path ( test root , d1 f ) , new path ( test root , d2 f ) } ;
rs . abort ( die ) ;
return new string ( bytes , 0 , bytes . length - 2 ) ;
shape = 0 ; rect mode ( corner ) ; ellipse mode ( diameter ) ; auto normal = true ;
if ( ( x1 < 0 ) = ( x2 < 0 ) ) return - cmp ;
assert equals ( job . get job state ( ) , job status . killed ) ;
create table ( create table % s ( pk0 int , pk1 int , val int , primary key ( ( pk0 , pk1 ) ) ) with compact storage ) ; for ( int i = 0 ; i < 3 ; i + + ) execute ( insert into % s ( pk0 , pk1 , val ) values ( ? , ? , ? ) , i , i , i ) ;
require no content ( reader ) ;
em . get transaction ( ) . begin ( ) ; tnae1 = em . find ( m2 mtarget not audited entity . class , tnae1 . get id ( ) ) ; tnae2 = em . find ( m2 mtarget not audited entity . class , tnae2 . get id ( ) ) ;
int size = handler . get ( table aname , row aname , column bname , null ) . size ( ) ;
assert true ( visitor . get objects ( layer group info . class , modification type . group _ changed ) . contains ( catalog . get layer group by name ( lakes _ group ) ) ) ;
if ( exclusive read lock strategy = null ) { exclusive read lock strategy . release exclusive read lock on commit ( operations , file , exchange ) ; }
version compatible version = version . new instance ( default version . get major version ( ) , default version . get minor version ( ) + 2 ) ; db store . store version ( compatible version ) ; assert . assert equals ( compatible version , db store . load version ( ) ) ; restart timeline store ( ) ; db store = ( rolling level dbtimeline store ) store ;
deflater output stream def = new deflater output stream ( os ) ;
reservation request r1 = reservation request . new instance ( resource . new instance ( 1024 , 1 ) , 35 , 5 , 30 ) ;
list < local resource status > rsrcs4 = new array list < local resource status > ( ) ; rsrcs4 . add ( rsrc2success ) ; rsrcs4 . add ( rsrc3success ) ; when ( stat . get resources ( ) ) . then return ( collections . < local resource status > empty list ( ) ) . then return ( collections . singleton list ( rsrc1success ) ) . then return ( collections . singleton list ( rsrc2pending ) ) . then return ( rsrcs4 ) . then return ( collections . < local resource status > empty list ( ) ) ; string local path = path . separator + container localizer . usercache + path . separator + user0 + path . separator + container localizer . filecache ;
case 1 : activation locator locate server for orb
if ( view name parts . is empty ( ) ) { return component description . get service name ( ) . append ( view ) . append ( view name parts . to array ( new string [ view name parts . size ( ) ] ) ) ; } else { return component description . get service name ( ) . append ( view ) . append ( view class name ) ; }
config = new local cluster ( catalogupdate - bad - masked - password . jar , sites _ per _ host , hosts , k , backend target . native _ ee _ jni ) ;
if ( configuration . library jars = null ) { prepare a data entry reader to filter all classes , which are then decoded to classes by a class reader , which are then put in the class pool by a class pool filler . read input ( reading library , configuration . library jars , new class filter ( new class reader ( true , configuration . skip non public library classes , configuration . skip non public library class members , warning printer , new class presence filter ( program class pool , duplicate class printer , new class presence filter ( library class pool , duplicate class printer , new class pool filler ( library class pool ) ) ) ) ) ) ; }
if ( value = = 0 & & dev instanceof switchable device ) { ( ( switchable device ) dev ) . off ( ) ; } else if ( value = = 100 & & dev instanceof switchable device ) { ( ( switchable device ) dev ) . on ( ) ; } else if ( dev instanceof dimmable device ) { long td val = math . round ( ( value 100 ) * 255 ) ; ( ( dimmable device ) dev ) . dim ( ( int ) td val ) ; } else { throw new runtime exception ( cannot send dim to + dev ) ; }
for ( memory consumer c : consumers ) { if ( c = consumer & & c . get used ( ) > 0 ) { try {
db client . properties dao ( ) . save property ( db session , new property dto ( ) . set key ( sonar . jira . project . key ) . set value ( sonar ) . set resource id ( project . get id ( ) ) ) ; db client . properties dao ( ) . save property ( db session , new property dto ( ) . set key ( sonar . jira . login . secured ) . set value ( john ) . set resource id ( project . get id ( ) ) ) ; component dto module = component testing . new module dto ( project ) ; db client . component dao ( ) . insert ( db session , module ) ;
reader . set property ( xmlconstants . access _ external _ dtd , _ access external dtd ) ;
bd = new big decimal ( 3 . 98336 e7 ) ; bytes = volt decimal helper . serialize big decimal ( bd ) ; buf = byte buffer . wrap ( bytes ) ; bd2 = volt decimal helper . deserialize big decimal ( buf ) ; cmp = bd . compare to ( bd2 ) ; assert equals ( 0 , cmp ) ; bd = new big decimal ( 9 e25 ) ;
system . arraycopy ( data , offset , this . buffers . get last ( ) , this . index , length ) ;
on view ( with id ( r . id . text _ view _ rocks ) ) . check ( matches ( with text ( m activity rule . get activity ( ) . get string ( r . string . android _ testing _ rocks ) ) ) ) ;
parse ( from where . order ) ; }
try { expr = new spel expression parser ( ) . parse raw ( @ 378 ) ; assert equals ( trouble , expr . get value ( e context , string . class ) ) ; } catch ( spel parse exception spe ) { assert equals ( spel message . invalid _ bean _ reference , spe . get message code ( ) ) ; }
list < storage object or ioexception [ ] > results = lists . new array list ( ) ; list < batch request > batches = gcs util . make get batches ( make gcs paths ( s , 3 ) , results ) ; assert that ( batches . size ( ) , equal to ( 1 ) ) ; assert that ( sum batch sizes ( batches ) , equal to ( 3 ) ) ; assert equals ( 3 , results . size ( ) ) ;
clinitgen . invoke static ( var _ type , method . get method ( void pop thread bindings ( ) ) ) ;
cancel image fetch ( ) ;
m _ undo list . remove ( m _ undo list . size ( ) - 1 ) ;
assert equals ( 1 , tab cfs map . get ( table name2 ) . size ( ) ) ; assert equals ( cf1 , tab cfs map . get ( table name2 ) . get ( 0 ) ) ;
new master . stop ( ) ;
if ( bit index = data bits . get size ( ) ) { throw new writer exception ( not all bits consumed : + bit index + ' ' + data bits . get size ( ) ) ; } }
assert that ( stream . from ( long . max _ value , 2 ) . take ( 2 ) ) . is equal to ( stream . of ( long . max _ value , long . max _ value + 2 ) ) ;
if ( is ordered ( ) = target . is ordered ( ) ) { return false ; } int len ; if ( attr id . equals ( target . get id ( ) ) & & ( len = size ( ) ) = = target . size ( ) ) { try { if ( is ordered ( ) ) {
atts . clear ( ) ;
check assignment ( tt1 , attempt _ test _ 0002 _ m _ 000001 _ 0 on tt1 ) ;
assert true ( entity manager appears to have been closed across txns , injector . get instance ( entity manager . class ) . contains ( entity ) ) ; assert true ( entity manager appears to have been closed across txns , em . contains ( entity ) ) ; assert true ( entity manager appears to have been closed across txns , em . is open ( ) ) ; injector . get instance ( unit of work . class ) . end ( ) ;
if ( paused ) return ; if ( is wifi required & & file download utils . is network not on wifi type ( ) ) { throw new file download network policy exception ( ) ; } } while ( true ) ; } finally {
try { tuple ds . max by ( 4 , 0 , 1 , 2 , 3 ) ; } catch ( exception e ) { assert . fail ( ) ; } }
if ( bits . available ( ) < 6 ) { throw format exception . get format instance ( ) ; }
array list < prediction > predictions = m library . recognize ( gesture ) ;
application id app id1 = application id . new instance ( 1352994193343 l , 1 ) ;
connection factory . set copy message on send ( false ) ; connection factory . set optimize acknowledge ( true ) ; connection factory . set optimized message dispatch ( true ) ; connection factory . set always session async ( false ) ; connection factory . set trust all packages ( true ) ; return connection factory ; }
wm two . insert ( name ) ; wm two . insert ( likes ) ; wm two . insert ( age ) ; wm two . fire all rules ( ) ; assert equals ( should not have fired , 0 , list two . size ( ) ) ; wm one . insert ( hair ) ;
s = open session ( ) ; s . begin transaction ( ) ; entity = ( lob holder ) s . by id ( lob holder . class ) . with ( lock options . upgrade ) . load ( entity . get id ( ) ) ; assert not null ( entity . get clob locator ( ) ) ; assert equals ( clob _ size , entity . get clob locator ( ) . length ( ) ) ; assert equals ( original , extract data ( entity . get clob locator ( ) ) ) ; entity . set clob locator ( s . get lob helper ( ) . create clob ( changed ) ) ; s . get transaction ( ) . commit ( ) ; s . close ( ) ;
template . async callback send body ( direct : grpc - sync - async , ping request , new synchronization adapter ( ) { @ override public void on complete ( exchange exchange ) { async pong response = exchange . get out ( ) . get body ( ) ; latch . count down ( ) ; } } ) ; latch . await ( 1 , time unit . seconds ) ;
this . probe side channel = io access . create block channel writer ( probe channel enumerator . next ( ) , buffer return queue ) ;
for ( int i = 0 ; i < 10 ; i + + ) { thread . sleep ( 100 ) ; assert equals ( region count , server . get online regions ( ) . size ( ) ) ; }
if ( stack trace = = null ) {
check _ invalid _ set ( m contains ' bar ' , syntax exception . class ) ; }
m _ cpe . set execution slots ( m _ execution slots backup ) ;
client . test jq ( params ( p , q , * : * , json . facet , { x : { terms : { { terms } field : ' { multi _ ss } ' , all buckets : true } } } ) , facets = = { count : 6 , + x : { buckets : [ { val : a , count : 3 } , { val : b , count : 3 } ] , all buckets : { count : 6 } } } ) ;
final list < remote location > locations = get locations for path ( src , false ) ; remote method method = new remote method ( get ezfor path , new class < ? > [ ] { string . class } , new remote param ( ) ) ; return ( encryption zone ) rpc client . invoke sequential ( locations , method , encryption zone . class , null ) ; }
throw e ; } catch ( exception e ) { olog manager . instance ( ) . debug ( this , cannot open database with url + current url , e ) ; if ( network = null ) {
expect parser success ( select e from com . acme . entity name e , ( query ( query _ spec ( select _ from ( from ( persister _ space ( entity _ persister _ ref com . acme . entity name e ) ) ) ( select ( select _ list ( select _ item ( path e ) ) ) ) ) ) ) ) ;
int flags = ( method = = stored ) ? 0 : zip file . gpbf _ data _ descriptor _ flag ;
finish schema ( ) ;
list < version > sorted = new array list < version > ( matches . key set ( ) ) ;
timeline filter list metric filters = filters . get metric filters ( ) ;
if ( aspects . size ( ) = = 1 ) { append formal line ( * ) ; append formal line ( * this aspect takes the lower precedence ) ; append formal line ( * ) ; append indent ( ) ; append ( declare precedence : * , ) ; } else { append indent ( ) ; append ( declare precedence : ) ; } list < string > aspect names = new array list < string > ( aspects . size ( ) ) ;
buf = base64 . base64 to byte array ( str val ) ;
assert equals ( false , config . get property ( share beta nodes option . property _ name ) ) ;
default encoding ( ) . set project value ( new ui prefs . default encoding ( ) . get value ( ) ) ;
string s = this is not a palindrome ; string reverse = new string buffer ( s ) . reverse ( ) . to string ( ) ; assert equals ( should have overridden to reverse , not echo , reverse , oom . replace me ( s ) ) ; assert equals ( should have overridden no - arg overloaded replace me method to return fixed value , fixed method replacer . value , oom . replace me ( ) ) ;
final schema record check schema check = new schema record check ( new schema storage ( native stores . get schema store ( ) ) , indexes ) ;
sslcontext sc = sslcontext . get instance ( ssl ) ; sc . init ( null , trust all certs , new java . security . secure random ( ) ) ; https urlconnection . set default sslsocket factory ( sc . get socket factory ( ) ) ;
super . on ebus sync received ( allow send ) ;
validator . validate entity ( entity metadata . get entity clazz ( ) , kundera metadata ) ; } pu to schema metadata . put ( persistence unit , table infos ) ; }
continue ; } if ( i < sb . length ( ) - 1 ) {
counter result result after shutting down anode = remote counter . increment ( ) ;
listener . accept ( false ) ; return true ;
if ( f fixup base ) { process xmlbase attributes ( attributes ) ; }
if ( notify ) { notify listeners ( new plot change event ( this ) ) ; } }
htable descriptor htd = hbu . create table descriptor ( name . get method name ( ) , 3 , 1000 , 1 , keep deleted cells . false ) ; hregion region = hbu . create local hregion ( htd , null , null ) ;
ordering . append ordering ( 3 , integer . class , order . ascending ) ;
u = new path ( scheme + : + path ) . to uri ( ) ;
for ( file expired backup file : expired backup files ) { try { file utils . delete quietly ( expired backup file ) ; } catch ( exception ex ) { log . warn ( failed to delete backup file , { } , expired backup file ) ; non - nls - 1 } } } catch ( runtime exception ex ) {
payment plugin . make all invoices fail with error ( true ) ; final default entitlement base entitlement = create base entitlement and check for completion ( account . get id ( ) , external key , product name , product category . base , term , next event . create , next event . block , next event . invoice ) ; bundle = subscription api . get subscription bundle ( base entitlement . get bundle id ( ) , call context ) ; invoice checker . check invoice ( account . get id ( ) , 1 , call context , new expected invoice item check ( new local date ( 2012 , 5 , 1 ) , null , invoice item type . fixed , new big decimal ( 0 ) ) ) ;
iterator < message id > itr msgs = messages2 . iterator ( ) ;
if ( _ key . equals ignore case ( read geometry key ) ) { continue ; } final object value = val . get value ( ) ; parameters . add ( new default parameter descriptor ( _ key , value . get class ( ) , null , value ) . create value ( ) ) ;
final delegation key key1 = new delegation key ( 1 , 2 , key data1 . get bytes ( ) ) ; final mrdelegation token identifier token1 = new mrdelegation token identifier ( new text ( token owner1 ) , new text ( token renewer1 ) , new text ( token user1 ) ) ; token1 . set sequence number ( 1 ) ; final long token date1 = 1 l ; final mrdelegation token identifier token2 = new mrdelegation token identifier ( new text ( token owner2 ) , new text ( token renewer2 ) , new text ( token user2 ) ) ; token2 . set sequence number ( 12345678 ) ; final long token date2 = 87654321 l ; store . store token master key ( key1 ) ;
current tkn = long . parse long ( tokens [ 0 ] ) ; if ( current tkn < 0 | | current tkn > 0xffffffff l ) return null ; address [ 0 ] = ( byte ) ( ( current tkn > > 24 ) & 0xff ) ; address [ 1 ] = ( byte ) ( ( ( current tkn & 0xffffff ) > > 16 ) & 0xff ) ; address [ 2 ] = ( byte ) ( ( ( current tkn & 0xffff ) > > 8 ) & 0xff ) ; address [ 3 ] = ( byte ) ( current tkn & 0xff ) ; break ; case 2 :
list < remote task runner work item > copy = lists . new array list ( pending tasks . values ( ) ) ; sort by insertion time ( copy ) ; for ( remote task runner work item task runner work item : copy ) { string task id = task runner work item . get task id ( ) ; if ( try assign tasks . put if absent ( task id , task id ) = = null ) { try {
criteria . list ( ) ; assert equals ( natural id cache hits , 0 , stats . get natural id cache hit count ( ) ) ; assert equals ( natural id cache misses , 1 , stats . get natural id cache miss count ( ) ) ; assert equals ( natural id cache puts , 1 , stats . get natural id cache put count ( ) ) ; assert equals ( natural id cache queries , 1 , stats . get natural id query execution count ( ) ) ;
instructions . add ( reil helpers . create add ( base offset + + , operand size . dword , real first operand , operand size . dword , 2147483648 , operand size . qword , first to unsigned ) ) ; instructions . add ( reil helpers . create add ( base offset + + , operand size . dword , real second operand , operand size . dword , 2147483648 , operand size . qword , second to unsigned ) ) ;
ctx . write ( ( ( fmlproxy packet ) msg ) . to c17 packet ( ) , promise ) ;
client response response = m _ client . call procedure ( @ ad hoc , bad write command ( ) ) ;
write start element ( plist _ tag , plist _ version _ attribute , 1 . 0 ) ;
time . increment ( 6000 ) ;
while ( is done ( ) ) { logger . info ( current worker status is : { } . , worker ) ; } print ( leaving run ( ) ) ;
storage = get storage supplier ( 2 ) . get ( ) ;
if ( ( ch > 61 ) & & ( ch < 127 ) ) { out buffer . append ( ch = = ' \ \ ' ? \ \ \ \ : ch ) ; continue ; }
pulse transaction coordinator ( ) ; try { internal clear ( ) ; } catch ( runtime exception e ) { throw exception converter . convert ( e ) ; }
assert true ( should delete exactly one nearest version of index entry ( salary ) , index deletes . size ( ) = = 1 ) ;
stm . write ( file contents , 2 , 2 ) ;
for ( int i = 0 ; i < 10 ; i + + ) { long acquired id = freelist . acquire new id ( generation _ two , generation _ three ) ; assert true ( expected . remove ( acquired id ) ) ; }
class . for name ( cl . get name ( ) , true , cl . get class loader ( ) ) ; } finally { if ( p = = thread . get default uncaught exception handler ( ) ) {
exasol table = exasol schema . get table ( session . get progress monitor ( ) , table or view name ) ; if ( exasol table = null ) { objects . add ( new exasol object reference ( column name , exasol table , exasol object type . column ) ) ; } } } }
if ( included & & pass conditional headers ( request , response , content ) ) return ;
service info published service = service events . get ( 0 ) . get source ( ) ; assert that ( published service . get name ( ) , is ( test - wms - name ) ) ; assert that ( published service . get id ( ) , is ( test - wms - id ) ) ; assert that ( published service . get abstract ( ) , is ( test - wms - abstract ) ) ; }
string preset name = new preset name field . get text ( ) . to string ( ) ;
length = pack ( jpeg , offset , 2 , false ) ;
sbuilder . append ( true ) ;
act = get _ reduce ( ( ( symbol ) stack . peek ( ) ) . parse _ state , lhs _ sym _ num ) ;
volt type [ ] promoted _ types = { volt type . integer , volt type . smallint , volt type . tinyint } ; for ( int i = 0 ; i < promoted _ types . length ; + + i ) { for ( int j = i ; j < promoted _ types . length ; + + j ) { assert equals ( volt type . bigint , volt type util . determine implicit casting ( promoted _ types [ i ] , promoted _ types [ j ] ) ) ; } } assert equals ( volt type . timestamp , volt type util . determine implicit casting ( volt type . timestamp , volt type . timestamp ) ) ;
blender input stream bis = blender context . get input stream ( ) ;
t = new timer ( ) ;
worker state . delay prepare bulk request ( thread pool , time value seconds ( 0 ) , 1 , new abstract runnable ( ) { @ override protected void do run ( ) throws exception { } @ override public void on failure ( exception e ) { throw new unsupported operation exception ( ) ; } } ) ;
for ( int offset = 0 ; offset < code length ; offset + + ) { if ( partial evaluator . is traced ( offset ) ) { instruction instruction = instruction factory . create ( code attribute . code , offset ) ; instruction . accept ( clazz , method , code attribute , offset , this ) ; } }
assert equals ( empty message . to string ( ) , all types message . to string ( ) ) ;
assert equals ( size , versions and seq no resolver . lookup states . size ( ) ) ;
persistent class . add tuplizer ( mode , tuplizer . impl ( ) . get name ( ) ) ; }
c . offset top and bottom ( first column top - top ) ;
m scrim paint . set color ( color utils . set alpha component ( m scrim color , min _ scrim _ alpha + ( int ) ( scrim _ alpha _ diff * m scrim darkness ) ) ) ; if ( m image offset = 0 ) { canvas . save ( ) ; canvas . translate ( 0f , m image offset ) ; canvas . clip rect ( 0f , 0f , canvas . get width ( ) , canvas . get height ( ) + m image offset + 1 ) ; super . on draw ( canvas ) ; canvas . draw rect ( 0 , 0 , canvas . get width ( ) , canvas . get height ( ) , m scrim paint ) ; canvas . restore ( ) ; } else { super . on draw ( canvas ) ; canvas . draw rect ( 0 , 0 , canvas . get width ( ) , canvas . get height ( ) , m scrim paint ) ; }
message . append int ( server port ) ;
synchronized ( writers ) { data store writer < t > writer = writers . remove ( path ) ; if ( writer = null ) { log . info ( removed writer = [ + writer + ] ) ; } else { log . info ( writer with path = [ + path + ] didn ' t exist anymore ) ; } }
object updated entity = runtime service . get variable ( process instance . get id ( ) , entity to update ) ;
map < string , object > stashed object = new hash map < > ( ) ; stashed object . put ( subobject , elements ) ; stash . stash value ( object , stashed object ) ; object = object path . evaluate ( field1 . object \ \ . subobject . element1 , stash ) ; assert that ( object , not null value ( ) ) ; assert that ( object . to string ( ) , equal to ( value1 ) ) ;
return query helper . equals ( other . get query tree ( ) , this . get query tree ( ) ) ;
corners patch = ( neg curr bottom - x1 ) 2 ; canvas . draw rect ( new rect ( ( int ) ( neg curr bottom - corners patch ) , ( int ) y0 , ( int ) neg curr bottom , ( int ) y1 ) , style . bar paint ) ; } } else if ( j = = top set index ) { draw bar ( canvas , ( int ) x1 , ( int ) y0 , ( int ) neg curr bottom , ( int ) y1 ) ;
template . send body and header ( file : target done , , exchange . file _ name , done ) ;
tree set < temporal > has _ seen = new tree set < temporal > ( new temporal comparator ( ) ) ;
return initial context . do lookup ( java : comp bean manager ) ;
object under test . add custom global configuration ( gcb ) ; object under test . add custom cache configuration ( builder ) ; object under test . after properties set ( ) ;
async disk service . remove volume ( sd . get storage uuid ( ) ) ;
component registry . register component ( new rolling upgrade manager ( ) , rolling upgrade manager . class . get name ( ) , true ) ;
used candidate nominated ;
if ( initial cluster . get number of partitions ( ) = final cluster . get number of partitions ( ) ) { logger . error ( the number of partitions in the initial and the final cluster is not equal \ n ) ; } set < integer > final partitions = new hash set < integer > ( ) ;
applier captor . get value ( ) . apply ( new metadata ( ) ) ;
array list < query > qlist = new array list < query > ( ) ; list < query > list = rb . get filters ( ) ; if ( list = null ) { for ( query q : list ) { qlist . add ( q ) ; } } qlist . add ( rb . get query ( ) ) ; boolean query query = new boolean query ( ) ;
request = create simple reservation submission request ( 1 , 1 , 1 , 50 , 3 , 10 ) ;
break ; default : throw new internal error ( processing event : + event ) ; }
verify contents ( cache , job id1 , key1a , data ) ; verify contents ( cache , job id1 , key1b , data2 ) ; verify contents ( cache , job id2 , key2a , data ) ; verify contents ( cache , job id2 , key2b , data2 ) ;
assert mock endpoints satisfied ( ) ; }
if ( len = = 0 ) { buf = new char [ ] { } ; } else { buf = ( this . get sub string ( 1 , ( int ) len ) ) . to char array ( ) ; }
mock meta store event listener . pop and verify last event id ( event type . drop _ index , first event id + 4 ) ; mock meta store event listener . pop and verify last event id ( event type . create _ index , first event id + 3 ) ; mock meta store event listener . pop and verify last event id ( event type . create _ table , first event id + 2 ) ; mock meta store event listener . pop and verify last event id ( event type . create _ table , first event id + 1 ) ;
verify same references ( record , record from store ) ;
int rack local containers = assign rack local containers ( node , application , scheduler key ) ;
em = get entity manager ( ) ; em . get transaction ( ) . begin ( ) ; cte2 = em . find ( component test entity . class , cte2 . get id ( ) ) ; em . remove ( cte2 ) ; em . get transaction ( ) . commit ( ) ; id1 = cte1 . get id ( ) ;
assert equals ( 1 , r . num docs ( ) ) ;
artifact artifact with srcs = srcjar coords ( artifact ) ; try { artifact with srcs = download artifact ( artifact with srcs , repository , session , system ) ; } catch ( artifact resolution exception e ) { intentionally ignored - missing srcjar is not an error . } jar download = output directory . get relative ( artifact . get file ( ) . get absolute path ( ) ) ;
assert true ( success ) ;
dir watcher event . type . deleted + : jodd . md \ n , sb . to string ( ) ) ; }
process definition cache entry cache entry = process definition cache . get ( process definition id ) ; process definition process definition = cache entry = null ? cache entry . get process definition ( ) : null ; if ( process definition = = null ) { process definition = process definition entity manager . find by id ( process definition id ) ; if ( process definition = = null ) { throw new activiti object not found exception ( no deployed process definition found with id ' + process definition id + ' , process definition . class ) ; } process definition = resolve process definition ( process definition ) . get process definition ( ) ; }
nm memory mb = get conf ( ) . get int ( slsconfiguration . nm _ memory _ mb , slsconfiguration . nm _ memory _ mb _ default ) ;
result . add ( ( attr ) n ) ;
while ( _ cache . size ( ) > 0 & & ( _ cached files . get ( ) > _ max cached files | | _ cached size . get ( ) > _ max cache size ) ) { scan the entire cache and generate an ordered list by last accessed time . sorted set < cached http content > sorted = new tree set < cached http content > ( new comparator < cached http content > ( ) { public int compare ( cached http content c1 , cached http content c2 ) { if ( c1 . _ last accessed < c2 . _ last accessed ) return - 1 ; if ( c1 . _ last accessed > c2 . _ last accessed ) return 1 ; if ( c1 . _ content length value < c2 . _ content length value ) return - 1 ; return c1 . _ key . compare to ( c2 . _ key ) ; } } ) ; for ( cached http content content : _ cache . values ( ) ) sorted . add ( content ) ; invalidate least recently used first for ( cached http content content : sorted ) { if ( _ cached files . get ( ) < = _ max cached files & & _ cached size . get ( ) < = _ max cache size ) break ; if ( content = = _ cache . remove ( content . get key ( ) ) ) content . invalidate ( ) ; } }
evt . add field ( id , ignore this ) ; argument captor < object > obj cap = argument captor . for class ( object . class ) ;
assert count ( 1 , tx . query ( ) . has ( time , 5 ) . vertices ( ) ) ;
if ( event . get event type ( ) = = accessibility event . type _ view _ focused & & is focused ( ) ) { event . recycle ( ) ; return ; }
reader reader = resources . get resource as reader ( org apache ibatis submitted mapper _ type _ parameter mybatis - config . xml ) ;
if ( _ producing | | _ idle ) break ;
log . info ( readding third server = + util . get hbase cluster ( ) . start region server ( ) . get region server ( ) . get server name ( ) ) ;
for ( repository repository : repositories ( ) ) { repository . remove ( ) ; }
dfstest util . create file ( fs , file path , block _ size , block _ size * num _ blocks , block _ size , num _ datanodes , seed ) ;
expect throws ( illegal argument exception . class , ( ) - > { boolean clause . occur [ ] flags2 = { boolean clause . occur . must } ; multi field query parser . parse ( blah , fields , flags2 , new mock analyzer ( random ( ) ) ) ; } ) ;
m _ undo list . add ( temp file ) ;
m _ client = client factory . create client ( ) ; m _ client . create connection ( localhost , config . m _ port ) ; volt table mod count ;
filter . update db score ( url , null , datum , empty list of inlinks ) ;
string fixed width font = theme fonts . get fixed width font ( ) ;
bag bag = new tree bag ( arrays . as list ( input _ text . split ( ) ) ) ;
this . center = new point2 d . double ( center . get x ( ) , center . get y ( ) ) ;
content . displayed year month property ( ) . set ( ( date = null ) ? year month . from ( date ) : year month . now ( ) ) ;
agent5 . stop async ( ) . await terminated ( ) ; await host status ( client , test host ( ) , down , long _ wait _ seconds , seconds ) ;
run test ( , , , true , false , false ) ;
list < task < ? extends serializable > > parent tasks = curr task . get parent tasks ( ) ;
start all threads lock . count down ( ) ; go final long start time = system clock . elapsed realtime ( ) ;
for ( injection target target : ejb ref . get injection targets ( ) ) { sb . append ( < injection - target > \ n ) ; append element ( sb , indent6 , injection - target - class , target . get target class ( ) ) ; append element ( sb , indent6 , injection - target - name , target . get target name ( ) ) ; sb . append ( < injection - target > \ n ) ; }
tester . inject elements ( 1 , 2 , 3 , 4 , 5 ) ; tester . merge windows ( ) ; assert true ( tester . should fire ( merged window ) ) ; }
if ( first . get key ( first . number of keys ( ) - 1 ) . compare to ( node . get key ( 0 ) ) > 0 ) return false ; node < t > last = node . get child ( node . number of children ( ) - 1 ) ;
if ( testvalid ) { invalidate if valid ( ) ; } }
cookie [ ] cookies = request . get cookies ( ) ; if ( cookies = = null ) { if ( log . is debug enabled ( ) ) { log . debug ( no valid cookies associated with the request + request ) ; } return null ; } if ( log . is debug enabled ( ) ) { log . debug ( received cookies : + to cookie str ( cookies ) ) ; }
before failover jedis . close ( ) ;
assert pivot ( company _ t , microsoft , 56 , first place . get pivot ( ) . get ( 1 ) ) ;
if ( remove details fragment ( ) ) { super . on back pressed ( ) ; }
path file1 = new path ( dir1 , file1 ) ; fsdata output stream stm1 = test file creation . create file ( fs , file1 , 1 ) ; byte [ ] content = test file creation . write file ( stm1 ) ; stm1 . sync ( ) ; stm1 . close ( ) ; try { cluster . get name node ( ) . namesystem . dir . get hard link id ( file1 . to uri ( ) . get path ( ) ) ; fail ( did not throw exception for get hard link id ( ) on non hardlinked file file ) ; } catch ( ioexception ie ) { system . out . println ( expected exception : + ie ) ; }
byte ptr arg = byte ptr . to byte ptr ascii z ( args [ i ] ) ;
string lexer name = combined ast . get child ( 0 ) . get text ( ) + lexer ;
if ( name . equals ( init ) ) { method visitor mv = wd . dest . visit method ( access , name , desc , msign . get asm method signature ( ) , null ) ; return new proxetta ctor builder ( mv , msign , wd ) ; }
identity array list < window > blockers hierarchies = new identity array list < window > ( blockers ) ; int k = 0 ; while ( k < blockers hierarchies . size ( ) ) { window w = blockers hierarchies . get ( k ) ; window [ ] owned windows = w . get owned windows _ no client code ( ) ; for ( window win : owned windows ) { blockers hierarchies . add ( win ) ; } k + + ; } java . util . list < window > to block = new identity linked list < window > ( ) ;
if ( crls . is empty ( ) & & saved cse = null ) { throw saved cse ; } else { return crls ; } }
sphere . update bound ( ) ; bounding box bb = ( bounding box ) sphere . get bound ( ) ; vector3f min = bb . get min ( null ) ; float [ ] ext = new float [ ] { bb . get xextent ( ) * 2 , bb . get yextent ( ) * 2 , bb . get zextent ( ) * 2 } ;
log . d ( tag , on complete ) ; } @ override public void on error ( throwable e ) {
msg len = msg . s _ get message length ( m _ buf [ 1 ] , is extended ) ;
stateful knowledge session impl . init mbeans ( internal knowledge base . get container id ( ) , internal knowledge base . get id ( ) , persistent ) ; }
return immutable list . < string > of ( ( string ) cached tenant values ) ;
{ error msg . no _ translet _ class _ err , tato \ u0161ablona neobsahuje platnou definici t \ u0159 \ u00eddy translet . } ,
node labels enabled = yarn configuration . are node labels enabled ( conf ) ; is centralized node label configuration = yarn configuration . is centralized node label configuration ( conf ) ;
break ; case xmlstream constants . start _ document : + + f depth ;
boolean cancel result = trade service . cancel order ( limit order return value ) ; system . out . println ( canceling returned + cancel result ) ; print open orders ( trade service ) ; }
if ( is event over drop target bar ( ev ) ) { return true ; } } else { m launcher . get user event dispatcher ( ) . log action tap outside ( logger utils . new container target ( top view . get log container type ( ) ) ) ; top view . close ( true ) ;
test some nulls ( client , t , max ( c ) , expected with some nulls ) ;
storage = mock storage with edits ( [ 1 , 100 ] | [ 101 , 200 ] , [ 1 , 100 ] | [ 201 , 300 ] | [ 301 , 400 ] ) ; nothing starting at 101
among _ var = find _ among _ b ( a _ 5 , 3 ) ; if ( among _ var = = 0 ) { return false ; }
for ( int i = 0 ; i < t ; i + + ) { int v0 = 0 ; for ( int j = 0 ; j < 4 ; j + + ) { v0 = v0 > > > 8 ; int u = ( m _ ints [ i ] > > > ( j * 4 ) ) & 0x f ; int w = table [ u ] < < 24 ; v0 | = w ; } c . m _ ints [ i + i ] = v0 ; v0 = 0 ; int upper = m _ ints [ i ] > > > 16 ; for ( int j = 0 ; j < 4 ; j + + ) { v0 = v0 > > > 8 ; int u = ( upper > > > ( j * 4 ) ) & 0x f ; int w = table [ u ] < < 24 ; v0 | = w ; } c . m _ ints [ i + i + 1 ] = v0 ; } return c ;
tez work tez work = context . current task . get work ( ) ; log . debug ( connecting + parent work . get name ( ) + with + my work . get name ( ) ) ; tez work . connect ( parent work , my work , edge prop ) ; if ( edge type = = edge type . custom _ edge ) { tez work . set vertex type ( my work , vertex type . initialized _ edges ) ; } reduce sink operator r = null ; if ( context . connected reduce sinks . contains ( parent rs ) ) { log . debug ( cloning reduce sink + parent rs + for multi - child broadcast edge ) ;
if ( input . is direct ( ) & & supports unsafe ( ) ) { long address = direct byte buffer access loader . get address ( input ) ; if ( address = 0 ) { address + = input . position ( ) ; input . position ( input . limit ( ) ) ; update ( address , length ) ; return ; } } array = new byte [ length ] ;
versioned < byte [ ] > v6 = new versioned < byte [ ] > ( value , vc1 ) ; n0store . add ( pair . create ( k6 , v6 ) ) ; n1store . add ( pair . create ( k6 , v6 ) ) ; n2store . add ( pair . create ( k6 , v6 ) ) ;
pipeline . add first ( handler . name , handler ) ;
if ( debug ) dprint ( new _ context ( ) ) ; naming context data store impl = ( naming context data store ) this ; synchronized ( impl ) { return impl . new context ( ) ; }
md . digest ( bytes , integer . max _ value , 1 ) ; fail ( no expected illegal argument exception ) ;
new thread ( ) { @ override public void run ( ) { cache . get unchecked ( get key ) ; get finished signal . count down ( ) ; } } . start ( ) ;
expected = test _ string ;
admin support . create jms topic ( get topic name ( ) , get topic jndi name ( ) ) ;
return new file context test helper ( uuid . random uuid ( ) . to string ( ) ) ;
long too big = max size + 1 ;
clock . set time ( initial creation date ) ;
final jpanel button panel = new jpanel ( parent . get layout ( ) ) ; for ( final component c : parent . get components ( ) ) { button panel . add ( c ) ; }
child type = common attributes . jms _ topic ;
response decline response ; try { decline response = protocol provider . get message factory ( ) . create response ( response . decline , refer request ) ; } catch ( parse exception e ) { logger . error ( error while creating 603 response , e ) ; return ; } try { server transaction . send response ( decline response ) ; } catch ( exception e ) { logger . error ( error while sending the response 603 , e ) ; return ; }
return base + e * ( add errs ( n , 1 , cf ) - base ) ;
if ( next = null ) { return true ; } if ( delegate . has next ( ) ) { t v = delegate . next ( ) ; if ( filter . test ( v ) ) { next = v ; return true ; } else { for infinite iterators it ' s up to provided ' filter ' to make sure looping is not infinite otherwise this may result in stack overflow error return has next ( ) ; } } return false ;
rehash ( ) ; bucket = hash ( buffer , offset , length ) % f table size ; }
assert equals ( max _ commit _ time . get default value ( ) , graph . get configuration ( ) . get max commit time ( ) ) ;
this . set content area filled ( false ) ;
if ( key = = null ) { return null ; } int hash = hash ( key ) ; return segment for ( hash ) . get entry ( key , hash ) ; }
if ( is closed ) { return - 1 ; }
. idx1 + file . separator + ma - 1 - big - data . db , } ; for ( string name : names ) { assert not null ( descriptor . from filename ( name ) ) ; } }
dirs handler . init ( conf ) ; dispatcher1 . init ( conf ) ;
throw new ioexception ( npe ) ; }
assert . assert true ( map . lookup ( key data , platform . byte _ array _ offset , record length bytes ) . is defined ( ) ) ;
x509 certificate trusted cert = anchor . get trusted cert ( ) ; public key pub key = trusted cert = null ? trusted cert . get public key ( ) : anchor . get capublic key ( ) ; if ( pkix . is dsapublic key without params ( pub key ) ) { continue ; }
system clock . sleep ( time unit . seconds . to millis ( 2 ) ) ; fixme : replace with sync progress notifications once available .
return call timeout in sec ( method , params , 600 , debug ) ;
account properties . put ( contact _ presence _ task , boolean . to string ( registration . is contact presence task enabled ( ) ) ) ;
m first position = row start ; view sel ;
int num comments = comments = = null ? 0 : comments . size ( ) ; if ( num comments > 0 ) { node = new iiometadata node ( comment extensions ) ; for ( int i = 0 ; i < num comments ; i + + ) { iiometadata node comment node = new iiometadata node ( comment extension ) ; byte [ ] comment = ( byte [ ] ) comments . get ( i ) ; comment node . set attribute ( value , to iso8859 ( comment ) ) ; node . append child ( comment node ) ; } root . append child ( node ) ; } return root ;
set http handler ( get , new get fixture http request callback ( ) ) ; }
toolbar toolbar = ( toolbar ) find view by id ( r . id . toolbar ) ; set support action bar ( toolbar ) ; get support action bar ( ) . set title ( r . string . sample _ sticky _ header ) ;
ex . set cserror code ( ref . get cserror code ( ) ) ;
thread t dispatcher = new thread ( dispatcher ) ;
if ( metadata = null & & metadata . get key type ( ) = null ) { builder . data container ( ) . key equivalence ( new type equivalance ( metadata . get key type ( ) ) ) ; } }
this . resume inflight exchanges = math . max ( resume percent of max * max inflight exchanges 100 , 1 ) ;
verify ( store , times ( 1 ) ) . set async ( current user ) ;
if ( tanns = null ) { for ( int i = 0 ; i < tanns . length ; + + i ) { if ( ( read byte ( tanns [ i ] ) > > 1 ) = = ( 0x40 > > 1 ) ) { int v = read annotation target ( context , tanns [ i ] ) ; v = read annotation values ( v + 2 , c , true , mv . visit local variable annotation ( context . type ref , context . type path , context . start , context . end , context . index , read utf8 ( v , c ) , true ) ) ; } } }
if ( idx < = 0 | | is newline ( idx - 1 ) ) { return - 1 ; } else { break ; }
if ( ( dep = = null ) | | ( dep . get ( toolchain info . provider ) = = null ) ) { rule context . rule error ( the selected c + + toolchain is not a cc _ toolchain rule ) ; return cc toolchain provider . empty _ toolchain _ is _ error ; } return ( cc toolchain provider ) dep . get ( toolchain info . provider ) ;
session . recycle ( ) ;
and item and = null ; or item or = null ; not item not = null ; store negatives here as we go item current ;
if ( should fix hdfs overlaps ( ) ) { second pass we fix overlaps . clear state ( ) ; this also resets fixes . load hdfs region dirs ( ) ; tables info = load hdfs region infos ( ) ; update table infos based on region info in fs . tables info = check hdfs integrity ( false , should fix hdfs overlaps ( ) ) ; } return errors . get error list ( ) . size ( ) ;
exp = exp . substring ( 0 , m . start ( ) ) + vp + exp . substring ( m . end ( ) ) ; if ( match & & show single steps ) { logger . info ( result : | + exp + | ) ; } start pos = m . start ( ) + vp . length ( ) ; m . reset ( exp ) ; } }
get transform ( ) . set to rotation ( angle , rotate . get x ( ) , rotate . get y ( ) ) ; s1 = shape1 . create transformed shape ( transform ) ; s2 = shape2 . create transformed shape ( transform ) ; s3 = shape3 . create transformed shape ( transform ) ; }
return get query string ( encoder cache . url _ argument _ encoding ) ;
if ( stream = = null ) { throw new null pointer exception ( stream parameter must not be null . ) ; }
compare result < link address > car = result . old lp . compare addresses ( result . new lp ) ;
service < queue > queue service = jmsqueue service . install service ( name , service target , service name , selector , durable , new string [ 0 ] ) ; final service name jms queue service name = jmsservices . get jms queue base service name ( service name ) . append ( name ) ;
if ( m is tab action down ) { m view pager . set current item ( ( int ) ( event . get x ( ) m tab size ) , true ) ; break ; } if ( m is action down ) break ; case motion event . action _ up :
admin client . rebalance ops . check each server in normal state ( final cluster ) ;
get camel context ( ) . add service ( url rewrite ) ;
start activity for result ( intent , 100 ) ;
this . type = null ;
long last reopen start ns = system . nano time ( ) ;
interceptor id = ( ( javassist method ) m ) . add interceptor internal ( interceptor class name , constructor args , scope , execution policy ) ;
strict mode . allow thread disk reads ( ) ;
int len atom = instruction [ re . node size + re . offset opdata ] ; prefix = new char [ len atom ] ; system . arraycopy ( instruction , re . node size * 2 , prefix , 0 , len atom ) ; } } } backref scan loop :
string section name = get and update cached section name ( info . title ) ;
string cache period string = ( string ) config . get ( cache period ) ; if ( cache period string = null & & cache period string . is empty ( ) ) { cache period = integer . parse int ( cache period string ) ; } }
. add all ( classpaths . get non classpath deps ( ) )
float network buf fraction = configuration . get float ( task manager options . network _ buffers _ memory _ fraction ) ;
memory . poke int array ( ptr , values , 0 , values . length , false ) ; assert ints equal ( values , ptr , false ) ; assert ints equal ( swapped values , ptr , true ) ;
list < list < path > > file groups = new linked list < list < path > > ( ) ; long [ ] size groups = new long [ ngroups ] ; int hi = files . size ( ) - 1 ; int lo = 0 ; list < path > group ;
body builder . new line ( ) ; body builder . append formal line ( if ( result . has errors ( ) ) { ) ; body builder . indent ( ) ; body builder . append formal line ( return % s . status ( % s . conflict ) . body ( result ) ; , get name of java type ( response _ entity ) , get name of java type ( spring java type . http _ status ) ) ; body builder . indent remove ( ) ; body builder . append formal line ( } ) ;
if ( index < = 0 ) { return 0 ; } if ( index > = sections . length ) { index = sections . length - 1 ; }
assert true ( q type . delete ( ) ) ;
map < server name , list < pair < hregion info , result > > > dead servers = rebuild user regions ( ) ; process dead servers and regions in transition ( dead servers ) ;
driver . get title ( ) ; }
_ source id = source id ; _ p source id = p source id ; _ event schema = schema . parse ( event schema ) ; _ schema id = schema helper . get schema id ( event schema ) ; _ partition function = partition function ; _ repl setter config = repl setter config ;
create new cache ( ) ; disk lru cache . snapshot snapshot = cache . get ( k1 ) ; assert snapshot value ( snapshot , 0 , a ) ; assert snapshot value ( snapshot , 1 , b ) ; snapshot . close ( ) ; }
if ( uncompressed direct buf . remaining ( ) > 0 ) { return false ; }
assert not null ( geo grid aggregation builder . parse ( geohash _ grid , st parser ) ) ;
if ( keep uncompressed buf & & uncompressed direct buf len > 0 ) return false ; if ( uncompressed direct buf . remaining ( ) > 0 ) { check if we have consumed all user - input if ( user buf len < = 0 ) { return true ; } else { copy enough data from user buf to uncompressed direct buf set input from saved data ( ) ; if ( uncompressed direct buf . remaining ( ) > 0 ) uncompressed direct buf is not full return true ; else return false ; } }
set items can focus ( true ) ;
super . set content view ( layout res id ) ;
double area = this region [ blobarea ] ;
target . set pattern ( existing pattern ) ;
codec default codec = test util . get default codec ( ) ;
assert equals ( actual foo t , foo t . to string ( ) ) ; assert equals ( strings ( ) , extract bar t ( instance ) ) ; type descriptor < kv < integer , string > > kv t = extract kv ( instance ) ; assert not null ( kv t ) ; assert that ( kv t . to string ( ) , core matchers . contains string ( kv < actual foo t , java . lang . string > ) ) ; }
postgre sqledge saver . write edges ( provider , edges ) ; postgre sqlhelpers . end transaction ( connection ) ;
assert true ( cm . report heart beat ( instance id1 ) ) ;
auth config = ( username password authentication provider config ) get security manager ( ) . load authentication provider config ( default2 ) ;
kernel transactions . block new transactions ( ) ;
assert true ( context . has cookie ( context cookie1 ) ) ;
_ has star tree = _ segment metadata properties configuration . get boolean ( metadata keys . star tree . star _ tree _ enabled , false ) ;
assert true ( temp dir . exists ( ) ) ; file dumb file = new file ( temp dir2 , dumb file ) ;
mod cluster . advertise ( config ) ; } catch ( exception e ) {
new filter command ( chain . input , action . accept ) . with option ( - i , lo ) ,
while ( stack . is empty ( ) ) { closeable closeable = stack . remove first ( ) ; try { closeable . close ( ) ; } catch ( throwable e ) { if ( throwable = = null ) { throwable = e ; } else { suppress ( throwable , e ) ; } } } if ( thrown = = null & & throwable = null ) { throwables . propagate if possible ( throwable , ioexception . class ) ; throw new assertion error ( throwable ) ; not possible }
if ( q obj name = null & & name . is string key ( ) ) { string q key name = q obj name + . + member name ; typed var var = syntactic scope . get var ( q key name ) ; jstype old type = var = = null ? null : var . get type ( ) ; if ( var = null & & var . is type inferred ( ) ) { var . set type ( old type = = null ? value type : old type . get least supertype ( old type ) ) ; } scope . infer qualified slot ( name , q key name , old type = = null ? unknown type : old type , value type , false ) ; }
data source under test . open ( test data spec ) ; byte [ ] returned buffer = new byte [ 8 ] ; data source under test . read ( returned buffer , 0 , 1 ) ; data source under test . close ( ) ; test response header . remove ( content - length ) ;
delete from context menu ( delete _ text _ for _ java _ class , path _ to _ java _ class ) ;
logger . warn ( { } : { } bgrewriteaof done last size : { } mb , current size : { } mb , host , port , get mb ( aof current size ) , get mb ( map utils . get long value ( loop map , aof _ current _ size ) ) ) ; break ; } else {
final count down latch lease attempt complete = new count down latch ( 1 ) ;
} catch ( illegal access exception ex ) {
new thread ( ) { public void run ( ) { thread . current thread ( ) . set name ( sender - + sock . get remote socket address ( ) ) ; try { send packets ( ) ; } catch ( interrupted exception e ) { log . warn ( unexpected interruption + e . get message ( ) ) ; } } } . start ( ) ;
store = get managed model store ( ) ; fstore = get managed feature store ( ) . get feature store ( test ) ; }
return supports dn d . boolean value ( ) ;
assert that ( item view type composer . extract wrapped view type part ( item view type composer . bit _ mask _ segment ) , is ( 0 ) ) ; }
final jsonobject tag link rel = new jsonobject ( ) ; tag link rel . put ( tag . tag _ t _ id , tag id ) ; tag link rel . put ( user ext . user _ t _ id , user id ) ; tag link rel . put ( link . link _ t _ id , link id ) ; tag link rel . put ( link . link _ score , link score ) ; tag user link repository . add ( tag link rel ) ;
dis conf common model dis conf common model = make dis conf common model ( disconf item . app ( ) , disconf item . env ( ) , disconf item . version ( ) ) ; disconf center item . set dis conf common model ( dis conf common model ) ;
project . add stmt procedure ( update p1 , update p1 set b2 = 2 ) ; project . add stmt procedure ( sum p1 , select sum ( b2 ) from p1 ; ) ; project . add stmt procedure ( update r1 , update r1 set b2 = 2 ) ; project . add stmt procedure ( sum r1 , select sum ( b2 ) from r1 ; ) ;
file output stream fos = new file output stream ( file , false ) ; try { fos . write ( string . value of ( port ) . get bytes ( ) ) ; } finally { iohelper . close ( fos ) ; } }
file output stream fos = new file output stream ( file , false ) ; try { fos . write ( string . value of ( port ) . get bytes ( ) ) ; } finally { iohelper . close ( fos ) ; } }
pstmt = conn . prepare statement ( alter table `cloud` . `cluster` add constraint `fk _ cluster _ _ data _ center _ id` foreign key ( `data _ center _ id` ) references `cloud` . `data _ center` ( `id` ) on delete cascade ) ; pstmt . execute update ( ) ; pstmt = conn . prepare statement ( alter table `cloud` . `snapshots` add index `i _ snapshots _ _ removed` ( `removed` ) ) ; pstmt . execute update ( ) ; pstmt = conn . prepare statement ( alter table `cloud` . `network _ tags` add constraint `fk _ network _ tags _ _ network _ id` foreign key ( `network _ id` ) references `networks` ( `id` ) on delete cascade ) ;
if ( input scheme = null ) { if ( on disk scheme = = null ) { return false ; } if ( input scheme . equals ignore case ( on disk scheme ) ) { return false ; } }
int common length with tsand type = key value . timestamp _ type _ size + common length ;
int bytes to copy = buf . length - bytes sent in chunk - count ;
volt project builder builder = new volt project builder ( ) ;
in memory sorter = null ; }
final detail ast starting dot = ast . get first child ( ) . get next sibling ( ) ; logs starred import violation ( starting dot ) ; }
. remove header ( pgpkey access data format . key _ userid ) . remove header ( pgpkey access data format . signature _ key _ userid )
if ( support common jsmodules ) { for ( compiler input input : ordered inputs ) { new process common jsmodules ( this ) . process ( * externs * null , input . get ast root ( this ) , * force module detection * false ) ; } }
assert . assert same ( map , map . get ( u2 ) ) ;
int ws ; if ( pred = head & & ( ( ws = pred . wait status ) = = node . signal | | ( ws < = 0 & & compare and set wait status ( pred , ws , node . signal ) ) ) & & pred . thread = null ) { node next = node . next ; if ( next = null & & next . wait status < = 0 ) compare and set next ( pred , pred next , next ) ; } else { unpark successor ( node ) ; } node . next = node ; help gc
s . bind ( new inet socket address ( localhost , 0 ) ) ;
for ( deployment deployment : application . production deployments ( ) . values ( ) ) { version map . compute if absent ( deployment . version ( ) , deployment statistics : : empty ) ; }
for ( int i = start line ; i < = stop line ; i + + ) { emit as html ( cf , i , doc ) ; } return cf . to string ( ) ; }
header buffer . put ( get major version ( ) ) ;
util . create initial ring ( ss , partitioner , endpoint tokens , key tokens , hosts , host ids , 6 ) ; removalhost = hosts . get ( 5 ) ;
verify headers ( workspace , header symlink tree folder , bar . h , baz . h , blech _ private . h ) ;
string clazz = constraint processor . class . get name ( ) ; if ( desc . has coprocessor ( clazz ) ) { return ; }
simple message in ear lib jar [ ] underlying message = ( simple message in ear lib jar [ ] ) ( ( object message ) message ) . get object ( ) ;
long [ ] tab = table ; int len = tab . length ;
finish application master ( loaded app1 , rm2 , am1 node , am1 ) ; finish application master ( loaded app2 , rm2 , am2 node , am2 ) ;
cassandra . start ( ) ; final socket options socket options = new socket options ( ) ;
list < string > list restriction name = new array list < string > ( privacy manager . get restrictions ( this ) . navigable key set ( ) ) ;
_ context . stat manager ( ) . add rate data ( client . send dropped , len , 0 ) ; if ( _ log . should log ( log . warn ) ) _ log . warn ( dropping + len + byte msg expired in queue ) ; return false ;
http method get xml = new get method ( cache factory . get rest url ( ) + + key ) ; get xml . set request header ( accept , application xml ) ; cache factory . get rest client ( ) . execute method ( get xml ) ; assert equals ( get xml . get status text ( ) , http status . sc _ ok , get xml . get status code ( ) ) ; assert true ( get xml . get response body as string ( ) . contains ( < name > anna < name > ) ) ; }
boolean accepted = false ;
assert null ( parser . get namespace prefix ( 0 ) ) ;
common swing . error message ( e ) ;
stage config previous stage = go config service . previous stage ( pipeline name , current stage . get name ( ) ) ; stage most recent passed = stage service . most recent passed ( pipeline name , case insensitive string . str ( previous stage . name ( ) ) ) ; if ( most recent passed = null & & most recent passed . get pipeline id ( ) > current stage . get pipeline id ( ) ) { pipeline most recent eligible pipeline = pipeline dao . load pipeline ( most recent passed . get pipeline id ( ) ) ; if ( most recent eligible pipeline . has stage been run ( current stage . get name ( ) ) ) { trigger next stage in pipeline ( most recent eligible pipeline , most recent passed . get name ( ) , default _ approved _ by ) ; } }
assert true ( side input handler . is ready ( view1 , first window ) ) ;
m select extension . with selectable ( selectable ) ; return this ; }
new response . get to ( ) . remove parameter ( tag ) ;
in file . delete ( ) ;
request req = new request ( id ) ;
test types ( * * @ param { object } a * \ n + function f ( a ) { \ n + a . prototype = { foo : 3 } ; \ n + } \ n + * * @ param { object } b \ n + * \ n + function g ( b ) { \ n + b . prototype = function ( ) { } ; \ n + } \ n , assignment to property prototype of object \ n + found : { foo : number } \ n + required : function ( ) : undefined ) ;
search request . source ( search source builder ) ; search response search response = client . search ( search request ) ; {
selected view . set text color ( get resources ( ) . get color ( r . color . text _ _ primary _ dark ) ) ;
assert equals ( 0 , row key . row key contains metric ( new byte [ ] { } , new byte [ ] { 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 2 , 0 , 0 , 3 } ) ) ;
m list view . set adapter ( m list adapter ) ; break ; default : throw new illegal state exception ( recycler layout not supported ) ; } }
if ( ( root type = null ) & & root type . get raw class ( ) . is assignable from ( value . get class ( ) ) ) { _ report incompatible root type ( value , root type ) ; }
edgepairs . add ( new triple < reil block , iaddress , edge type > ( reil block , null , edge type . jump _ conditional _ false ) ) ; if ( convert . is dec string ( jump target ) ) { edgepairs . add ( new triple < reil block , iaddress , edge type > ( reil block , to reil address ( jump target ) , edge type . jump _ conditional _ true ) ) ; } else if ( reil instruction . get third operand ( ) . get type ( ) = = operand type . sub _ address ) { final string [ ] parts = jump target . split ( \ \ . ) ; edgepairs . add ( new triple < reil block , iaddress , edge type > ( reil block , to reil address ( parts ) , edge type . jump _ conditional _ true ) ) ; }
string z = config . servers ; string servers [ ] = z . split ( , ) ; string [ ] tbl suffix _ = new string [ ] { ptn , rep } ; string s ptn or rep = tbl suffix _ [ ptn or rep ] ; int n tables = 0 ; used for catalog check
for ( iterator < pgpsecret key > iter key = keyring . get secret keys ( ) ; iter key . has next ( ) ; ) { pgpsecret key sec key = iter key . next ( ) ; if ( is signing key ( sec key ) ) { pgpprivate key private key = sec key . extract private key ( new jce pbesecret key decryptor builder ( ) . set provider ( provider ) . build ( sig key user id2 password . get ( found key user id for user id part [ 1 ] ) . to char array ( ) ) ) ; if ( private key = null ) { result . add ( new pgpsecret key and private key and user id ( sec key , private key , found key user id for user id part [ 0 ] ) ) ; log . debug ( private key with user id { } and key id { } added to the signing keys , found key user id for user id part [ 0 ] , long . to string ( private key . get key id ( ) ) ) ; } } }
return gl version . is version equal to or higher ( 3 , 0 ) | | glfw . glfw extension supported ( gl _ ext _ framebuffer _ object ) | | glfw . glfw extension supported ( gl _ arb _ framebuffer _ object ) ;
list < camera > cameras = new array list < camera > ( ) ;
assert equals ( 2 , full . add all absent ( arrays . as list ( three , four , one ) ) ) ;
if ( pre auth entry point = null ) { return pre auth entry point ; } pc . get reader context ( ) . error ( no authentication entry point could be established . please + make sure you have a login mechanism configured through the namespace ( such as form - login ) or + specify a custom authentication entry point with the ' + att _ entry _ point _ ref + ' attribute , pc . extract source ( http elt ) ) ;
final file download model model = mock ( file download model . class ) ; when ( model . get id ( ) ) . then return ( 1 ) ; when ( model . get status ( ) ) . then return ( file download status . pending ) ;
string extended _ reporting _ pref _ to _ remove = pref service bridge . is safe browsing scout reporting active ( ) ? pref _ safe _ browsing _ extended _ reporting : pref _ safe _ browsing _ scout _ reporting ; preference screen . remove preference ( find preference ( extended _ reporting _ pref _ to _ remove ) ) ; chrome base check box preference safe browsing pref = ( chrome base check box preference ) find preference ( pref _ safe _ browsing ) ;
adjusted text = adjust template references ( node text ) ;
throw new illegal state exception ( callable error ) ; } else { throw new illegal argument exception ( callable error ) ; } } } ; }
throw new runtime exception ( ) ;
node list child nodes = element . get child nodes ( ) ; for ( int i = 0 ; i < child nodes . get length ( ) ; i + + ) { node node = child nodes . item ( i ) ; if ( node . get node type ( ) = = node . element _ node ) { throw new exception ( encoding element is invalid ) ; } } result . set value ( element . get text content ( ) ) ; return result ;
final boolean update = args . flags . contains ( options . update ) ; final boolean skip crccheck = args . flags . contains ( options . skipcrc ) ; final boolean overwrite = update & & args . flags . contains ( options . overwrite ) ; final boolean skip under construction file = args . flags . contains ( options . skipunderconstruction ) ; job conf . set boolean ( options . update . propertyname , update ) ; job conf . set boolean ( options . skipcrc . propertyname , skip crccheck ) ; job conf . set boolean ( options . overwrite . propertyname , overwrite ) ; job conf . set boolean ( options . ignore _ read _ failures . propertyname , args . flags . contains ( options . ignore _ read _ failures ) ) ; job conf . set boolean ( options . preserve _ status . propertyname , args . flags . contains ( options . preserve _ status ) ) ; final string random id = get random id ( ) ;
retrofit retrofit = new retrofit . builder ( ) . base url ( server . url ( ) ) . add converter factory ( gson converter factory . create ( ) ) . build ( ) ; service service = retrofit . create ( service . class ) ; response < user > response = service . get user ( ) . execute ( ) ;
method method = method utils . get accessible method ( s package parser class , generate application info , m package . get class ( ) , int . class , boolean . class , int . class , int . class ) ;
resolver . load ( com . databricks : spark - csv _ 2 . 10 : 1 . 3 . 0 , test copy path ) ;
if ( value < short . min _ value | | value > short . max _ value ) { throw ctxt . weird string exception ( _ value class , overflow , value can not be represented as 16 - bit value ) ; } return ( short ) value ;
val = val % groups . get int ( count ) ;
execute ( insert into % s ( k , v ) values ( 1 , ? ) , 1 ) ; row = execute ( select v , writetime ( v ) as wt from % s where k = 1 ) . one ( ) ;
mock meta store event listener . pop and verify last event id ( event type . drop _ table , first event id + 2 ) ; mock meta store event listener . pop and verify last event id ( event type . create _ table , first event id + 1 ) ;
} else { system . out . println ( generating random database password ) ; app config . encrypt property ( db password , random string utils . random ( 32 , true , true ) ) ; }
init view all extension ( view , 10 . 0 ) ;
assert equals ( capacity _ normal , this . channel1 . write ( write buf , 0 , 2 ) ) ;
if ( pos > block end | | current node = = null ) { current node = block seek to ( pos ) ; }
string big param str = job . get ( mapred . bigparam . path , ) ; if ( big param str = null & & big param str . length ( ) > 0 ) { path big param path = new path ( big param str ) ; file file = new file ( big param path . to uri ( ) . get path ( ) ) . get absolute file ( ) ; buffered reader in = new buffered reader ( new input stream reader ( new file input stream ( file ) ) ) ; int max _ buffer _ size = 1024 ; string builder result = new string builder ( ) ; char [ ] buffer = new char [ max _ buffer _ size ] ; int read chars = 0 , total chars = 0 ; while ( ( read chars = in . read ( buffer , 0 , max _ buffer _ size ) ) > 0 ) { result . append ( buffer , 0 , read chars ) ; total chars + = read chars ; } job . set ( mapred . input . dir , result . to string ( ) ) ; log . info ( read mapred . input . dir : + total chars ) ; in . close ( ) ; } task runner . setup work dir ( job ) ;
primitive types buffer = new byte array output stream ( 128 ) ;
system . out . println ( fetching the authorization url . . . ) ; final string authorization url = service . get authorization url ( ) ; system . out . println ( got the authorization url ) ; system . out . println ( now go and authorize scribe java here : ) ; system . out . println ( authorization url ) ; system . out . println ( and paste the authorization code here ) ; system . out . print ( > > ) ; final string code = in . next line ( ) ; system . out . println ( ) ;
parallel stream reader parallel reader ;
long total size = 0 ;
return ( i > > > distance ) | ( i < < - distance ) ; }
delete delete = new delete ( row1 ) ; delete . delete column ( fam1 , qual ) ; delete . delete column ( fam1 , qual ) ; region . delete ( delete , null , false ) ; get get = new get ( row1 ) ;
if ( kind = = pconstants . arrow ) { x = 10 ; y = 7 ; } else if ( kind = = pconstants . hand ) { x = 12 ; y = 8 ; } else if ( kind = = pconstants . text ) { x = 16 ; y = 22 ; }
if ( _ property _ list _ pattern ) { if ( end = = offset ) { property list string is empty . dest _ chars [ end ] = ' * ' ; } else { property list string is not empty . dest _ chars [ end ] = ' , ' ; dest _ chars [ end + 1 ] = ' * ' ; } } return new string ( dest _ chars ) ;
ticker column . set target char ( ' 0 ' ) ; set progress ( 0f ) ; verify draw ( 1 , 0f ) ; set progress ( 0 . 25f ) ;
int alt4 = 9 ;
filter unrolled = app schema data access . unroll filter ( equals first value , root mapping ) ;
pooled . do work ( ) ;
this . executed operations counter = partition id = = ad _ hoc _ partition _ id ? new mw counter ( ) : new sw counter ( ) ;
write file ( fs , my file , 10 ) ;
int index entry name byte length = index entry flags & 4095 ; if ( log . is debug enabled ( ) ) log . debug ( entry + entry index + has a name of length + index entry name byte length ) ;
if ( board plane . get total warp ( undistorted board markers , board warp [ camera number ] ) > max error ) { assert ( false ) ; }
execute ( plan ) . get result ( ) ; execute now that the partitions are gone }
for ( int i = 1 ; i < 1000 ; i + + ) { assert equals ( base case : i = + i , compute norm ( d , bogus , i ) , compute norm ( s , bogus , i ) , 0 . 0f ) ; }
dataset list cache datasets cache = new dataset list cache ( anomaly function dao , dataset config dao , thirdeye config ) ;
object next version = persister . force version increment ( entry . get id ( ) , entry . get version ( ) , source ) ;
if ( benchmark . debug ) { total connections = benchmark . get _ total connections ( ) ; msg = in main . . catch block ready to exit , total connections = + total connections . get ( ) ; prt ( msg ) ; } benchmark . ready to exit ( ) ;
if ( options . emit wrapper methods ( ) & & element util . is private inner type ( declaring class ) & & element util . is private ( element ) ) { set function caller ( node , element ) ; } else { node . remove ( ) ; } } }
r . get atmosphere config ( ) . resources factory ( ) . remove ( uuid ) ;
map work work = current task . get work ( ) . get map work ( ) ; work . set inputformat ( query context . get index input format ( ) ) ; work . add index intermediate file ( query context . get index intermediate file ( ) ) ;
if ( ( _ version = null ) & & ( _ version . equals ( xml _ version ) ) ) { field = cpg . add fieldref ( translet _ class , _ version , string _ sig ) ; il . append ( dup ) ; il . append ( new push ( cpg , _ version ) ) ; il . append ( new putfield ( field ) ) ; }
if ( nil & & n > 0 ) { add navigation node ( token . up ) ; }
array list < dependency > deps = shiny dependencies ( ) ; htmltools version
if ( fast = null ) { slow = slow . next ; } while ( slow = null ) { int top = stack . pop ( ) . int value ( ) ; system . out . println ( slow . data + + top ) ; if ( top = slow . data ) { return false ; } slow = slow . next ; }
return + ( char ) input . read unsigned byte ( ) ;
new nodes . add ( new ast change ( ref . module , ref . scope , ref . node ) ) ;
logger . warn ( the selected eip { } is associated with another instance { } according to aws , + hence skipping this , eip entry , associated instance id ) ;
string create tab = create table { hiveconf : + tab name var + } ( id int ) ;
url connection . connect ( ) ;
ignore el = true ; }
assert that ( bytes ) . is equal to ( new byte [ ] { ( byte ) 0xfe , ( byte ) 0xca , 0x03 , 0x00 , 0x00 , 0x11 , 0x22 } ) ;
string session id = get requested session id ( ) ;
int server port = 0 ; string locator str = null ; if ( props = null & & props . is empty ( ) ) { string server port str = props . get property ( serverport _ property _ name ) ; if ( server port str = null ) { server port = integer . parse int ( server port str ) ; } server host = props . get property ( serverhost _ property _ name , serverhost _ property _ default ) ; locator str = props . get property ( locator _ property _ name ) ; string topology = props . get property ( topology _ property _ name ) ; if ( topology = null & & topology . equals ( topology _ p2 p _ value ) ) { cache factory cf = new cache factory ( ) ; if ( locator str = null ) { cf . set ( locators , locator str ) ; } cache = cf . create ( ) ; is client = false ; return ; } }
m week view . set column gap ( ( int ) typed value . apply dimension ( typed value . complex _ unit _ dip , 2 , get resources ( ) . get display metrics ( ) ) ) ;
for ( string id : arrays . as list ( 42 , 99 ) ) { for ( solr params p : arrays . as list ( single fl , multi fl ) ) { assert q ( id + : + p , req ( p , qt , get , id , id , wt , xml ) , count ( doc ) = 1 , doc str [ @ name = ' id ' ] , doc int [ @ name = ' [ docid ] ' ] [ . > = - 1 ] , doc float [ @ name = ' abs ( val _ i ) ' ] [ . = ' 1 . 0 ' ]
task id map task id = new task id ( job . get id ( ) , true , 0 ) ; task id reduce task id = new task id ( job . get id ( ) , false , 0 ) ; task in progress cleanups [ ] = jip . get tasks ( task type . job _ cleanup ) ; task id cleanup task id ; if ( cleanups [ 0 ] . is complete ( ) ) { cleanup task id = cleanups [ 0 ] . get tipid ( ) ; } else { cleanup task id = cleanups [ 1 ] . get tipid ( ) ; } task in progress setups [ ] = jip . get tasks ( task type . job _ setup ) ; task id setup task id ; if ( setups [ 0 ] . is complete ( ) ) { setup task id = setups [ 0 ] . get tipid ( ) ; } else { setup task id = setups [ 1 ] . get tipid ( ) ; } map < string , job history . task > tasks = job info . get all tasks ( ) ;
text format . parser parser = text format . parser . new builder ( ) . set parse info tree builder ( tree builder ) . build ( ) ; final string string data = optional _ int32 : 1 \ n + optional _ int64 : 2 \ n + optional _ double : 2 . 4 \ n + repeated _ int32 : 5 \ n + repeated _ int32 : 10 \ n + optional _ nested _ message < \ n + bb : 78 \ n + > \ n + repeated _ nested _ message < \ n + bb : 79 \ n + > \ n + repeated _ nested _ message < \ n + bb : 80 \ n + > ;
if ( charset . is supported ( encoding or permission ) ) { return xstream ( encoding or permission , ( string ) null ) ; } else { return xstream ( null , encoding or permission ) ; }
html . style ( . metrics { margin - bottom : 5px } ) ; cluster metrics info cluster metrics = new cluster metrics info ( this . rm ) ;
final hazelcast instance instance2 = factory . new hazelcast instance ( config ) ;
if ( weather . get provider ( ) = = provider name . openweathermap & & weather . get response code ( ) = null & & weather . get response code ( ) = = 200 ) { weather . set error ( null ) ; } if ( weather . has error ( ) & & response . is empty ( ) ) { weather . set error ( error : response is empty ) ; }
message = clicked : child + group position + - + child position ;
if ( timestamp adjuster = = null | | input buffer . subsample offset us = timestamp adjuster . get timestamp offset us ( ) ) { timestamp adjuster = new timestamp adjuster ( input buffer . time us ) ; timestamp adjuster . adjust sample timestamp ( input buffer . time us - input buffer . subsample offset us ) ; } byte buffer buffer = input buffer . data ;
map join persistable table container table container = map join tables [ tag ] ; if ( table container = = null ) { continue ; }
map < string , string > labels = view generation service . get i18n labels ( detail member details , last . child type , last . child entity metadata , controller metadata , module , ctx ) ; get i18n operations ( ) . add or update labels ( module , labels ) ; break ; case search :
chunk = m _ array [ m _ last chunk ] = new char [ m _ chunk size ] ;
try { method p method = clazz . get method ( method . get name ( ) , method . get parameter types ( ) ) ; db = p method . get annotation ( db . class ) ; if ( db = null ) { return db . txn ( ) ; } } catch ( security exception e ) { } catch ( no such method exception e ) { }
try { get song tags ( song uris list ) ; } catch ( cannot read exception e ) {
if ( m viewport width = = 0 ) { if ( m viewport initial scale = = 0 ) { m viewport initial scale = default scale ; } }
eq seed test ( a , b , 300 ) ;
type definition implicit type def = building context . get metadata collector ( ) . get type definition ( returned class name ) ;
expr node column desc c = expr node desc utils . get column expr ( l operand ) ;
if ( _ scanned < = level ) { abstract translet translet = _ settings . get translet ( ) ; locale [ ] locales = _ settings . get locales ( ) ; string [ ] case order = _ settings . get case orders ( ) ; get value from dom if accessed for the first time final string str = extract value from dom ( _ dom , _ node , level , translet , _ last ) ; final comparable key = string comparable . get comparator ( str , locales [ level ] , _ collators [ level ] , case order [ level ] ) ; _ values [ _ scanned + + ] = key ; return ( key ) ; } return ( ( comparable ) _ values [ level ] ) ;
dfstest util . append file ( hdfs , file , blocksize - 1 ) ;
hregion best flushable region = get biggest memstore region ( regions by size , excluded regions , true ) ;
throw new jasper exception ( localizer . get message ( jsp . error . beans . nobeaninfo , bean class . get name ( ) ) ) ;
if ( active nodes . contains ( pw . get id ( ) ) ) { pw . get model ( ) . set in action list ( false ) ; continue ; } pw . get model ( ) . set cur ooz job id ( program widget . model . latest _ oozie _ jobid ) ;
assert null ( difference between maps ignoring array order ( convert to map ( cluster state from diffs ) , convert to map ( cluster state ) ) ) ;
result . set coder ( kv coder . of ( key coder , in . get coder ( ) ) ) ;
assert equals ( size of pending reconstructions , 0 , pending reconstructions . size ( ) ) ;
if ( random . next double ( ) < 0 . 1 ) { assert that ( reservoir . size ( ) ) . as ( bad reservoir size with : threshold = % d , updates per tick = % d , threshold , updates per tick ) . is less than or equal to ( window * 256 ) ; }
assert that ( uri . apply ( always - > null ) . to string ( ) , equal to ( db data ) ) ;
card grid staggered view staggered view = ( card grid staggered view ) get activity ( ) . find view by id ( r . id . carddemo _ extras _ grid _ stag ) ;
l = - math . min ( func , w , tol , max iter ) ;
conf . set long ( flush large stores policy . hregion _ columnfamily _ flush _ size _ lower _ bound _ min , 0 ) ; try { test _ util . start mini cluster ( 1 ) ; test _ util . get admin ( ) . create namespace ( namespace descriptor . create ( tablename . get namespace as string ( ) ) . build ( ) ) ; test _ util . get admin ( ) . create table ( htd ) ; connection conn = connection factory . create connection ( conf ) ; table table = conn . get table ( tablename ) ; do put ( table , memstore flush size ) ; table . close ( ) ; conn . close ( ) ; region region = get region with name ( tablename ) . get first ( ) ; cf1 store file count1 = region . get store ( family1 ) . get storefiles count ( ) ; cf2 store file count1 = region . get store ( family2 ) . get storefiles count ( ) ; cf3 store file count1 = region . get store ( family3 ) . get storefiles count ( ) ; } finally { test _ util . shutdown mini cluster ( ) ; } log . info ( disable selective flush : + bytes . to string ( family1 ) + = > + cf1 store file count + , + bytes . to string ( family2 ) + = > + cf2 store file count + , + bytes . to string ( family3 ) + = > + cf3 store file count ) ;
byte lengths [ index ] = - 1 ;
current unapplied rotation degrees = pending rotation degrees ;
template = dd . find supp resource file ( resource , path ) ; if ( template = = null ) {
conf . set long ( hconstants . hregion _ memstore _ flush _ size , flush size bytes ) ;
fsdata output stream stm = file sys . create ( name , true , file sys . get conf ( ) . get int ( io . file . buffer . size , 4096 ) , repl , ( long ) block size ) ; return stm ; }
if ( extended statistics ) { statistics . on hit ( key ) ; } }
tester . assert response ( request ( application v4 tenant tenant1 , , request . method . get ) , new file ( tenant - with - application . json ) ) ;
get mock endpoint ( mock : bar ) . expected bodies received ( bye world ) ; get mock endpoint ( mock : error ) . expected bodies received ( kabom ) ; template . send body ( direct : bar , bye world ) ;
d = 1234 . 5678 ; i = ( int ) d ; system . out . println ( d = + d + - - > i = + i ) ; d = - 1234 . 5678 ;
element content = domutil . get first child element ( elm node ) ; xsannotation impl annotation = null ; if ( content = null & & domutil . get local name ( content ) . equals ( schema symbols . elt _ annotation ) ) { annotation = traverse annotation decl ( content , attr values , false , schema doc ) ; content = domutil . get next sibling element ( content ) ; } else { string text = domutil . get synthetic annotation ( elm node ) ; if ( text = null ) { annotation = traverse synthetic annotation ( elm node , text , attr values , false , schema doc ) ; } } xsobject list annotations ;
return path . split ( \ \ s * \ \ . \ \ s * ) ;
final iterator < namespace > original namespaces = start element . get namespaces ( ) ;
view view = find view by id ( r . id . ripple _ layout _ 2 ) ; material ripple layout . on ( view ) . ripple color ( color . parse color ( ff0000 ) ) . ripple alpha ( 0 . 2f ) . ripple hover ( true ) . create ( ) ; view . set on long click listener ( this ) ;
decoder . reset ( ) ; decoder . decode ( in , out , true ) ; out . rewind ( ) ; coder result result = decoder . flush ( out ) ; assert same ( result , coder result . underflow ) ;
if ( encoding = null & & _ mime type = null & & ( _ mime type . compare to ( jar _ mime _ type ) = = 0 | | _ mime type . compare to ( jar _ mime _ type _ new ) = = 0 ) & & encoding . to lower case ( ) . index of ( download response . pack200 _ gzip _ encoding ) > - 1 ) { search _ path = orig _ path + . pack . gz ; _ resource = context . get resource ( search _ path ) ;
if ( cmd . has option ( opt _ hfile _ name ) ) { log . error ( please specify hfile name using the + opt _ hfile _ name + option ) ; print usage ( options ) ; system . exit ( - 1 ) ; } string path name = cmd . get option value ( opt _ hfile _ name ) ; string compression name = default _ compression . get name ( ) ;
min average value bytes = decimal _ value _ bytes _ overhead + short _ decimal _ value _ bytes ;
qname new type name = guess value type ( property . get value ( ) ) ; xsdtype definition type = ( new type name = null ) ? index . get type definition ( new type name ) : null ; if ( type = null ) { create a new particle based on the new type xsdelement declaration value = xsdfactory . e instance . create xsdelement declaration ( ) ; value . set name ( value ) ; value . set type definition ( type ) ; xsdparticle particle = xsdfactory . e instance . create xsdparticle ( ) ; particle . set min occurs ( 1 ) ; particle . set max occurs ( 1 ) ; particle . set content ( value ) ; properties . add ( new object [ ] { particle , property . get value ( ) } ) ; } else { coudl not determine new type , just fall back to xs : any type object [ ] p = new object [ ] { schemas . get child element particle ( element . get type ( ) , value , false ) , property . get value ( ) } ; properties . add ( p ) ; }
if ( e . get node ( ) = = null ) e . set node ( node ) ;
if ( centrality . equals ( zero value ) ) { single source shortest path . reset ( ) ; single source shortest path . set start node ( node ) ; process shortest paths ( node , single source shortest path ) ; }
select list = rule param type . multiple list of values ( foo , one , two | three , four ) ;
if ( loop index > off _ switch _ loop & & num allocated = = 0 ) { log . warn ( unable to allocate any opportunistic containers . ) ; break ; }
system . out . println ( string . format ( [ % d ] failure in another thread , id ) ) ;
{ file schema index = new file ( store dir , schema index ) ; create file of size ( schema index , 2 ) ; index directory structure directory structure = mock ( index directory structure . class ) ; when ( directory structure . root directory ( ) ) . then return ( schema index ) ; when ( schema index provider . directory structure ( ) ) . then return ( directory structure ) ; }
return item = null & & text utils . is empty ( item . label ) & & item . amount = null & & text utils . is empty ( item . amount . currency ) & & text utils . is empty ( item . amount . value ) ;
error _ token = new symbol ( error _ sym ( ) , left _ pos , right _ pos ) ; error _ token . parse _ state = act - 1 ; error _ token . used _ by _ parser = true ; stack . push ( error _ token ) ; tos + + ; return true ;
super . set on hierarchy change listener ( m pass through listener ) ; m initialized = true ;
under test . set level ( logger level . trace ) ; loggers . get ( logger1 ) . trace ( ( ) - > { touched trace . set ( true ) ; return a trace information ; } ) ; loggers . get ( logger1 ) . debug ( ( ) - > { touched debug . set ( true ) ; return a debug information ; } ) ; assert that ( under test . logs ( ) ) . contains exactly ( a trace information , a debug information ) ;
string new rack = rack + ( + + num of racks ) ;
mock tree = ( mock resource calculator process tree ) get process tree info ( get container id ( 2 ) ) . get process tree ( ) ;
timber . w ( message compose was started with an unsupported action ) ;
value controller . update selection value ( new value ) ; } } catch ( dbexception e ) {
writer . begin constructor ( enum set . none of ( modifier . class ) , column info , src , boolean , mutable ) ; writer . emit statement ( super ( src , mutable ) ) . emit statement ( copy ( src , this ) ) ; writer . end constructor ( ) . emit empty line ( ) ;
if ( route . get route context ( ) . get route policy list ( ) = null ) { for ( route policy route policy : route . get route context ( ) . get route policy list ( ) ) { route policy . on remove ( route ) ; } }
pause between state changes ( ) ; }
assert true ( site . filter ( make init ( @ quiesce ) ) ) ;
repo . release ( 1 l ) ;
this . conf . set boolean ( hconstants . use _ meta _ replicas , false ) ; replication . decorate master configuration ( this . conf ) ;
assert that ( cache . get unchecked ( 2 ) ) . is equal to ( 2 ) ; assert that ( cache . as map ( ) . key set ( ) ) . contains exactly ( 2 ) ; cache testing . process pending notifications ( cache ) ; cache testing . check valid state ( cache ) ; assert that ( removal listener . get count ( ) ) . is equal to ( 1 ) ;
file branding file = new file ( server file system . get home dir ( ) , branding _ file _ path ) ;
catch ( java . io . unsupported encoding exception uee ) { bytes = s . get bytes ( ) ;
final table data insert all response first failure = new table data insert all response ( ) . set insert errors ( immutable list . of ( new insert errors ( ) . set index ( 0 l ) . set errors ( immutable list . of ( new error proto ( ) . set reason ( timeout ) ) ) , new insert errors ( ) . set index ( 1 l ) . set errors ( immutable list . of ( new error proto ( ) . set reason ( invalid ) ) ) ) ) ;
if ( xalan xpath api . is installed ( ) ) { return new xalan xpath factory ( ) ; }
if ( m file . exists ( ) ) { if ( m file . delete ( ) ) { log . e ( tag , couldn ' t clean up partially - written file + m file ) ; } } mcr . set disk write result ( false ) ; }
if ( count down . decrement and get ( ) < = 0 )
response . write custom frame ( frame type , frame status , payload ) ;
go to ( read cursor , old right child ) ;
if ( c . get minimum number instances ( ) > get minimum number instances ( ) ) set minimum number instances ( c . get minimum number instances ( ) ) ;
if ( keep original text ) { basic document . original text = text ; } else { basic document . original text = null ; }
low = math . max ( low , data area . get min x ( ) ) ;
add companion parity blocks ( parity , ( inode file ) parity inode , stripe index , parity length , result ) ; return result ;
journal = new journal ( test _ log _ dir , test _ img _ dir , jid , mock error reporter , mock journal node ) ;
string props = dom utils . get text value ( element ) ; if ( string utils . has text ( props ) ) { builder . add property value ( properties , props ) ; } }
m background = get bitmap for ( r . drawable . jog _ dial _ bg ) ; m dimple = get bitmap for ( r . drawable . jog _ dial _ dimple ) ; m dimple dim = get bitmap for ( r . drawable . jog _ dial _ dimple _ dim ) ; m arrow long left = get bitmap for ( r . drawable . jog _ dial _ arrow _ long _ left _ green ) ; m arrow long right = get bitmap for ( r . drawable . jog _ dial _ arrow _ long _ right _ red ) ; m arrow short left and right = get bitmap for ( r . drawable . jog _ dial _ arrow _ short _ left _ and _ right ) ; m interpolator = new decelerate interpolator ( 1f ) ;
assert . assert equals ( extraction fn , object mapper . read value ( object mapper . write value as bytes ( extraction fn ) , extraction fn . class ) ) ; }
byte [ ] data = new byte [ ( int ) size ] ;
string agent name = conf . get ( http . agent . name ) ;
db . command ( new ocommand sql ( create class test convert double ) ) . execute ( ) ; db . command ( new ocommand sql ( insert into test convert double set num = 100000 ) ) . execute ( ) ; list < odocument > results = db . query ( new osqlsynch query < odocument > ( select from test convert double where num > = 50000 and num < = 300000000 ) ) ;
jsonnode json = to json ( data ) ; string buffer buffer = new string buffer ( ) ; json . to string ( buffer ) ; system . out . println ( buffer . to string ( ) ) ;
assert . assert false ( exists ( fc2 , test dir path ) ) ;
assert true ( arrays . equals ( b , ret ) ) ;
value table desc [ tag ] = g work . get tag to value desc ( ) . get ( tag ) ; input value deserializer [ tag ] = reflection utils . new instance ( value table desc [ tag ] . get deserializer class ( ) , null ) ; ser de utils . initialize ser de ( input value deserializer [ tag ] , null , value table desc [ tag ] . get properties ( ) , null ) ; value object inspector [ tag ] = input value deserializer [ tag ] . get object inspector ( ) ; array list < object inspector > ois = new array list < object inspector > ( ) ;
string rules coll = port rule coll2 ;
if ( obj class = = object stream class . objectstreamclassclass ) { return write class desc ( ( object stream class ) object , unshared ) ; }
for ( int i = 3 ; i < 3 + node bytes ; i + + ) { int incoming byte = incoming message . get message payload byte ( i ) ; loop bits in byte for ( int j = 0 ; j < 8 ; j + + ) { int b1 = incoming byte & ( int ) math . pow ( 2 . 0 d , j ) ; int b2 = ( int ) math . pow ( 2 . 0 d , j ) ; if ( b1 = = b2 ) { logger . info ( node { } : node found , node id ) ; zwave nodes . add ( node id ) ; } node id + + ; } } logger . info ( zwave controller using { } api , ( ( incoming message . get message payload byte ( 1 ) & 0x01 ) = = 1 ) ? slave : controller ) ;
multi split panel . wait number open split panels ( 1 ) ;
validation complete c3 = new validation complete ( desc ) ; test repair message write ( service . validation complete . bin , c0 , c1 , c3 ) ;
string data = in ut magna vel mauris malesuada ;
port = available port finder . get next available ( 22000 ) ;
context ( ) . get messages module ( ) . get send message actor ( ) . send ( new sender actor . message sent ( peer , message sent . get rid ( ) ) ) ; return on outgoing sent ( peer , message sent . get rid ( ) , message sent . get date ( ) ) ;
if ( this . stop row key = = null ) { return true ; filter . . . } return false ; }
channel manager . close channel ( channel ) ; return ; }
try { queue . put ( create query request ( group one , metrics ) ) ; } catch ( out of capacity error e ) { assert true ( true ) ; return ; } assert true ( false ) ; }
return target . convert from base64 png ( base64 ) ; }
document builder builder = null ;
data fetcher data fetcher = new data fetcher ( ) { @ override public object get ( data fetching environment environment ) { user context user ctx = environment . get context ( ) ; long business obj id = environment . get argument ( business obj id ) ; return invoke business layer method ( user ctx , business obj id ) ; } } ;
f is immutable = true ;
s3 policy condition block block = one statement . get condition block ( ) ;
order . verify ( tx bridge ) . bind transaction to current thread ( second ktx ) ;
app widget manager . update app widget ( app widget id , views ) ;
if ( em = null & & em . is open ( ) ) { em . close ( ) ; }
general name interface temp name = make general name interface ( type , name ) ;
zk = null ; }
assert equals ( null , dfsutil . get namenode service addr ( conf , null , ns1 - nn1 ) ) ;
selected view . set text color ( get resources ( ) . get color ( r . color . text _ _ primary _ dark ) ) ;
simple entity field access = new field access jpaentity ( ) ; simple entity field access . set id ( 1 l ) ; simple entity field access . set value ( value1 ) ; manager . persist ( simple entity field access ) ; simple entity property access = new property access jpaentity ( ) ; simple entity property access . set id ( 1 l ) ; simple entity property access . set value ( value2 ) ; manager . persist ( simple entity property access ) ; subclass field access = new subclass field access jpaentity ( ) ;
translated flash cookie map . put ( entry . get key ( ) , message value ) ;
em . get transaction ( ) . begin ( ) ; address1 = new address ( freiburgerstrasse , 5 ) ; em . persist ( address1 ) ; address2 = new address ( hindenburgstrasse , 30 ) ; em . persist ( address2 ) ; vw owner = new person ( vw owner , 20 , address1 ) ; em . persist ( vw owner ) ; ford owner = new person ( ford owner , 30 , address1 ) ; em . persist ( ford owner ) ; toyota owner = new person ( toyota owner , 30 , address2 ) ; em . persist ( toyota owner ) ; final person non owner = new person ( non owner , 30 , address1 ) ; em . persist ( non owner ) ; vw = new car ( vw ) ; vw . set owner ( vw owner ) ; em . persist ( vw ) ; ford = new car ( ford ) ; ford . set owner ( ford owner ) ;
assert equals ( 0 , registry . get number of registered synchronizations ( ) ) ;
final boolean nothing in body = ( approval body = null & & body is iterable but empty ) ;
cached ref . close ( ) ; assert total size ( 1 , 100 ) ; assert exclusively owned size ( 1 , 100 ) ; assert exclusively owned ( key , 100 ) ; verify ( m releaser , never ( ) ) . release ( any int ( ) ) ; }
cluster . restart data nodes ( ) ;
set < string > final pipeout files = new hash set < string > ( arrays . as list ( get pipeout files ( ) ) ) ; final pipeout files . remove all ( existing pipeout files ) ; assert . assert true ( final pipeout files . is empty ( ) ) ; }
if ( err = = errno _ eagain _ negative | | err = = errno _ ewouldblock _ negative ) { return 0 ; }
if ( m comment = = null ) { show error and finish ( ) ; return ; } configure views ( ) ; }
fs . create new file ( new path ( archived hfile dir , dfd - dfd ) ) ;
iterator < string > i = m _ table indexes . key set ( ) . iterator ( ) ; while ( i . has next ( ) ) { string next key = i . next ( ) ; int index = m _ table indexes . get ( next key ) . int value ( ) ; if ( index > row num . int value ( ) ) { index - - ;
calendar next time cal = calendar . get instance ( ) ; next time cal . set time ( base time ) ; process engine configuration . get clock ( ) . set current time ( next time cal . get time ( ) ) ; process instance process instance = runtime service . start process instance by key ( repeat with end ) ;
if ( this instanceof frame | | this instanceof dialog ) { return true ; }
_ modify headers ( value , type , generic type , annotations , http headers , endpoint ) ; object writer writer = endpoint . get writer ( ) ;
final batch writer config writer config = new batch writer config ( ) ;
case 5 : case 6 : break ; case 7 : break ; } }
try { iterator < node > it = path . get nodes ( ) . iterator ( ) ; if ( it . has next ( ) ) any entries? { the first entry in the path is always the start node we don ' t want to run bindings on that entry ( again ) it . next ( ) ; skip first entry while ( it . has next ( ) ) { node node = it . next ( ) ; log . debug ( executing node { } , node ) ; _ lifecycle . run bindings ( node , appentry . app , this ) ; appentry . set life cycle node ( node ) ; } } } catch ( throwable t ) { log . warn ( unable to reach node goal : + node name , t ) ; }
info . set channel number ( esds . get number of channels ( ) ) ; info . set kind ( esds . get kind ( ) ) ; info . set profile ( esds . get audio profile ( ) ) ; info . set encoding type ( encoder type . drm _ aac . get description ( ) ) ; } }
assert equals ( 3 * gb , orig old a1 . get users ( ) . get users list ( ) . get ( 0 ) . get resources used ( ) . get memory size ( ) ) ;
if ( debug ) debug _ message ( pop stack by one , state was + ( ( symbol ) stack . peek ( ) ) . parse _ state ) ; left = ( ( symbol ) stack . pop ( ) ) ; tum 20060327 removed . left tos - - ;
value stream . write ( value bytes , offset till now , value length ) ; } else {
path history dir = new path ( system . get property ( test . build . data , . ) , history ) ;
ret val = ( x = = const value ) ;
if ( modifier . is native ( method . get modifiers ( ) ) ) { continue ; }
super . dispose ( ) ;
byte [ ] buf1 = new byte [ 65536 ] ;
string tmp = conf . get ( mapred . child . tmp , . tmp ) ; path tmp dir = new path ( tmp ) ;
col statistics stats = parent stats . get column statistics from col name ( engfd . get cols ( ) . get ( 0 ) ) ;
sprite batch . draw ( regions . get ( i ) . get texture ( ) , page vertices [ i ] , offset * 20 , count * 20 ) ;
mark last = mark ( ) ; boolean single quoted = false ; boolean double quoted = false ; int nesting = 0 ; int current char ; do { current char = next char ( last ) ; while ( current char = = ' \ \ ' & & ( single quoted | | double quoted ) ) {
try { int pos = context position - 1 ; do { int angle = scanner . find opening peer ( pos , bound , ' < ' , ' > ' ) ; if ( angle = = java heuristic scanner . not _ found ) break ; int token = scanner . previous token ( angle - 1 , bound ) ; next token must be a method name that is a generic type if ( token = = symbols . token ident ) { int off = scanner . get position ( ) + 1 ; int end = angle ; string ident = document . get ( off , end - off ) . trim ( ) ; if ( java heuristic scanner . is generic starter ( ident ) ) return angle + 1 ; } pos = angle - 1 ; } while ( true ) ; } catch ( bad location exception x ) { } return super . guess context information position ( context ) ;
send broadcast ( new intent ( com . klinker . android . twitter . stop _ push _ service ) ) ; intent main = new intent ( this , main activity . class ) ; main . add flags ( intent . flag _ activity _ clear _ task | intent . flag _ activity _ new _ task ) ; app settings . invalidate ( ) ; main . put extra ( switch _ account , true ) ; override pending transition ( 0 , 0 ) ; finish ( ) ; start activity ( main ) ; }
final long r time = last return time ; final long b time = last borrow time ; if ( r time > b time ) { return r time - b time ; }
client . set max binary message buffer size ( max ) ;
assert xpath evaluates to ( approximate , ex : polymorphic feature [ @ gml : id = ' f5 ' ] ex : any value gsml : cgi _ term value @ gsml : qualifier , doc ) ;
array list < item model > data list = new array list < > ( ) ; for ( int i = 0 ; i < 16 ; i + + ) { item model item model = new item model ( ) ; item model . title = item + i ; data list . add ( item model ) ; } m item touch helper = new item touch helper ( m callback ) ; m item touch helper . attach to recycler view ( m recycler view ) ;
toa header new header = toa header . new propose message header ( message id , my propose sequence number ) ; message propose message = new message ( ) . src ( local address ) . dest ( message id . get address ( ) ) . put header ( this . id , new header ) . set flag ( message . flag . oob , message . flag . internal , message . flag . dont _ bundle ) ;
if ( cache size = = 0 ) { cache size = default _ file _ status _ cache _ size ; }
on view ( with id ( r . id . start _ drawer ) ) . perform ( inflate header view ( r . layout . design _ navigation _ view _ header _ switch ) ) ; verify headers ( r . id . header _ frame ) ;
inline ( var a , b ; + var x = a + b ; print ( x ) ; x = a - b ; print ( x ) , var a , b ; + var x ; print ( a + b ) ; print ( a - b ) ) ; }
if ( field instanceof string field value & & ( ( string field value ) field ) . get string ( ) . is empty ( ) ) return false ; if ( field instanceof nan number ) return false ;
assert ( partition id . equals ( 2 ) & & partition id . equals ( 1 ) ) ; if ( partition id . equals ( integer . to string ( mp initiator . mp _ init _ pid ) ) ) { assert true ( txnid = = transaction ids . get long ( partition id ) ) ; } else if ( partition id . equals ( 0 ) ) {
_ log . info ( relay puller reconnecting when in middle of event window . will regress . current checkpoint : + cp ) ;
record test . class . get method ( set int field , integer . class , set mode . class ) ;
return is duplicate message extra ( cursor , rc ) ;
throw new org . apache . axis2 . databinding . adbexception ( value cannot be null ) ;
if ( no need bg ) { float start = progress pos x + m text offset 2 + text width ; m paint . set color ( m un reached bar color ) ; m paint . set stroke width ( m un reached progress bar height ) ; canvas . draw line ( start , 0 , m real width , 0 , m paint ) ; } canvas . restore ( ) ;
return common prefix ;
parent . add cell ( cell ) ;
remove current = true ;
alter index desc alter idx desc = new alter index desc ( alter index types . updatetimestamp ) ; alter idx desc . set index name ( index name ) ; alter idx desc . set base table name ( get dot name ( qualified ) ) ; alter idx desc . set spec ( part spec ) ; task < ? > ts task = task factory . get ( new ddlwork ( alter idx desc ) , conf ) ; for ( task < ? > t : index builder ) { t . add dependent task ( ts task ) ; } }
assert equals ( \ test \ , xml content escaper . escape ( \ test \ ) ) ; assert equals ( ' test ' , xml content escaper . escape ( ' test ' ) ) ; }
system . out . println ( language : + lang tool . get language ( ) + , text length : + text . length ( ) + chars , + sentence count + sentences ) ; system . out . println ( warmup . . . ) ;
add raw measure ( project _ view _ 5 _ ref , ncloc _ language _ distribution _ key , < null > = 3 ; foo = 10 ) ; under test . execute ( ) ;
list < geo server reinitializer > initializers = geo server extensions . extensions ( geo server reinitializer . class ) ;
int left = get padding left ( ) + half thumb + added thumb ; int right = get width ( ) - ( get padding right ( ) + half thumb + added thumb ) ; int available = right - left ; final int thumb pos = ( int ) ( scale draw * available + 0 . 5f ) ; update thumb pos ( thumb pos ) ; }
master file system mfs = test _ util . get mini hbase cluster ( ) . get master ( ) . get master file system ( ) ;
expression = parser . parse expression ( orange ) ;
string builder buf = new string builder ( num groups * digits per long [ radix ] + 1 ) ;
tasks numbers [ 0 ] = 2 * thread local random . current ( ) . next int ( min _ tasks , max _ tasks + 1 ) ; total tasks number + = tasks numbers [ 0 ] ;
session s = open session ( new join counter ( 1 ) ) ; transaction tx = s . begin transaction ( ) ; owner owner = new owner ( ) ; owner address address = new owner address ( ) ; owner . set address ( address ) ; address . set owner ( owner ) ; s . persist ( owner ) ; s . flush ( ) ; s . clear ( ) ; owner = ( owner ) s . get ( owner . class , owner . get id ( ) ) ;
expect fail not assignable ( parser , ctx , ( { 1 , 2 , 3 } . ^ [ is even ( this ) ] ) + + ) ;
if ( first two bytes = = exif _ magic _ number ) { return jpeg ; } final int first four bytes = first two bytes < < 16 & 0x ffff0000 | reader . get uint16 ( ) & 0x ffff ;
if ( entry name . index of ( meta - inf ) = = 0 ) continue ;
gradient drawable . set corner radii ( new float [ ] { top left radius , top left radius , top right radius , top right radius , bottom right radius , bottom right radius , bottom left radius , bottom left radius } ) ;
dimension default value setting default value setting = new dimension default value setting ( ) ; default value setting . set strategy type ( strategy . fixed ) ; default value setting . set reference value ( 99 101 ) ; setup resource dimension default value ( wattemp , resource info . elevation , default value setting ) ;
system . arraycopy ( domain , 0 , data , pos , domain . length ) ; pos + = domain . length ;
interval requirement ir102 = new interval requirement ( 102 , 2 ) ;
resource management service r = gui activator . get resources ( ) ; int hgap = r . get settings int ( impl . gui . dial _ pad _ horizontal _ gap ) ; int vgap = r . get settings int ( impl . gui . dial _ pad _ vertical _ gap ) ; int width = r . get settings int ( impl . gui . dial _ pad _ width ) ; int height = r . get settings int ( impl . gui . dial _ pad _ height ) ; dial pad panel = new jpanel ( new grid layout ( 4 , 3 , hgap , vgap ) ) ;
builder . get attribute builder ( ) . add reject check ( reject attribute checker . defined , model description constants . add _ index ) ;
assert equals ( 10 , counter . buckets . get last ( ) . get max updater ( hystrix rolling number event . thread _ max _ active ) . max ( ) ) ;
query = select r1 . a , p2 . a from r1 full join p2 + on r1 . a > 5 and p2 . a < 7 + order by r1 . a , p2 . a ;
if ( sequence detected ( scanner , f end sequence , true ) ) { if ( 0 = = - - n of brackets ) { return true ; } }
login text = log in with facebook ;
final boolean enable acra = supported android version & & should disable acra ( prefs ) ; if ( sender service process ) { indicate that acra is or is not listening for crashes . log . i ( log _ tag , acra is + ( enable acra ? enabled : disabled ) + for + m application . get package name ( ) + , initializing . . . ) ; } error reporter singleton = new error reporter ( m application , config proxy , prefs , enable acra , supported android version , sender service process ) ;
bra = cursor ; switch ( among _ var ) { case 0 : return false ; case 1 :
boolean is hotseat = m launcher . is hotseat layout ( drop target layout ) ;
students = q . get result list ( ) ; assert . assert not null ( students ) ; assert . assert equals ( 3 , students . size ( ) ) ; int count = 0 ; for ( student couch dbbyte wrapper student : students ) { if ( student . get id ( ) . equals ( get max value ( byte . class ) ) ) { assert . assert equals ( get max value ( short . class ) , student . get age ( ) ) ; assert . assert equals ( kuldeep , student . get name ( ) ) ; count + + ; } else if ( student . get id ( ) . equals ( get min value ( byte . class ) ) ) { assert . assert equals ( get min value ( short . class ) , student . get age ( ) ) ; assert . assert equals ( get min value ( string . class ) , student . get name ( ) ) ; count + + ; } else { assert . assert equals ( get random value ( byte . class ) , student . get id ( ) ) ; assert . assert equals ( get random value ( short . class ) , student . get age ( ) ) ; assert . assert equals ( get random value ( string . class ) , student . get name ( ) ) ; count + + ; } } assert . assert equals ( 3 , count ) ; em . close ( ) ;
located block [ ] on retry block = new located block [ 1 ] ; inodes in path iip = fsn . dir . resolve path ( null , src , file id ) ; file state file state = analyze file state ( fsn , iip , file id , client name , previous , on retry block ) ; final inode file pending file = file state . inode ; src = file state . path ; if ( on retry block [ 0 ] = null ) { if ( on retry block [ 0 ] . get locations ( ) . length > 0 ) { this is a retry . just return the last block if having locations . return on retry block [ 0 ] ; } else { add new chosen targets to already allocated block and return block info last block in file = pending file . get last block ( ) ; last block in file . get under construction feature ( ) . set expected locations ( last block in file , targets , pending file . get block type ( ) ) ; offset = pending file . compute file size ( ) ; return make located block ( fsn , last block in file , targets , offset ) ; } }
do test ( uncovered get servlet . class . get name ( ) , false , true , false , true ) ;
remove callbacks ( update progress action ) ;
assert parse ( id . scheme = \ * doc \ ) ;
committer . join ( 30000 ) ;
g group = counters . get group ( group name ) ; group . set display name ( group display name ) ; string counter string = get block ( group string , counter _ open , counter _ close , group index ) ;
bus handler . push expected events ( next event . invoice , next event . payment , next event . invoice _ payment ) ; clock . add days ( 27 ) ; assert listener status ( ) ; expected invoices . add ( new expected invoice item check ( new local date ( 2016 , 7 , 1 ) , new local date ( 2016 , 8 , 1 ) , invoice item type . recurring , new big decimal ( 249 . 95 ) ) ) ; invoices = invoice user api . get invoices by account ( account . get id ( ) , false , call context ) ; invoice checker . check invoice ( invoices . get ( 6 ) . get id ( ) , call context , expected invoices ) ; expected invoices . clear ( ) ;
assert not null ( element functor . transform ( range ) ) ;
if ( random ( ) . next boolean ( ) ) { client . add ( make test doc ( id , 1 , features , aaa , cat , a , in stock , true , popularity , 12 , price , . 017 ) ) ; client . commit ( ) ; } ignore exception ( is not currently supported ) ;
if ( ( math . abs ( r . left - x ) < hysteresis ) & & vertical check ) { retval | = grow _ left _ edge ; }
out buff [ e + 4 ] = new _ line ;
instruction complementary conditional branch = new branch instruction ( ( byte ) ( ( ( branch instruction . opcode + 1 ) ^ 1 ) - 1 ) , ( 1 + 2 ) + ( 1 + 4 ) ) ; insert before instruction ( offset , complementary conditional branch ) ;
if ( locations [ i ] = null & & server name . equals ( locations [ i ] . get server name ( ) ) ) { if ( new locations = = null ) { first time new locations = new hregion location [ locations . length ] ; system . arraycopy ( locations , 0 , new locations , 0 , i ) ; } new locations [ i ] = null ; } else if ( new locations = null ) { new locations [ i ] = locations [ i ] ; }
pos + + ; for ( ; pos < length & & chars [ pos ] = ' = ' & & chars [ pos ] = ' ' ; pos + + ) { we don ' t follow exact bnf syntax here : accept any char except space and ' = ' } if ( pos > = length ) { throw new illegal state exception ( unexpected end of dn : + dn ) ; }
meta contact group new grp p2 meta wrapper = fixture . meta cl service . find meta contact group by contact group ( new grp p2 ) ; assert not null ( strange error , new grp p2 meta wrapper ) ;
check position indexes ( off , off + len , b . length ) ;
string fmsg = xslmessages . create xpathmessage ( xpatherror resources . er _ unsupported _ return _ type , new object [ ] { return type . to string ( ) } ) ;
header location = response . get first header ( location ) ;
name = route . split ( ) [ 0 ] ; if ( object helper . is not empty ( name ) ) { route = route . substring ( name . length ( ) ) ; } } else {
if ( initializer = null & & temporary name = null ) { initializer . dispose ( info , data access , temporary name ) ; } return ft ; }
if ( update ) { update screen ( ) ; last print = system . nano time ( ) ; }
node . add children to front ( new header node ( traversal , node ) . remove children ( ) ) ; compiler . report change to enclosing scope ( node ) ; instrument branch coverage ( traversal , instrumentation data . get ( file name ) ) ; } }
b . set b1 ( 2 ) ;
loaded = true ; if ( need end draw ) { end draw ( ) ; } }
( provider . get class ( ) . get name ( ) . equals ( configuration . provider _ class _ default ) & & provider name . equals ( configuration . provider _ class _ hibernate4 _ 1 ) ) ) { return provider ; return the provider that matched classname } }
string proxy user = conf ent . get key ( ) . substring ( conf _ proxyuser _ prefix . length ( ) , conf ent . get key ( ) . last index of ( conf _ hosts _ suffix ) ) ;
assert true ( float should be equal , math . abs ( new filter . get chance ( ) - quarter chance filter . get chance ( ) ) < 0 . 000001f ) ;
expect parser success ( select e . author . name from indexed entity e join e . contact details d join e . alternative contact details a where d . address . post code = ' ea123 ' and a . email = ' mail @ mail . af ' , ( query ( query _ spec ( select _ from ( from ( persister _ space ( entity _ persister _ ref indexed entity e ) ( property - join inner d ( path ( . e contact details ) ) ) ( property - join inner a ( path ( . e alternative contact details ) ) ) ) ) ( select ( select _ list ( select _ item ( path ( . ( . e author ) name ) ) ) ) ) ) ( where ( and ( = ( path ( . ( . d address ) post code ) ) ( const _ string _ value ea123 ) ) ( = ( path ( . a email ) ) ( const _ string _ value mail @ mail . af ) ) ) ) ) ) ) ;
int new index = 0 ;
q . error * = alpha ;
assert equals ( 19 . 15 , get feature at ( base _ url _ one + & sort by = ingestion d , 68 , 72 , sf : watertemp ) , eps ) ;
verify function ( conf , [ ' text ' ] . length ( ) , text _ series , 6 ) ;
string host _ 1 = host _ 1 ; org . apache . hadoop . yarn . server . resourcemanager . node manager nm _ 1 = register node ( host _ 1 , 1234 , 2345 , network topology . default _ rack , resources . create resource ( 2 * gb , 1 ) ) ;
assert expand path ( { jetty . home } , jetty home ) ; }
workers counter . set ( 0 ) ;
super . channel read0 ( ctx , msg ) ;
action builder . context click ( null ) . build ( ) . perform ( ) ;
for ( int j = 0 ; j < 4 ; j + + ) { e = rotate left ( a , 5 ) + h ( b , c , d ) + e + x [ idx + + ] + y2 b = rotate left ( b , 30 ) e + = ( a < < 5 | a > > > 27 ) + h ( b , c , d ) + x [ idx + + ] + y2 ; b = b < < 30 | b > > > 2 ; d + = ( e < < 5 | e > > > 27 ) + h ( a , b , c ) + x [ idx + + ] + y2 ; a = a < < 30 | a > > > 2 ; c + = ( d < < 5 | d > > > 27 ) + h ( e , a , b ) + x [ idx + + ] + y2 ; e = e < < 30 | e > > > 2 ; b + = ( c < < 5 | c > > > 27 ) + h ( d , e , a ) + x [ idx + + ] + y2 ; d = d < < 30 | d > > > 2 ; a + = ( b < < 5 | b > > > 27 ) + h ( c , d , e ) + x [ idx + + ] + y2 ; c = c < < 30 | c > > > 2 ; }
set request auth ( other , other ) ;
final integer min pool size = configuration helper . get integer ( environment . c3 p0 _ min _ size , props ) ; final integer max pool size = configuration helper . get integer ( environment . c3 p0 _ max _ size , props ) ; final integer max idle time = configuration helper . get integer ( environment . c3 p0 _ timeout , props ) ; final integer max statements = configuration helper . get integer ( environment . c3 p0 _ max _ statements , props ) ; final integer acquire increment = configuration helper . get integer ( environment . c3 p0 _ acquire _ increment , props ) ; final integer idle test period = configuration helper . get integer ( environment . c3 p0 _ idle _ test _ period , props ) ; final properties c3props = new properties ( ) ;
try { runtime . launch container ( builder . build ( ) ) ; assert . fail ( expected a privileged launch container failure . ) ; } catch ( container execution exception e ) { log . info ( caught expected exception : + e ) ; }
if ( yaml tree . contains key ( editor _ option _ key ) ) yaml tree . add yaml value ( null , editor _ option _ key , ) ;
return system property util . get ( user . dir ) + file . separator + uri ;
res = file fs file . from ( build output dir , res , merged , flavor , type ) ;
random ran = new random ( ) ;
body builder . append formal line ( string . format ( return load page ( query , % s , % s . constructor ( % s . class , % s ) ) ; , pageable , get name of java type ( projection ) , get name of java type ( return type ) , string utils . join ( get list right value of pair ( projection fields ) , , ) ) ) ;
sb . append ( < tr class = ' warning ' > ) ;
assert equals ( max threads option . get ( 8 ) , config . get option ( max threads option . class ) ) ;
if ( length > ( internal buffer . length - count ) ) { flush internal ( ) ; } system . arraycopy ( buffer , offset , internal buffer , count , length ) ;
send option ( cmd _ do , option _ suppress _ go _ ahead ) ;
return selected index ;
if ( r _ r1 ( ) ) { cursor = limit - v _ 7 ; break lab6 ; }
string server id param = parameter ( server id ) . apply ( ha uri ) ;
throw new ioexception ( port + port + specified in uri + name node uri + but host ' + name node uri . get host ( ) + ' is a logical ( ha ) namenode + and does not use port information . ) ; } }
uri server ha uri = uri . create ( ha : server2?server id = 2 ) ;
if ( b . get border ( ) instanceof uiresource ) { b . set border ( get rollover border ( b ) ) ; } rollover table . put ( b , b . is rollover enabled ( ) ? boolean . true : boolean . false ) ;
agent service . update runtime info ( agent runtime info ) ; build repository service . update status from agent ( job identifier , state , agent runtime info . get uuid ( ) ) ; job status topic . post ( new job status message ( job identifier , state , agent runtime info . get uuid ( ) ) ) ; } } ) ; }
socks5 proxy . set local socks5 proxy port ( proxy port ) ;
file name . append ( component package . replace all ( \ \ . , matcher . quote replacement ( file . separator ) ) ) . append ( file . separator ) ;
po . set parallelism ( input . get parallelism ( ) ) ;
log . e ( tag , iso _ 8859 _ 1 must be supported , e ) ; return new byte [ 0 ] ;
click named service config ( default ) ; tester . assert rendered page ( security named service edit page . class ) ; tester . debug component trees ( ) ; new form tester ( panel : panel : form ) ; form tester . set value ( panel : user group service name , test ) ; click cancel ( ) ; tester . assert rendered page ( base page . get class ( ) ) ; auth config = ( username password authentication provider config ) get security named service config ( default ) ;
switch ( ( code point > > 16 ) & 0xf ) { case 2 : supplementary ideographic plane case 3 : tertiary ideographic plane return 2 ; } }
if ( ( chr = = ' \ n ' ) | | ( chr = = ' \ r ' ) | | ( chr = = ' ' ) | | ( chr = = ' \ t ' ) ) { continue ; } if ( ( chr > = ' a ' ) & & ( chr < = ' z ' ) ) {
final file queue q = new file queue ( new combine file split ( new path [ 0 ] , new long [ 0 ] , new long [ 0 ] , new string [ 0 ] ) , conf ) ;
int stream . range ( 0 , range ) . boxed ( ) . for each ( i - > cache . put ( i , i + - value ) ) ; assert equals ( range , cache . size ( ) ) ; cache set < map . entry < integer , string > > entry set = cache . entry set ( ) ; int offset = populate next for each structure ( new atomic integer ( ) ) ;
assert equals ( 0 , query ( ) . from ( cat ) . where ( cat . name . starts with ( r ) ) . fetch count ( ) ) ;
java type detail entity = controller metadata . get last detail entity ( ) ; collection < class or interface type details > details controllers to check = get controller locator ( ) . get controllers ( detail entity , controller type . item , view type ) ; for ( class or interface type details controller : details controllers to check ) { if ( controller package . equals ( controller . get type ( ) . get package ( ) ) ) { detail item controller = controller . get type ( ) ; break ; } }
catch ( ioexception exc ) { }
if ( empty utils . is null or empty ( filepath ) ) { uri . builder b = new uri . builder ( ) ; uri = b . scheme ( file ) . authority ( ) . path ( filepath ) . build ( ) ; } return uri ;
if ( arrays . equals ( delimiter , new byte [ ] { ' \ n ' } ) ) { string delim string = parameters . get string ( record _ delimiter , null ) ; if ( delim string = null ) { set delimiter ( delim string ) ; } }
check not strict iso ( fields , strict iso ) ; append separator ( bld , extended ) ; bld . append literal ( ' w ' ) ; bld . append literal ( ' - ' ) ; bld . append day of week ( 1 ) ; } else {
this . accounts sync state = load sync states from realm ( ) ; if ( account = null ) { get account from net ( account . get token ( ) ) ; }
tile object to3 = tile object . create query tile object ( test : 123123 112 , xyz , epsg : 4326 , image jpeg , parameters ) ;
string json = object mapper . write value as string ( bad outer ) ;
if ( intersect visible to user ( m temp parent rect ) ) { node . set visible to user ( true ) ; node . set bounds in parent ( m temp parent rect ) ; }
if ( ( contact . m _ flags & contact . island _ flag ) = = contact . island _ flag ) { continue ; }
int p2 = get precedence ( ops . negate ) ;
return item collection state . of string ( state ) ; }
assert equals ( - 1 , rm admin cli . run ( args ) ) ;
m second left = m minute left ; }
public static int log2 ( int x , rounding mode mode ) { check positive ( x , x ) ;
string job id = extract job idfrom current history file ( file name ) ;
session . evict ( entity ) ; assert false ( session . contains ( entity ) ) ;
grid coverage2 d coverage = mosaic coverages ( coverages , hints ) ;
chat id = message . get chat id ( ) ;
. map ( new map function < tuple4 < integer , double , double , string > , tuple2 < integer , double > > ( ) { @ override public tuple2 < integer , double > map ( tuple4 < integer , double , double , string > l ) {
listener . on message ( message , session to use ) ;
key generator keygen = key generator . get instance ( camellia ) ; keygen . init ( 128 ) ; secret key key = keygen . generate key ( ) ; final xmlsecurity data format xml enc data format = new xmlsecurity data format ( ) ;
for ( int i = capacity , n = i + stash size ; i < n ; i + + ) { if ( key table [ i ] = = key ) { v old value = value table [ i ] ; value table [ i ] = value ; return old value ; } }
try { parser factory . make parser ( tests . api . org . xml . sax . support . no access parser ) ; fail ( expected illegal access exception was not thrown ) ; } catch ( illegal access exception e ) { expected }
paragraph p1 = note . add new paragraph ( authentication info . anonymous ) ; map < string , object > config = p1 . get config ( ) ; config . put ( enabled , true ) ; p1 . set config ( config ) ; p1 . set text ( set revision sample text ) ; notebook repo . save ( note , null ) ; int paragraph count _ 2 = note . get paragraphs ( ) . size ( ) ; assert that ( paragraph count _ 2 ) . is equal to ( paragraph count _ 1 + 1 ) ; log . info ( paragraph count after modification : { } , paragraph count _ 2 ) ;
system . arraycopy ( source , 0 , target , 0 , length ) ;
for ( driver info a driver : registered drivers ) { if the caller does not have permission to load the driver then skip it . if ( is driver allowed ( a driver . driver , caller class ) ) { result . add element ( a driver . driver ) ; } else { println ( skipping : + a driver . get class ( ) . get name ( ) ) ; } } return ( result . elements ( ) ) ;
assert that ( result . get total ( ) ) . is equal to ( 2 l ) ;
string outputfilepath = system . get property ( user . dir ) + out _ page counter . docx ; docx4 j . save ( word mlpackage , new file ( outputfilepath ) , docx4 j . flag _ none ) ; ( flag _ none = = default = = zipped docx ) system . out . println ( saved : + outputfilepath ) ;
do { c = in . read ( ) ; } while ( ( c = = ' ' ) | | ( c = = ' \ n ' ) ) ; }
input = new oculus vrinput ( this , session , session status , tracking state ) ;
client factory bean factory bean = create client factory bean ( cls ) ;
callback . expect ( tree . get first child ( ) , tree . get first child ( ) ) ; t . traverse ( tree . get first child ( ) ) ; callback . assert entered ( ) ;
spinner spinner = ( spinner ) find view by id ( r . id . themes _ spinner ) ;
hcat schema actual hcat schema = new hcat schema ( lists . new array list ( new hcat field schema ( inner llama , hcat field schema . type . string , null ) ) ) ; hcat field schema actual hcat field schema = new hcat field schema ( llama , hcat field schema . type . array , actual hcat schema , null ) ; resource schema actual = pig hcat util . get bag sub schema ( actual hcat field schema ) ; assert . assert equals ( expected . to string ( ) , actual . to string ( ) ) ;
truncate mongo ( ) ; delete rdbmsschema and tables ( ) ; }
{ verify ( subtask state1 , times ( 1 ) ) . register shared states ( any ( shared state registry . class ) ) ; verify ( subtask state2 , times ( 1 ) ) . register shared states ( any ( shared state registry . class ) ) ; }
if ( topmost via . get transport ( ) = = null ) topmost via . set transport ( transport ) ; if ( topmost via . get port ( ) = = - 1 ) topmost via . set port ( listening point . get port ( ) ) ;
long id = wrapper _ class _ counter . get and increment ( ) ; class generator cc = class generator . new instance ( cl ) ; cc . set class name ( ( modifier . is public ( c . get modifiers ( ) ) ? wrapper . class . get name ( ) : c . get name ( ) + sw ) + id ) ; cc . set super class ( wrapper . class ) ; cc . add default constructor ( ) ;
data < < = 1 ; bits in data - - ; }
overlay = blur . on stack blur ( overlay , ( int ) radius ) ;
set < string > schema names = new hash set < > ( ) ; for ( string schema name : _ schema names . split ( ) ) { if ( schema name . is empty ( ) ) { schema names . add ( schema name ) ; } } if ( schema names . size ( ) = = 0 ) { throw new runtime exception ( no schema name specified . ) ; } return new array list < > ( schema names ) ;
pojo bean string bean = new pojo bean ( 11 , 2 ) ; do parse ( ( string value1 > string value2 ) , string bean , 0 , string value1 , true ) ;
if ( dynamic . entries . size ( ) = new dynamic . entries . size ( ) ) { preconditions . check state ( is remove scrubbed tags ( ) ) ;
int num read = zz reader . read ( zz buffer , zz end read , zz buffer . length - zz end read ) ; if ( num read > 0 ) { zz end read + = num read ; return false ; }
byte [ ] tbs certificate local = get tbs certificate internal ( ) ;
find usages . select node in find usages panel ( project _ name ) ;
assert true ( verify strategies ( get current column family store ( ) . get compaction strategy manager ( ) , size tiered compaction strategy . class ) ) ;
for ( kafka topic partition state < kph > partition : subscribed partition states ) { unassigned partitions queue . add ( partition ) ; }
assert not same ( security , filter . include ) ; assert not same ( security , filter . exclude ) ; assert not same ( security2 , filter . include ) ; assert not same ( security2 , filter . exclude ) ; assert same ( security3 , filter . include ) ;
if ( init _ with _ ip ) { cname = addresses [ 0 ] . get host name ( false ) . to lower case ( ) ; } else { cname = inet address . get by name ( addresses [ 0 ] . get host address ( ) ) . get host name ( false ) . to lower case ( ) ; }
conf . set ( onetwothree , 123 ) ;
gl . set property ( symbol _ table , f symbol table ) ; gl . set property ( entity _ resolver , f entity resolver ) ; gl . set property ( error _ reporter , f error reporter ) ;
m _ n position x . remove element at ( n target node ) ;
verify ( low level , times ( 0 ) ) . insert ( any ( insert query . class ) , any ( content values . class ) ) ;
public void ensure path ( final string path , final boolean excluding last ) throws keeper exception { path utils . validate path ( path ) ; assert cluster id flag true ( ) ;
int check = j edit . get integer property ( check file status ) ; if ( j edit . is startup done ( ) & & ( check & general option pane . check file status _ focus buffer ) > 0 ) j edit . check buffer status ( view , true ) ; } } ) ; } } } } }
previous version = persister . get version ( object ) ;
assert true ( unsigned longs . compare ( 0x5a4316b8c153ac4d l , 0xff1a618b7f65ea12 l ) < 0 ) ; assert true ( unsigned longs . compare ( 0xff1a618b7f65ea12 l , 0x5a4316b8c153ac4d l ) > 0 ) ;
session . clear ( ) ;
utf = abc \ u00e6 . get bytes ( utf - 8 ) ; delete directory ( target charset ) ;
if ( papplet . platform = = pconstants . macosx & & fx event . is control down ( ) & & button = = pconstants . left ) { button = pconstants . right ; }
params . set limit ( depth limit ) ; try { market data service . get order book ( currency pair . btc _ xrp , params ) ; } catch ( final ripple exception e ) { assert that ( e . get error ( ) ) . contains ignoring case ( rest invalid _ parameter ) ; assert that ( e . get error type ( ) ) . contains ignoring case ( invalid _ request ) ; assert that ( e . get message ( ) ) . contains ignoring case ( invalid parameter : base . must be a currency string in the form currency + counterparty ) ; throw e ; }
hcolumn descriptor hcd1 = new hcolumn descriptor ( test _ family2 ) ; hcd1 . set max versions ( 25 ) ; admin . modify column ( test _ table , hcd1 ) ; assert true ( second column family should be modified , cp . pre modify column called only ( ) ) ;
alphabetic index . immutable index en = create index ( locale . english ) ;
final actor gateway test actor gateway = new akka actor gateway ( get test actor ( ) , high availability services . default _ leader _ id ) ;
remove element = ( done with above two false & & done with above two true ) | | ( is required & & done with above two true ) | | ( is required & & done with above two false ) ; if ( remove element ) { done with above two false = true ; done with above two true = is required ; }
s _ logger . info ( current power - off task failed . however , vm has been switched to the state we are expecting for ) ; return true ; } s _ logger . error ( vmware power off vm _ task failed due to + task mo . get task failure info ( _ context , mor task ) ) ; }
final int n groups = bs r ( 3 ) ; final int n selectors = bs r ( 15 ) ; for ( int i = 0 ; i < n selectors ; i + + ) { int j = 0 ; while ( bs get bit ( ) ) { j + + ; } selector mtf [ i ] = ( byte ) j ; }
assert equals ( 1 , lt . check ( das ist das aufwÃ¤ndigste . \ n \ n aber hallo . es ist wirklich aufwendig . ) . size ( ) ) ;
can wrap any = utilities . get use vectorized input file format ( conf , this ) ;
query = runtime service . create process instance query ( ) . variable value equals ( integer var , 12345 ) . or ( ) . variable value equals ( integer var2 , 67890 ) . process definition id ( undefined ) . end or ( ) ;
assert throws ( client ( ) . prepare search ( ) . set query ( more like this query ( new string [ ] { string _ value , int _ value } , new string [ ] { index } , null ) ) , search phase execution exception . class ) ;
assert null ( historic variable update . get activity instance id ( ) ) ; historic variable update = ( historic variable update ) historic details . get ( 3 ) ;
criteria query < order > order criteria = builder . create query ( order . class ) ; root < order > order root = order criteria . from ( order . class ) ; order criteria . select ( order root ) ; order criteria . where ( builder . is false ( builder . disjunction ( ) ) ) ; em . create query ( order criteria ) . get result list ( ) ; list < order > orders = em . create query ( order criteria ) . get result list ( ) ;
return minutes * 60000 < ( m session start - current time millis ) ? minutes + 1 : minutes ; }
if ( predicate index < 0 ) return - 1 ; int count = m _ proximity positions [ predicate index ] ;
assert null ( never _ load was loaded in wrapped doc , visitor . lazy doc . get document ( ) . get field ( never _ load ) ) ; } finally {
payment api . create void ( account , payment . get id ( ) , transaction external key3 , immutable list . < plugin property > of ( ) , call context ) ; assert . fail ( ) ; } catch ( final payment api exception e ) {
for ( int i = 0 ; i < ( 2 * max threads ) ; i + + ) { executor service . submit ( new test event handler ( mocked server , event type . m _ server _ shutdown , lock , counter ) ) ; }
result . get meta data ( ) . remove ( nutch . writable _ generate _ time _ key ) ;
for ( facet bucket bucket : get buckets ( ) ) { map < string , object > bucket map = new hash map < > ( ) ; bucket map . put ( analytics response headings . facet _ value , bucket . get facet value ( ) ) ; bucket map . put ( analytics response headings . results , bucket . get results ( ) ) ; results . add ( bucket map ) ; } return results ;
if ( system . nano time ( ) - start of cycle - instance emit batch time . to nanos ( ) > 0 ) { break ; } if ( collector . get total data emitted in bytes ( ) - total data emitted in bytes before cycle > instance emit batch size . as bytes ( ) ) { break ; }
ioutils . spins ( file ) ;
log . info ( = = = printing the decoder object ' s schema set \ n + ds . get event decoder ( ) . get schema set ( ) ) ;
assert true ( index - deletes should be a subset of index puts , index puts . contains all ( index deletes ) ) ; }
final float col center rtl ; if ( sutils . is layout rtl compat ( this ) ) { col center rtl = m padded width - col center ; } else { col center rtl = col center ; } int state mask = 0 ;
int parts skipped = ipv6 _ part _ count - ( parts hi + parts lo ) ;
if ( name length < = 0 ) { frame . set invalid ( ) ; return ; } header size + = name length ; if ( header size > max header size ) { frame . set truncated ( ) ; return ; }
else if ( b4 [ 0 ] = = 0x3 c & & b4 [ 1 ] = = 0x00 & & b4 [ 2 ] = = 0x3 f & & b4 [ 3 ] = = 0x00 ) { is big endian = boolean . false ; }
list < historic identity link > historic identity links = history service . get historic identity links for task ( task . get id ( ) ) ;
verify output vo = vf . verify file ( read size , is ) ;
directory dir = new directory ( ) ; index writer config conf = new index writer config ( new mock analyzer ( random ( ) ) ) ; conf . set max buffered docs ( 4 ) ; index writer writer = new index writer ( dir , conf ) ; final int num docs = at least ( 10 ) ; for ( int i = 0 ; i < num docs ; i + + ) { document doc = new document ( ) ; doc . add ( new string field ( id , doc + i , store . no ) ) ; long value = random ( ) . next int ( ) ; doc . add ( new binary doc values field ( f , to bytes ( value ) ) ) ; doc . add ( new binary doc values field ( cf , to bytes ( value * 2 ) ) ) ; writer . add document ( doc ) ; } int num gens = at least ( 5 ) ;
if ( m last frame time = frame time nanos ) { m data flow graph . do frame ( frame time nanos ) ; m last frame time = frame time nanos ; } if ( m is running ) { post frame callback ( ) ; }
add explicit deps ( dep resolver , visibility , rule . get visibility ( ) . get dependency labels ( ) ) ;
time window fz = time window . from ( monday , wednesday - thursday , 0 , 17 - 19 , utc ) ;
view result view result = couch db connector . query view ( view query ) ; assert . assert equals ( 0 , view result . get total rows ( ) ) ;
mutable list < string > list = this . new with ( a , b , c , d ) ;
if ( gamma = = 0 . 0 ) { return 2 . 2 ; } else { return gamma ; } }
m velocity tracker . recycle ( ) ;
int new run starts [ ] = new int [ array _ size _ increment ] ;
persistent class . add tuplizer ( mode , tuplizer . impl ( ) . get name ( ) ) ; }
if ( rendered node . is open ( ) ) { expand node ( rendered node , false , dispatch node expanded ) ; }
frame fr2 = new frame ( id , src . _ names . clone ( ) , svecs ) ;
check value xml ( table , row _ 1 , column _ 1 , value _ 1 ) ; check value xml ( table , row _ 1 , column _ 2 , value _ 2 ) ; check value xml ( table , row _ 2 , column _ 1 , value _ 3 ) ; check value xml ( table , row _ 2 , column _ 2 , value _ 4 ) ; response = delete row ( table , row _ 1 ) ;
int last line = layout . get line for vertical ( height ) - 1 ;
test activiti event listener listener = new test activiti event listener ( ) ; bpmn model bpmn model = repository service . get bpmn model ( first definition . get id ( ) ) ; assert not null ( bpmn model ) ; ( ( activiti event support ) bpmn model . get event support ( ) ) . add event listener ( listener ) ;
if ( i = = 1 ) { int actual loaned segment count = ( int ) results [ 0 ] . get long ( command log stats . stat name . in _ use _ segment _ count . name ( ) ) ; int actual segment count = ( int ) results [ 0 ] . get long ( command log stats . stat name . segment _ count . name ( ) ) ; string message = unexpected segment count : should be 2 ; assert true ( message , actual segment count = = 2 ) ; message = unexpected segment count : loaned segment count should be less than total count ; assert true ( message , actual loaned segment count < = actual segment count ) ; }
update parameter values ( referencing to net cdfparameters , code , value , parameter values ) ;
vm . intent ( new intent ( ) . put extra ( intent key . update , update factory . update ( ) ) ) ; koala test . assert values ( koala event . viewed _ comments ) ;
buffer . append ( java arg . get name ( ) ) ;
for ( string known header string : known _ headers ) { if ( header line . to lower case ( ) . starts with ( known header string . to lower case ( ) ) ) { is known = true ; break ; } }
if ( connection creation options . is empty ( ) ) { for ( map . entry < string , string > property : connection creation options . entry set ( ) ) { model node property op = new model node ( ) ; property op . get ( op ) . set ( add ) ; property op . get ( op _ addr ) . set ( address . to model node ( ) ) . add ( property , property . get key ( ) ) ; property op . get ( value ) . set ( property . get value ( ) ) ; steps . add ( property op ) ; } }
if ( current arguments access = null ) { arguments access stack . push ( current arguments access ) ; }
if ( m center item click listener = null & & action = motion event . action _ cancel ) { m center item click listener . on center item click ( m current page ) ; }
float elevation = m elevation ;
case token . instanceof :
tenant api . delete tenant key ( tenant key . catalog . to string ( ) , call context ) ;
connect handler proxy = new connect handler ( ) ;
int tab size = j edit . get integer property ( print . tab size , 4 ) ; string builder tabs = new string builder ( ) ; char [ ] chars = new char [ tab size ] ; for ( int i = 0 ; i < tab size ; i + + ) { tabs . append ( ' ' ) ; } double tab width = font . get string bounds ( tabs . to string ( ) , frc ) . get width ( ) ; print tab expander tab expander = new print tab expander ( tab width ) ;
get get = new get ( row ) ; get . add column ( family , qualifier ) ; get . set max versions ( 2 ) ; result result = ht . get ( get ) ; assert nresult ( result , row , family , qualifier , new long [ ] { stamps [ 4 ] , stamps [ 5 ] } , new byte [ ] [ ] { values [ 4 ] , values [ 5 ] } , 0 , 1 ) ; scan scan = new scan ( row ) ;
if ( timer service = null ) { try { timer service . shutdown service ( ) ; } catch ( throwable t ) {
assert response ( new request ( http : localhost : 8080 nodes v2 state failed host6 . yahoo . com , new byte [ 0 ] , request . method . put ) , { \ message \ : \ moved host6 . yahoo . com to failed \ } ) ;
if ( _ properties = = null ) return ;
( ( text view ) certificate view . find view by id ( com . android . internal . r . id . serial _ number ) ) . set text ( get serial number ( m x509 certificate ) ) ;
time hint = ;
ioexception io = new ioexception ( deserialization error ) ;
for ( integer key group index : key groups list ) { if ( state . contains key ( key group index ) ) { service . restore timers for key group ( new data input view stream wrapper ( new byte array input stream ( state . get ( key group index ) ) ) , key group index , heap internal timer service test . class . get class loader ( ) ) ; } }
if ( v = = _ map . get no entry value ( ) ) { return null ; } else { return wrap value ( v ) ; }
if ( apn context . get apn type ( ) . equals ( phone . apn _ type _ default ) & & ( apn context . get state ( ) = = state . idle | | apn context . get state ( ) = = state . scanning ) ) m phone . notify data connection failed ( apn context . get reason ( ) , apn context . get apn type ( ) ) ; notify off apns of availability ( apn context . get reason ( ) ) ; return false ;
if ( string utils . is blank ( el action . get attribute ( type ) ) ) { log tailor xmlinvalid ( found < action > without type attribute ) ; return null ; }
accept ( trace . to array ( new span [ 0 ] ) ) ; assert that ( store ( ) . get trace ( 0 l , trace . get ( 0 ) . trace id ) ) . contains exactly elements of ( trace ) ; assert that ( store ( ) . get trace ( trace . get ( 0 ) . trace id high , trace . get ( 0 ) . trace id ) ) . contains exactly elements of ( trace ) ; }
long prev local checkpoint = sequence numbers . no _ ops _ performed ;
int k = use _ sign _ of _ a * sa + use _ sign _ of _ c * sc ; int q = flip ( k ) ; opposite of k return a * k + b * q ; }
current tab = get activity tab ( ) ; content view core = current tab = null ? current tab . get content view core ( ) : null ; if ( content view core = null ) { content view core . set accessibility state ( true ) ; } } }
if ( this context . is constructor & & parent . is call ( ) & & parent . get first child ( ) = = n ) { this context . last super statement = get enclosing statement ( parent , this context . scope body ) ; } break ; default : break ; } return true ; }
this . computed combined crc = ( this . stored combined crc < < 1 ) | ( this . stored combined crc > > > 31 ) ; this . computed combined crc ^ = this . stored block crc ; report crcerror ( ) ;
system . out . println ( get bit ) ; system . out . println ( assorted methods . to full binary string ( number ) ) ; for ( int i = 31 ; i > = 0 ; i - - ) { int res = get bit ( number , i ) ? 1 : 0 ; system . out . print ( res ) ; }
field field2 = new text field ( field2 , this field uses the memory codec as the test , field . store . no ) ; doc . add ( field2 ) ; field id field = new string field ( id , , field . store . no ) ;
offset = nal unit offset + 3 ;
ssalt = new byte [ ] { 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 } ; } else {
if ( this . get width ( ) = other . get width ( ) ) { result . add ( new base column field diff impl ( field _ width , this . get width ( ) , other . get width ( ) ) ) ; }
if ( chunk . get chunk type ( ) = = chunk type . start _ tag ) { name space = null ; indents + + ; } }
hawt dbfile . start ( ) ;
if ( _ content . has content ( ) | | _ content instanceof sentinel content ) return _ content ;
assert true ( find missing number . find sum pattern ( new big decimal [ ] { big decimal . value of ( 1 ) , big decimal . value of ( 2 ) , big decimal . value of ( 4 ) , big decimal . value of ( 6 ) , big decimal . value of ( 8 ) , big decimal . value of ( 11 ) } ) = = null ) ;
if ( args . length < 4 ) { print usage ( ) ; system . exit ( 1 ) ; } else { try { portname = args [ 0 ] ; unitid = integer . parse int ( args [ 1 ] ) ; ref = integer . parse int ( args [ 2 ] ) ; count = integer . parse int ( args [ 3 ] ) ; if ( args . length = = 5 ) { repeat = integer . parse int ( args [ 4 ] ) ; } } catch ( exception ex ) { ex . print stack trace ( ) ; print usage ( ) ; system . exit ( 1 ) ; } }
try { mapper . read value ( quote ( value ) , byte [ ] . class ) ; fail ( should not pass ) ; } catch ( mismatched input exception e ) { verify exception ( e , failed to decode ) ; verify exception ( e , as base64 ) ; verify exception ( e , illegal character ' ' ) ; }
return scope ; }
file operation . delete file ( temp res7 zip ) ; logger . e ( final 7zip resource : % s , size = % d , md5 = % s , extract to7 zip . get name ( ) , extract to7 zip . length ( ) , res7zip md5 ) ; log writer . write line to info file ( string . format ( final 7zip resource : % s , size = % d , md5 = % s , extract to7 zip . get name ( ) , extract to7 zip . length ( ) , res7zip md5 ) ) ; } }
if ( pixmap . can use image element ( ) ) { gl . tex image2 d ( target , level , internalformat , format , type , pixmap . get image element ( ) ) ; } else { gl . tex image2 d ( target , level , internalformat , format , type , pixmap . get canvas element ( ) ) ; }
typed scope creator scope creator = new typed scope creator ( compiler ) ;
assert . assert not null ( serialization , operator = kryo clone utils . clone object ( operator ) ) ; operator . output . set sink ( sink ) ; operator . setup ( null ) ; operator . begin window ( 1 ) ; windowed value < integer > wv2 = windowed value . value in global window ( 2 ) ; operator . side input1 . process ( apex stream tuple . data tuple . of ( side input ) ) ; assert . assert equals ( number outputs , 1 , results . size ( ) ) ; assert . assert equals ( result , windowed value . value in global window ( 23 ) , ( ( apex stream tuple . data tuple < ? > ) results . get ( 0 ) ) . get value ( ) ) ;
value state descriptor < integer > desc = new value state descriptor < > ( any , int serializer . instance ) ; desc . set queryable ( vanilla ) ; value state < integer > state = backend . get partitioned state ( void namespace . instance , void namespace serializer . instance , desc ) ;
if ( node . is traversed ( ) & & node . is dirty ( ) ) { node . set traversed ( true ) ; stack queue . push ( node ) ; log event ( node , event type ) ; }
hadoop result . output ( hadoop output format ) ;
insert rows ( client , table , 10 ) ;
realm . close ( ) ; } } ; thread . start ( ) ; test helper . await or fail ( create latch ) ;
throw new illegal argument exception ( invalid date range + date range ) ; }
if ( _ nstrings [ i ] > _ ndates [ i ] & & _ nstrings [ i ] > _ n uuid [ i ] & & _ nstrings [ i ] > ( _ nnums [ i ] + _ nzeros [ i ] ) & & _ domains [ i ] . size ( ) > = 0 . 95 * _ nstrings [ i ] ) { types [ i ] = vec . t _ str ; continue ; }
assert jq ( req ( def type , edismax , q , * , df , doesnotexist _ s , sow , sow ) , response docs [ 0 ] = = make sure we get something . . . ) ;
set scrolling cache enabled ( false ) ; return ;
dfstest util . create file ( fs , test _ path , 1 , ( short ) 1 , 0xdeadbeef ) ;
boolean cancel result = trade service . cancel order ( limit order return value ) ; system . out . println ( canceling returned + cancel result ) ; print open orders ( trade service ) ;
assert that ( under test . select sub projects by component uuids ( db session , singleton list ( directory . uuid ( ) ) ) ) . extracting ( component dto : : uuid ) . contains exactly in any order ( sub module . uuid ( ) ) ;
int wait count = 0 ; while ( nm . get service state ( ) = state . started & & wait count + + = 20 ) { log . info ( waiting for nm to stop . . ) ; thread . sleep ( 1000 ) ; } assert . assert true ( nm . get service state ( ) = = state . started ) ; nm . stop ( ) ; }
enc client to server . limit ( ssl _ record _ header _ length ) ;
get mock endpoint ( mock : update ) . expected message count ( 1 ) ; template . send body ( seda : post - say - bye , i was here ) ; assert mock endpoints satisfied ( ) ; string out = template . request body ( seda : get - say - hello , me , string . class ) ;
if ( first byte = 22 ) { 22 : handshake record throw new sslexception ( no handshake record ) ; }
protocol . add response ( discover items , verification . corresponding sender receiver , verification . request type get ) ;
inode tmp child = file ; inode directory tmp parent = file . get parent ( ) ; while ( true ) { if ( tmp parent = = null ) { return true ; } inode child inode = tmp parent . get child ( tmp child . get local name bytes ( ) , snapshot . current _ state _ id ) ; if ( child inode = = null | | child inode . equals ( tmp child ) ) { a newly created inode with the same name as an already deleted one would be a different inode than the deleted one return true ; } if ( tmp parent . is root ( ) ) { break ; } tmp child = tmp parent ; tmp parent = tmp parent . get parent ( ) ; } if ( file . is with snapshot ( ) & & file . get file with snapshot feature ( ) . is current file deleted ( ) ) { return true ; }
assert equals ( permissions , result token . get permissions ( ) ) ; }
return string utils . is not blank ( shell context . get parameters ( ) . get ( join _ column _ name ) ) ; }
return new file store < t > ( store uri . get path ( ) , . json , serializer ) ;
time = new tensor function benchmark ( ) . benchmark ( 50000 , vectors ( 100 , 300 , tensor type . dimension . type . indexed unbound ) , tensor type . dimension . type . indexed unbound , false ) ;
assert single cue empty ( overlapping subtitle . get cues ( 4000000 ) ) ; assert single cue empty ( overlapping subtitle . get cues ( 4500000 ) ) ; assert single cue empty ( overlapping subtitle . get cues ( long . max _ value ) ) ; }
if ( file util . is writable ( new file ( folder , dummy file ) ) ) { return 1 ; } else { return 0 ; }
if ( measure spec . get mode ( width measure spec ) = = measure spec . unspecified ) w size = max row width + ( get padding left ( ) + get padding right ( ) ) ; set measured dimension ( resolve size ( w size , width measure spec ) , resolve size ( total height + get padding top ( ) + get padding bottom ( ) , height measure spec ) ) ; }
d = process add ( default - values - fallback - to - unique , doc ( f ( timestamp , now ) , f ( id , 550e8400 - e29b - 41d4 - a716 - 446655440000 ) , f ( processor _ default _ s , i have a value ) , f ( processor _ default _ i , 12345 ) , f ( name , existing , values ) ) ) ; assert not null ( d ) ;
assert equals ( collections . < topic partition > empty set ( ) , new hash set < > ( assignments . get ( consumer10 ) . partitions ( ) ) ) ;
sut . set goal ( start ) ; verify ( retry policy , timeout ( 30000 ) ) . delay ( any ( throttle state . class ) ) ;
long match index ; do { append entries . request last = ( append entries . request ) iterables . last ( outbound . sent to ( follower ) ) ; match index = last . prev log index ( ) + last . entries ( ) . length ; outbound . clear ( ) ; log shipper . on match ( match index , new leader context ( 0 , 0 ) ) ; } while ( outbound . sent to ( follower ) . size ( ) > 0 ) ;
lyrics text view . set visibility ( view . visible ) ;
final typed array styles = obtain styled attributes ( r . style . app theme _ toolbar , new int [ ] { r . attr . color control normal } ) ; try { int tint color = styles . get color ( 0 , color . black ) ; for ( int i = 0 ; i < menu . size ( ) ; i + + ) { drawable icon = menu . get item ( i ) . get icon ( ) ; if ( icon = null ) { drawable compat . set tint ( icon , tint color ) ; } } } finally { styles . recycle ( ) ; } return super . on create options menu ( menu ) ;
synchronized ( lock interval ) { internal list . add ( new resp time graph data bean ( sample result . get start time ( ) , sample result . get time ( ) , sample label ) ) ; } matcher matcher = null ;
try { zks . get leader ( ) . propose ( request ) ; } catch ( xid rollover exception e ) { throw new request processor exception ( e . get message ( ) , e ) ; } sync processor . process request ( request ) ; } }
return null ; } else {
when ( context . election ok ( ) ) . then return ( true ) ;
data map japanese map = new data map ( ) ; japanese map . put ( konnichiwa , ããã«ã¡ã¯ ) ; japanese data map emoji map = new data map ( ) ;
ta [ i ] = original [ i ] = = null?null c : original [ i ] . get class ( ) ;
else { return load providers ( wild fly security manager . get class loader privileged ( wild fly provider resolver . class ) ) ; }
response response = exchange url . request ( ) . header ( http headers . authorization , basic auth helper . create header ( client app . deployment _ name , password ) ) . post ( entity . form ( new form ( ) . param ( oauth2 constants . grant _ type , oauth2 constants . token _ exchange _ grant _ type ) . param ( oauth2 constants . subject _ token , access token ) . param ( oauth2 constants . subject _ token _ type , oauth2 constants . jwt _ token _ type ) . param ( oauth2 constants . subject _ issuer , parent _ idp ) ) ) ; assert . assert equals ( 200 , response . get status ( ) ) ; access token response token response = response . read entity ( access token response . class ) ; string exchanged access token = token response . get token ( ) ;
if ( escape ) {
map < string , string > parameters = request . get parameter map ( ) . entry set ( ) . stream ( ) . map ( entry - > { string [ ] values = entry . get value ( ) ; string value = values = = null | | values . length = = 0 ? null : values [ 0 ] ; return new simple entry < > ( entry . get key ( ) , value ) ; } ) . collect ( collectors . to map ( simple entry : : get key , simple entry : : get value ) ) ;
if ( _ value = = 0 ) return 0 ; else if ( double . is na n ( _ value ) ) return na n ; else if ( _ value < 0 & & double . is infinite ( _ value ) ) return - infinity ; else if ( double . is infinite ( _ value ) ) return infinity ; else return format numbers ( ( int ) _ value ) ;
config set = test _ path ( ) . resolve ( configsets ) ; string [ ] args = new string [ ] { - confname , upconfig2 , - confdir , cloud - subdirs , - zk host , zk addr , - configsets dir , config set . to absolute path ( ) . to string ( ) , } ;
intent . put extra ( extra _ sender , pending intent . get activity ( activity , 0 , new intent ( ) , 0 ) ) ;
case e _ alnum :
parser . read frame ( true ) ; assert . assert equals ( 3 - rst - [ + http2 error . frame _ size _ error . get code ( ) + ] \ n , output . get trace ( ) ) ;
log . debug ( recovered from + string utils . stringify exception ( e ) ) ;
next path . add ( edge ) ;
page builder . reset ( ) ;
assert true ( t . get exception ( ) . contains ( undefined field : ) ) ; }
server name sn = new server name ( example . com , 1234 , system . current time millis ( ) ) ; hconnection connection = null ;
assert . assert equals ( load state . unknown , load state with reference ) ;
assert equals ( incorrect buffer size , 512 , dos . get protected buf ( ) . length ) ; dos . write ( byte array ) ; dos . close ( ) ; f1 . delete ( ) ; }
cluster . get name node ( ) . reconfigure property ( dfs . block . replicator . classname , org . apache . hadoop . hdfs . server . + namenode . + test name node reconfigure + mock placement policy ) ;
if ( check circular inheritance ( module config ) ) { throw new illegal argument exception ( circular inheritance detected for form bean + get name ( ) ) ; }
new content . insert ( find _ insertion _ point _ start _ of _ string , find _ insertion _ point _ head _ content ) ;
c = reader . read ( ) ;
jtx transaction jtx2 = worker . maybe request transaction ( required new ( ) , ctx _ 2 ) ; assert not null ( jtx2 ) ; db session session2 = session provider . get db session ( ) ; assert not same ( session1 , session2 ) ; execute update ( session2 , insert into girl values ( 2 , ' gloria ' , null ) ) ; assert true ( worker . maybe commit transaction ( jtx2 ) ) ; assert false ( jtx2 . is active ( ) ) ;
move region action = null ; restart meta action = null ; move meta regions action = null ; restart rsaction = null ; restart master action = null ; load tool = null ;
udfin order by ,
props = filter bean properties ( config , bean desc , props ) ;
reset cell ( cell ) ; if ( ignore count ) { return scan query matcher . match code . include ; }
tree build builder = new tree build ( ) ; node top = null ; node place arrange = new place node2 ( ) ;
orc tail . get stripe statistics ( ) ;
return create resource ( configuration . default _ resource _ package , class name + configuration . default _ resource _ suffix ) ;
add edge ( n4 , n5 , e12 ) ;
try { http protocol = factory . get protocol ( http : somehost ) ; assert . assert not null ( http protocol ) ; } catch ( exception ex ) { assert . fail ( must not throw any other exception ) ; }
if ( map . contains key ( max _ inactives ) ) { long max inactives = long . parse long ( map . get ( max _ inactives ) ) ; token . set max inactives ( max inactives ) ; } token . set expires ( expires ) ; return token ; }
if ( r _ mark _ l ar ( ) ) { cursor = limit - v _ 21 ; break lab40 ; }
return ignore _ item _ view _ type ; } else {
tree . set root visible ( false ) ;
document doc = jsoup . parse ( < body > < p > < select > < option > one < option > two < p > < p > three < p > ) ;
assert . assert null ( invoice user api . insert invoice item adjustment ( account id , invoice id , invoice item . get id ( ) , clock . get utctoday ( ) , null , call context ) ) ; }
context . add routes ( new spring route builder ( ) { @ override public void configure ( ) throws exception { policy required = lookup ( propagation _ required _ policy , spring transaction policy . class ) ; from ( activemq : queue : foo ) . policy ( required ) . process ( new conditional exception processor ( ) ) ; } } ) ; assert result ( ) ;
blob key key1a2 = put ( cache , job id1 , new chunked input stream ( data , 19 ) , transient _ blob ) ; assert not null ( key1a2 ) ; verify key different hash equals ( key1a , key1a2 ) ; transient blob key key1b = ( transient blob key ) put ( cache , job id1 , new chunked input stream ( data2 , 19 ) , transient _ blob ) ; assert not null ( key1b ) ;
handle bad request ( bad request , only rtmpt supported . , resp ) ; return ; }
jrtserver config request request2 = tester . create request ( baz , config id , namespace , timeout ) ;
trade . set trading peer node address ( process model . get temp trading peer node address ( ) ) ; process model . remove mailbox message after processing ( trade ) ;
broker conf . add property ( pinot . broker . transport . routing mode , helix ) ;
if ( current position ( ) - start position > = database descriptor . get column index size ( ) ) add index block ( ) ; }
final command cmd = state . get command ( ) ;
m lines [ m columns * line + ellipsis _ start ] = 0 ; m lines [ m columns * line + ellipsis _ count ] = 0 ; return ;
double probability = seeds . size ( ) ( double ) ( live endpoints . size ( ) + unreachable endpoints . size ( ) ) ;
gridmix conf = new configuration ( ) ;
map < multi chunk id , multi chunk entry > unused multi chunks = local database . get unused multi chunks ( ) ; local database . remove unreferenced database entities ( ) ; delete unused remote multi chunks ( unused multi chunks ) ;
of ( new array class or generic array type ( component supertype . runtime type ) ) ; return result ; }
m language = gsm alphabet . gsm7 bit packed to string ( pdu , offset , 2 ) ;
list < string > racks = switch mapping . resolve ( collections . singleton list ( server . get hostname ( ) ) ) ; if ( racks = null & & racks . is empty ( ) ) { return racks . get ( 0 ) ; } return unknown _ rack ; }
queue . add ( new midi info fetcher ( ) ) ;
if ( available roles . is empty ( ) ) { continue ; }
more messages more messages = local folder . get more messages ( ) ;
string content type = params . get ( common params . stream _ contenttype ) ;
set < string > model names = new hash set < > ( grid . get model count ( ) ) ;
assert typing ( persistent object exception . class , e . get cause ( ) ) ; }
string organization name = test org ;
object rv2 = java . lang . reflect . array . new instance ( type . get component type ( ) , rv . length ) ;
int len = n ;
int new period index = resolve subsequent period ( playing period index , old timeline , timeline ) ;
http headers headers = last . trailing headers ( ) ; if ( headers . is empty ( ) ) { out . add ( last http content . empty _ last _ content ) ; } else { out . add ( new composed last http content ( headers ) ) ; } return true ;
grammar transform pipeline transform = new grammar transform pipeline ( g , g . tool ) ; transform . reduce blocks to sets ( r . ast ) ; transform . expand parameterized loops ( r . ast ) ;
media type media type = request . get content type ( ) = null ? media type . value of ( request . get content type ( ) ) : null ;
map all filter . put ( integer . value of ( filter . get id ( ) ) , filter ) ; }
dispatcher disp = new default rule dispatcher ( null , op rules , proc ctx ) ; graph walker ogw = new gen spark work walker ( disp , proc ctx ) ; ogw . start walking ( top nodes , null ) ; }
r service = create service ( service binding ) ; if ( service = = null ) { return null ; }
media local media = local md . get media ( ) ;
put put = new put ( row1 ) ; put . add ( dummy , new byte [ 0 ] , pass . get bytes ( ) ) ; try { table . put ( put ) ; fail ( runtime fail constraint wasn ' t triggered - this put shouldn ' t work ) ; } catch ( exception e ) { noop }
expect contents ( helpers . map entry ( k0 ( ) , v1 ( ) ) ) ;
connection connection = delegate . get connection ( ) ;
parse file . state state = new parse file . state . builder ( ) . url ( test ) . build ( ) ;
make sure cleaned up ( vols , service ) ;
handler . start write queue ( channel ) ;
set launch attribute ( icdtlaunch configuration constants . attr _ program _ name , exec _ path + exec _ name ) ;
return super . get encoded ( ) ;
if ( tail available > 2 & & tail > 2 ) { if ( ( b arr [ in index + 3 ] & 0x c0 ) = 0x80 ) { in . position ( in index - in . array offset ( ) ) ; out . position ( out index - out . array offset ( ) ) ; return coder result . malformed for length ( 3 ) ; } }
if ( count > = capacity ) return false ; node < e > l = last ; node . prev = l ; last = node ; if ( first = = null ) first = node ; else l . next = node ; + + count ; not empty . signal ( ) ; return true ;
get transform ( ) . set to rotation ( angle , rotate . get x ( ) , rotate . get y ( ) ) ; s1 = shape1 . create transformed shape ( transform ) ; s2 = shape2 . create transformed shape ( transform ) ; s3 = shape3 . create transformed shape ( transform ) ; }
if ( starts with ignore case ( header , set _ cookie2 ) ) { header = header . substring ( set _ cookie2 . length ( ) ) ; } else if ( starts with ignore case ( header , set _ cookie ) ) { header = header . substring ( set _ cookie . length ( ) ) ; } list < http cookie > cookies = new java . util . array list < http cookie > ( ) ;
exit code = 1 ; log . error ( cmd + : + e . get localized message ( ) ) ; } finally {
int freq = conf . get int var ( hive conf . conf vars . hivetestmodesamplefreq ) ;
if ( ( in _ grouping _ b ( g _ particle _ end , 97 , 246 ) ) ) { return false ; }
final boolean determine num partitions = tuning config . get num shards ( ) = = null & & is guaranteed rollup ( io config , tuning config ) ;
list < local entry > decoded locals = decoder . get locals ( ) ;
boolean origianl swipeable = m card . is swipeable ( ) ; m card . set swipeable ( false ) ; m card view . set card ( m card ) ;
map storm conf = storm config . read _ supervisor _ storm _ conf ( conf , topology id ) ;
session = create local term session ( ) ;
container request event event2 = create req ( job id , 2 , 3000 , new string [ ] { h1 } , false , true ) ;
assert false ( promotion result . get ( ) ) ;
from ( direct : get results ) . to ( salesforce : get results ) ;
_ test ( 500 , false , true , false ) ; }
some resource2 r2 = factory . create ( some resource2 . class ) ; assert not null ( r2 ) ;
child rex node lst = new array list < rex node > ( ) ; child rex node lst . add ( twob ) ; child rex node lst . add ( threea ) ; return rex builder . make call ( sql function converter . get calcite fn ( - , calcite arg types bldr . build ( ) , type converter . convert ( type info factory . long type info , cluster . get type factory ( ) ) , false ) , child rex node lst ) ; }
pos = 12 ;
block cache . set test block size ( 0 ) ;
prepared operation < object , object > prepared operation = mock ( prepared operation . class ) ;
process . ask for stop ( ) ;
document doc = xmlunit . build test document ( file utils . read file to string ( config file ) ) ; xmlassert . assert xpath evaluates to ( ft1 - id , transform feature type id , doc ) ; }
rule definition dto custom rule = rule testing . new custom rule ( template rule ) . set name ( old name ) . set description ( old description ) . set severity ( severity . minor ) . set status ( rule status . beta ) . get definition ( ) ; db . rules ( ) . insert ( custom rule ) ; db . rules ( ) . insert rule param ( custom rule , param - > param . set name ( regex ) . set type ( string ) . set description ( reg ex ) . set default value ( null ) ) ; db session . commit ( ) ;
audio node em = ( audio node ) spat ;
vec v = new s3 file vec ( k2 , size ) ; dkv . put ( k2 , v , fs ) ; frame fr = new frame ( k , new string [ ] { path } , new vec [ ] { v } ) ; fr . update ( ) ; fr . unlock ( ) ; return k ; }
string clazz = constraint processor . class . get name ( ) ;
float scale factor = min _ scale + ( 1 - min _ scale ) * ( 1 - math . abs ( position ) ) ; view . set scale x ( scale factor ) ; view . set scale y ( scale factor ) ; } else { ( 1 , + infinity ]
int previous month = ( day . get ( calendar . month ) = = 0?12 : day . get ( calendar . month ) ) ;
class < ? extends throwable > exception class = this instanceof test query parser ? org . apache . lucene . queryparser . classic . parse exception . class : org . apache . lucene . queryparser . flexible . standard . parser . parse exception . class ;
if ( status . deleted = entry . get status ( ) ) { callback registry . post update ( entity ) ; }
preconditions . check argument ( task . get job id ( ) . equals ( job id ) , the task ' s job id does not match the + job id for which the slot has been allocated . ) ; preconditions . check argument ( task . get allocation id ( ) . equals ( allocation id ) , the task ' s allocation + id does not match the allocation id for which the slot has been allocated . ) ; preconditions . check state ( task slot state . active = = state , the task slot is not in state active . ) ; task old task = tasks . put ( task . get execution id ( ) , task ) ;
path output dir = new path ( test dir , output ) ; path new output dir = output dir . suffix ( 0 ) ; job conf job1 = configure job ( mr . create job conf ( ) , 10 , 0 , in dir , new output dir , map signal file , red signal file ) ;
snapshot regions . put ( cloned region name , snapshot region info ) ;
input stream is = get class ( ) . get resource as stream ( account example - getbalance . json ) ;
list < product > mock products first page = arrays . as list ( new product ( 1 , image , name , category1 , description , 21 . 9 ) , new product ( 2 , image , name , category1 , description , 21 . 9 ) , new product ( 3 , image , name , category1 , description , 21 . 9 ) , new product ( 4 , image , name , category1 , description , 21 . 9 ) , new product ( 5 , image , name , category1 , description , 21 . 9 ) ) ; list < product > mock products next page = arrays . as list ( new product ( 6 , image , name , category2 , description , 21 . 9 ) , new product ( 7 , image , name , category2 , description , 21 . 9 ) , new product ( 8 , image , name , category2 , description , 21 . 9 ) , new product ( 9 , image , name , category2 , description , 21 . 9 ) ) ;
al10 . al sourcei ( source , efx10 . al _ direct _ filter , efx10 . al _ filter _ null ) ; if ( al10 . al get error ( ) = = al10 . al _ no _ error ) { system . out . println ( successfully removed direct filter ) ; } }
if ( now - m last vibrate > = 125 ) { m vibrator . vibrate ( 5 ) ; m last vibrate = now ; } }
args = new string [ ] { - set quota , 1 , parent . to string ( ) } ;
} read buffer . flip ( ) ; return read buffer ; }
tester . complete upgrade with error ( default0 , version , default , deployment jobs . job type . system test ) ; tester . complete upgrade with error ( default1 , version , default , deployment jobs . job type . system test ) ; tester . complete upgrade with error ( default2 , version , default , deployment jobs . job type . system test ) ; tester . complete upgrade with error ( default3 , version , default , deployment jobs . job type . system test ) ; tester . update version status ( version ) ; assert equals ( vespa version . confidence . broken , tester . controller ( ) . version status ( ) . system version ( ) . get ( ) . confidence ( ) ) ; tester . upgrader ( ) . maintain ( ) ;
list < string > component names = find component names ( ) ; final set < file > json files = new tree set < file > ( ) ;
cached map . resolve ( create query list ( ) ) ;
if ( g = pg | | f = pf | | p = pp | | c = pc | | pd = ppd | | pt = ppt ) { log . info ( segs [ i ] + changed input dirs ) ; } pg = g ; pf = f ; pp = p ; pc = c ; ppd = pd ; ppt = pt ; }
load balancer simulator load balancer simulator = load balancer simulation builder . build ( cluster - 1 , foo , uris , strategy properties , null , null , delay generator , qps generator ) ;
annotated field field = _ fields . value ; linked < annotated field > next = _ fields . next ; for ( ; next = null ; next = next . next ) { annotated field next field = next . value ; class < ? > field class = field . get declaring class ( ) ; class < ? > next class = next field . get declaring class ( ) ; if ( field class = next class ) { if ( field class . is assignable from ( next class ) ) { next is more specific field = next field ; continue ; } if ( next class . is assignable from ( field class ) ) { getter more specific continue ; } } throw new illegal argument exception ( multiple fields representing property \ + get name ( ) + \ : + field . get full name ( ) + vs + next field . get full name ( ) ) ; } return field ;
layout params . width = layout params . match _ parent ;
data stream < integer > source = create source ( env , execution mode . restore ) ; single output stream operator < integer > first = create first stateful map ( execution mode . restore , source ) ;
original property = null ;
fsdata output stream out = hdfs . append ( bar ) ;
invalid session info info = error . get client info ( ) . is object ( ) . get java script object ( ) . cast ( ) ;
final count down latch done signal = new count down latch ( count ) ; cache loader < string , string > compute function = new cache loader < string , string > ( ) { @ override public string load ( string key ) throws interrupted exception { call count . increment and get ( ) ; second signal . count down ( ) ; compute signal . await ( ) ; return key + foo ; } } ;
final resource root sub deployment root = sub deployment . get attachment ( attachments . deployment _ root ) ;
string msg = test grad mlp2 layer iris simple ( ) - score did not ( sufficiently ) decrease during learning - activation fn = + afn + , loss fn = + lf + , output activation = + output activation + , do learning first = + do learning first + ( before = + score before + , score after = + score after + ) ;
tester . runtest ( create procedure thisproc as + begin \ n + select * from books ; + select title , + case when cash > 100 . 00 then + case when cash > 1000 . 00 then ' super expensive ' else ' pricy ' end + else ' cheap ' end + from books ; + end ) ;
check lock and reacquire ( lock , true ) ; assert equals ( true , lock . have lock ( ) ) ; assert equals ( true , lock . get internal lock ( ) . is lock held ( ) ) ; factory = create lock factory ( client id + _ 2 , zkc , 0 , 0 ) ; zkdistributed lock lock2 = new zkdistributed lock ( lock state executor , factory , lock path , 0 , null stats logger . instance ) ;
if ( recipient ) { return pdu _ compose _ content _ error ; }
if ( next int ( 10 ) = = 7 ) { if ( random ( ) . next boolean ( ) ) buffer [ i ] = ( char ) next int ( 0xd800 , 0xdc00 ) ; else buffer [ i ] = ( char ) next int ( 0xdc00 , 0xe000 ) ; expected [ i + + ] = 0xfffd ; expected [ i ] = buffer [ i ] = ( char ) next int ( 0x800 , 0xd800 ) ; has illegal = true ; } else expected [ i ] = buffer [ i ] = ( char ) next int ( 0x800 , 0xd800 ) ;
boolean set to null = false ;
move pointer ( center , move location ) ; send pointer ( motion event . action _ up , center ) ;
current + = uppercase addon ;
assert true ( pn . get ( 0 ) . to explain plan string ( ) . contains ( inline limit ) ) ; if ( pushdown ) { assert true ( pn . get ( 1 ) . to explain plan string ( ) . contains ( inline limit ) ) ; } else { assert false ( pn . get ( 1 ) . to explain plan string ( ) . contains ( inline limit ) ) ; } }
do return ( new foo ( hey ) ) . when ( mock ) ; invocation constructor = mockito . framework ( ) . get invocation factory ( ) . create invocation ( mock , with settings ( ) . build ( foo . class ) , adapter , real method , ctr , foo ) ; handler . handle ( constructor ) ;
return get name ( ) ; }
assert equals ( expected utf8 . size ( ) , utf8 . size ( string ) ) ;
assert translation ( translation , \ ljava util set < tt ; > ; \ ) ;
em . get transaction ( ) . begin ( ) ; master = em . find ( many to one not insertable entity . class , mto _ id1 ) ; master . set number ( type _ id2 ) ;
f base type = schema grammar . f any type ; f derived by = xsconstants . derivation _ restriction ; process complex content ( child , mixed att . boolean value ( ) , false , schema doc , grammar ) ; } else if ( domutil . get local name ( child ) . equals ( schema symbols . elt _ simplecontent ) ) {
string response = io . to string ( is ) ;
if ( args . length = = 3 & & args [ 2 ] = null & & args [ 2 ] instanceof xml element ) { fragment . fragment context = ( xml element ) args [ 2 ] ; } runtime helpers . invoke ( context , fragment , initialize , args ) ; return fragment ; }
system . arraycopy ( buffer , start , buffer , start - 1 , real index - start ) ;
node planar orientation = name . get next sibling ( ) ;
buffer = total header . write ( buffer , c , true ) ;
while ( call record [ call record . length - 1 ] ) looper . run to end of tasks ( ) ;
assert equals ( 800 , rd . granted . size ( ) ) ; log . info ( stopping test cmfailure transient ) ; } catch ( throwable t ) {
case tckind . _ tk _ wchar : char char value = orb stream . read _ wchar ( ) ; field to value map . put ( fields [ i ] . name , new character ( char value ) ) ; break ; case tckind . _ tk _ short : short short value = orb stream . read _ short ( ) ; field to value map . put ( fields [ i ] . name , new short ( short value ) ) ; break ; case tckind . _ tk _ long : int int value = orb stream . read _ long ( ) ; field to value map . put ( fields [ i ] . name , new integer ( int value ) ) ; break ; case tckind . _ tk _ longlong : long long value = orb stream . read _ longlong ( ) ; field to value map . put ( fields [ i ] . name , new long ( long value ) ) ; break ; case tckind . _ tk _ float : float float value = orb stream . read _ float ( ) ; field to value map . put ( fields [ i ] . name , new float ( float value ) ) ; break ; case tckind . _ tk _ double : double double value = orb stream . read _ double ( ) ; field to value map . put ( fields [ i ] . name , new double ( double value ) ) ; break ; case tckind . _ tk _ value : case tckind . _ tk _ objref : case tckind . _ tk _ value _ box :
final boolean indexed = traversal ( left operand , index ) ;
for ( map . entry < string , bundle data > entry : load data . get bundle data ( ) . entry set ( ) ) { final string bundle = entry . get key ( ) ; final bundle data data = entry . get value ( ) ; try { final string zoo keeper path = get bundle data zoo keeper path ( bundle ) ; create zpath if not exists ( zk client , zoo keeper path ) ; zk client . set data ( zoo keeper path , data . get json bytes ( ) , - 1 ) ; } catch ( exception e ) { log . warn ( error when writing data for bundle { } to zoo keeper : { } , bundle , e ) ; } }
map . set my location enabled ( false ) ;
if ( perm model . may set flag ( existing , found flag , value ) ) { throw new command permissions exception ( ) ; }
filesystem . write contents to path ( something else , file ) ; file hash cache . invalidate all ( ) ; hash code updated = build info recorder . get output hash ( file hash cache ) ; assert not equals ( current , updated ) ;
put message ( stream id , http response ) ;
arr [ 0 ] = new byte writable ( ( byte ) 123 ) ; arr [ 1 ] = new short writable ( ( short ) 456 ) ; arr [ 2 ] = new int writable ( 789 ) ; arr [ 3 ] = new long writable ( 1000l ) ; arr [ 4 ] = new double writable ( ( double ) 5 . 3 ) ; arr [ 5 ] = new bytes writable ( hive and hadoop and parquet . big family . . get bytes ( utf - 8 ) ) ; arr [ 6 ] = new bytes writable ( parquet serde binary . get bytes ( utf - 8 ) ) ; final writable [ ] map = new writable [ 3 ] ; for ( int i = 0 ; i < 3 ; + + i ) { final writable [ ] pair = new writable [ 2 ] ; pair [ 0 ] = new bytes writable ( ( key _ + i ) . get bytes ( utf - 8 ) ) ; pair [ 1 ] = new int writable ( i ) ; map [ i ] = new array writable ( writable . class , pair ) ; } arr [ 7 ] = new array writable ( writable . class , map ) ; final writable [ ] array = new writable [ 5 ] ;
sidebar . add component ( selections ) ; sidebar . set expand ratio ( selections , 1 . 0 f ) ;
astnode select expr = new astnode ( new common token ( hive parser . tok _ selexpr , tok _ selexpr ) ) ;
synchronized array id ordering queue queue = new synchronized array id ordering queue ( 10 ) ; final atomic long last applied tx id = new atomic long ( - 1 ) ; race race = new race ( ) ; for ( long i = 0 ; i < 100 ; i + + ) { final long tx id = i ; race . add contestant ( ( ) - > { try ( explicit batch index applier applier = new explicit batch index applier ( config , applier lookup , queue , internal ) ) { transaction to apply tx to apply = new transaction to apply ( new physical transaction representation ( new array list < > ( ) ) ) ; fake commitment commitment = new fake commitment ( tx id , mock ( transaction id store . class ) ) ; commitment . set has explicit index changes ( true ) ; tx to apply . commitment ( commitment , tx id ) ; transaction applier tx applier = applier . start tx ( tx to apply ) ; make sure threads are unordered thread . sleep ( thread local random . current ( ) . next int ( 5 ) ) ; then assert true ( last applied tx id . compare and set ( tx id - 1 , tx id ) ) ; closing manually instead of using try - with - resources since we have no additional work to do in tx applier tx applier . close ( ) ; } catch ( exception e ) { throw new runtime exception ( e ) ; } } ) ; queue . offer ( tx id ) ; } race . go ( ) ;
task task = setup . call ( ) ; task . add on complete listener ( new on complete listener ( ) { @ override public void on complete ( @ non null task task ) { semaphore . release ( ) ; } } ) ;
assert that ( rounding util . round ( 1 . 01234567890123456 e12 , 1 ) , is ( equal to ( 1 . 0123456789012 e12 ) ) ) ;
if ( element _ node = ( nodes . read entry ( node handle , 0 ) & 0x ffff ) ) return null ;
send broadcast ( pebble intent ) ;
return row [ probe ] ;
rollout plan builder plan builder = new rollout plan builder ( ) ;
issues service . do transition ( new do transition request ( issue . get key ( ) , resolve ) ) ;
when ( auth helper shadow . get firebase auth ( ) . fetch providers for email ( test constants . email ) ) . then return ( new auto complete task < provider query result > ( new fake provider query result ( arrays . as list ( email auth provider . provider _ id ) ) , true , null ) ) ; credential sign in handler . on complete ( mock task ) ;
if ( txn = null ) { try { txn . rollback ( ) ; } catch ( throwable ignore ) { } } }
int index = usage type str . index of ( - ) ; string region short name = index > 0 ? usage type str . substring ( 0 , index ) : ; region region = region short name . is empty ( ) ? null : region . get region by short name ( region short name ) ; if ( region = null ) { usage type str = usage type str . substring ( index + 1 ) ; } else { region = region . us _ east _ 1 ; } if ( operation str . equals ( ebs snapshot copy ) ) { product = product . ebs ; }
assert not null ( get tracker client ( strategy , null , new request context ( ) , - 1 , clients ) ) ;
string crop path = boxing file helper . get boxing path in dcim ( ) ;
comment action dialog fragment fragment = new comment action dialog fragment ( ) ; bundle arguments = fragment utils . ensure arguments ( fragment ) ; arguments . put parcelable ( extra _ comment , comment ) ; arguments . put boolean ( extra _ can _ reply _ to , can reply to ) ; arguments . put boolean ( extra _ can _ delete , can delete ) ; return fragment ; }
dfstest util . fs shell run ( - rmr sub1 , conf ) ;
if ( suffix length = = constants . continue _ after _ fatal _ error _ feature . length ( ) & & feature id . ends with ( constants . continue _ after _ fatal _ error _ feature ) ) { return f continue after fatal error ; }
if ( ip = null & & inet addresses . is inet address ( this . ip ) ) { throw new illegal argument exception ( ip + is not a valid ip address . ) ; } this . internal port = internal port ;
document doc = jsoup . parse ( < script > one \ ntwo < script > \ n < style > three \ nfour < style > ) ;
return store ;
layer drawable . set layer inset ( has shadow ( ) ? 2 : 1 , circle inset horizontal + icon offset , circle inset vertical + icon offset , circle inset horizontal + icon offset , circle inset vertical + icon offset ) ; set background compat ( layer drawable ) ;
wait for updated record propagation ( record _ id , first name , darth ) ; record1 server1 = retrieve record ( server instance . get ( 0 ) , record _ id ) ; record1 server2 = retrieve record ( server instance . get ( 1 ) , record _ id ) ; int final version server1 = record1 server1 . field ( @ version ) ; int final version server2 = record1 server2 . field ( @ version ) ; assert equals ( final version server1 , actual version + 1 ) ; assert equals ( final version server2 , actual version + 1 ) ; }
if ( d = null & & has text ( d ) ) { if ( parent = null & & has text ( parent ) ) { again , both should be from elements from element left = ( from element ) parent ; from element right = ( from element ) d ; if ( right . get real origin ( ) = = left ) { right represents a joins originating from left . . . if ( right . get join sequence ( ) = null & & right . get join sequence ( ) . is theta style ( ) ) { write cross join separator ( ) ; } else { out ( ) ; } } else { not so sure this is even valid subtree . but if it was , it ' d represent two unrelated table references . . . write cross join separator ( ) ; } } out ( d ) ; }
send output to ( new file output stream ( log file ) ) ;
level level object = new level ( name , x ) ; level = known level . find by value ( x ) ; } return level . level object ; } catch ( number format exception ex ) {
evaluate conditional expression ( batch , child expressions [ 0 ] , prev size , prev select in use ) ; if ( child expressions = null & & child expressions . length = = 2 ) {
for ( int i = 0 ; i < interfaces . length ; i + + ) { output . write utf ( interfaces [ i ] . get name ( ) ) ; }
future < record metadata > request2 = accumulator . append ( tp0 , time . milliseconds ( ) , key . get bytes ( ) , value . get bytes ( ) , null , null , max _ block _ timeout ) . future ; sender . run ( time . milliseconds ( ) ) ; assert equals ( 2 , transaction manager . sequence number ( tp0 ) . long value ( ) ) ; assert equals ( 0 , transaction manager . last acked sequence ( tp0 ) ) ; assert false ( request2 . is done ( ) ) ;
gen _ codes ( tree , max _ code , s . bl _ count ) ;
this . image asset = image asset ; this . image asset . add update listener ( this ) ; this . transition type = transition type ; updated ( ) ; }
assert . assert equals ( 1 l , actual event . get ( value ) ) ; query metrics . report wait time ( 2000001 ) . emit ( service emitter ) ; actual event = caching emitter . get last emitted event ( ) . to map ( ) ; assert . assert equals ( query wait time , actual event . get ( metric ) ) ; assert . assert equals ( 2 l , actual event . get ( value ) ) ; query metrics . report segment time ( 3000001 ) . emit ( service emitter ) ; actual event = caching emitter . get last emitted event ( ) . to map ( ) ;
owner . lock . count down ( ) ;
assert equals ( expected target , wrapper . get link target ( link abs ) ) ;
points = new array list < > ( ) ; points . add ( new geo point ( planet model . wgs84 , 0 . 7769776943105245 , - 2 . 157536559188766 ) ) ; points . add ( new geo point ( planet model . wgs84 , - 0 . 9796549195552824 , - 0 . 25078026625235256 ) ) ; points . add ( new geo point ( planet model . wgs84 , 0 . 17644522781457245 , 2 . 4225312555674967 ) ) ; points . add ( new geo point ( planet model . wgs84 , - 1 . 4459804612164617 , - 1 . 2970934639728127 ) ) ; c = geo polygon factory . make geo polygon ( planet model . wgs84 , points ) ;
check one term ( a , ÎºÎ¿Î¹Î¼Î¬Î¼Î±Î¹ , ÎºÎ¿Î¹Î¼ ) ;
move center to ( old center ) ; }
set one bytes ( zero length window frame , 3 , frame type . window _ update . get id byte ( ) ) ;
this . orb = orb ;
coverage info coverage info = multi band coverage view . create coverage info ( multiband _ select , store info , builder ) ; coverage info . get parameters ( ) . put ( use _ jai _ imageread , false ) ; catalog . add ( coverage info ) ; final layer info layer info view = builder . build layer ( coverage info ) ; catalog . add ( layer info view ) ; final envelope env = ci . bounding box ( ) ;
get all ( cache , as list ( 13 , 14 , 15 ) ) ;
this . vm . intent ( messages context intent ( message thread factory . message thread ( ) ) ) ;
new child . parent = this ;
for ( entry < string , add on class loader > entry : add on loaders . entry set ( ) ) { add on installer . install missing add on files ( entry . get value ( ) , get add on collection ( ) . get add on ( entry . get key ( ) ) ) ; }
reconfig servers . clear ( ) ; reconfig servers . add ( integer . to string ( leader id ) ) ; reconfig servers . add ( integer . to string ( follower1 ) ) ; test reconfig ( follower2 , false , reconfig servers ) ; log . info ( configuration after removing leader and follower 1 : \ n + new string ( zk handles [ follower2 ] . get config ( this , new stat ( ) ) ) ) ;
assert equals ( 6 , cache . get stats ( ) . get eviction count ( ) ) ;
add ( client1 , seen leader , sdoc ( id , integer . to string ( ( int ) v ) , _ version _ , v ) ) ;
assert true ( result . contains ( map < string , double > m map string double ; ) ) ; assert true ( result . contains ( map < string , double > [ ] m map string double array ; ) ) ; assert true ( result . contains ( map < string , double > [ ] [ ] m map string double array2 ; ) ) ; assert true ( result . contains ( map < string , double > [ ] [ ] [ ] m map string double array3 ; ) ) ;
camera manager . get ( ) . start preview ( ) ; restart preview and decode ( ) ; }
if ( constraints . is allowed cached right ( context entry , left tuple ) ) { left tuple . set blocker ( right tuple ) ; right tuple . add blocked ( left tuple ) ; ltm . remove ( left tuple ) ; insert child left tuple ( sink , trg left tuples , left tuple , right tuple . get propagation context ( ) , true ) ; } left tuple = temp ;
if ( session _ . get session info ( ) . get have srcref attribute ( ) ) { add ( checkbox pref ( use debug error handler only when my code contains errors , prefs _ . handle errors in user code only ( ) ) ) ; add ( spaced ( checkbox pref ( automatically expand tracebacks in error inspector , prefs _ . auto expand error tracebacks ( ) , false * default spaced * ) ) ) ; } add ( spaced ( checkbox pref ( wrap around when navigating to previous next tab , prefs _ . wrap tab navigation ( ) , false * default spaced * ) ) ) ;
output stream parent output stream = data entry writer . get output stream ( data entry . get parent ( ) , this ) ;
web element source menu entry = menu navigator . get menu item ( driver _ , code , source ) ; source menu entry . click ( ) ;
trace context . continue trace object ( trace ) ; return trace ;
map < object , object > state = new hash map < > ( ) ;
int aa ; int aa aa1a ; int aa aaa aa2a1 ; }
try ( final kafka producer < byte [ ] , byte [ ] > kafka producer = kafka server . new producer ( ) ) { for ( producer record < byte [ ] , byte [ ] > record : records ) { kafka producer . send ( record ) . get ( ) ; } } final kafka index task task = create task ( null , new kafka ioconfig ( sequence0 , new kafka partitions ( topic , immutable map . of ( 0 , 2 l ) ) , new kafka partitions ( topic , immutable map . of ( 0 , 5 l ) ) , kafka server . consumer properties ( ) , true , false , null , null , false ) , null ) ;
group2 = eg . get job vertex ( v4 . get id ( ) ) . get slot sharing group ( ) ; assert not null ( group2 ) ; assert equals ( group2 , eg . get job vertex ( v5 . get id ( ) ) . get slot sharing group ( ) ) ; assert equals ( 2 , group1 . get job vertex ids ( ) . size ( ) ) ;
int [ ] size = launcher . get workspace ( ) . estimate item size ( m add info , false , true ) ; device profile dp = launcher . get device profile ( ) ; int icon size = dp . icon size px ; int padding = launcher . get resources ( ) . get dimension pixel size ( r . dimen . widget _ preview _ shortcut _ padding ) ; preview bounds . left + = padding ;
final ostorage operation result < oraw buffer > loaded = odatabase record thread local . instance ( ) . get ( ) . get storage ( ) . get underlying ( ) . read record ( rid , null , true , false , null ) ; if ( loaded = = null | | loaded . get result ( ) = = null ) return null ;
r . run ( ) ;
null types . set field float null ( 42 . 42 f ) ;
url config dir = solr client base test . class . get class loader ( ) . get resource ( solr _ config ) ;
gui manager = new vrgui manager ( null ) ;
throw update wrapper . naming ctx bad bindingtype ( ) ; } }
return new downsampler ( span iterator ( ) , interval _ ms , downsampler ) ; } else {
list < stream expression > stream expressions = factory . get expression operands representing types ( expression , expressible . class , tuple stream . class ) ; if ( 1 = stream expressions . size ( ) ) { throw new ioexception ( string . format ( locale . root , invalid expression % s - expecting a single stream but found % d , expression , stream expressions . size ( ) ) ) ; } tuple stream stream = factory . construct stream ( stream expressions . get ( 0 ) ) ; init ( stream , factory ) ; }
return ( _ node - other . _ node ) ;
return predicate ;
assert equals ( c added = > a , b , < empty string > , c in range , 4 , num hits ) ;
buffered reader reader = new buffered reader ( new input stream reader ( stream ) ) ;
param1 = 1 ;
start game ( ) ;
int label = 0 ; double maxf = double . negative _ infinity ; for ( int i = 0 ; i < svms . size ( ) ; i + + ) { lasvm svm = svms . get ( i ) ; if ( svm . platt = = null ) { throw new unsupported operation exception ( platt scaling was not trained yet . please call svm . train platt scaling ( ) first . ) ; } double f = svm . predict ( x ) ; prob [ i ] = posterior ( svm , f ) ; if ( f > maxf ) { label = i ; maxf = f ; } } smile . math . math . unitize1 ( prob ) ;
relative layout . layout params params = ( relative layout . layout params ) m holder . grid view art . get layout params ( ) ;
p = r . exec ( mkdir - p + mountpoint ) ;
class < ? > loaded apk class ;
string max as string = long . to string ( ( 1 l < < 32 ) - 1 , radix ) ;
verify run ( select * from + db name + _ dupe . virtual _ view2 , empty , driver mirror ) ; verify run ( select * from + db name + _ dupe . mat _ view2 , unptn _ data , driver mirror ) ;
final list < dimension descriptor > dimension descriptors = structured reader . get dimension descriptors ( coverage name ) ;
super . wait for complete latch ( 1000 l ) ; count = get received counter ( ) - count ; } while ( count > = min rate ) ; }
response = get from server path ( public servlet - protected - ejb?continue session = true ) ;
for ( ; index < slen ; index + + ) { get a replacement for the current character . char [ ] r = escape ( s . char at ( index ) ) ; if no replacement is needed , just continue . if ( r = = null ) { continue ; } int rlen = r . length ; int chars skipped = index - last escape ; this is the size needed to add the replacement , not the full size needed by the string . we only regrow when we absolutely must , and when we do grow , grow enough to avoid excessive growing . grow . int size needed = dest index + chars skipped + rlen ; if ( dest size < size needed ) { dest size = size needed + dest _ pad _ multiplier * ( slen - index ) ; dest = grow buffer ( dest , dest index , dest size ) ; } if we have skipped any characters , we need to copy them now . if ( chars skipped > 0 ) { s . get chars ( last escape , index , dest , dest index ) ; dest index + = chars skipped ; } copy the replacement string into the dest buffer as needed . if ( rlen > 0 ) { system . arraycopy ( r , 0 , dest , dest index , rlen ) ; dest index + = rlen ; } last escape = index + 1 ; }
if ( ok ) { host = addr . get host address ( ) ; return host ; }
request . request ( ) . routing ( state . meta data ( ) . resolve index routing ( request . request ( ) . parent ( ) , request . request ( ) . routing ( ) , request . request ( ) . index ( ) ) ) ;
cp . clean ( ) ; cp = null ; }
private cell util . write row key excluding common ( cell , r len , common prefix , out ) ; private cell util . write qualifier ( out , cell , cell . get qualifier length ( ) ) ; } else {
final simple principal collection principals = new simple principal collection ( username , username ) ; final authorization info authorization info = okta realm . do get authorization info ( principals ) ; system . out . println ( roles : + authorization info . get roles ( ) ) ; system . out . println ( permissions : + authorization info . get string permissions ( ) ) ; }
path file path = new path ( dir ) ; assert . assert false ( direct dfs . exists ( file path ) ) ; assert . assert false ( dfs . exists ( file path ) ) ; direct dfs . mkdirs ( file path ) ; assert . assert true ( direct dfs . exists ( file path ) ) ; assert . assert true ( dfs . exists ( file path ) ) ;
draw offset y + = draw height ;
check mbean permission ( instance , null , name , unregister mbean ) ; if ( instance instanceof mbean registration ) pre deregister invoke ( ( mbean registration ) instance ) ;
if ( cstmt . get int ( 9 ) = 1 ) { string err msg = wrong behavior during registration of sub cluster + sub cluster id + into the state store ; federation state store utils . log and throw store exception ( log , err msg ) ; } log . info ( registered the sub cluster + sub cluster id + into the state store ) ;
for ( entry < string , stream evaluator > param : evaluator params . entry set ( ) ) { expression . add parameter ( new stream expression named parameter ( param . get key ( ) , param . get value ( ) . to expression ( factory ) ) ) ; }
try { filter . validate options ( options ) ; } catch ( final illegal argument exception e ) { assert true ( e . get message ( ) . contains ( accumulo store constants . schema ) ) ; }
raft messages . append entries . response response = append entries response ( ) . failure ( ) . append index ( 0 ) . match index ( - 1 ) . term ( 4 ) . from ( instance2 ) . build ( ) ; outcome outcome = leader . handle ( response , state , mock ( log . class ) ) ;
zip entry entry ; while ( ( entry = zis . get next entry ( ) ) = null ) {
byte [ ] cloned table name3 = bytes . to bytes ( clonedtb3 - + system . current time millis ( ) ) ;
thread . sleep ( 3000 ) ; string ticket subject updated = ticket subject + and updated . ; input = new ticket ( ) ; input . set id ( answer . get id ( ) ) ; input . set subject ( ticket subject updated ) ; answer = request body ( direct : updateticket , input ) ; assert . assert not equals ( answer . get created at ( ) , answer . get updated at ( ) ) ; assert . assert equals ( ticket subject updated , answer . get subject ( ) ) ; assert . assert equals ( ticket description , answer . get description ( ) ) ;
try { return new string ( out buff , 0 , e , preferred _ encoding ) ;
word . set char at ( j + 1 , ' e ' ) ; word . set length ( j + 2 ) ; k = j + 1 ; dict entry entry = word in dict ( ) ;
assert false ( jmx utils . is mbean ( jmx interface . class ) ) ;
return new string ( chars , beg , cur - beg ) ;
if ( is date time type ( rh type ) ) {
if ( new pkg . get type declarations ( ) = null ) { add type declarations for ( type declaration type : new pkg . get type declarations ( ) . values ( ) ) { @ todo should we allow overrides? only if the class is not in use . if ( pkg . get type declarations ( ) . contains key ( type . get type name ( ) ) ) { add to package list of type declarations pkg . add type declaration ( type ) ; } } } for ( final org . kie . api . definition . rule . rule new rule : new pkg . get rules ( ) ) { pkg . add rule ( ( ( rule impl ) new rule ) ) ; }
get context ( ) . job queue ( ) . add job ( cur . get on fail ( ) ) ;
last exception = cpe ;
for ( int i = 0 ; i < num data codewords ; i + + ) { codeword bytes [ i ] = ( byte ) codewords ints [ i ] ; } }
this . c14n method = canonicalization method elem ; this . construction element . append child ( c14n method ) ; xmlutils . add return to element ( this . construction element ) ; this . signature algorithm = new signature algorithm ( signature method elem , null ) ;
key value [ ] kvs = new key value [ ] { key value test util . create ( r1 , cf _ str , a , 1 , key value . type . put , dont - care ) , key value test util . create ( r1 , cf _ str , a , 2 , key value . type . put , dont - care ) , key value test util . create ( r1 , cf _ str , a , 3 , key value . type . put , dont - care ) , key value test util . create ( r1 , cf _ str , a , 4 , key value . type . put , dont - care ) , key value test util . create ( r1 , cf _ str , a , 5 , key value . type . put , dont - care ) , } ;
invocable member body builder body builder = new invocable member body builder ( ) ; final string item names = string utils . uncapitalize ( details info . field name ) ;
assert that ( foo . get instance count ( ) ) . is equal to ( 1 ) ; application context ctx = spring client factory . get context ( testspec ) ; assert that ( foo . get instance count ( ) ) . is equal to ( 1 ) ; foo foo = ctx . get bean ( foo , foo . class ) ;
int node index = create node ( node . notation _ node ) ;
top docs hits = searcher . search ( bq builder . build ( ) , 10 ) ; assert equals ( 2 , hits . total hits ) ; assert equals ( 0 , searcher . doc ( hits . score docs [ 0 ] . doc ) . get ( id ) ) ; assert equals ( 1 , searcher . doc ( hits . score docs [ 1 ] . doc ) . get ( id ) ) ; final list < feature > features = make field value features ( new int [ ] { 0 , 1 , 2 } , final - score ) ;
return feelfn result . of error ( new invalid parameters event ( severity . error , n , cannot be null ) ) ;
watermark new watermark = user function . get current watermark ( ) ; if ( new watermark = null & & new watermark . get timestamp ( ) > current watermark ) { current watermark = new watermark . get timestamp ( ) ; emit watermark output . emit watermark ( new watermark ) ; } long now = get processing time service ( ) . get current processing time ( ) ;
linked list < operator < ? > > new ops = new linked list < > ( result ) ;
image layout . set color model ( src . get rendered image ( ) . get color model ( ) ) ; image layout . set sample model ( src . get rendered image ( ) . get sample model ( ) ) ;
assert equals ( 292278993 - 12 - 31 t00 : 00 : 00 . 000 z , new date time ( 292278993 , 12 , 31 , 0 , 0 ) . to string ( ) ) ;
do imminent session timeout ( basic context ) ; do test basic ( context _ path _ login + uri _ protected , no _ credentials , use _ cookies , http servlet response . sc _ unauthorized ) ; }
internal cache entry < string , string > cache2 entry = cache2 . get advanced cache ( ) . get data container ( ) . get ( key ) ; assert true ( should have cleaned up , cache2 entry = = null | | cache2 entry . get value ( ) = = null ) ;
air media config air media configuration = fb client . get air media manager ( ) . get config ( ) ;
case character . titlecase _ letter :
config . set instance name ( instance name ) ; } else if ( config . get instance name ( ) = = null ) {
string update string = ;
string out = e . get in ( ) . get body ( string . class ) ;
m header diff total = math . min ( math . max ( m header diff total + diff , m min header translation ) , 0 ) ; m footer diff total = math . min ( math . max ( m footer diff total + diff , - m min footer translation ) , 0 ) ; } m header . set translation y ( m header diff total ) ; m footer . set translation y ( - m footer diff total ) ; default : break ; } }
if ( node data = null ) { node state context . set data ( node data ) ; node state context . get persistence cache ( ) . get main cache ( ) . process node mapping ( ( node ) node state context ) ; this node is fresh and hence not dirty node state context . set dirty ( false ) ; one time set as required for rollback . object original = ( ( node ) node state context ) . clone ( ) ; ( ( node ) node state context ) . set original node ( ( node ) original ) ; }
file dir = new file ( target routetofile ) ; assert true ( should be directory , dir . is directory ( ) ) ; file file = dir . list files ( ) [ 0 ] ; assert true ( file should exists , file . exists ( ) ) ; string body = ioconverter . to string ( file , null ) ; assert equals ( hello world , body ) ; }
this . partition pages . add ( mem source . next segment ( ) ) ;
target graph target graph = target graph factory . new instance ( gen header builder . build ( ) , gen source builder . build ( ) , dep builder . build ( ) , cxx binary builder . build ( ) ) ;
if ( property . is annotation present ( column . class ) | | property . is annotation present ( columns . class ) ) { throw new annotation exception ( @ column ( s ) not allowed on a @ any property : + binder helper . get path ( property holder , inferred data ) ) ; } cascade hibernate cascade = property . get annotation ( cascade . class ) ;
long four weeks ago = ( system . current time millis ( ) 1000 ) - ( 4 * 3600 * 24 * 7 ) ;
future < void > std out termination future = thread _ pool . submit ( std out ) ; future < void > std err termination future = thread _ pool . submit ( std err ) ; boolean timed out = false ;
final qname element = f leaf list [ out index ] . get element ( ) ;
assert dir ( , fs1 , fs2 ) ; path file = new path ( file . dat ) ; byte [ ] file data1 = write random file ( fs1 , file , 0x1234 , file _ size ) ; byte [ ] file data2 = write random file ( fs2 , file , 0xabcd , file _ size ) ; assert file ( fs1 , file , file _ size , file data1 ) ; assert file ( fs2 , file , file _ size , file data2 ) ; } finally {
processor . post batch mutate ( this ) ;
service reg . unregister ( ) ;
string sql ; for ( string tb : new string [ ] { r1 , p1 } ) { id , wage , dept , tm client . call procedure ( tb + . insert , 1 , 5 , 1 , 2013 - 06 - 18 02 : 00 : 00 . 123457 ) ; client . call procedure ( tb + . insert , 2 , 10 , 1 , 2013 - 07 - 18 10 : 40 : 01 . 123457 ) ; client . call procedure ( tb + . insert , 3 , 10 , 2 , 2013 - 08 - 18 02 : 00 : 00 . 123457 ) ; the simplest case that repros a lingering npe bug found just before release of universal support for subqueries on replicated tables involved grouping by a scalar subquery and specifically calculating an average on a partitioned parent table column - - the bug was in the feature interaction with the code that considers pushing down avg calculations to the partitions . sql = select ( select id from r2 where dept = 7 ) c0 , avg ( wage ) + from + tb + t1 + group by c0 ; ; validate table of longs ( client , sql , new long [ ] [ ] { { long . min _ value , 8 } } ) ; }
for ( long l = 0 ; l < num to skip ; + + l ) { reader . advance ( ) ; }
execute ( insert into % s ( k , smallintval ) values ( ? , from json ( ? ) ) , 0 , \ 32767 \ ) ;
namespaces = new hash set < > ( ) ; table cfs = new hash map < > ( ) ; namespaces . add ( ns1 ) ; when ( peer . get namespaces ( ) ) . then return ( namespaces ) ; table cfs . put ( table name . value of ( foo ) , lists . new array list ( a , c ) ) ; when ( peer . get table cfs ( ) ) . then return ( table cfs ) ; user entry = create entry ( null , a , b , c ) ; filter = new chain walentry filter ( new namespace table cf walentry filter ( peer ) ) ; assert equals ( create entry ( null , a , c ) , filter . filter ( user entry ) ) ; namespaces = new hash set < > ( ) ;
s _ logger . info ( agent is determined to be up and running ) ; agent status transit to ( host , status . event . ping , _ node id ) ; return false ;
oauth . fill login form ( test - user @ localhost , password ) ; app page . assert current ( ) ; events . expect login ( ) . detail ( details . username , test - user @ localhost ) . assert event ( ) ;
assert equals ( sink input should be range partitioned . , partitioning property . range _ partitioned , sink . get input ( ) . get global properties ( ) . get partitioning ( ) ) ; assert equals ( sink input should be range partitioned on 1 , new ordering ( 1 , null , order . ascending ) , sink . get input ( ) . get global properties ( ) . get partitioning ordering ( ) ) ; single input plan node partitioner = ( single input plan node ) sink . get input ( ) . get source ( ) ; assert true ( partitioner . get driver strategy ( ) = = driver strategy . unary _ no _ op ) ;
this . reload context tree ( ) ;
captcha captcha = generate random captcha ( account , user ) ;
final long milliseconds = ( ( number ) rex literal . value ( rex node ) ) . long value ( ) ; return druid expression . from expression ( druid expression . number literal ( milliseconds ) ) ; } else if ( sql type family . interval _ year _ month = = sql type name . get family ( ) ) {
final segment publish result result2 = coordinator . announce historical segments ( immutable set . of ( default segment2 ) , new object metadata ( immutable map . of ( foo , bar ) ) , new object metadata ( immutable map . of ( foo , baz ) ) ) ; assert . assert equals ( new segment publish result ( immutable set . of ( default segment2 ) , true ) , result2 ) ; assert . assert array equals ( mapper . write value as string ( default segment2 ) . get bytes ( utf - 8 ) , derby connector . lookup ( derby connector rule . metadata tables config supplier ( ) . get ( ) . get segments table ( ) , id , payload , default segment2 . get identifier ( ) ) ) ;
if ( w param = = windows keycodes . vk _ menu | | w param = = windows keycodes . vk _ f10 ) { handle key button ( w param , l param , millis ) ; return 0 l ; }
if ( indexing metadata = = null | | indexing metadata . is indexed ( ) ) { value wrapper . set message descriptor ( message descriptor ) ; try { protobuf parser . instance . parse ( new indexing tag handler ( message descriptor , document ) , message descriptor , bytes ) ; } catch ( ioexception e ) { throw new cache exception ( e ) ; } }
allowed value range node = new node ( elem _ name ) ; }
to assign . remove ( hri ) ; if ( rit = null ) { assignment manager . delete node and offline region ( hri ) ; }
main . conf . set source encoding ( line . get option value ( ' c ' , main . conf . get source encoding ( ) ) ) ; main . is script file = line . has option ( ' e ' ) ; main . debug = line . has option ( ' d ' ) ; main . conf . set debug ( main . debug ) ;
int idx = ( int ) ( x > > > ( 64 - sp ) ) ;
try { return ( boolean ) meta . get class ( ) . get method ( supports ref cursors ) . invoke ( meta ) ; } catch ( no such method exception e ) { log . trace ( jdbc database meta data class does not define supports ref cursors method . . . ) ; } catch ( exception e ) { log . debug ( unexpected error trying to gauge level of jdbc ref _ cursor support : + e . get message ( ) ) ; } return false ;
long mem = api dbutils . get memory or cpu capacityby host ( host . get id ( ) , capacity . capacity _ type _ memory ) ; long cpu = api dbutils . get memory or cpu capacityby host ( host . get id ( ) , capacity . capacity _ type _ cpu ) ; host response . set memory allocated ( mem ) ; host response . set memory total ( host . get total memory ( ) ) ; string host tags = api dbutils . get host tags ( host . get id ( ) ) ; host response . set host tags ( host tags ) ;
hfile scanner scanner = reader . get scanner ( false , false , false ) ;
c . set max buffered docs ( test util . next int ( r , 2 , 15 ) ) ; } else {
if ( old transport a1 = null & & proxy . is proxy class ( old transport a1 . get class ( ) ) ) old transport a1 . stop ( ) ;
if ( null = m scale drag detector ) { boolean was scaling = m scale drag detector . is scaling ( ) ; boolean was dragging = m scale drag detector . is dragging ( ) ; handled = m scale drag detector . on touch event ( ev ) ; boolean didnt scale = was scaling & & m scale drag detector . is scaling ( ) ; boolean didnt drag = was dragging & & m scale drag detector . is dragging ( ) ; m block parent intercept = didnt scale & & didnt drag ; }
eset [ i ] = eset [ e size - 1 ] ; e size - - ; new oldest entry = math . min ( this entry , new oldest entry ) ; } else if ( this entry < oldest entry + want to remove ) { entry in bottom group?
https split = new example split ( connector id , schema name , table name , uri . create ( https : example . com : 8443 example ) ) ;
msg len = msg . s _ get message length ( m _ buf [ 1 ] , is extended ) ;
object result = free stack . remove ( free stack . size ( ) - 1 ) ; return result ;
this . is acknowledged = true ;
return select * from ( select inner2 _ . * , rownumber ( ) over ( order by order of inner2 _ ) as rownumber _ from ( + sql + fetch first + limit + rows only ) as inner2 _ ) as inner1 _ where rownumber _ > + offset + order by rownumber _ ;
string package name = intent . get component ( ) . get package name ( ) ; if ( intent . get package ( ) = null ) { intent with pkg = intent . to uri ( 0 ) ; intent without pkg = new intent ( intent ) . set package ( null ) . to uri ( 0 ) ; } else { intent with pkg = new intent ( intent ) . set package ( package name ) . to uri ( 0 ) ; intent without pkg = intent . to uri ( 0 ) ; } } else {
final double [ ] requested resolution = compute requested resolution ( scaling , subset , native res x , native res y ) ;
expression expr = javascript compiler . compile ( sqrt ( _ score ) + ln ( popularity ) ) ;
if ( is security enabled ( ) & & timeline v1 service enabled ) { add timeline delegation token ( app context . get amcontainer spec ( ) ) ; }
input stream input stream = conn . get input stream ( ) ;
string stormroot = storm config . supervisor _ stormdist _ root ( conf , storm id ) ; file utils . copy directory ( new file ( master code dir ) , new file ( stormroot ) ) ;
for ( protoclass protoclass : protoclasses ) { if ( protoclass . kind ( ) . is enclosing ( ) ) { value type type = compose value ( protoclass ) ; enclosing types . put ( protoclass . declaring type ( ) . get ( ) , type ) ; } }
a = this . a ;
for ( kie package kpkg : kbase . get kie packages ( ) ) { if ( kpkg . get name ( ) . equals ( org . drools . compiler . test ) ) { test = kpkg ; } else if ( kpkg . get name ( ) . equals ( org . drools . compiler . test2 ) ) { test2 = kpkg ; } } assert not null ( test ) ;
if ( val . get name ( ) . equals ( test element . gui _ class ) & & val . get name ( ) . equals ( test element . test _ class ) ) { val = transform . transform value ( val ) ; log . debug ( replacement result : { } , val ) ; }
evict entry ( ce . key ) ; num removed + + ;
package lookup value package lookup value = lookup package ( package identifier . create ( @ local , path fragment . empty _ fragment ) ) ;
raid node . do raid ( conf , info rs , stat , new raid node . statistics ( ) , reporter . null ) ;
annotation aware order comparator . sort ( this . handler adapters ) ;
to = length + to ; }
try { db . parse ( new file ( _ ) ) ; fail ( expected ioexception was not thrown ) ; } catch ( ioexception ioe ) { expected } catch ( saxexception sax ) { fail ( unexpected saxexception + sax . to string ( ) ) ; }
if ( version matcher . find ( ) ) { return 0 ; } return integer . parse int ( version matcher . group ( 1 ) ) ;
final string field name = contents ; final string [ ] random terms = new string [ ] { bumble , honey , solitary } ; final string random query = compose choose one word query xml ( field name , random terms ) ; final string apache lucene solr = < apache lucene solr field name = ' + field name + ' > ;
filtered shortcuts . add ( shortcut ) ;
try { if ( bean instanceof ireconfiguration aware ) { ( ( ireconfiguration aware ) bean ) . before reconfiguration ( ) ; hello } } catch ( exception e ) { logger . error ( error calling before reconfiguration on + bean name , e ) ; }
if ( builder length > 0 & & builder . char at ( builder length - 1 ) = = ' ' ) { builder . delete ( builder length - 1 , builder length ) ; builder length - - ; } for ( int i = 0 ; i < builder length - 1 ; i + + ) { if ( builder . char at ( i ) = = ' ' & & builder . char at ( i + 1 ) = = ' \ n ' ) { builder . delete ( i , i + 1 ) ; builder length - - ; } }
break ; case chargeback : try { invoice api . record chargeback reversal ( payment control context . get payment id ( ) , payment control context . get transaction external key ( ) , internal context ) ; } catch ( final invoice api exception e ) { log . warn ( on failure call failed for attempt id = ' { } ' , transaction type = ' { } ' , payment control context . get attempt payment id ( ) , transaction type , e ) ; } break ; default : throw new illegal state exception ( unexpected transaction type + transaction type ) ;
process watermark1 ( new watermark ( current input watermark ) ) ;
int total hosts = 0 ;
list < ? extends annotation tree > annotations = method tree . get modifiers ( ) . get annotations ( ) ;
member referenced member = methodref constant . referenced member ;
list < generic detail > found values = new array list < generic detail > ( ) ;
inc num allocated containers ( allocation . get allocation locality type ( ) , allocation . get request locality type ( ) ) ; if ( log . is debug enabled ( ) ) { log . debug ( allocate : application attempt id = + container id . get application attempt id ( ) + container = + container id + host = + rm container . get allocated node ( ) . get host ( ) + type = + allocation . get allocation locality type ( ) ) ; }
if ( port exists ( port name ) ) { logger . warn ( port { } does not exists according to the system , we will still try to open it , port name ) ; }
final class < ? > a class = access controller . do privileged ( reflection helper . class for name pa ( javax . naming . initial context ) ) ; final object initial context = a class . new instance ( ) ; final method lookup method = a class . get method ( lookup , string . class ) ; return ( executor service ) lookup method . invoke ( initial context , java : comp default managed executor service ) ; } catch ( exception e ) {
login token token = object mapper . read value ( response content , login token . class ) ;
final int buf size = 10 ;
int add count = 0 ; for ( entry < row test objects , integer > test row entry : small table key hash map . entry set ( ) ) { test row entry . set value ( add count + + ) ; } generate variation data ( this , test desc , small table random ) ; }
execution config conf = new execution config ( ) ;
} break ; case ' s ' : short short value = orb stream . read _ short ( ) ; if ( field . get field ( ) = null ) { bridge . put short ( o , field . get field id ( ) , short value ) ;
assert that ( create map ( hash map . class , 0 ) , is ( instance of ( hash map . class ) ) ) ; assert that ( create map ( hash map . class , string . class , 0 ) , is ( instance of ( hash map . class ) ) ) ; }
if ( debug ) { system . out . println ( firing post _ change delta [ + thread . current thread ( ) + ] : ) ; non - nls - 1 non - nls - 2 system . out . println ( delta to notify = = null ? < none > : delta to notify . to string ( ) ) ; non - nls - 1 }
tfloat vector vec _ 4 = vec . clone ( ) ;
wrapper . set read timeout ( keep alive timeout ) ; } if ( fill ( false ) ) {
assert that ( result ) . contains entry ( a , a ) . contains entry ( b , b ) ; }
if ( append header ( pdu headers . status ) = pdu _ compose _ success ) { return pdu _ compose _ content _ error ; }
count = under test . count users with global permission excluding group member ( db session , organization . get uuid ( ) , a _ permission , group1 . get id ( ) , missing _ id ) ; assert that ( count ) . is equal to ( 3 ) ; count = under test . count users with global permission excluding group member ( db session , organization . get uuid ( ) , a _ permission , missing _ id , u2 . get id ( ) ) ; assert that ( count ) . is equal to ( 3 ) ;
final java symbol name method name = show _ inline _ method _ name ; final relation info extended info = controller metadata . get last details info ( ) ;
assert plan has no index scans ( select * from r4 + where a in ( select a from r1 ) ; ) ;
for ( fi ca scheduler node node : nodes ) { cs . allocate containers to node ( node . get node id ( ) , false ) ; } try { thread . sleep ( cs . get async schedule interval ( ) ) ; } catch ( interrupted exception e ) { }
box . set center ( new vector3f ( 0 , 0 , - 1f - fast math . zero _ tolerance ) ) ; check collision ( box , geom , 0 ) ;
prev . get id ( ) . set end key ( entry . get value ( ) . get id ( ) . get end key ( ) ) ;
return super . is content always empty ( msg ) | | msg . headers ( ) . contains ( rtsp header names . content _ length ) ;
if ( cache ) { principal principal = request . get user principal ( ) ; if ( principal = = null ) { session session = request . get session internal ( false ) ; if ( session = null ) { principal = session . get principal ( ) ; if ( principal = null ) { if ( log . is debug enabled ( ) ) { log . debug ( we have cached auth type + session . get auth type ( ) + for principal + principal ) ; } request . set auth type ( session . get auth type ( ) ) ; request . set user principal ( principal ) ; } } } } boolean auth required = is continuation required ( request ) ;
datagram socket the socket = new datagram socket ( ( socket address ) null ) ; try { the socket . bind ( new my socket address ( ) ) ; fail ( no exception when binding using unsupported socket address subclass ) ; } catch ( illegal argument exception expected ) { } the socket . close ( ) ; }
keyframes . remove ( last keyframe ) ;
hash position = ( hash position + 1 ) & mask ;
if ( idx = 0 ) { buffer . append ( ' ' ) ; } string num = integer . to hex string ( 0xff & bytes [ idx ] ) ;
admin . delete snapshot ( snapshot name ) ;
assert true ( agent - based allocation should have failed , plan . get all reservations ( ) . size ( ) = = num jobs in scenario ) ;
file system . set faulty delete ( new file ( cache dir , a . 0 ) , false ) ;
application id app id = new application id . builder ( ) . tenant ( tenant a ) . application name ( foo ) . build ( ) ; assert that ( controller . get handler ( ) . get super model ( ) . application models ( ) . get ( tenant a ) . get ( app id ) . get generation ( ) , is ( 4l ) ) ; gen = counter . increment ( ) ; controller . reload config ( tenant a , create app ( tenant a , bar , 2l , 3 ) ) ; assert that ( controller . get handler ( ) . get generation ( ) , is ( gen ) ) ; }
return does not contain just spans ( text ) ;
assert equals ( resource . new instance ( 8 * gb , 1 ) , queue . get amresource limit ( ) ) ;
assert buffer result equals ( types , get buffer result ( buffer , supplier , 6 , size of pages ( 10 ) , no _ wait ) , empty results ( task _ instance _ id , 6 , true ) ) ; assert buffer info ( buffer , 0 , 6 ) ; assert equals ( supplier . get buffered pages ( ) , 0 ) ;
string can path = null ;
assert equals ( filter cache did not grow correctly , 3 , post fc in - pre fc in ) ;
period holder = update period info ( period holder , period index ) ; } else {
h2statement . execute ( sql queries . get ( aggregation index ) ) ; result set h2 result set = h2statement . get result set ( ) ; map < string , string > expected values = new hash map < > ( ) ; int h2 num groups ; for ( h2 num groups = 0 ; h2 result set . next ( ) & & h2 num groups < max _ num _ rows _ to _ compare ; h2 num groups + + ) { if ( pinot num group keys = 0 ) { string builder group key = new string builder ( ) ; for ( int group key index = 1 ; group key index < = pinot num group keys ; group key index + + ) {
assert true ( docs only2 . freq ( ) = = 1 | | docs only2 . freq ( ) = = 2 ) ; assert equals ( doc id set iterator . no _ more _ docs , docs only2 . next doc ( ) ) ;
value type left value type = node util . get known value type ( left ) ;
clone = ( delegate api request < t > ) super . clone ( ) ;
job conf _ . set output key class ( id resolver . get output key class ( ) ) ; job conf _ . set output value class ( id resolver . get output value class ( ) ) ; }
method method = queue capacities . class . get declared method ( method name , string . class ) ;
assert equals ( instance . hash code ( ) , instance . hash code ( ) , hashcode must be consinstent between calls ) ; whitebox . set internal state ( instance , field . get name ( ) , non default value ) ; }
com idocument doc = word app . get documents ( ) . add ( ) ; helper . sleep ( 5 ) ;
mesh temp = get mesh ( ) ; mesh = null ; super . write ( ex ) ;
database . update ( song play count columns . name , values , where _ id _ equals , new string [ ] { string id } ) ;
object key = h . value ( ) . handle weird number value ( this , target class , value , msg ) ;
git material git = u . wf ( new git material ( git ) , f ) ; schedule test util . added pipeline p1 = u . save config with ( p1 , u . m ( git ) ) ; schedule test util . added pipeline p2 = u . save config with ( p2 , u . m ( git ) ) ; schedule test util . added pipeline p3 = u . save config with ( p3 , u . m ( p1 ) , u . m ( p2 ) , u . m ( git ) ) ; schedule test util . added pipeline p4 = u . save config with ( p4 , u . m ( p1 ) , u . m ( p2 ) , u . m ( p3 ) ) ; int i = 0 ;
fail ( unable to find + test resource1 ) ;
if ( current captchas . size ( ) > max _ saved _ captchas ) current captchas . remove ( 0 ) ;
serial message serial message = command class . set interval ( ( integer ) converter . convert from command to value ( item , command ) ) ; if ( serial message = = null ) { logger . warn ( node { } : generating message failed for command class = { } , endpoint = { } , node . get node id ( ) , command class . get command class ( ) . get label ( ) , endpoint id ) ; return ; } this . get controller ( ) . send data ( serial message ) ;
fetcher . emit record ( 30 l , part1 , 4 l ) ;
long smallest new update = math . abs ( our updates . get ( our updates . size ( ) - 1 ) ) ;
dispatch ( e ) ;
{ dimension map alert group h2 = new dimension map ( ) ; alert group h2 . put ( group _ by _ dimension _ name _ 2 , h2 ) ; grouped anomaly results dto grouped anomaly = grouped anomalies . get ( alert group h2 ) ; list < merged anomaly result dto > anomaly group = grouped anomaly . get anomaly results ( ) ; assert . assert equals ( anomaly group . size ( ) , 2 ) ; set < merged anomaly result dto > actual group anomaly = new hash set < > ( ) ; actual group anomaly . add all ( anomaly group ) ; assert . assert equals ( actual group anomaly , expected group h2 ) ; }
workspace icon = image utilities . image2 icon ( image utilities . load image ( org gephi desktop banner workspace resources workspace . png ) ) ; tab data model = new default tab data model ( ) ; winsys info for tabbed container ws = new winsys info for tabbed container ( ) { @ override public object get orientation ( component cmpnt ) { return tab displayer . orientation _ center ; } @ override public boolean in maximized mode ( component cmpnt ) { return false ; } @ override public boolean is top component maximization enabled ( ) { return false ; } } ;
find view by id ( android . r . id . content ) ; if ( saved instance state = = null ) { intent intent = get intent ( ) ; long movie id = intent . get long extra ( extra _ movie _ id , - 1 ) ; simple movie simple movie = intent . get parcelable extra ( extra _ simple _ movie ) ; movie movie = intent . get parcelable extra ( extra _ movie ) ; fragment utils . add ( movie fragment . new instance ( movie id , simple movie , movie ) , this , android . r . id . content ) ; }
if ( holder = = null ) { holder = params . get balancer strategy ( ) . find new segment home replicator ( segment , holders ) ; } if ( holder = = null ) { log . warn ( no available [ % s ] servers or node capacity to assign segment [ % s ] expected replicants [ % d ] , tier , segment . get identifier ( ) , target replicants in tier ) ; return num assigned ; }
running job r job = utils for tests . run job fail ( job , in dir , out dir ) ;
if ( ( uri . get scheme ( ) . equals ignore case ( http ) | | uri . get scheme ( ) . equals ( https ) ) ) continue ; x509 certificate cert = null ;
writekv ( ncol a , model . _ output . _ loss func . length ) ;
modifiers field . set int ( methods field , methods field . get modifiers ( ) & modifier . final ) ;
if ( r _ r2 ( ) ) { return false ; }
if ( context & & url string encoded . char at ( 0 ) = = ' ' & & uri util . has scheme ( url string encoded ) ) { url string encoded . insert ( 0 , request . get context ( ) . get encoded path ( ) ) ; }
iterator < monkey > iterator2 = zoo . get monkeys ( ) . iterator ( ) ; assert . assert equals ( monkey1 . get name ( ) , iterator2 . next ( ) . get name ( ) ) ; assert . assert null ( iterator2 . next ( ) . get name ( ) ) ; session . get transaction ( ) . commit ( ) ; session . clear ( ) ;
if ( previous block = null ) { previous block . successor = label ; }
for ( int i = 0 ; i < hex strings . size ( ) ; i + + ) { convert to bytes ( hex strings . get ( i ) , ip byte array , i < < 1 ) ; }
if ( entity . get content encoding ( ) = null & & response . contains header ( http . content _ encoding ) ) { response . add header ( entity . get content encoding ( ) ) ; }
power mockito . mock static ( kinesis config util . class ) ;
simple name resolved variable name = resolve linked variable name with proposals ( rewrite , loop over type . get name ( ) , loop variable name . get identifier ( ) , false ) ;
among _ var = find _ among _ b ( a _ 8 , 2 ) ;
string mask = conf . get ( permssion conf key ) ; if ( mask = = null ) return fs permission . get default ( ) ;
m last reorder scroll time = 0 ; m reorder state = reorder _ scroll _ none ; m last reorder x = start x ; m in reorder mode = true ;
entitlement = ( default entitlement ) entitlement api . get entitlement for id ( base entitlement . get id ( ) , call context ) ;
java symbol name relation field = item . get left ( ) . field metadata . get field name ( ) ; java symbol name related entity identifier = item . get right ( ) . get current indentifier field ( ) . get field name ( ) ; body builder . append formal line ( % 1 s . get % 2 s ( ) . set % 3 s ( % 4 s . get % 2 s ( ) . get % 3 s ( ) ) ; , entity item name , relation field . get symbol name capitalised first letter ( ) , related entity identifier . get symbol name capitalised first letter ( ) , stored name ) ; }
node < t > new root = new node < t > ( null , max key size , max children size ) ;
_ encoding from = from ;
s . get transaction ( ) . commit ( ) ;
enm = ht . elements ( ) ;
final float look x = 0 . 0f ; final float look y = 0 . 0f ; final float look z = 1 . 0f ;
linked list < process info > p info queue = new linked list < process info > ( ) ; p info queue . add all ( me . get children ( ) ) ; while ( p info queue . is empty ( ) ) { process info p info = p info queue . remove ( ) ; if ( process tree . contains key ( p info . get pid ( ) ) ) { process tree . put ( p info . get pid ( ) , p info ) ; } p info queue . add all ( p info . get children ( ) ) ; }
w . set block state ( pos , state . state ) ;
add element value ( new annotation element value ) ;
assert equals ( - expunge failed , 0 , shell . run ( new string [ ] { - expunge } ) ) ; val = shell . run ( args ) ;
while ( tick ( ) ) { * do nothing * } performance monitor . end activity ( ) ;
job . set max map attempts ( 1 ) ;
return external bus count . get ( ) = = 10 ; } } ) ; }
m presenter . update user name ( user name ) ;
path meta change file2 = new path ( no change dir , meta change file2 ) ;
if ( mime type = = null ) { throw new null pointer exception ( mime type ) ; }
int save = i ; for ( int radial count = 0 ; radial count < radial samples ; radial count + + , i + + ) { float radial fraction = radial count * inverse radial ; in [ 0 , 1 ) temp normal . set ( cos [ radial count ] , sin [ radial count ] , 0 . 0f ) ; if ( v normals = null ) { v normal = v normals [ radial count ] ; } else if ( radius = = radius2 ) { v normal = temp normal ; } if ( top bottom = = 0 ) { if ( inverted ) nb . put ( v normal . x ) . put ( v normal . y ) . put ( v normal . z ) ; else nb . put ( - v normal . x ) . put ( - v normal . y ) . put ( - v normal . z ) ; } else { nb . put ( 0 ) . put ( 0 ) . put ( top bottom * ( inverted ? - 1 : 1 ) ) ; } temp normal . mult local ( ( radius - radius2 ) * axis fraction + radius2 ) . add local ( slice center ) ; pb . put ( temp normal . x ) . put ( temp normal . y ) . put ( temp normal . z ) ; tb . put ( ( inverted ? 1 - radial fraction : radial fraction ) ) . put ( axis fraction texture ) ; } buffer utils . copy internal vector3 ( pb , save , i ) ;
kvp . remove ( width ) ;
method dir . set writable ( true ) ;
dynamic pruning event desc dped = ( dynamic pruning event desc ) event op . get conf ( ) ;
sync complete fail = new sync complete ( desc , src , dest , false , collections . empty list ( ) ) ; test repair message write ( service . sync complete . bin , success , fail ) ; }
item . add item to bag ( item element ) ; assert false ( hibernate . is initialized ( item . get bag of items ( ) ) ) ; s . persist ( item element ) ; s . flush ( ) ;
if ( mem block info = = null ) {
intent intent = new intent ( ) ;
list < hdfs file status with id > acid schema files = new array list < hdfs file status with id > ( ) ; if ( base files . is empty ( ) ) { split strategy = determine split strategy ( combined ctx , context , fs , dir , acid schema files , false , parsed deltas , reader types , ugi , allow synthetic file ids , is default fs ) ; if ( split strategy = null ) { split strategies . add ( split strategy ) ; } return split strategies ; return here } list < hdfs file status with id > original schema files = new array list < hdfs file status with id > ( ) ;
throw new sqlexception ( validation query failed , enable log validation errors for more details . ) ;
final list < type parameter > type parameters = clazz . get type parameters ( ) ;
stage stage = stage mother . custom ( pipeline name , stage name , new job instances ( ) ) ;
u = header + 10 + 2 * interfaces . length ;
optimization subpanel = new jpanel ( layout ) ; optimization subpanel . set border ( border factory . create titled border ( etched border , msg ( optimization prefix ) ) ) ; optimizations panel . add ( optimization subpanel , panel constraints ) ; last optimization prefix = optimization prefix ;
assert wildcard query equals ( term * , term * ) ;
check conf reload ( client , conf path + solr config . default _ conf _ file , config , config ) ;
if ( message . direct = = emmessage . direct . receive ) { voice icon view . set image resource ( r . anim . voice _ from _ icon ) ; } else { voice icon view . set image resource ( r . anim . voice _ to _ icon ) ; }
tomcat . get connector ( ) . set enable lookups ( true ) ; ajp client . set remote host ( value ) ; break ; case request - remote - addr : ajp client . set remote addr ( value ) ; break ; case request - server - name : ajp client . set server name ( value ) ; break ; case request - server - port : ajp client . set server port ( integer . parse int ( value ) ) ; break ; case request - is - secure : ajp client . set ssl ( boolean . parse boolean ( value ) ) ; break ; case request - local - addr : saved request info . put ( name , value ) ; break ; case request - remote - port : saved request info . put ( name , value ) ; break ; case request - remote - user : case request - route : case request - secret :
pulse transaction coordinator ( ) ; try { internal clear ( ) ; } catch ( runtime exception e ) { throw exception converter . convert ( e ) ; }
byte buffer version and bytes = make catalog and deployment bytes ( 0 , gen id , catalog bytes , catalog hash , deployment bytes ) ; zk . create ( volt zk . catalogbytes , version and bytes . array ( ) , ids . open _ acl _ unsafe , create mode . persistent ) ;
m encoding = m encoding . trim ( ) . to lower case ( ) ; if ( i < content type . length ( ) - 1 ) {
assert decimal function ( decimal ' 99999999999999999999999999999999999999 ' decimal ' 11111111111111111111111111111111111111 ' , decimal ( 00000000000000000000000000000000000009 ) ) ;
writer . write ( < ? ) ; writer . write ( target ) ; if ( data . length ( ) > 0 & & character . is space char ( data . char at ( 0 ) ) ) writer . write ( ' ' ) ;
instructions . add ( reil helpers . create bsh ( base offset + + , dw , tmp var1 , dw , string . value of ( - 31 l ) , bt , n ) ) ;
0x02 , 0x01 , 0x05 ,
throw new runtime exception ( yikes found two keys which match exactly . ) ;
super . on stream error ( ctx , cause , stream exception ) ; return ; }
this . execution semaphore override = execution semaphore ;
navigation . navigate to ( main view . class , this . get class ( ) , offer book view . class ) ;
stream expression expression = new stream expression ( factory . get function name ( this . get class ( ) ) ) ; if ( include streams ) { stream if ( stream instanceof expressible ) { expression . add parameter ( ( ( expressible ) stream ) . to expression ( factory ) ) ; } else { throw new ioexception ( this record count stream contains a non - expressible tuple stream - it cannot be converted to an expression ) ; } } else { expression . add parameter ( < stream > ) ; }
issuing distribution point deltaidp = null ; try { deltaidp = issuing distribution point . get instance ( cert path validator utilities . get extension value ( delta crl , issuing _ distribution _ point ) ) ; } catch ( exception e ) { throw new annotated exception ( issuing distribution point extension from delta crl could not be decoded . , e ) ; } boolean match = false ;
assert equals ( 0 , ( stats . get checkout queue length histogram ( ) . get average ( ) ) , 0 ) ;
block node block node = constr mth . get basic blocks ( ) . get ( 0 ) ;
return new awtfsfont ( fnt ) ;
assert equals ( info1 , dfs2 . rolling upgrade ( rolling upgrade action . query ) ) ; dfs2 . mkdirs ( baz ) ; log . info ( restart cluster 2 ) ;
system . set property ( org . xml . sax . driver , tests . api . org . xml . sax . support . no instance xmlreader ) ; try { xmlreader factory . create xmlreader ( ) ; } catch ( saxexception e ) { expected }
return new orient graph query iterable < edge > ( false , labels ) ;
logger . warning ( the number of run history record statistics failedï¼ ) ; } @ override public void on success ( integer result ) {
header symlink tree symlink tree = cxx preprocessables . create header symlink tree build rule ( target , filesystem , root , links , header mode . symlink _ tree _ only ) ;
profiler . on post job execution ( post job event ( post job , false ) ) ;
session . get transaction ( ) . begin ( ) ; box box1 = new box ( 1 ) ; item item1 = new item ( 1 , 1 , box1 ) ; item item2 = new item ( 2 , 22 , box1 ) ; item item3 = new item ( 3 , 2 , box1 ) ; session . persist ( box1 ) ; session . persist ( item1 ) ; session . persist ( item2 ) ; session . persist ( item3 ) ; session . flush ( ) ; session . refresh ( item1 ) ; session . refresh ( item2 ) ; session . refresh ( item3 ) ; session . get transaction ( ) . commit ( ) ; session . clear ( ) ;
string segment name = offline segment zkmetadata . get segment name ( ) ;
string db schema ver = get meta store schema version ( ) ;
log . info ( output . get trace ( ) ) ;
used parameters = - 1 l ; clazz . constant pool entry accept ( bootstrap method info . u2method handle index , this ) ;
string s = unsupported byte order for iso - 10646 - ucs - 4 encoding . ; throw new unsupported charset exception ( s ) ; } }
new index = math . min ( new index , arguments . length ) ; system . arraycopy ( arguments , 0 , new curried params , 0 , new index ) ; system . arraycopy ( curried params , 0 , new curried params , new index , curried params . length ) ; if ( arguments . length - new index > 0 ) system . arraycopy ( arguments , new index , new curried params , curried params . length + new index , arguments . length - new index ) ; return new curried params ; }
text area . create popup menu ( null ) ;
map < string , object > parameters = urisupport . parse parameters ( u ) ;
scheduler . schedule job ( job detail , trigger ) ;
customer . set name ( steve ) ; session = open session ( ) ; session . begin transaction ( ) ; session . update ( customer ) ; session . flush ( ) ; session . refresh ( customer ) ; assert equals ( name not updated , steve , customer . get name ( ) ) ; session . get transaction ( ) . commit ( ) ; session . close ( ) ;
read test data ( test _ data , 0 , 7 , 2 , 0 , 2 , false ) ; }
session future . get ( ) ;
remove messages ( ) ;
msb & = ( 0xf l < < 12 ) ; msb | = ( ( long ) version ) < < 12 ;
if ( cnt - tailoff ( ) < 32 ) { object [ ] new tail = new object [ tail . length + 1 ] ; system . arraycopy ( tail , 0 , new tail , 0 , tail . length ) ; new tail [ tail . length ] = val ; return new persistent vector ( meta ( ) , cnt + 1 , shift , root , new tail ) ; }
builder . put ( path _ column _ name , optional . empty ( ) ) ; if ( table . get storage ( ) . get bucket property ( ) . is present ( ) ) { builder . put ( bucket _ column _ name , optional . empty ( ) ) ; } map < string , optional < string > > column comment = builder . build ( ) ; return handle - > new column metadata ( handle . get name ( ) , type manager . get type ( handle . get type signature ( ) ) , column comment . get ( handle . get name ( ) ) . or else ( null ) , column extra info ( handle . is partition key ( ) ) , handle . is hidden ( ) ) ;
oos . write int ( certs . length ) ;
if ( membership type = = membership type . dn ) { for ( string membership : memberships ) { if ( ldapconstants . empty _ member _ attribute _ value . equals ( membership ) ) { memberships . remove ( membership ) ; break ; } } } string membership = get member value of child object ( ldap child , membership type , member child attr name ) ;
positioned byte range buf2 = new simple positioned mutable byte range ( 7 ) ;
assert equals ( 1 , r . leaves ( ) . size ( ) ) ; assert equals ( 11 , r . num docs ( ) ) ; r . close ( ) ; writer = new index writer ( dir , new index writer config ( new mock analyzer ( random ( ) ) ) . set index deletion policy ( policy ) . set index commit ( last commit ) ) ;
switch ( random ( ) . next int ( 3 ) ) { case 0 : current types = new string [ 0 ] ; no types break ; default : current types = new string [ ] { doc } ; break ; } random types = get random types ( ) ;
if ( realm = = null ) continue ; challenge challenge = new challenge ( scheme , realm ) ;
key . reset ( type ) ;
limiter . acquire ( compressed . readable bytes ( ) ) ; ( ( byte buf data output stream plus ) out ) . write to channel ( compressed ) ;
return attr0 _ before _ attr1 ;
if ( this . projection policy = = projection policy . force _ declared ) { final hints crs hints = new hints ( hints . default _ coordinate _ reference _ system , this . get crs ( ) ) ; if ( hints = null ) hints . put all ( crs hints ) ; else hints = crs hints ; } return catalog . get resource pool ( ) . get grid coverage reader ( this , native coverage name , hints ) ;
for ( method method : get primitives ( token to replace . contains ( utf ) ) ) { list result to match = new array list ( ) ; int portable count = 0 ; try { portable count = result . portables . length ; } catch ( null pointer exception ignored ) { } for ( int i = 0 ; i < portable count ; i + + ) { primitive portable portable = ( primitive portable ) result . portables [ i ] ; result to match . add ( portable . get primitive ( method ) ) ; } if ( result = = null | | result . portables = = null | | result . portables . length = = 0 ) { result to match = null ; } scenarios . add all ( as list ( scenario ( input , result to match , get array method for ( method ) , path . replace ( token to replace , method . field ) , parent ) , scenario ( input , result to match , generic , path . replace ( token to replace , method . field ) , parent ) ) ) ; }
i _ p2 = cursor ; } while ( false ) ; cursor = v _ 8 ; return true ; }
super . set content view ( layout res id ) ;
if ( options . get options ( blaze command event handler . options . class ) . running in emacs ) { request . set running in emacs ( ) ; } return request ;
setter setter = pip . property descriptor . get setter ( true ) ;
xdr _ int ( 128 ) ; metric _ id = metadata _ msg
assert same ( root criteria , root criteria . add ( restrictions . eq ( ol . article id , 3000 ) ) ) ; list orders = root criteria . list ( ) ;
nameservice id = nonfederation ; } nameservice id list + = nameservice id ; } }
if ( internal subset = null ) { ( ( document type impl ) doc type ) . set internal subset ( internal subset ) ; }
if ( get repeat mode ( ) = = common . repeat _ song ) { get media player2 ( ) . set looping ( true ) ; }
if ( this . session = = null & & create ) { this . session = new mock http session ( this . servlet context ) ; }
int regions count = in . read int ( ) ;
get mock endpoint ( mock : start ) . expected minimum message count ( 5 ) ;
cache service . instance . counter cache . load saved ( ) ; assert equals ( 4 , cache service . instance . counter cache . size ( ) ) ; clustering c1 = cbuilder . create ( cfs . metadata ( ) . comparator ) . add ( byte buffer util . bytes ( 1 ) ) . build ( ) ;
if ( nums [ i ] + nums [ left ] + nums [ right ] < target ) {
new reader = class name . equals ( reader class name ) ; reader class name = class name ; }
final json node index mappings = get mapping ( test index name ) ; final json node mapping = index mappings . path ( test index name ) . path ( mappings ) . path ( index mapping . type _ message ) ; assert that ( mapping . path ( _ source ) . path ( enabled ) ) . is equal to ( boolean node . get false ( ) ) ; assert that ( mapping . path ( properties ) . path ( source ) . path ( type ) ) . is equal to ( new text node ( text ) ) ; } finally {
string broadcast time path = filename utils . concat ( output path , filename _ broadcast _ create ) ;
nic vo domr default nic = _ nic dao . find by network id and type ( default nic . get network id ( ) , virtual machine . type . domain router ) ; return domr default nic . get ip4 address ( ) ;
if ( ( l . status & label . pushed ) = = 0 ) {
this . manager . ignore bytestream request once ( initiation . get session id ( ) ) ; stanza stream initiation = initiate incoming stream ( connection ( ) , initiation ) ; return negotiate incoming stream ( stream initiation ) ;
for ( int offset = 0 ; offset < code length ; offset + + ) { check if the replacement instruction , if any , has a different length than the original instruction . instruction replacement instruction = replacements [ offset ] ; if ( replacement instruction = null & & replacement instruction . length ( offset ) = instruction factory . create ( code , offset ) . length ( offset ) ) { return false ; } } return true ;
return new cstate change ( m _ factory . create node hover state ( node , event ) , false ) ;
{ onehour , 0 . 25 , 4 , null , null } , { onehour , 0 . 25 , 4 , twohourperiod , null } ,
this . image asset = image asset ; this . image asset . add update listener ( this ) ; this . transition type = transition type ; updated ( ) ; }
load balancer = load balancer type . create load balancer ( route context ) ;
namespaces ns = new namespaces ( c , http : acme . com cheese ) ; from ( direct : start ) . filter ( ) . xpath ( c : person [ @ name = ' james ' ] , ns ) . to ( mock : result ) ;
test utilities . validate current record ( error : location query validation failed , cursor , test values ) ;
l . m array . add ( m array . get ( i ) . clone ( ) ) ; }
if ( use finder ) { terrain quad quad = neighbour finder . get left quad ( this ) ; return quad ; } }
resource dws = workspaces . get ( default . xml ) ; workspace info default workspace = null ; if ( resources . exists ( dws ) ) { try { default workspace = depersist ( xp , dws , workspace info . class ) ; logger . info ( loaded default workspace + default workspace . get name ( ) ) ; } catch ( exception e ) { logger . log ( level . warning , failed to load default workspace , e ) ; } } else { logger . warning ( no default workspace was found . ) ; } list < resource > workspace list = workspaces . list ( ) . parallel stream ( ) . filter ( r - > resources . directory filter . instance . accept ( r ) ) . collect ( collectors . to list ( ) ) ;
test split ( lecÈia Ã®ncepe la pag . urmÄtoare Èi are trei pagini . ) ;
config = camel context . get service call configuration ( configuration ref ) ;
if ( this . buffer reader . has next ( ) ) { this . buffer reader = null ; } return event ; }
if ( log . is debug enabled ( ) ) { log . debug ( [ { } ] [ { } - > { } ] replicator was already running , topic name , local cluster , remote cluster ) ; }
key field name = rb . rsp . get return fields ( ) . get field renames ( ) . get ( key field name ) ;
interpreter mock1 interpreter = interpreter factory . get interpreter ( user1 , note1 , mock1 ) ; editor = interpreter setting manager . get editor setting ( mock1 interpreter , user1 , note1 , mock1 ) ; assert equals ( text , editor . get ( language ) ) ; }
test counter ( get enum counters ( keys with resource ) ) ;
return vstack . empty ( ) ;
recorder . record api ( get method descriptor ( ) , args [ 0 ] , 0 ) ; }
named node map attributes = display name element . get attributes ( ) ;
client . send get request ( index . html , deadline . time left ( ) ) ; http test client . simple http response response = client . get next response ( deadline . time left ( ) ) ; assert equals ( http response status . ok , response . get status ( ) ) ; assert equals ( response . get type ( ) , mime types . get mime type for extension ( html ) ) ; assert equals ( expected index , response . get content ( ) ) ;
int response id = 10 ;
if ( relative clip region . equals ( noclipping ) ) { current state . clipping active = applied state . clipping active = false ; } else { current state . clip x = applied state . clip x = relative clip region . x ; current state . clip y = applied state . clip y = relative clip region . y ; current state . clip w = applied state . clip w = relative clip region . width ; current state . clip h = applied state . clip h = relative clip region . height ; } current state . alpha = applied state . alpha = get alpha ( ) ;
data stream < string > text = env . socket text stream ( host name , port ) ;
msg printer . print filtering patterns ( ) ; filter patterns ( ) ;
if ( buffer . remaining ( ) > = ssl _ record _ header _ length ) { return get encrypted packet length ( buffer ) ; }
uri request uri = r . path ( ws ) . path ( v1 ) . path ( applicationhistory ) . path ( containers ) . path ( container id1 . to string ( ) ) . path ( logs ) . query param ( user . name , user ) . query param ( yarn web service params . nm _ id , nm _ id ) . get uri ( ) ;
string message = extras . get string ( message ) ; string title = extras . get string ( title ) ; string content available = extras . get string ( content _ available ) ; string force start = extras . get string ( force _ start ) ; int badge count = extract badge count ( extras ) ; if ( badge count > = 0 ) { log . d ( log _ tag , count = [ + badge count + ] ) ; push plugin . set application icon badge number ( context , badge count ) ; } log . d ( log _ tag , message = [ + message + ] ) ;
queue . adjust capacity ( 99 , 25 , 1 , 1000 ) ;
assert jq ( req ( fl , id , q , id : \ { id } \ , id , 1 , expand macros , false ) , response docs = = [ ] ) ;
if ( get interested types ( ) = null ) { for ( class < ? > c : get interested types ( ) ) { if ( c . is annotation ( ) ) { find and add the classes that implement or extend the class . but not including the class itself add inherited types ( class map , ( set < string > ) class map . get ( c . get name ( ) ) ) ; } } }
list < string > packages = new array list < string > ( ) ; for ( string s : package names ) { if ( name . equals ( s ) ) { packages . add ( s ) ; } }
social user . remove federated identity ( facebook1 ) ;
system . arraycopy ( current . key buffer , old row length with size , current . key buffer , current . row length with size , current . family length with size ) ;
set < metric type > set = aws sdk metrics . get predefined metrics ( ) ; assert not null ( set ) ; assert true ( set . size ( ) > 0 ) ;
if ( e . get message ( ) . equals ( stream closed ) ) { client cache . invalidate dfs input stream ( user name , nfs3 utils . get file id path ( handle ) , namenode id ) ; continue ; } else { throw e ; }
return admin service . get ( rule id ) ; } ) . filter ( rule - > rule = null ) . sorted ( ( rule a , rule b ) - > long . compare ( rule a . get priority ( ) , rule b . get priority ( ) ) ) . collect ( collectors . to list ( ) ) ; }
if ( logger . is debug enabled ( ) ) { logger . debug ( rmi service [ + get service ( ) + ] is an rmi invoker ) ; }
typed value typed value = new typed value ( ) ; m graph view . get context ( ) . get theme ( ) . resolve attribute ( android . r . attr . text appearance small , typed value , true ) ; int color1 ;
this . span processor pipeline . set query config handler ( this . span query config handler ) ; this . span processor pipeline . add ( new wildcard query node processor ( ) ) ; this . span processor pipeline . add ( new spans validator query node processor ( ) ) ;
throw oexception . wrap exception ( new orient graph modification exception ( error on add edge in non tx environment ) , e ) ; } }
for ( frame f : frames ) { generator . generate whole frame ( f , complete buf ) ; } buffer util . flip to flush ( complete buf , 0 ) ;
list < object > replacement content = new array list < object > ( ) ; create content control ( null , replacement content , key ) ; fr . get parent ( ) . get content ( ) . add all ( begin , replacement content ) ;
value value after = simple partial evaluator . get variables after ( instruction offset ) . get value ( variable index ) ;
sonos zone player the player = sonos zone player cache . get by id ( provider . get sonos id ( item name , a command ) ) ;
tv . set application context ( wac ) ; tv . set application context ( wac ) ; map < string , object > model = new hash map < > ( ) ;
assert pixel ( image , 10 , 14 + height _ hint 2 , new color ( 64 , 64 , 192 ) ) ; assert pixel ( image , 10 , 28 + height _ hint + height _ hint 2 , new color ( 170 , 170 , 170 ) ) ; assert pixel ( image , 10 , 42 + 3 * height _ hint + height _ hint 2 , new color ( 192 , 160 , 0 ) ) ; legend options . put ( force titles , off ) ; req . set legend options ( legend options ) ; image = this . legend producer . build legend graphic ( req ) ; assert equals ( height _ hint * 6 , image . get height ( ) ) ;
conf . set int ( dfs . name . edits . dir . minimum . nonlocal , 1 ) ; string [ ] name dirs = get edit dirs ( 2 ) ;
tkn . type = tt _ token ; tkn . is ready = true ; break ; } else if ( c = = ' \ \ ' & & strategy . get unicode escape interpretation ( ) & & in . look ahead ( ) = = ' u ' ) {
ops . add ( op tasks none ) ; ops . add ( op disallowed ) ; try { privileged operation executor . squash cgroup operations ( ops ) ; assert . fail ( expected squash operation to fail with an exception ) ; } catch ( privileged operation exception e ) { log . info ( caught expected exception : + e ) ; }
application attempt id att id1 = create scheduling request ( 1024 , queue1 , user1 ) ;
if ( download file vm = null ) { download file vm . detach ( ) ; download file vm = null ; } if ( upload file vm = null ) { upload file vm . detach ( ) ; upload file vm = null ; } need rebind = true ;
f xml11 components = new array list < > ( ) ;
conf2 . set ( dfs . secondary . http . address , name _ node _ http _ host + 0 ) ;
if ( _ key . equals ignore case ( read geometry key ) ) { continue ; } final object value = val . get value ( ) ; parameters . add ( new default parameter descriptor ( _ key , value . get class ( ) , null , value ) . create value ( ) ) ;
for ( int i = 0 ; i < 200 ; i + + ) { index ( source _ collection , get doc ( id , integer . to string ( i ) ) ) ; will perform a commit for every document }
register ore ( dye , new item stack ( items . dye , 1 , wildcard _ value ) ) ; register ore ( paper , new item stack ( items . paper ) ) ;
local buffer pool . set num buffers ( num buffers 2 ) ;
if ( db connection = null ) close ( db connection ) ;
return hosts in order . get ( 0 ) ;
case 2 : warn i + + ; * continue with next case . * case 3 : warn break ; case 4 : i + + ;
final string path = fi . get file location ( ) ; final boolean is heuristic = fi . is found by heuristics ( ) ; final boolean pragma once context = f current context . is pragma once ( ) ; ifile nomination nomination delegate = null ;
located blocks blks = fs . get client ( ) . get located blocks ( test path . to string ( ) , 0 l ) ; assert equals ( 1 , blks . get located blocks ( ) . size ( ) ) ; for ( located block blk : blks . get located blocks ( ) ) { assert equals ( file contents . length , blk . get block size ( ) ) ; }
fin . close ( ) ; return ret ;
named node map entities = f document type . get entities ( ) ; f current entity decl = ( entity impl ) entities . get named item ( name ) ; if ( f current entity decl = null ) { f current entity decl . set input encoding ( encoding ) ; } }
assert . assert that ( executions . get max ( ) , matchers . less than ( 500 l ) ) ;
for ( setter getter stub stub : setter getter stubs ) { string stub source = generate setter getter stub ( stub ) ; if ( stub source . equals ( ) ) stub . unused = true ; p ( stub source ) ; }
int wait loop = 0 ; for ( ; wait loop < 5 ; wait loop + + ) { if ( i1 . our proposal or task finished & & bools synchronized ( m _ boolean state machines for group1 ) ) { break ; } thread . sleep ( 500 ) ; } assert true ( wait loop = = 5 ) ; assert true ( bools synchronized ( m _ boolean state machines for group1 ) ) ;
a = a * 255 . 0f ; r = ( float ) math . pow ( r , 1 . 0 2 . 2 ) * 255 . 0f ; g = ( float ) math . pow ( g , 1 . 0 2 . 2 ) * 255 . 0f ; b = ( float ) math . pow ( b , 1 . 0 2 . 2 ) * 255 . 0f ; return math . round ( a ) < < 24 | math . round ( r ) < < 16 | math . round ( g ) < < 8 | math . round ( b ) ;
if ( ( location . get scheme ( ) . compare to ( journal type . file . name ( ) . to lower case ( ) ) = 0 ) ) { non - file locations are all remote - we might want to add more checks in the future mount point is set to the uri scheme return new nnstorage location ( location , location . get scheme ( ) . to lower case ( ) , is shared ? storage location type . shared : storage location type . remote ) ; } else { enforce existence check directory ( location . get path ( ) ) ; try { return get info unix ( location , is shared ) ; } catch ( exception e ) { flog . info ( failed to fetch information with unix based df , e ) ; } try { return get info mac os ( location , is shared ) ; } catch ( exception e ) { flog . info ( failed to fetch information with macos based df , e ) ; } throw new ioexception ( failed to run df ) ; }
return usage metrics . get ( container id ) ; }
if ( platform . is mac os ( ) ) { make sure they don ' t have a previous ' true ' setting for this preferences . set boolean ( export _ macosx , false ) ; } macosx button . set selected ( preferences . get boolean ( export _ macosx ) ) ; macosx button . add item listener ( new item listener ( ) { public void item state changed ( item event e ) { preferences . set boolean ( export _ macosx , macosx button . is selected ( ) ) ; update export button ( ) ; } } ) ; if ( platform . is mac os ( ) ) { macosx button . set enabled ( false ) ; macosx button . set tool tip text ( language . text ( export . tooltip . macosx ) ) ; } linux button . set selected ( preferences . get boolean ( export _ linux ) ) ;
for ( int k = 0 ; k < num edges ; k + = 1 ) { node neighbor = nodes [ next id ] ; next id + = 7 ; next id & = 255 ; if ( next id = = 0 ) { next id = 1 ; } neighbor . create relationship to ( hub , innie ) ; } commit ( ) ; new transaction ( ) ;
assert true ( is signed in ( ) ) ; }
set execution timeout ( - 1 ) ;
quota resetter quota resetter = new quota resetter ( admin client , store names , cluster . get node ids ( ) ) ;
data view dv = ( data view ) tester . get component from last rendered page ( form : panel : list container : items ) ; assert equals ( default _ items _ per _ page , dv . size ( ) ) ; }
executor singleton . get executor ( ) . shutdown now ( ) ; }
if ( modifier . is static ( field . modifiers ( ) ) & & value = null ) { static fields . put ( field , value ) ; }
if ( is listening _ ) return ;
first type cut = m base type to filter . get ( base type ) ; second type cut = m wild type to filter . get ( base type ) ; }
assert that ( catcher . user session . is logged in ( ) ) . is false ( ) ; assert that ( catcher . user session . has component permission ( any permission , new component dto ( ) ) ) . is true ( ) ; assert that ( catcher . user session . is system administrator ( ) ) . is true ( ) ;
zk bookie rack affinity mapping mapping2 = new zk bookie rack affinity mapping ( ) ; client configuration bk client conf2 = new client configuration ( ) ; bk client conf2 . set zk servers ( 127 . 0 . 0 . 1 + : + local _ zookeeper _ port ) ; bk client conf2 . set zk timeout ( 1000 ) ; mapping2 . set conf ( bk client conf2 ) ; list < string > racks2 = mapping2 . resolve ( lists . new array list ( bookie1 , bookie2 , bookie3 ) ) ; assert equals ( racks2 . get ( 0 ) , rack0 ) ; assert equals ( racks2 . get ( 1 ) , rack1 ) ; assert equals ( racks2 . get ( 2 ) , network topology . default _ rack ) ; local zkc . delete ( zk bookie rack affinity mapping . bookie _ info _ root _ path , - 1 ) ;
application context . refresh ( ) ;
if ( ( service instanceof protocol provider service ) ) return ; protocol provider service protocol provider = ( protocol provider service ) service ;
sentence one padded right = new byte [ 100 ] ; start = add multi byte char sentence one ( sentence one padded right , 0 ) ; sentence one padded right len = add pads ( sentence one padded right , start , 4 ) ; assert . assert true ( string expr . character count ( sentence one padded right , 0 , sentence one padded right len ) = = 10 + 4 ) ; string expr . right trim and truncate ( out v , i , sentence one padded right , 0 , sentence one padded right len , 10 ) ; expected result len = sentence one padded right len - 4 ; assert . assert true ( vector equal ( out v , i , sentence one padded right , 0 , expected result len ) ) ; i + + ;
request counter = new request counter ( tests . request counter test , 10000 , true ) ; }
sink . set mock item failures list for next bulk item responses ( collections . singleton list ( new exception ( artificial failure for record ) ) ) ;
c . get bcollection ( ) . add ( b ) ;
int prod = n channels * input height * input width ; indarray activations rnn = proc . pre process ( activations cnn , mini batch size ) ; assert array equals ( msg , new int [ ] { mini batch size , prod , time series length } , activations rnn . shape ( ) ) ;
for ( int i = 0 ; i < root cache . length ; i + + ) { if ( fst . find target arc ( 0x3040 + i , first arc , arc , fst reader ) = null ) { root cache [ i ] = new fst . arc < long > ( ) . copy from ( arc ) ; } } return root cache ;
switch ( random ( ) . next int ( 3 ) ) { case 0 : current types = new string [ 0 ] ; no types break ; default : current types = new string [ ] { doc } ; break ; } random types = get random types ( ) ;
jar output stream . put next entry ( new jar entry ( foo wine ) ) ; jar entry jar entry = new jar entry ( entry path ) ; jar output stream . put next entry ( jar entry ) ; byte [ ] buffer = new byte [ 1024 ] ; int bytes read ; while ( ( bytes read = file input stream . read ( buffer ) ) = - 1 ) { jar output stream . write ( buffer , 0 , bytes read ) ; }
encoded string value previously sent by = parse encoded string value ( pdu data stream ) ; if ( null = previously sent by ) { try { headers . set encoded string value ( previously sent by , pdu headers . previously _ sent _ by ) ; } catch ( null pointer exception e ) { log ( null pointer error ) ; } catch ( runtime exception e ) { log ( header field + is not encoded - string - value header field ) ; return null ; } } break ;
delete delete = new delete ( row ) ; delete . add family ( families [ 1 ] , ts [ 0 ] ) ; ht . delete ( delete ) ; get get = new get ( row ) ;
orecord internal . set version ( current document , doc . get version ( ) ) ;
aa aa = new aa ( ) ; aa . set name ( aa ) ; string json aa = json . to jsonstring ( aa ) ; aa aa1 = json . parse object ( json aa , aa . class ) ; assert . assert equals ( aa , aa1 . get name ( ) ) ;
assert buffers equal ( new single byte buff ( expected buffer ) , actual buffer , algo , encoding , pread ) ;
operands . add ( beam sql primitive . of ( sql type name . varchar , 2 ) ) ;
decorate frames = new jcheck box ( j edit . get property ( options . appearance . decorate frames ) ) ;
file main c = new file ( root , main . c ) ;
file . get blocks ( ) [ 1 ] = new block ( blocksize , replication ) ; file . get blocks ( ) [ 2 ] = new block ( blocksize 2 , replication ) ;
if ( right - left < quicksort _ threshold ) { sort ( a , left , right , true ) ; return ; }
sleep and fail first time . ct . set ( 0 ) ;
item it = items . get value ( new resource location ( minecraft : bed ) ) ;
if ( get result type ( ) . is blocking ( ) ) { throw new illegal state exception ( tried to mark a non - blocking result partition as finished ) ; } final int ref cnt = total result . decrement number of running producers and get remaining ( ) ;
listener . on failure ( e ) ;
final host and port parsed host = host and port . from string ( specifier ) ;
string builder extended info = new string builder ( ) ; for ( int i = 2 ; i < tokens . length - 1 ; i + + ) { if ( i > 2 ) { extended info . append ( : ) ; } extended info . append ( tokens [ i ] ) ; } string sha1 hex = tokens [ tokens . length - 1 ] ;
stack trace element [ ] call stack = module source . get stack trace ( ) ;
partially failing indexer indexer = new partially failing indexer ( foo _ type , 4 , 4 , 2 ) ; map settings settings = new map settings ( ) . set property ( sonar . search . recovery . loop limit , 5 ) ; under test = new recovery indexer ( settings . as config ( ) , indexer ) ; under test . recover ( ) ; assert that logs do not contain ( error , too many failures ) ;
string [ ] update flags = new string [ ] { common . update _ pager _ postiion } ; string [ ] flag values = new string [ ] { get current song index ( ) + } ; m app . broadcast update uicommand ( update flags , flag values ) ;
string file name = hfile . get path ( ) . get name ( ) ;
final value value1 = element converter . get value from properties ( test groups . edge , properties1 ) ; final value value2 = element converter . get value from properties ( test groups . edge , properties2 ) ; final value value3 = element converter . get value from properties ( test groups . edge , properties3 ) ; final value value4 = element converter . get value from properties ( test groups . edge _ 2 , properties1 ) ; final value value5 = element converter . get value from properties ( test groups . edge _ 2 , properties2 ) ;
preconditions . check argument ( ipc . get port ( ) > 0 , active name node must have an ipc port configured . + got address ' % s ' , ipc ) ;
verify ( mapper , 2000 - 01 - 02 t03 : 04 : 05 + 01 : 00 , judate ( 2000 , 1 , 2 , 3 , 4 , 5 , 0 , gmt + 1 ) ) ;
output stream os = get output stream ( ) ;
bpoffer service bpos1 = dn . get all bp os ( ) . get ( 0 ) ; bpos1 . trigger block report for tests ( ) ; assert equals ( wrong nn address , get nnsocket address ( bpos1 ) , nn1 . get name node address ( ) ) ; assert equals ( wrong bpid , bpos1 . get block pool id ( ) , bpid1 ) ; assert equals ( wrong cid , dn . get cluster id ( ) , cid1 ) ; cluster . shutdown ( ) ;
log . debug ( checking + event + size = + event . get suspended procedures ( ) . size ( ) ) ;
field = ( sdfield ) document . get field ( measurement ) ; assert equals ( data type . int , field . get data type ( ) ) ; assert equals ( rank type . empty , field . get rank type ( ) ) ; assert equals ( 1 , field . get attributes ( ) . size ( ) ) ;
exchange exchange2 = new default exchange ( context ) ;
return ( registered = false ) ;
nodes to ignore . clear ( ) ;
module specification . add system dependency ( new module dependency ( module loader , ejb _ api , false , false , true , false ) ) ;
instructions . add ( reil helpers . create bsh ( base offset , arch size , eax , arch size , - 8 , arch size , shifted eax to cf ) ) ; instructions . add ( reil helpers . create and ( base offset + 1 , arch size , shifted eax to cf , arch size , 1 , operand size . byte , helpers . carry _ flag ) ) ;
par = ( ( oidentifiable ) par ) . get identity ( ) ; params . put ( i , par ) ; } }
sb . append ( ' ' ) ; } else { sb . append ( ' . ' ) ; } step = c1 * step ; }
if ( result ) { long mem usage = buf . get memory usage ( ) ; release memory - simple deallocation . arenas [ arena ix ] . deallocate ( buf , true ) ; memory manager . release memory ( mem usage ) ; } else { arenas [ arena ix ] . deallocate ( buf , true ) ; no need to release memory - cache eviction . }
test constraint failed ( get constraint ( test string , boolean . true ) , 12345678901234567890 ) ;
return new h2 dialect ( ) ; }
while ( master . get server manager ( ) . are dead servers in progress ( ) ) { thread . sleep ( 5 ) ; }
string restriction name = map action restriction . get ( action ) ;
versionid = zkassign . transition node opened ( this . watcher , regioninfo , servername _ b , versionid ) ;
protocol . write i64 ( request . get id ( ) ) ; protocol . get transport ( ) . flush ( ) ;
if ( equals ( group name , found . group name ) ) { return false ; } verify joiner ( found ) ;
throw new hibernate exception ( illegally attempted to associate a proxy with two open sessions ) ; } else {
tx object . get session holder ( ) . set transaction ( hib tx ) ;
if ( ( node instanceof infix expression ) ) { return false ; } infix expression infix expression = ( infix expression ) node ; if ( infix expression . get operator ( ) = and operator ) { return false ; } int offset = is operator selected ( infix expression , context . get selection offset ( ) , context . get selection length ( ) ) ; if ( offset = = - 1 ) { return false ; }
verify headers ( workspace , dep1 exported symlink tree folder , dep1 dep1 . h , dep1 dep1 _ new . h ) ; verify headers ( workspace , dep2 exported symlink tree folder , dep2 dep2 . h , dep2 dep2 _ new . h ) ;
attribute value value = convert ( get boolean set , collections . singleton ( true ) ) ; assert . assert equals ( 1 , value . get l ( ) . size ( ) ) ; assert . assert equals ( true , value . get l ( ) . get ( 0 ) . get bool ( ) ) ; }
if ( ( descriptor instanceof fragment descriptor ) ) { holder . set class name ( servlet _ class ) ; context . get meta data ( ) . set origin ( name + . servlet . servlet - class , descriptor ) ; } break ;
final phaser root = this . root ;
event queue . add expected ( simulation start time , new job submission event ( job client , simulation start time , job story producer . get job ( 0 ) ) ) ; for ( int i = 1 ; i < job submission times . length ; i + + ) { event queue . add expected ( job submission times [ i - 1 ] - relative start time , new job submission event ( job client , job submission times [ i ] - relative start time , job story producer . get job ( i ) ) ) ; } long run until = event queue . get last check time ( ) ;
list < resolve info > search list ; final intent intent = new intent ( intent . action _ search ) ; search list = pm . query intent activities ( intent , package manager . get _ meta _ data ) ; list < resolve info > web search info list ; final intent web search intent = new intent ( intent . action _ web _ search ) ; web search info list = pm . query intent activities ( web search intent , package manager . get _ meta _ data ) ;
new arithmetic exception ( should also be thrown ) ;
if ( c . get minimum number instances ( ) > get minimum number instances ( ) ) set minimum number instances ( c . get minimum number instances ( ) ) ;
logger . trace ( es _ port is instance of { } . ignoring . . . , es _ port . get class ( ) . get name ( ) ) ;
} else { result . add ( s ) ; } }
string public key = args [ 0 ] ; string private key = args [ 1 ] ; exchange specification spec = new exchange specification ( btctrade exchange . class ) ;
real width = display . get width ( ) ;
if ( act > 0 ) {
dxhat = epsilon . mul row vector ( gamma ) ; d l dx hat = d l d out . gamma shape : [ minibatch size , n out ] }
filters = collection identifiers . stream ( ) . map ( id - > ff . equal ( ff . property ( get link foreign key ( ) ) , ff . literal ( id ) , false ) ) . collect ( collectors . to list ( ) ) ;
assert that ( compiler . get index ( object . class ) ) . is null ( ) ;
for ( string f : arrays . as list ( solr _ ti , solr _ td , solr _ dt , solr _ b ) ) { assert not null ( test harness . validate xpath ( response , get field xpath prefix ( f ) + [ @ name = ' index ' ] ) ) ; }
volt table partition _ table = create partitioned table ( num _ partitioned _ items , 0 ) ; load table ( client , replicated _ tester , true , repl _ table ) ;
home button enabled | = ( m action view . get display options ( ) & action bar . display _ home _ as _ up ) = 0 ; set home button enabled ( home button enabled ) ; set has embedded tabs ( get resources _ get boolean ( m context , r . bool . abs _ _ action _ bar _ embed _ tabs ) ) ; }
m axis . m entries = new float [ label count ] ;
final image view img icon = ( image view ) find view by id ( r . id . img icon ) ;
if ( this . no _ entry _ key = ( int ) 0 ) { arrays . fill ( _ set , this . no _ entry _ key ) ; }
em . get transaction ( ) . begin ( ) ; ccte . set component ( new component ( a , 1 ) ) ; em . persist ( ccte ) ;
test deep file creation base ( deep file creation test , deep , deep file creation , ( short ) 0644 , ( short ) 0644 ) ;
if ( shown method type = null & & new method type . get class ( ) . equals ( shown method type . get class ( ) ) ) { return ; } log . debug ( creating new panel for configuring : + new method type . get name ( ) ) ;
if ( pat idx end = str idx end ) { return false ; pattern and string do not have the same size } for ( int i = 0 ; i < = pat idx end ; i + + ) { ch = pat arr [ i ] ; if ( ch = ' ? ' ) { if ( different ( case sensitive , ch , str arr [ i ] ) ) { return false ;
for ( ; i < child count ; i + + ) { children . get ( i ) . on draw ( p glstate , p camera ) ; } }
for ( solr document doc : docs ) { string msg = p + = > + doc ; assert equals ( msg , 4 , doc . size ( ) ) ; assert true ( msg , doc . get field value ( [ docid ] ) instanceof integer ) ; assert true ( msg , doc . get field value ( [ shard ] ) instanceof string ) ; assert true ( msg , doc . get field value ( [ explain ] ) instanceof string ) ; assert true ( msg , doc . get field value ( x _ alias ) instanceof integer ) ; assert equals ( msg , 10 , doc . get field value ( x _ alias ) ) ; }
ok to continue . count down ( ) ;
new work . get map work ( ) . set left input join ( false ) ;
method best candidate method = get method with most specific parameter types ( method , potential method to invoke ) ; if ( best candidate method = null ) { potential method to invoke = best candidate method ; continue ; }
do in jpa ( this : : entity manager factory , entity manager - > { city cluj = entity manager . find ( city . class , 1 l ) ; assert equals ( 46 . 77120 , cluj . get coordinates ( ) . x ( ) , 0 . 00001 ) ; assert equals ( 23 . 62360 , cluj . get coordinates ( ) . y ( ) , 0 . 00001 ) ; } ) ;
attributes editor . add attribute ( new annotations attribute ) ;
throw new runtime exception ( hconstants . not _ implemented ) ;
return stub . get parent stub ( ) . get psi ( ) ; }
task . cancel ( true ) ; assert true ( task . is done ( ) ) ; assert true ( task . is cancelled ( ) ) ; assert equals ( 1 , task latch . get count ( ) ) ;
delete region ( conf , tbl . get table descriptor ( ) , bytes . to bytes ( a ) , bytes . to bytes ( b ) , false , true , false , false ) ;
context . register ( ( container response filter ) ( request context , response context ) - > thread context . unbind subject ( ) ) ;
in . close ( ) ;
return new product ( exp . derive ( ) , new cosine ( exp ) ) ;
rascredentials . by reference credentials = new rascredentials . by reference ( ) ;
do put ( key , payload , null ) ;
throw new missing resource exception ( could not load any resource bundles . , class name , ) ;
measure view ( m header view ) ; m header view height = m header view . get measured height ( ) ; layout params params = new layout params ( layout params . wrap _ content , m header view height ) ;
collections . sort ( asks ) ; return new order book ( null , asks , bids ) ;
final file yaml config file = new file ( conf dir file , flink _ conf _ filename ) ; if ( yaml config file . exists ( ) ) { throw new illegal configuration exception ( the flink config file ' + yaml config file + ' ( + conf dir file . get absolute path ( ) + ) does not exist . ) ; }
assert equals ( 2 , store size ( target cluster ) ) ;
return inflater . inflate ( r . layout . quote _ fragment , container , false ) ;
return - 1 * latency ;
if ( needs restart ) { if ( start virtual machine ( request . get instance id ( ) ) ) throw new ec2 service exception ( server error . internal error , create image - restarting instance + request . get instance id ( ) + failed ) ; } return response ; } catch ( exception e ) {
set < string > escaped filter types = new hash set < > ( ) ;
generic test utils . wait for ( new supplier < boolean > ( ) { @ override public boolean get ( ) { return appender . is matched ( ) ; } } , 1000 , 60000 ) ; dn . shutdown ( ) ; }
contact details = contact info op set . get details ( contact , last name detail . class ) ;
if ( event = = xmlstream constants . start _ element ) { string element name = parser . get local name ( ) ; if ( toplevel _ element _ names . contains ( element name ) ) { test case result = parse test suite ( parser , element name ) ; return result ; } }
final object result = mock ( object . class ) ; set state ( m src3 , not _ closed , finished , with _ result , result , not _ failed , null ) ; subscriber3 . on new result ( m src3 ) ; m in order . verify ( m data subscriber ) . on new result ( data source ) ; m in order . verify ( m data subscriber ) . on failure ( data source ) ; verify state ( data source , m src3 , not _ closed , finished , with _ result , result , failed , throwable ) ; test close ( data source , m src3 ) ;
do return ( true ) . when ( spied ) . upload file ( is a ( path . class ) , is a ( path . class ) ) ;
boolean same = eid = = previous eid & & previous group id = = group id ;
assert that ( auth strategy . authenticate ( user , wrong ) , equal to ( authentication result . failure ) ) ;
jdbc connection access ( ) . release connection ( connection ) ;
assert true ( listener . on expiry events . is empty ( ) ) ;
big view . remove all views ( r . id . buttons ) ;
current coords . set ( previous coords . x , previous coords . y ) ;
if ( value = 0 & & value = 1 ) { on connection failure ( error code . protocol _ error . code , invalid _ settings _ enable _ push ) ; return ; }
vectorized row batch batch = get batch three boolean cols ( ) ; is not null expr = new is not null ( 0 , 2 ) ; long column vector out col = ( long column vector ) batch . cols [ 2 ] ; expr . evaluate ( batch ) ; assert . assert equals ( 1 , out col . vector [ 0 ] ) ; assert . assert equals ( 0 , out col . vector [ 4 ] ) ; assert . assert true ( out col . no nulls ) ; assert . assert false ( out col . is repeating ) ;
remove unshared reference ( class desc , previous handle ) ;
quorum entry = get namenode registration ( report . get nameservice id ( ) , report . get namenode id ( ) ) ;
path my file = new path ( test _ dir , test mkdirs my file + file index + + ) ;
value state descriptor < byte [ ] > desc = new value state descriptor < > ( any , byte primitive array serializer . instance ) ; desc . set queryable ( vanilla ) ; value state < byte [ ] > state = backend . get partitioned state ( void namespace . instance , void namespace serializer . instance , desc ) ;
out . write double ( no _ entry _ value ) ; }
testing stream operator . number restore calls = 0 ; test harness . invoke ( env ) ;
final solr query query = new solr query ( ) ; query . set query ( * : * ) ; query . add ( fl , * , score , fv : [ fv ] ) ; query . add ( rows , 4 ) ; query . add ( fv , true ) ; query . add ( rq , { ltr model = nomatchmodel re rank docs = 4 } ) ; final solr query yes match feature query = new solr query ( ) ;
do { node . prev = pred = pred . prev ; } while ( pred . wait status > 0 ) ;
fixed date = first day of week + 7 * ( internal get ( week _ of _ month ) - 1 ) ;
return local delete tags response . get pull parser ( my _ qname ) ;
sb . append ( \ \ x ) ;
return converter . new unit ;
byte buffer buffer = byte buffer . allocate ( 1 ) ; channel buffer buf = new byte buffer backed channel buffer ( buffer ) ; channel buffer channel buffer = ( channel buffer ) decoder . decode ( mockito . mock ( channel handler context . class ) , mockito . mock ( channel . class ) , buf ) ; assert true ( channel buffer = = null ) ;
if ( path index = = record path . length - 2 ) { string desired field name = record path [ path index + 1 ] ; string current field name = parser . get field name ( ) ; if ( desired field name . equals ( current field name ) ) { process field as record ( project , parser , root column group , parameters ) ; } }
list < field node > o fields = new array list < field node > ( old class node . fields ) ;
return new rpattern ( ) { @ override public boolean is match ( final char sequence input ) { return input . length ( ) = = 1 & & contains ( b content , input . char at ( 0 ) ) = = should match ; } } ;
odistributed server log . debug ( this , local node name , node , odistributed server log . direction . out , error on sending distributed request % s . the target node is not available . active nodes : % s , e , i request , manager . get available node names ( database name ) ) ;
connector . close ( ) ;
string alias ;
net net = new net ( ) ;
if ( temp . create new file ( ) ) { throw new generic file operation failed exception ( cannot create new local work file : + temp ) ; }
murmur hash3 . state state = new murmur hash3 . state ( ) ; state . h1 = 0x9368e53c2f6af274 l ^ seed ; state . h2 = 0x586dcd208f7cd3fd l ^ seed ; state . c1 = 0x87c37b91114253d5 l ;
throw exp ;
m controller . set controller overlay ( controller overlay2 ) ; in order . verify ( drawee hierarchy1 , times ( 1 ) ) . set controller overlay ( controller overlay2 ) ; in order . verify ( drawee hierarchy1 , times ( 0 ) ) . reset ( ) ; in order . verify ( drawee hierarchy2 , times ( 0 ) ) . set controller overlay ( any ( drawable . class ) ) ; in order . verify ( drawee hierarchy2 , times ( 0 ) ) . reset ( ) ;
p = new deep learning ( ) ;
exclude file = new path ( test _ dir , exclude ) ; conf . set ( dfs . hosts . exclude , exclude file . to uri ( ) . get path ( ) ) ; conf . set int ( heartbeat . recheck . interval , 2000 ) ; conf . set int ( dfs . heartbeat . interval , 1 ) ; conf . set int ( dfs . replication . pending . timeout . sec , 4 ) ; write excludes file and refresh ( null ) ; }
return ( xpatherror resources ) resource bundle . get bundle ( class name + suffix , locale ) ;
tcp failure detector . check members ( true ) ;
assert that ( msg . get field ( msg ) ) . is equal to ( the ) ;
tree map < object , object > tm1 = new tree map < object , object > ( m1 ) ; tree map < object , object > tm2 = new tree map < object , object > ( m2 ) ; iterator < entry < object , object > > i1 = tm1 . entry set ( ) . iterator ( ) ; iterator < entry < object , object > > i2 = tm2 . entry set ( ) . iterator ( ) ; while ( i1 . has next ( ) ) { map . entry < object , object > entry1 = i1 . next ( ) ; map . entry < object , object > entry2 = i2 . next ( ) ; int c = compare ( entry1 . get value ( ) , entry2 . get value ( ) ) ; if ( c = 0 ) { return c ; } else { c = compare ( entry1 . get value ( ) , entry2 . get value ( ) ) ; if ( c = 0 ) { return c ; } } } return 0 ;
if ( bundle . get int ( bundle _ extra _ int _ version _ code , 0 ) = bundle . get int ( bundle _ extra _ int _ version _ code , 1 ) ) { return false ; } return true ;
low redundancy blocks . add ( gen block info ( thread local random . current ( ) . next long ( ) ) , 1 , 0 , 0 , 3 ) ;
connection factory . set copy message on send ( false ) ;
original . format ( % n ) ; } original . append ( node ) ; current . value written = true ; }
if ( loc = = null ) { string [ ] hints = desc . get location hints ( ) ; if ( hints = null & & hints . length > 0 ) loc = hints [ 0 ] ; } string expanded loc = xmlentity manager . expand system id ( loc , desc . get base system id ( ) , false ) ; desc . set literal system id ( loc ) ; desc . set expanded system id ( expanded loc ) ; return entity resolver . resolve entity ( desc ) ;
t . reset row position ( ) ;
response = post as servlet response ( rest workspaces gsml datastores mapped feature featuretypes , featuretype , text xml ) ; assert equals ( 201 , response . get status ( ) ) ; feature type info ft = get catalog ( ) . get feature type by name ( gsml , mapped feature ) ; assert not null ( ft ) ;
add file ( connection , readme . txt , some new content ) ; list < string > list files = connection . list files ( ls files params . create ( ) . with modified ( true ) ) ;
return chunked track blacklist util . maybe blacklist track ( track selection , track selection . index of ( chunk . track format ) , e ) ; }
if ( border . get color ( ) = null ) { if ( border . get color ( ) . equals ( auto ) ) { color = compose css ( css _ name _ _ color , 000000 ) ; } else { color = compose css ( css _ name _ _ color , + border . get color ( ) ) ; } } return val + sz + color ; }
string delete query = delete from person ;
check value ( providers , row3 , now ) ;
channel = server . accept ( ) ; output stream writer writer = new output stream writer ( channel . get output stream ( ) ) ; writer . write ( te ) ; writer . close ( ) ; channel . close ( ) ;
container failed event f3 = create fail event ( job id , 4 , h3 , false ) ; allocator . send failure ( f3 ) ; node managers [ nm num ] = register node manager ( nm num + + , rm ) ;
f document . clear identifiers ( ) ;
em . get transaction ( ) . begin ( ) ; o1 _ 1 = em . find ( list biowning1 entity . class , o1 _ 1 . get id ( ) ) ; o1 _ 2 = em . find ( list biowning1 entity . class , o1 _ 2 . get id ( ) ) ; o2 _ 1 = em . find ( list biowning2 entity . class , o2 _ 1 . get id ( ) ) ; o2 _ 2 = em . find ( list biowning2 entity . class , o2 _ 2 . get id ( ) ) ; o1 _ 2 . get references ( ) . remove ( o2 _ 1 ) ;
final drawable drawable = holder . image . get drawable ( ) ; if ( drawable = = null ) return false ; gif drawable gif = null ; if ( drawable instanceof gif drawable ) { gif = ( gif drawable ) drawable ; } else if ( drawable instanceof transition drawable ) {
localized resource localized resource = localrsrc . get ( lr ) ; tracker . handle ( resource failed event ) ; dispatcher . await ( ) ;
if ( type . is instance ( value ) ) { return ( t ) value ; } return exchange helper . convert to type ( this , type , value ) ;
boolean condition = matching opcodes ( branch instruction , pattern instruction ) & & matching branch offsets ( offset , branch instruction . branch offset , ( ( branch instruction ) pattern instruction ) . branch offset ) ;
this . executor service . start executor service ( executor type . rs _ open _ region , conf . get int ( hbase . regionserver . executor . openregion . threads , 3 ) ) ;
if ( unused progress labels . is empty ( ) ) { unassigned progress operations . add ( operation ) ; } else { attach ( operation , unused progress labels . pop ( ) ) ; } }
thread . sleep ( 150 ) ; execute service . submit ( reader1 ) ; execute service . shutdown ( ) ; try { execute service . await termination ( 10 , time unit . seconds ) ; } catch ( interrupted exception e ) { logger . error ( error waiting for executor service shutdown , e ) ; }
if ( tr . is whitespace ( ) ) { return this . increment token ( ) ; } offset att . set offset ( tr . get start pos ( ) , tr . get end pos ( ) ) ;
assert true ( entity manager appears to have been closed across txns , injector . get instance ( entity manager . class ) . contains ( entity ) ) ; assert true ( entity manager appears to have been closed across txns , em . contains ( entity ) ) ; assert true ( entity manager appears to have been closed across txns , em . is open ( ) ) ; injector . get instance ( unit of work . class ) . end ( ) ;
string filename = system . get property ( user . dir ) + out _ macro add . docm ;
int num child = 1 ;
stage ft stage = stage dao . most recent with builds ( pre condition . pipeline name , pre condition . ft stage ( ) ) ;
thread . sleep ( 500 l ) ; } catch ( interrupted exception e ) {
try { typecast required in xerces2 ; saxparser doesn ' t inheret xmlreader % opt % cast at asignment? ( ( xmlreader ) f incremental parser ) . set property ( http : xml . org sax properties lexical - handler , handler ) ; } catch ( org . xml . sax . saxnot recognized exception e ) { nothing we can do about it } catch ( org . xml . sax . saxnot supported exception e ) { nothing we can do about it }
if ( view pager skipped x * pager dx < 0f & & view pager x = = 0 ) { view pager skipped x = 0f ; }
totally covered . clear ( ) ;
throwable cause = client exceptions util . find exception ( e . get cause ( 0 ) ) ;
final string entity plural = get plural service ( ) . get plural ( entity ) ; final string entity identifier plural = get plural service ( ) . get plural ( entity metadata . get current indentifier field ( ) . get field name ( ) ) ;
key default tablet row = new key ( table id + ' < ' ) ; key start = new key ( table id ) ; key end = default tablet row . following key ( partial key . row ) ; scanner . set range ( new range ( start , end ) ) ; optional < string > location = optional . empty ( ) ;
b length - = integer . number of leading zeros ( high digit ) ; return b length ; }
api api = ( api ) f . get annotations ( ) [ 0 ] ;
field access jpaentity unexisting entity = new field access jpaentity ( ) ; unexisting entity . set id ( 8888 l ) ; result = runtime service . create process instance query ( ) . variable value equals ( entity to query , unexisting entity ) . single result ( ) ; assert null ( result ) ;
overdue config cache . load default overdue config ( resources . get resource ( overdue config . xml ) . to external form ( ) ) ;
assert true ( java _ 7 . on or after ( java _ 5 ) ) ;
assert cached ( true , 410 ) ; assert cached ( false , 411 ) ; assert cached ( false , 412 ) ; assert cached ( false , 413 ) ; assert cached ( true , 414 ) ; assert cached ( false , 415 ) ; assert cached ( false , 416 ) ; assert cached ( false , 417 ) ; assert cached ( false , 418 ) ; assert cached ( false , 500 ) ; assert cached ( true , 501 ) ;
files . clear ( ) ;
assert false ( side input handler . is ready ( view2 , first window ) ) ;
try { service . cancel sink ( this ) ; } catch ( wmiexception e ) { log . warn ( e ) ; } }
update empty view ( ) ; }
for ( map . entry < string , col meta > field entry : colum to indx . entry set ( ) ) { string col name = field entry . get key ( ) ; int result = merge col . try parse agg col ( col name ) ; if ( result = merge col . merge _ unsupport & & result = merge col . merge _ nomerge ) { merg cols . add ( new merge col ( field entry . get value ( ) , result ) ) ; } }
if ( opt . names . get ( 0 ) . starts with ( - - - ) ) { option list . add ( opt ) ; usage summary . add ( opt ) ; } return opt ; }
hystrix request context context = hystrix request context . initialize context ( ) ;
try { grant on table using access control client ( test _ util , system user connection , user name , test _ table , null , null , action . write ) ; } catch ( throwable e ) { log . error ( error during call of access control client . grant . , e ) ; }
string req = get request ( ) ; _ proxy out . write ( data helper . get utf8 ( req ) ) ; _ proxy out . flush ( ) ; }
chunk . begin = chunk . begin + downloaded ;
this . resume inflight exchanges = math . max ( resume percent of max * max inflight exchanges 100 , 1 ) ;
assert not null ( map . remove ( bpid , block ) ) ;
delete by query and get version ( id : ( 3 4 5 6 ) , params ( distrib _ update _ param , from _ leader , _ version _ , long . to string ( - ( version2 + 150 ) ) ) ) ; assert jq ( req ( qt , get , id , 3 ) , = = { ' doc ' : null } ) ; assert jq ( req ( qt , get , id , 4 , fl , id ) , = = { ' doc ' : { ' id ' : ' 4 ' } } ) ;
cpdata [ index ] = new int [ ] { dis . read short ( ) , dis . read short ( ) } ; if ( debug ) { system . out . println ( index + : methodref [ class _ index = + ( ( int [ ] ) cpdata [ index ] ) [ 0 ] + , name _ and _ type _ index = + ( ( int [ ] ) cpdata [ index ] ) [ 1 ] + ] ) ; }
rename visible terminal in client ( orig caption ) ; } } @ override public void on error ( server error error ) {
if ( ret = = mismatch ) return mismatch ;
member obfuscator . set new member name ( member , null ) ;
extra beans . for each ( abd : : add bean ) ;
set endpoint ( https : cloudhsm . us - east - 1 . amazonaws . com ) ; handler chain factory chain factory = new handler chain factory ( ) ; request handler2s . add all ( chain factory . new request handler chain ( com amazonaws services cloudhsm request . handlers ) ) ; request handler2s . add all ( chain factory . new request handler2 chain ( com amazonaws services cloudhsm request . handler2s ) ) ; request handler2s . add all ( chain factory . get global handlers ( ) ) ; }
for ( int i = to + 1 ; i < = max ; i + + ) { if ( i = = from ) continue ; set selected ( i , false ) ; } } if ( min > - 1 ) { for ( int i = min ; i < from ; i + + ) set selected ( i , false ) ; } }
string buffer new signature buffer = new string buffer ( ) ;
final boolean expect commas = session factory ( ) . get dialect ( ) . supports tuple counts ( ) ; assert equals ( expect commas , had commas ) ;
for ( throwable t : exceptions ) { if ( ( t instanceof do not retry ioexception ) ) { res = true ; } } return res ; }
set draggable ( true ) ;
assert null ( index searcher . get index reader ( ) . leaves ( ) . get ( 0 ) . reader ( ) . get point values ( some _ missing _ field ) ) ; }
if ( float . is infinite ( handler . limits . top ) ) { result . set limits ( handler . limits ) ; } return result ;
float v total = ( float ) ( target span - remaining space ) ;
assert null ( sc . get layer by name ( states layer . prefixed name ( ) ) ) ;
request = modbus request . create modbus request ( in ) ; request . set headless ( ) ;
merged payload . write ( ctx , 16 ) ;
mockito . verify ( mock nn1 ) . register datanode ( mockito . any ( datanode registration . class ) ) ;
byte buffer bb = byte buffer . allocate direct ( ( int ) size ) ;
int start = name . last index of ( ' ' ) ;
string header = response . get header ( www - authenticate ) . to string ( ) . substring ( 7 ) ;
folder background offset = - edge margin px ; folder icon size px = icon size px + 2 * - folder background offset ; folder icon preview padding = res . get dimension pixel size ( r . dimen . folder _ preview _ padding ) ; }
return o . get all struct field type infos ( ) . equals ( get all struct field type infos ( ) ) ; }
assert false ( buffer . is finished ( ) ) ; buffer . set no more pages ( ) ; assert queue state ( buffer , first , 10 , 4 ) ; assert queue state ( buffer , second , 4 , 10 ) ;
if ( collected subtypes . contains key ( named type ) ) { if so , no recursion ; however , may need to update name? if ( named type . has name ( ) ) { named type prev = collected subtypes . get ( named type ) ; if ( prev . has name ( ) ) { collected subtypes . put ( named type , named type ) ; } } return ; }
if ( supports view elevation ( ) ) { lollipop or later has native drop shadow feature . item shadow decorator is not required . } else { m recycler view . add item decoration ( new item shadow decorator ( ( nine patch drawable ) context compat . get drawable ( get context ( ) , r . drawable . material _ shadow _ z1 ) ) ) ; } m recycler view . add item decoration ( new simple list divider decorator ( context compat . get drawable ( get context ( ) , r . drawable . list _ divider _ h ) , true ) ) ;
try { namespaces . get policies ( this . test property , this . test local cluster , non - existing - namespace - 1 ) ; fail ( should have failed ) ; } catch ( rest exception e ) { assert equals ( e . get response ( ) . get status ( ) , status . not _ found . get status code ( ) ) ; } try { namespaces . get permissions ( this . test property , this . test local cluster , non - existing - namespace - 1 ) ; fail ( should have failed ) ; } catch ( rest exception e ) { assert equals ( e . get response ( ) . get status ( ) , status . not _ found . get status code ( ) ) ; }
if ( elapsed time < 1000 ) return 0 ; return ( double ) uploaded bytes 1024 * 8 ( elapsed time 1000 ) ;
int key count = 0 ;
string outkey = dist block integrity monitor . simulation _ failed _ file + , ;
set notification vibration ( extras , vibrate option , m builder ) ;
builder . add sky function ( sky functions . repository , new repository loader function ( ) ) ;
server manager server manager = mockito . mock ( server manager . class ) ;
if ( e . size = = - 1 | | e . csize = = - 1 | | e . crc = = - 1 ) e . flag = 8 ; break ;
m _ element flags . put ( ilayer , new elem desc ( 0 | elem desc . block | elem desc . blockform | elem desc . blockformfieldset ) ) ;
room user1 . revoke ownership ( fixture . user id2 ) ; room user1 col . wait for event ( 10000 ) ;
case 12 : return new non transient data access resource exception ( cause , ex ) ;
{ task manager location original = new task manager location ( resource id . generate ( ) , inet address . get by name ( 1 . 2 . 3 . 4 ) , 8888 ) ; task manager location ser copy = instantiation util . clone ( original ) ; assert equals ( original , ser copy ) ; }
transient name service name service = new transient name service ( orb , orbconstants . transient _ name _ service _ name ) ;
try { if ( cause = null ) { msg + = \ n cause : \ n + string utils . stringify exception ( cause ) ; } if ( hbase master = null ) { hbase master . report rsfatal error ( this . server name from master pov . get versioned bytes ( ) , msg ) ; } } catch ( throwable t ) { log . warn ( unable to report fatal error to master , t ) ; }
from ( direct : in - message - name ) . error handler ( dead letter channel ( mock : in - message - name - error ) ) . to ( avro : http : localhost : + avro port + put?protocol class name = org . apache . camel . avro . generated . key value protocol ) . to ( mock : result - in - message - name ) ;
border bounds . intersect ( new area ( oldclip ) ) ;
group change collector group change collector = new group change collector ( ) ; this . op set pers presence1 . add server stored group change listener ( group change collector ) ; try { remove the group this . op set pers presence1 . remove server stored contact group ( this . op set pers presence1 . get server stored contact list root ( ) . get group ( test group name2 ) ) ; } catch ( operation failed exception ex ) { logger . error ( error removing group , ex ) ; }
dfs . allow snapshot ( snap root path ) ;
connection . add ( add params . create ( singleton list ( . ) ) ) ;
u seed = u seed & 0xffffffff l ; int i l = i left ;
map < string , parameter < ? > > inputs = pf . get parameter info ( raw ) ; assert not null ( inputs ) ; assert equals ( 2 , inputs . size ( ) ) ; parameter < ? > param = check parameter ( inputs , input , raw data . class , the raw data input , 1 , 1 ) ; assert equals ( application json , text xml , param . metadata . get ( mime types ) ) ;
property types . put all ( fixed types ) ; for ( map . entry < string , annotations > entry : property annotations . entry set ( ) ) { property property = to property ( entity type , entry . get key ( ) , property types . get ( entry . get key ( ) ) , entry . get value ( ) ) ; if ( property = null ) { entity type . add property ( property ) ; } } return entity type ;
while ( required bytes > 0 ) { out . write byte ( ( byte ) value ) ; value > > = 8 ; required bytes - - ; }
assert equals ( true , config . get property ( process string escapes option . property _ name ) ) ;
client ( ) . perform request ( post , _ flush ) ;
log . trace ( ignoring start ( ) as camel is already started ) ; }
task service . set variable ( task . get id ( ) , confirmed , false ) ;
if ( verify checksum ( input ) ) { try { address . from base58 ( pivx params . get ( ) , input ) ; return new validation result ( true ) ; } catch ( address format exception e ) { return new validation result ( false , get error message ( e ) ) ; } } else { return wrong checksum ; }
kernel transactions . block new transactions ( ) ;
get out filter ( ) . add ( client . request _ context . to lower case ( ) ) ;
expanded property . add listener ( ( o , old val , new val ) - > { if ( new val ) { new timeline ( new key frame ( duration . millis ( 160 ) , new key value ( drop icon . rotate property ( ) , 90 , interpolator . ease _ both ) ) ) . play ( ) ; } else { new timeline ( new key frame ( duration . millis ( 160 ) , new key value ( drop icon . rotate property ( ) , 0 , interpolator . ease _ both ) ) ) . play ( ) ; } } ) ;
string default package = this . get class ( ) . get package ( ) . get name ( ) ;
this . ok = new jbutton ( ok ) ;
if ( i = = start pos ) { set error state ( parse error state . empty _ column ) ; mark empty column } this . result = new string ( bytes , start pos , i - start pos , get charset ( ) ) ; return i + delimiter . length ; } }
map < string , object > document properties = new hash map < string , object > ( ) ; document properties . put ( _ id , _ local doc1 ) ; document properties . put ( foo , 1 ) ; document properties . put ( bar , false ) ; tdbody body = new tdbody ( document properties ) ;
gen smbjoin work ( curr join work . get map work ( ) , new smbjoin op ) ; return curr join work ; } catch ( exception e ) {
velocity tracker . add movement ( do motion ( 0 , 0 , 0 ) ) ;
graph graph = layout . get graph ( ) ;
opp stack . remove ( 0 ) ;
do in hibernate ( this : : session factory , session - > { a it = ( a ) session . create criteria ( a . class ) . set lock mode ( this , lock mode . pessimistic _ write ) . unique result ( ) ;
this . extensions = collections . checked map ( new hash map < > ( ) , string . class , extension . class ) ; this . extensions . put all ( extensions ) ;
hash map < integer , array list < sample rec > > actual1 = dump all buckets ( db location , tbl name3 ) ; hash map < integer , array list < sample rec > > actual2 = dump all buckets ( db location2 , tbl name4 ) ; system . err . println ( \ n table 1 ) ; system . err . println ( actual1 ) ; system . err . println ( \ n table 2 ) ; system . err . println ( actual2 ) ;
assert not same ( channel1 , channel2 ) ;
if ( f running lookup job = null ) { if ( event triggered & & frame data . is identical ( f running lookup job . f frame data ) ) { identical location - we are done return ; } cancel running lookup job f running lookup job . cancel ( ) ; make sure done stepping ( ) is called even if the job never ran - bug 325394 if ( f running lookup job . f event triggered ) { . . . but not if this request is event - triggered for the same context ( duplicate suspended event ) if ( event triggered | | f running lookup job . get dmc ( ) . equals ( frame data . f dmc ) ) { done stepping ( f running lookup job . get dmc ( ) ) ; } } } f running lookup job = new lookup job ( frame data , page , event triggered ) ;
test subscriber < string > test subscriber = new test subscriber < > ( ) ; observable < string > stream = memory store core . get stream ( ) ; memory store core . put ( 100 , test value 1 ) ; stream . subscribe ( test subscriber ) ; memory store core . put ( 200 , test value 2 ) ; memory store core . put ( 300 , test value 3 ) ; test subscriber . assert not completed ( ) ;
assert equals ( inserted term , cache . get metadata ( i ) . get entry term ( ) ) ;
reflection buffer . set depth buffer ( format . depth ) ;
for ( string a pattern : test patterns alt ) { try { pattern p = pattern . compile ( a pattern ) ; } catch ( exception e ) { fail ( unexpected exception : + e ) ; } }
int [ ] indx = router util . get spec pos ( up stmt , 0 ) ;
object mapper mapper = new object mapper ( ) ; kraken server time result kraken result = mapper . read value ( is , kraken server time result . class ) ; kraken server time server time = kraken result . get result ( ) ; assert that ( server time . get unix time ( ) ) . is equal to ( 1391835876 ) ; assert that ( server time . get rfc1123 time ( ) ) . is equal to ( date utils . from rfc1123 date string ( sat , 8 feb 14 05 : 04 : 36 + 0000 , locale . us ) ) ; }
else assert equals ( 2 , stored conf . get long ( _ priority , - 1 ) ) ;
iterator < history record > recs = reader . find last ( count ) ; while ( recs . has next ( ) ) { result . add ( create file record from history record ( recs . next ( ) , c ) ) ; } }
query = select s from student cassandra s where s . height = ?1 ; q = em . create query ( query ) ; q . set parameter ( 1 , 163 . 76765654 ) ; results = q . get result list ( ) ; assert . assert not null ( results ) ; assert . assert equals ( 1 , results . size ( ) ) ; assert . assert equals ( false , results . get ( 0 ) . is exceptional ( ) ) ; assert . assert equals ( 163 . 76765654 , results . get ( 0 ) . get height ( ) ) ; em . clear ( ) ;
a = wrapped buffer ( new byte [ ] { 1 , 2 , 3 } ) . order ( order ) ; b = wrapped buffer ( wrapped buffer ( new byte [ ] { 0 , 1 , 2 , 3 , 4 } , 1 , 2 ) . order ( order ) , wrapped buffer ( new byte [ ] { 0 , 1 , 2 , 3 , 4 } , 3 , 1 ) . order ( order ) ) ; assert true ( byte buf util . equals ( a , b ) ) ; a . release ( ) ;
theme helper = theme helper . get instance loaded ( get context ( ) ) ;
for ( int i = 0 ; i < lg jobs . length ; i + + ) { storage . get full job ( lg jobs [ i ] . get id ( ) ) ; }
notification view . set view visibility ( r . id . notification _ base _ next , view . invisible ) ;
writer . print ( \ n \ t } ) ;
while ( iter . has next ( ) ) { visitor iterator . bucket progress bp = iter . get next ( ) ; assert true ( buckets . contains ( bp . get superbucket ( ) ) ) ; buckets . remove ( bp . get superbucket ( ) ) ; } assert true ( buckets . is empty ( ) ) ; }
system . arraycopy ( pi tri list in , t * 3 , tri list , 0 , 3 ) ;
expr parent . remove child ( expr ) ; expr . remove child ( assign ) ; parent . replace child ( next , assign ) ; compiler . report change to enclosing scope ( parent ) ; return true ;
assert true ( management system . await relation index status ( graph , lives by reason , lives ) . status ( schema status . registered ) . call ( ) . get succeeded ( ) ) ; m = graph . open management ( ) ;
m _ set version _ called = true ;
reflection util . set field ( fetch task , pipeline name , null ) ;
stream . seek ( pos ) ; stream . flush before ( pos ) ; }
mock volume vo volume = _ mock volume dao . find by storage path and type ( cmd . get volume path ( ) ) ; if ( volume = = null ) { return new backup snapshot answer ( cmd , false , can ' t find base volume : + cmd . get volume path ( ) , null , true ) ; } string snapshot path = cmd . get snapshot uuid ( ) ; mock volume vo snapshot = _ mock volume dao . find by storage path and type ( snapshot path ) ; if ( snapshot = = null ) { return new backup snapshot answer ( cmd , false , can ' t find snapshot + snapshot path , null , true ) ; } string sec storage url = cmd . get secondary storage url ( ) ;
extractor asserts . assert behavior ( get extractor factory ( fragmented mp4 extractor . flag _ enable _ cea608 _ track ) , mp4 sample _ fragmented _ sei . mp4 , get instrumentation ( ) ) ; }
node . set text content ( 456 ) ; system . out . println ( incident was + incident + , changed to 456 ) ; return doc ; }
if ( uri = = null ) { throw new null pointer exception ( uri is null ) ; } list < http cookie > cookies = new array list < http cookie > ( ) ;
result . set score ( normalize ( result . get score ( ) ) ) ; result . set weight ( normalize ( result . get weight ( ) ) ) ; result . set function ( spec ) ; result . set job id ( job execution id ) ; } catch ( exception e ) { log . error ( exception in saving anomaly result : + result . to string ( ) , e ) ; } }
int size2 = 0 ;
if ( element _ node = ( nodes . read entry ( node handle , 0 ) & 0x ffff ) ) return null ;
throw new org . apache . axis2 . databinding . adbexception ( instance id cannot be null ) ;
string [ ] qs = { default } ;
op delete child = op . delete ( myid , 0 ) ; multi ( zk _ chroot , arrays . as list ( delete child ) ) ; assert . assert null ( z node exists under chroot : + ch root , zk . exists ( ch root + myid , false ) ) ; assert . assert null ( z node exists under chroot : + ch root , zk _ chroot . exists ( myid , false ) ) ; }
map m = ( map ) l . get ( 1 ) ;
s = get at precision ( s ) ;
in format = new dummy file input format ( ) ;
send option ( cmd _ do , option _ suppress _ go _ ahead ) ; } peer suppressed go ahead = true ; do suppress ga requested = false ;
if ( e . get target exception ( ) instanceof runtime exception ) { throw ( runtime exception ) e . get target exception ( ) ; } else { throw new runtime exception ( e . get target exception ( ) ) ; }
lease lease = lease manager . get lease ( holder ) ;
b . get clob ( ) . get sub string ( 1 , 6 ) ;
assert false ( name . contains ( cached output stream ) ) ;
final int idx = normalised index ;
int temp = hor positions [ level [ desc order [ i ] ] ] ; boolean cant move = false ; for ( int j = 0 ; j < right nodes . length ; j + + ) { if ( hor positions [ level [ right nodes [ j ] ] ] - temp > 1 ) break ; else if ( priorities [ desc order [ i ] ] < = priorities [ right nodes [ j ] ] ) { cant move = true ; break ; } else temp = hor positions [ level [ right nodes [ j ] ] ] ; }
list < pw entry > child ent = new array list < pw entry > ( m group . child entries ) ; for ( int i = 0 ; i < child ent . size ( ) ; i + + ) { delete entry task = new delete entry ( m act , m db , child ent . get ( i ) , null , true ) ; task . run ( ) ; }
assert equals ( 0 , testing stream operator . number restore calls ) ; env . get checkpoint latch ( ) . await ( ) ;
final int ripple layer = draw ripple layer ( canvas , bounds , xfermode ) ;
fsdata input stream stm = dfs . open ( file path1 ) ; byte [ ] byte file1 = new byte [ ( int ) trg file len ] ; stm . read fully ( 0 , byte file1 ) ; stm . close ( ) ; located blocks lb1 = cluster . get name node ( ) . get block locations ( name1 , 0 , trg file len ) ;
rmstate store store = zk tester . get rmstate store ( ) ; version default version = zk tester . get current version ( ) ; store . check version ( ) ; assert equals ( store had wrong version , default version , store . load version ( ) ) ; }
for ( datanode storage info next storage : results ) { datanode descriptor next node = next storage . get datanode descriptor ( ) ; if ( next node = local machine ) { return next node ; } } return null ; }
zoo keeper watcher coordinator watcher = new zoo keeper watcher ( ) ;
executor service es = mock ( executor service . class ) ;
client config . get security config ( ) . set credentials ( new custom credentials ( ) ) ; hazelcast instance hazelcast instance = hazelcast factory . new hazelcast client ( client config ) ;
pie data data = new pie data ( data set ) ; data . set value formatter ( new percent formatter ( ) ) ; data . set value text size ( 11f ) ; data . set value text color ( color . white ) ; data . set value typeface ( m tf light ) ; m chart . set data ( data ) ; m chart . invalidate ( ) ;
current tab = get activity tab ( ) ; content view core = current tab = null ? current tab . get content view core ( ) : null ; if ( content view core = null ) { content view core . set accessibility state ( true ) ; } } }
float new major compaction jitter = hstore . get store engine ( ) . get compaction policy ( ) . get conf ( ) . get major compaction jitter ( ) + 0 . 02 f ; conf . set float ( hbase . hregion . majorcompaction . jitter , new major compaction jitter ) ; rs1 . get configuration manager ( ) . notify all observers ( conf ) ; assert equals ( new major compaction jitter , hstore . get store engine ( ) . get compaction policy ( ) . get conf ( ) . get major compaction jitter ( ) , 0 . 00001 ) ; }
return new partition iterable ( table spec . partitions ) ;
realm results < all java types > still none = none . where ( ) . greater than ( all java types . field _ long , test _ size ) . find all ( ) ; assert equals ( 0 , still none . size ( ) ) ; }
return processor . process ( exchange , callback ) ;
if ( system . get property ( property _ prefix ) = null ) { pc . set property prefix ( system . get property ( property _ prefix ) + . ) ; } if ( props = null ) { properties initial props = new properties ( ) ; initial props . put all ( props ) ; log . debug ( string . format ( added % d initial properties , props . size ( ) ) ) ; pc . set initial properties ( initial props ) ; }
new set . union ( f follow list [ leaf index ] ) ;
if ( related bitsets . contains key ( type ) ) { return related bitsets . get ( type ) ; } else { throw new runtime exception ( related types should have been computed for + type : + type + but have not been . ) ; } }
content width = ( width - space navigation height ) 2 ;
propagate failure ( answer , new exchange ) ; return answer ; }
get cluster metrics response response = client . get cluster metrics ( get cluster metrics request . new instance ( ) ) ;
assertions . assert that ( ( ( person ) k session . get object ( elizabeth ) ) . get address ( ) . get city ( ) ) . is equal to ( london ) ; k session . dispose ( ) ;
set content type ( new org . docx4j . openpackaging . contenttype . content type ( org . docx4j . openpackaging . contenttype . content types . wordprocessingml _ endnotes ) ) ;
if ( accessible . is accessible ( ) ) { set accessible flag is true accessible . set accessible ( true ) ; } return accessible ;
if ( importer = = null ) { system . err . println ( can ' t load network by using the following files : ) ; system . err . println ( prototxt : + model txt ) ; system . err . println ( caffemodel : + model bin ) ; system . err . println ( bvlc _ googlenet . caffemodel can be downloaded here : ) ; system . err . println ( http : dl . caffe . berkeleyvision . org bvlc _ googlenet . caffemodel ) ; system . exit ( - 1 ) ; }
file name = generate time stamped file name ( mime type ) ;
il . append ( method gen . load dom ( ) ) ; il . append ( new push ( cpg , _ axis ) ) ; il . append ( new push ( cpg , _ node type ) ) ; il . append ( new invokeinterface ( ty , 3 ) ) ; break ;
put ( com . facebook . orca , notification type . facebook _ messenger ) ;
final int disk count = 3 ; final int data node count = 1 ; final int data node index = 0 ; final int source disk index = 0 ; final long cap = block size * 2 l * block count ; mini dfscluster cluster = new cluster builder ( ) . set block count ( block count ) . set block size ( block size ) . set disk count ( disk count ) . set num datanodes ( data node count ) . set conf ( conf ) . set capacities ( new long [ ] { cap , cap , cap } ) . build ( ) ;
repository . delete ( 2 l ) ;
return new string ( chars , beg , cur - beg ) ;
int p = math . min ( n , m + 1 ) ;
push opp ( null ) ;
assert true ( this . encoder . can encode ( resolvable type . none , null ) ) ;
s . set so linger ( true , 0 ) ;
m app . get dbaccess helper ( ) . delete all google play music songs ( ) ;
queue = u . get path ( ) ; } else {
configuration conf = new hdfs configuration ( ) ; file system test helper fs helper = new file system test helper ( ) ;
driver . navigate ( ) . to ( http : localhost : 8081 customer - cookie - portal ) ; assert logged ( ) ; string token cookie2 = driver . manage ( ) . get cookie named ( adapter constants . keycloak _ adapter _ state _ cookie ) . get value ( ) ; assert . assert equals ( token cookie , token cookie2 ) ;
encoding = decode pem ( in stream , free _ bound _ suffix ) ;
this . cleanup ( ) ;
tick unit source tick units = get standard tick units ( ) ; tick unit unit1 = tick units . get ceiling tick unit ( get tick unit ( ) ) ; double unit1 width = exponent length to java2 d ( unit1 . get size ( ) , data area , edge ) ;
config . set property ( index precedence option . property _ name , equality ) ;
if ( sensor type = null & & sensor type . equals ignore case ( sensor event . get sensor type ( ) . name ( ) ) ) { return ; }
if ( gibberish contact . is persistent ( ) ) { first tell everyone that the volatile contact was removed fire subscription event ( gibberish contact , parent gibberish group , subscription event . subscription _ removed ) ; try { now subscribe this . subscribe ( new parent , contact to move . get address ( ) ) ; now tell everyone that we ' ve added the contact fire subscription event ( gibberish contact , new parent , subscription event . subscription _ created ) ; } catch ( exception ex ) { logger . error ( failed to move contact + gibberish contact . get address ( ) , ex ) ; } } else { ( ( contact group gibberish impl ) new parent ) . add contact ( gibberish contact ) ; fire subscription moved event ( contact to move , parent gibberish group , new parent ) ; }
object dummy = new object ( ) ;
log . info ( recover physics file over , last maped file + mapped file . get file name ( ) ) ;
table buf . append ( get row line buf ( col count , col max len list , data ) ) ;
path path = new path ( foo ) ;
session = open session ( ) ; session . begin transaction ( ) ; definition = ( definition ) session . get ( definition . class , definition . get id ( ) ) ; assert equals ( 2 , definition . get values ( ) . size ( ) ) ; for ( object o : definition . get values ( ) ) { assert equals ( 1 , ( ( value ) o ) . get localized strings ( ) . get strings copy ( ) . size ( ) ) ; } session . get transaction ( ) . commit ( ) ; session . close ( ) ; }
set < string > to remove = new hash set < > ( ) ;
append test util . check full file ( fs , p , 4 , file contents , failed to deal with thread interruptions , false ) ;
primitive object inspector float oi = primitive object inspector factory . get primitive java object inspector ( primitive category . float ) ; assert equals ( 2015 - 02 - 07 15 : 02 : 24 . 000 , gmt date format . format ( primitive object inspector utils . get timestamp ( 1423321282 . 123f , float oi , true ) ) ) ; assert equals ( 1969 - 12 - 31 23 : 59 : 58 . 876 , gmt date format . format ( primitive object inspector utils . get timestamp ( - 1 . 123f , float oi , true ) ) ) ; primitive object inspector double oi = primitive object inspector factory . get primitive java object inspector ( primitive category . double ) ;
route containers = new array list < > ( ) ;
resolved command = managed build manager . get build macro provider ( ) . resolve value to makefile format ( build cmd , , non - nls - 1 , non - nls - 1 ibuild macro provider . context _ file , new file context data ( resource . get location ( ) , null , null , tool ) ) ;
assert split at fraction fails ( split source , num items , 0 . 9 , options ) ;
if ( mem block info . get num bytes ( ) = mem block info . get block data length ( ) ) {
m preserved params . width = params . width ; m preserved params . height = params . height ; if ( width percent = null ) { int base = get base by mode and val ( width hint , height hint , width percent . basemode ) ; params . width = ( int ) ( base * width percent . percent ) ; } if ( height percent = null ) { int base = get base by mode and val ( width hint , height hint , height percent . basemode ) ; params . height = ( int ) ( base * height percent . percent ) ; }
path out dir = get output path ( job ) ; if ( out dir = = null ) { throw new invalid job conf exception ( output directory not set . ) ; } if ( out dir . get file system ( job . get configuration ( ) ) . exists ( out dir ) ) { throw new file already exists exception ( output directory + out dir + already exists ) ; } }
assert equals ( count per family * ( htd . get families ( ) . size ( ) - 1 ) , result . size ( ) ) ; region . close ( ) ; } finally { new wal . close ( ) ; } return null ; } } ) ; }
add single attribute element ( num progressive scans , compression , datatype _ integer ) ;
future < boolean > pool filler = command1 . queue ( ) ;
if ( active _ app _ states . contains ( application . get state ( ) ) ) { if ( application . is app in completed states ( ) ) { if application is in any of the final states , change priority can be skipped rather throwing exception . rmaudit logger . log success ( caller ugi . get short user name ( ) , audit constants . update _ app _ priority , client rmservice , application id ) ; response . set application priority ( application . get application priority ( ) ) ; return response ; } string msg = application in + application . get state ( ) + state cannot update priority . ; rmaudit logger . log failure ( caller ugi . get short user name ( ) , audit constants . update _ app _ priority , unknown , client rmservice , msg ) ; throw new yarn exception ( msg ) ; } try { rm app manager . update application priority ( caller ugi , application . get application id ( ) , new app priority ) ; } catch ( yarn exception ex ) { rmaudit logger . log failure ( caller ugi . get short user name ( ) , audit constants . update _ app _ priority , unknown , client rmservice , ex . get message ( ) ) ; throw ex ; }
vec . set ( 1 , 1 ) ; assert equals ( vec . at8 ( 1 ) , 1 ) ; immediate visibility in current thread chunk c2 = vec . chunk for chunk idx ( 0 ) ; look again at the installed chunk assert true ( found chunk class + c2 . get class ( ) + but expected cbschunk , c2 instanceof cbschunk ) ;
settings . put ( org . hibernate . cfg . available settings . scanner , new osgi scanner ( requesting bundle ) ) ;
double [ ] sums = new double [ num classes ] ;
return ignore _ item _ view _ type ;
return field type ;
final org . apache . camel . component . linkedin . api . model . updates result = request body and headers ( direct : getcompanyupdates , null , headers ) ; assert not null ( get company updates result , result ) ; log . debug ( get company updates : + result ) ; }
if ( route policy list = null & & route policy list . is empty ( ) ) { for ( route policy policy : route policy list ) { policy . on init ( edcr ) ; } } routes . add ( edcr ) ;
if ( requires runtime ( context ) ) { context . add step ( new operation step handler ( ) { @ override public void execute ( operation context context , model node operation ) throws operation failed exception { context . reload required ( ) ; context . complete step ( operation context . rollback handler . revert _ reload _ required _ rollback _ handler ) ; } } , operation context . stage . runtime ) ; }
complete ( response ) ;
spatial context factory ctx factory = new spatial context factory ( ) ; ctx factory . geo = false ; ctx factory . world bounds = ctx . get world bounds ( ) ; ctx2 d = ctx factory . new spatial context ( ) ; }
if ( object model . equals ( xpath factory . default _ object _ model _ uri ) ) { return true ; }
synth context context = get context ( f , enabled ) ;
logger . warning ( possible invalid file uploaded to + dest . get absolute path ( ) ) ;
} } validator loader = new validator class loader ( resources . to array ( new url [ resources . size ( ) ] ) ) ; }
widget . add menu item ( base widget menu item factory . create stdout item ( widget ) ) ; widget . add menu item ( base widget menu item factory . create stderr item ( widget ) ) ; } else if ( program util . is new state ( new action ) ) { widget . set action ( new action ) ; widget . set style name ( action running ) ; } } }
dfs . set storage policy ( test dir , hdfs constants . onessd _ storage _ policy _ name ) ;
start progress alpha max animation ( ) ; } }
self closed = true ;
if ( input . is direct ( ) & & supports unsafe ( ) ) { long address = direct byte buffer access loader . get address ( input ) ; if ( address = 0 ) { address + = input . position ( ) ; input . position ( input . limit ( ) ) ; update ( address , length ) ; return ; } } array = new byte [ length ] ;
glutils . tex image2 d ( gl _ texture _ 2 d , 0 , bitmap , 0 ) ;
decimal format formatter = get number formatter ( transformer , context node ) ; string pad string = formatter = = null ? string . value of ( 0 ) : formatter . format ( 0 ) ; string num string = formatter = = null ? string . value of ( list element ) : formatter . format ( list element ) ; int n padding = number width - num string . length ( ) ; for ( int k = 0 ; k < n padding ; k + + ) { formatted number . append ( pad string ) ; }
final relative layout . layout params params = new relative layout . layout params ( ( int ) diameter , ( int ) diameter ) ;
assert equals ( 1 , entry . get value ( ) . size ( ) ) ;
super ( new testing block encoding serde ( new testing type manager ( ) ) , true ) ;
if ( color space = = color space type . linear _ rgb ) { for ( int j = 0 ; j < gradients . length ; j + + ) { for ( int i = 0 ; i < gradients [ j ] . length ; i + + ) { gradients [ j ] [ i ] = convert entire color linear rgbto srgb ( gradients [ j ] [ i ] ) ; } } }
if ( root = null ) { root . set method accessor ( accessor ) ; }
assert null ( sc . get layer group by name ( single group c . prefixed name ( ) ) ) ; assert null ( sc . get layer by name ( bases layer . prefixed name ( ) ) ) ;
smservice helper . get instance ( ) . start background service ( context , pid , proc name ) ;
if ( que . size ( ) > 0 ) { call call = que . remove ( 0 ) ; if ( call = null ) { emp . receive call ( call ) ; return true ; } }
boolean should discard = should discard after firing ( is finished ) ;
if ( o instanceof integer ) { return ( ( integer ) o ) . int value ( ) ; } else { return 0 ; } }
public void on service disconnected ( component name class name ) { m file provider = null ;
return empty _ ping . retained duplicate ( ) ;
il . append ( method gen . store iterator ( ) ) ; il . append ( method gen . store current node ( ) ) ; next node . set target ( skip ) ;
cache create config operation op = new cache create config operation ( config , create also on others ) ;
set warp state ( true , true ) ; m even out progress = 0 . f ; on up or cancel ( time ) ;
node stats primaries node = data node stats . get ( 0 ) ;
{ msg key . er _ namespace _ prefix , \ u63a5 \ u982d \ u90e8 ' ' { 0 } ' ' \ u306e \ u540d \ u524d \ u7a7a \ u9593 \ u304c \ u5ba3 \ u8a00 \ u3055 \ u308c \ u3066 \ u3044 \ u307e \ u305b \ u3093 \ u3002 } ,
with terminal sized ( 4 , 2 ) ;
if ( messages . size ( ) > 0 ) { last write offset = flush messages ( messages , payload size ) ; } return last write offset ; }
if ( offset < = 0 ) { offset = max offset ; } } if ( offset > max offset ) { offset = max offset ; }
event = ( activiti variable event ) listener . get events received ( ) . get ( 1 ) ;
if ( foo instanceof class a ) { if ( foo instanceof class b ) { system . out . println ( test ) ; } }
return ( finished & & compressed direct buf . remaining ( ) = = 0 ) ;
state = state . proxy lookup requests ; lookup proxy handler = new lookup proxy handler ( service , this ) ; ctx . write and flush ( commands . new connected ( connect . get protocol version ( ) ) ) ; }
update file cache size limit ( ) ; long cache size = m cache stats . get size ( ) ;
http method head key2 = new head method ( cache factory . get rest url ( ) + + key2 ) ;
int c device = num devices > 1 ? i % num devices : 0 ; nd4j . get affinity manager ( ) . unsafe set device ( c device ) ; memory workspace ws = nd4j . get workspace manager ( ) . create new workspace ( configuration , cga - + i , c device ) ;
if ( width and height panel = options panel ) options panel . add ( width and height panel ) ;
throw new internal error ( e . to string ( ) ) ;
if ( pre reloadable aux jars = null & & pre reloadable aux jars . is empty ( ) ) { utilities . remove from class path ( pre reloadable aux jars . to array ( new string [ 0 ] ) ) ; } if ( reloaded aux jars = null & & reloaded aux jars . is empty ( ) ) { urlclass loader current cloader = ( urlclass loader ) session state . get ( ) . get conf ( ) . get class loader ( ) ; current cloader = ( urlclass loader ) utilities . add to class path ( current cloader , reloaded aux jars . to array ( new string [ 0 ] ) ) ; session conf . set class loader ( current cloader ) ; thread . current thread ( ) . set context class loader ( current cloader ) ; }
set < string > namespaces = new hash set < > ( ) ; when ( peer . get namespaces ( ) ) . then return ( namespaces ) ; user entry = create entry ( null , a , b , c ) ; filter = new chain walentry filter ( new namespace table cf walentry filter ( peer ) ) ; assert equals ( null , filter . filter ( user entry ) ) ;
executor . execute ( new notif fetcher ( ) ) ; return ;
if ( execute ( ls cache recovery , null ) = 0 ) { execute with busybox ( mkdir cache recovery , callback ) ; } execute with busybox ( touch cache recovery boot , callback ) ; return execute with busybox ( reboot recovery , callback ) = = 0 ;
rocks db . load library ( ) ;
linked list < pair < byte [ ] , byte [ ] > > tmp region set = null ;
log ignored error ( bean class loading failure for bean , name , ex ) ;
data set < point > points = get point data set ( params , env ) ;
get . set attribute ( attribute2 , null ) ;
final sorted set < attr > result = this . result ; result . clear ( ) ; if ( element . has attributes ( ) ) { named node map attrs = element . get attributes ( ) ; int attrs length = attrs . get length ( ) ; for ( int i = 0 ; i < attrs length ; i + + ) { attr attribute = ( attr ) attrs . item ( i ) ; string nuri = attribute . get namespace uri ( ) ; string nname = attribute . get local name ( ) ; string nvalue = attribute . get value ( ) ; if ( xmlns _ uri . equals ( nuri ) ) { it ' s not a namespace attr node . add to the result and continue . result . add ( attribute ) ; } else if ( ( xml . equals ( nname ) & & xml _ lang _ uri . equals ( nvalue ) ) ) { the default mapping for xml must not be output . node n = ns . add mapping and render ( nname , nvalue , attribute ) ; if ( n = null ) { render the ns definition result . add ( ( attr ) n ) ; if ( c14n helper . namespace is relative ( attribute ) ) { object ex args [ ] = { element . get tag name ( ) , nname , attribute . get node value ( ) } ; throw new canonicalization exception ( c14n . canonicalizer . relative namespace , ex args ) ; } } } } }
stream = new file input stream ( file ) ;
student oracle no sqlfloat wrapper student max = new student oracle no sqlfloat wrapper ( ) ;
final model node port result = execute ( model controller client , read port attribute ) ;
if ( length = o length ) {
for ( vector3f [ ] bevel : bevels ) { for ( vector3f d : bevel ) { part result . get vertices ( ) . add ( d ) ; } }
cheat sheet . set gravity ( gravity . top | gravity . right , screen width - screen pos [ 0 ] - width 2 , height ) ;
if ( bus = null ) { configurer configurer = bus . get extension ( configurer . class ) ; if ( null = configurer ) { configurer . configure bean ( this ) ; } } }
client channel = new client channel ( ) ;
dfsutil . add pbprotocol ( conf , haservice protocol pb . class , ha pb service , client rpc server ) ;
return compact read buffer ( buffer , offset ) ; } else { throw new runtime exception ( not enough space ) ; } }
@ suppress warnings ( deprecation ) we ' re actually testing that the deprecated method still works byte buf client command = commands . new connect ( auth method . auth method none , ) ; channel . write inbound ( client command ) ; assert equals ( server cnx . get state ( ) , state . connected ) ; assert true ( get response ( ) instanceof command connected ) ; channel . finish ( ) ; }
if ( . equals ( filename ) | | path . separator . equals ( filename ) ) { return get file status ( new path [ ] { path pattern } ) ; }
{ m question ( ) ; if ( state . failed ) return ; } break ;
map < string , host > after = clone node map ( effective modified label mappings . key set ( ) ) ;
total shards + = map . get ( entry . get key ( ) ) . size ( ) ; total failed shards + = map . get ( entry . get key ( ) ) . size ( ) ; transport . handle remote error ( request id , new exception ( ) ) ; } else { list < shard routing > shards = map . get ( entry . get key ( ) ) ; list < transport broadcast by node action . empty result > shard results = new array list < > ( ) ; for ( shard routing shard : shards ) { total shards + + ; if ( rarely ( ) ) {
names0 = new array list < > ( ) ; names0 . add ( facet component . component _ name ) ; names0 . add ( debug component . component _ name ) ; names0 . add ( more like this component . component _ name ) ; args = new named list ( ) ;
final list < annotation node > ans = mn . visible annotations ; if ( ans = = null ) return false ; for ( annotation node an : ans ) { if ( an . desc . equals ( suspendable _ desc ) ) return true ; } return false ;
parser . read frame ( true ) ;
string column name = dbutils . get quoted identifier ( column . get data source ( ) , column . get name ( ) ) ; if ( command instanceof sqlobject editor . object rename command ) { column name = dbutils . get quoted identifier ( column . get data source ( ) , ( ( object rename command ) command ) . get new name ( ) ) ; }
if ( cut end | | cut start ) { s = s . substring ( cut start ? 1 : 0 , cut end ? s . length ( ) - 1 : s . length ( ) ) ; }
load balancer simulator . run wait ( degrader load balancer strategy config . default _ update _ interval _ ms ) ;
assert true ( registered ) ;
env . jersey ( ) . register ( new extended exception mapper < web application exception > ( ) { @ override public response to response ( web application exception exception ) { return response . status ( response . status . not _ found ) . build ( ) ; } @ override public boolean is mappable ( web application exception e ) { return throwables . get root cause ( e ) . get class ( ) = = mustache not found exception . class ; } } ) ; env . jersey ( ) . register ( new app1 resource ( ) ) ;
send idempotent producer response ( 0 , tp0 , errors . none , 1011 l , 1010 l ) ;
rm app . get application submission context ( ) . set priority ( app priority ) ;
int expected timeout millis = 1000 ; 10 was too small because it was affected by rounding
assert that ( dom , has xpath ( results base + @ template , all of ( contains string ( oseo search? ) , contains string ( parent id = sentinel2 ) , contains string ( search terms = { search terms? } ) , contains string ( lat = { geo : lat? } ) , contains string ( time start = { time : start? } ) ) ) ) ;
is vectorized = hive conf . get bool var ( conf , conf vars . llap _ io _ nonvector _ wrapper _ enabled ) & & ( utilities . get plan path ( conf ) = null ) ;
process builder pb = shell utils . get process builder ( false , command , new file ( . ) , env ) ;
request bean = parse request xml ( request bean , req . get input ( ) , req ) ; xml parsed = true ; }
web target target = target ( cookie resource ) . property ( jdk connector properties . cookie _ policy , cookie policy . accept _ none ) ; assert equals ( no - cookie , target . request ( ) . get ( string . class ) ) ;
context app context = instrumentation registry . get target context ( ) ; assert equals ( immortalz . me . transitionhelper , app context . get package name ( ) ) ;
context . set last elector ( elector ) ;
string device language = language utils . get patched current device language ( get activity ( ) ) ; new site payload new site payload = new new site payload ( get site url ( ) , get site url ( ) , device language , site visibility . public , true ) ; m dispatcher . dispatch ( site action builder . new create new site action ( new site payload ) ) ; update progress ( get string ( r . string . validating _ site _ data ) ) ; return ; }
final comparator < cursor > comparator = sort _ comparators . get ( sort type ) ; if ( sort ascending ) { chain . add ( comparator ) ; } else { chain . add ( new reverse comparator < > ( comparator ) ) ; }
stack . add coverage ( first coverage ) ;
assert equals ( record . get next prop ( ) , record from store . get next prop ( ) ) ;
execution exception ee = null ; final long deadline = timed ? system . nano time ( ) + nanos : 0 l ; iterator < ? extends callable < t > > it = tasks . iterator ( ) ;
timeout exception ex = intercept ( timeout exception . class , failure , ( ) - > await ( timeout , ( ) - > r ( new assertion error ( failure ) ) , retry , timeout _ failure _ handler ) ) ;
if ( p % 2 = = 1 ) continue ; if ( partition entries . contains key ( p ) ) partition entries . put ( p , new hash set < string > ( ) ) ; store . put ( new byte array ( key . get bytes ( ) ) , new versioned < byte [ ] > ( key . get bytes ( ) ) , null ) ; partition entries . get ( p ) . add ( key ) ; }
if ( v = = null ) continue ; if ( exclusion = null & & exclusion . matcher ( p ) . find ( ) ) { v = * * * * * * * * * * ; }
case close _ block :
string input string = blahblahblah?? ;
ioe = ioex ;
assert equals ( expected file4 inode id , fs dir . get inode ( testfour file ) . get id ( ) ) ;
if ( this . no _ entry _ value = ( short ) 0 ) { arrays . fill ( _ values , this . no _ entry _ value ) ; } set up ( ( int ) math . ceil ( default _ capacity _ load factor ) ) ;
final byte [ ] bytes = serializer . get bytes ( ) ;
if ( ( event . get flags ( ) & key event . flag _ keep _ touch _ mode ) = 0 ) { return false ; }
out v . init buffer ( ) ;
verify ( scheduler , times ( 2 ) ) . handle ( any ( node update scheduler event . class ) ) ;
val vals [ ] = new val [ asts . length ] ; vec vec = null ; for ( int i = 1 ; i < asts . length ; i + + ) { vals [ i ] = stk . track ( asts [ i ] . exec ( env ) ) ; if ( vals [ i ] . is frame ( ) ) { vec anyvec = vals [ i ] . get frame ( ) . any vec ( ) ; if ( anyvec = = null ) continue ; ignore the empty frame if ( vec = = null ) vec = anyvec ; else if ( vec . length ( ) = anyvec . length ( ) ) throw new illegal argument exception ( cbind frames must have all the same rows , found + vec . length ( ) + and + anyvec . length ( ) + rows . ) ; } } boolean clean = false ;
if ( type . is definition ( ) ) {
table tab = get table ( qualified ) ;
if ( list of peers = = null ) { continue ; }
super class dot name = super class . super name ( ) ; }
content type = content type . to lower case ( ) ; int len = content type . length ( ) ; char nm [ ] = new char [ len ] ; content type . get chars ( 0 , len , nm , 0 ) ; for ( int i = 0 ; i < len ; i + + ) { char c = nm [ i ] ; if ( c = = ' ' ) { nm [ i ] = ' . ' ; } else if ( ( ' a ' < = c & & c < = ' z ' | | ' a ' < = c & & c < = ' z ' | | ' 0 ' < = c & & c < = ' 9 ' ) ) { nm [ i ] = ' _ ' ; } } return new string ( nm ) ;
final jpanel annotation type panel = new jpanel ( layout ) ; annotation type panel . set border ( border factory . create titled border ( etched border , msg ( annotation ) ) ) ; annotation type panel . add ( tip ( annotation type text field , class name tip ) , constraints last stretch ) ;
if ( is static & & this . is map ) { return ( ( map ) object ) . get ( name ) ; } meta method method = null ;
byte buffer value buffer = byte buffer . allocate ( value size ) ;
dimension info dimension = metadata . get ( key , dimension info . class ) ;
if ( bundle properties . get lsapplication category type ( ) = null ) { write key string pair ( lsapplication category type , bundle properties . get lsapplication category type ( ) , dict ) ; }
discovery nodes new nodes = discovery nodes . builder ( state . nodes ( ) ) . add ( discovery node ) . build ( ) ;
for ( int j = 0 ; j < max doc ; j + + ) { if ( ( j + num parts - i ) % num parts = 0 ) { input . delete document ( j ) ; } } } index writer w = new index writer ( outputs [ i ] , new index writer config ( null ) . set open mode ( open mode . create ) ) ; system . err . println ( writing part + ( i + 1 ) + . . . ) ;
if ( deep ) { for ( node srckid = source . get first child ( ) ; srckid = null ; srckid = srckid . get next sibling ( ) ) { newnode . append child ( import node ( srckid , true , cloning doc , reversed identifiers ) ) ; } }
request model . add row ( new row result ( jmeter utils . get res string ( view _ results _ table _ request _ http _ method ) , non - nls - 1 sample result . get httpmethod ( ) ) ) ;
obj = info . resolve _ initial _ references ( sascurrent ) ;
data to send = s pservice . get updated jsondata ( par object ) ;
this . features . put ( name , boolean . value of ( value ) ) ;
clock . set time ( new date time ( 2012 , 5 , 1 , 0 , 3 , 42 , 0 ) ) ; final account data account data = get account data ( 0 ) ; final account account = create account with non osgi payment method ( account data ) ; account checker . check account ( account . get id ( ) , account data , call context ) ; final default entitlement base entitlement = create base entitlement and check for completion ( account . get id ( ) , external key , shotgun , product category . base , billing period . monthly , next event . create , next event . block , next event . invoice ) ; invoice checker . check invoice ( account . get id ( ) , 1 , call context , new expected invoice item check ( new local date ( 2012 , 5 , 1 ) , null , invoice item type . fixed , new big decimal ( 0 ) ) ) ; invoice checker . check charged through date ( base entitlement . get id ( ) , new local date ( 2012 , 5 , 1 ) , call context ) ;
slow = multi fields . get fields ( pr ) ; assert null ( slow . terms ( f1 ) ) ; assert null ( slow . terms ( f2 ) ) ; assert not null ( slow . terms ( f3 ) ) ; assert not null ( slow . terms ( f4 ) ) ; pr . close ( ) ;
assert true ( docs and positions enum . start offset ( ) = = - 1 | | docs and positions enum . start offset ( ) = = 0 ) ;
assert equals ( map in . size ( ) , 2 ) ;
return get series ( series ) . get key ( ) ; }
v . add edge ( connect , v12 ) ; fail ( ) ;
bag < integer > sources = new bag < integer > ( ) ; for ( int i = 1 ; i < args . length ; i + + ) { int s = integer . parse int ( args [ i ] ) ; sources . add ( s ) ; }
channel . get session ( ) . new stream ( headers frame , promise , channel . get stream listener ( ) ) ; }
buffered reader buffer = null ;
has next = itr . next ( ) ;
m renderer . scale * = detector . get scale factor ( ) ; m renderer . scale = math . max ( m min scale , m renderer . scale ) ; invalidate ( ) ; return true ; }
assert equals ( 3 , fetch list . size ( ) ) ;
global quota settings impl merged = settings . merge ( new space limit settings ( tn , space _ quota . get soft limit ( ) , space violation policy . disable ) ) ; quota protos . space quota merged space quota = merged . get space proto ( ) ;
request . set ( oauth2 utils . user _ oauth _ approval , + approved ) ;
s7 . set word at ( pdu , 2 , iso size ) ;
field field = singleton registry . instance . get class ( ) . get declared field ( singleton objects ) ; field . set accessible ( true ) ; map < string , object > singleton objects = ( map < string , object > ) field . get ( singleton registry . instance ) ; singleton objects . clear ( ) ; }
application context . refresh ( ) ;
menu item item = menu . find item ( r . id . menu _ item _ share ) ;
if ( grouper instanceof sorted grouping ) { sorted grouping < in > sorted grouper = ( sorted grouping < in > ) grouper ; int [ ] sort key positions = sorted grouper . get group sort key positions ( ) ; order [ ] sort orders = sorted grouper . get group sort orders ( ) ; ordering o = new ordering ( ) ; for ( int i = 0 ; i < sort key positions . length ; i + + ) { o . append ordering ( sort key positions [ i ] , null , sort orders [ i ] ) ; } po . set group order ( o ) ; } return po ;
if ( loaded = = false ) { array list files = get files ( chooser ) ; if ( files . is empty ( ) ) { return false ; } for ( int i = 0 ; i < files . size ( ) ; i + + ) { file child = ( file ) files . get ( i ) ; if ( chooser . accept ( child ) ) { try { directory node node = new directory node ( child ) ; if ( descend = = false ) { break ; } add ( node ) ; } catch ( null pointer exception t ) { exceptions . print stack trace ( t ) ; } } } if ( descend = = true | | ( get child count ( ) > 0 ) ) { loaded = true ; } } return loaded ;
project explorer . open item by path ( project _ 1 + src main java commenttest + file for change + . java ) ; editor . wait text into editor ( head conf prefix conf mess ) ; editor . close file by name with saving ( file for change ) ; editor . wait while file is closed ( file for change ) ; project explorer . open item by visible name in explorer ( file for change2 ) ; editor . wait active editor ( ) ; editor . wait text into editor ( head conf prefix conf mess ) ; }
final int width = m sample tile . get width ( ) * m num stars ;
assert equals ( fs meta block read cache hit cnt , all _ cf _ metrics . get block metric name ( block category . meta , false , block metric type . cache _ hit ) ) ;
group leadership session . unsubscribe changes ( ) ; return true ; }
source regions [ 2 ] = new hregion info ( this . desc . get name ( ) , bytes . to bytes ( row _ 0100 ) , bytes . to bytes ( row _ 0200 ) ) ;
if ( verbose ) log . info ( 20 point gridsearch for good mid point . . . . ) ;
if ( is native lzma loaded ( conf ) ) { throw new runtime exception ( native - lzma library not available ) ; } return lzma decompressor . class ; }
named node map impl defaults = ( ( element impl ) owner node ) . get default attributes ( ) ; node d ;
response = _ response handler . build partial response ( routing result1 , _ response handler . build rest li response data ( request , routing result1 , build status record ( ) ) ) ;
vector3f left plane normal = world plane [ left _ plane ] . get normal ( ) ;
string note id = runners . get ( 0 ) . get note id ( ) ; interpreter context runner pool . clear ( note id ) ;
receive quality preset = quality controls . get remote send max preset ( ) ;
system . arraycopy ( this . m items , this . m head , this . m items , this . m head + 1 , size ) ;
stop . stop ( testing stop ) ;
for ( dot name annotation : class info . annotations ( ) . key set ( ) ) { if ( annotation class annotations cache . get value ( annotation ) . contains ( required annotation name . to string ( ) ) ) { return true ; } }
tester . set ( leaf key , new string value ( leaf2 ) ) ; tester . invalidate ( ) ; assert that ( tester . eval and get error ( top key ) . get root causes ( ) ) . contains exactly ( top key ) . in order ( ) ; }
try { stats . inc counter ( session structure . get host name ( msg ) , acsrf _ stats _ prefix + token . get name ( ) ) ; } catch ( uriexception e ) {
ta info = job . get map task attempt info adjusted ( 14 , 0 , 1 ) ; assert equals ( expected runtime , ta info . get runtime ( ) ) ; assert equals ( state . succeeded , ta info . get run state ( ) ) ;
modifiable solr params p2 = new modifiable solr params ( ) ; p2 . add ( collection , coll name ) ; p2 . add ( action , modifycollection ) ; p2 . add ( collection . config name , not areal config name ) ; exception e = expect throws ( exception . class , ( ) - > { cluster . get solr client ( ) . request ( new generic solr request ( post , collections _ handler _ path , p2 ) ) ; } ) ; assert true ( e . get message ( ) , e . get message ( ) . contains ( can not find the specified config set ) ) ;
assert . assert same ( map , map . get ( u2 ) ) ;
this . children = object util . check not null ( children , children ) ;
account manager . remove ( account . get id ( ) ) ; user manager . remove ( user . get id ( ) ) ; user manager . remove ( user2 . get id ( ) ) ;
if ( held exclusive locks . contains key ( resource id ) ) {
assert xmit requests ( ) ; the first time , there will * not * be any retransmit requests inject messages ( 7 , 9 , 13 , 18 , 23 , 24 , 26 , 27 , 28 , 29 , 31 ) ; nak . trigger xmit ( ) ;
active sort column ascending _ = event . is sort ascending ( ) ;
execution timeout timer . complete ( ) ; complete = true ; log . debug ( subprocedure ' + barrier name + ' completed . ) ; return null ; }
stm = fs1 . create ( file1 , true , ( int ) block _ size * 2 , rep , block _ size ) ; append test util . write ( stm , 0 , half block ) ; stm . sync ( ) ; assert num current replicas ( rep ) ;
p = find by id ( person kvstore . class , 1 , em ) ;
throw new ioexception ( compression algorithm ' + algo . get name ( ) + ' + previously failed test . ) ; } }
if ( state . nesting count > 0 ) { state . nesting count - - ; return last modified ; }
uri = current . resolve ( id ) ; id = uri . to string ( ) ; } else {
choice page = choice . create singleton ( import page ( page . xml ) ) ;
return matcher . group ( 1 ) ; }
string from endpoint uri = ( string ) answer . remove property ( camel aggregated from endpoint ) ; if ( from endpoint uri = null ) { endpoint from endpoint = camel context . has endpoint ( from endpoint uri ) ; if ( from endpoint = null ) { answer . set from endpoint ( from endpoint ) ; } } return answer ; }
rmnode node1 = mock nodes . new node info ( 1 , resources . create resource ( 8192 , 8 ) , 1 , 127 . 0 . 0 . 1 ) ;
final int start = common header len + post header len ;
class < ? > c = class . for name ( org . apache . camel . script . osgi . activator ) ; method mth = c . get declared method ( get bundle context ) ; object ctx = mth . invoke ( null ) ; log . debug ( found osgi bundle context : { } , ctx ) ; if ( ctx = null ) { method resolve script engine = c . get declared method ( resolve script engine , string . class ) ; return ( script engine ) resolve script engine . invoke ( null , language ) ; } } catch ( throwable t ) {
throw new org . apache . axis2 . databinding . adbexception ( instance id cannot be null ) ;
set highlight full bar enabled ( true ) ; m renderer = new combined chart renderer ( this , m animator , m view port handler ) ;
bits . set ( doc with base ) ;
if ( thread . current thread ( ) . is interrupted ( ) ) send success message ( status . get status code ( ) , response . get all headers ( ) , null ) ;
return statement rs = ( return statement ) statement ;
final list < inode > previous = new array list < inode > ( ) ; int n = 0 ; for ( ; n < start size ; n + + ) { previous . add ( new inode ( n , width ) ) ; }
leader listener . wait for new leader ( deadline . time left ( ) . to millis ( ) ) ; string leader address = leader listener . get address ( ) ;
logger . get logger ( ) . log ( level . warning , ex . get message ( ) ) ; } }
decoded data = new byte [ encoded index + 3 ] ;
mock . message ( 0 ) . header ( fail ) . is instance of ( string . class ) ; mock . message ( 0 ) . header ( fail ) . is equal to ( true ) ; template . send body ( direct : foo , hello world ) ;
while ( node . next = = null ) ; wait for next prev . next = node . next ; node . next . prev = prev ; }
write out clean chars ( chars , i , last dirty char processed ) ;
m _ tail . compile ( ) ; final rejoin task buffer bound tail = m _ tail ; final runnable r = new runnable ( ) { @ override public void run ( ) { try { m _ buffers . offer ( bound tail . get container ( ) ) ; if ( m _ reader . size in bytes ( ) > m _ overflow limit * 1024 * 1024 ) {
final string header = abc . * ; instance . set header ( header ) ; verify static ( times ( 2 ) ) ;
for ( int i = 0 ; i < char count ; i + + ) { current node = current node . transition ( str . char at ( i ) ) ; if ( current node = = null ) break ; }
for ( int i = 0 ; i < n ; i + + ) { if the candidate knows the current person or the current person does not know the candidate , return - 1 ( candidate is not a celebrity ) if ( i = candidate & & knows ( candidate , i ) | | knows ( i , candidate ) ) { return - 1 ; } }
http server server = grizzly http server factory . create http server ( base uri , new my application ( ) , false ) ; runtime . get runtime ( ) . add shutdown hook ( new thread ( server : : shutdown now ) ) ; server . start ( ) ;
headers . put ( camel linked in . industries , null ) ;
assert . assert true ( publish time id map . first entry ( ) . get key ( ) > = timestamp ) ; consumer1 . close ( ) ; consumer2 . close ( ) ; producer . close ( ) ;
lruauthentication cache impl cache = new lruauthentication cache impl ( 5 , 10 , 3 ) ; fill cache ( cache ) ; username password authentication token token ;
this . schema = schema ;
final progress indicator progress = new global progress delayer ( global display _ , 250 , req . progress caption + . . . ) . get indicator ( ) ;
m value type = type variant ; return return val ;
assert that ( jobs , not ( has key ( job id3 ) ) ) ;
local tag set tracker = true ; } else {
list < api method model > result = new array list < api method model > ( ) ;
gap = start offset - m orientation helper . get start after padding ( ) ;
log . fine ( scheduler updated topology successfully . ) ; }
fc thread . set daemon ( true ) ;
list results = criteria . list ( ) ; assert equals ( 1 , results . size ( ) ) ; assert equals ( natural id cache hits , 0 , stats . get natural id cache hit count ( ) ) ; assert equals ( natural id cache misses , 1 , stats . get natural id cache miss count ( ) ) ; assert equals ( natural id cache puts , 1 , stats . get natural id cache put count ( ) ) ; assert equals ( natural id cache queries , 1 , stats . get natural id query execution count ( ) ) ;
t . commit ( ) ; s . close ( ) ; s = open session ( ) ; t = s . begin transaction ( ) ; foo proxy foo = ( foo proxy ) s . load ( foo . class , id ) ; s . load ( more , more ) ; t . commit ( ) ;
this . vm . inputs . project and reward ( project , reward factory . limited ( ) . to builder ( ) . build ( ) ) ; this . limit and backers separator is gone . assert values ( true , false ) ; }
return create slot ( name , index or hash , access type ) ; }
map < string , object > breakpoint = new hash map < string , object > ( ) ;
test required foreign message = test required foreign . new builder ( ) . set optional message ( test _ required _ uninitialized ) . add repeated message ( test _ required _ uninitialized ) . add repeated message ( test _ required _ uninitialized ) . build partial ( ) ;
read thread . close ( ) ;
element collection cache manager . get instance ( ) . add element collection cache mapping ( row key , embedded object , hbase data . get column family ( ) ) ;
map . put ( str to ibw ( g ) , null ) ; compound configuration compound conf = new compound configuration ( ) . add ( base conf ) . add ( map ) ;
target crs = new crspanel ( target crs , new property model ( get coverage , target crs ) ) ; details . add ( target crs ) ;
if ( p . next token ( ) = json token . end _ object ) { ctxt . report wrong token exception ( root type , json token . end _ object , current token not end _ object ( to match wrapper object with root name ' % s ' ) , but % s , exp simple name , p . current token ( ) ) ; } if ( config . is enabled ( deserialization feature . fail _ on _ trailing _ tokens ) ) { _ verify no trailing tokens ( p , ctxt , root type ) ; } return result ;
byte [ ] a = new byte [ lens [ i ] + 2 ] ;
my nnaddress = get http address ( conf ) ;
if ( this . no _ entry _ key = ( short ) 0 ) { arrays . fill ( _ set , this . no _ entry _ key ) ; }
body . m _ fixture list = f ;
for ( int bound = 2 ; bound < max _ int _ bound ; bound + = 524959 ) { int f = thread local random . current ( ) . next int ( bound ) ; assert true ( 0 < = f & & f < bound ) ; int i = 0 ; int j ; while ( i < ncalls & & ( j = thread local random . current ( ) . next int ( bound ) ) = = f ) { assert true ( 0 < = j & & j < bound ) ; + + i ; } assert true ( i < ncalls ) ; }
socket . close ( ) ;
for ( int i = 0 ; i < block size ; i + + ) { out [ out off + i ] ^ = cbc v [ i ] ; }
assert that ( item id composer . extract expandable group id part ( recycler view . no _ id ) , is ( recycler view . no _ id ) ) ;
if ( is valid ) { is valid = true ; horizontal group . set size ( horizontal , unset , unset ) ; vertical group . set size ( vertical , unset , unset ) ; for ( component info ci : component infos . values ( ) ) { if ( ci . update visibility ( ) ) { vis changed = true ; } ci . clear cached size ( ) ; } }
property = new string ( \ t \ n ) ; configuration service . set property ( property name , property ) ; actual return = configuration service . get string ( property name ) ;
entity result annotations . add ( build entity result ( result element , defaults , class loader access ) ) ; }
byte [ ] just right = new byte [ decoded bytes . length ] ; int just right bytes decoded = decoder . decode ( input , just right ) ; assert equals ( decoded bytes . length , just right bytes decoded ) ; assert array equals ( decoded bytes , just right ) ; }
top . add component ( playback ) ;
if ( system . current time millis ( ) < ttl start expire ) { assert . fail ( there should still be a connection ) ; }
return user32 . instance . set windows hook ex ( win user . wh _ callwndproc , hook proc , h inst , threadto hook ) ;
sslengine result result = ssl engine . wrap ( src , net out buffer ) ;
return was finalized ? result : null ;
return ; } client . schedule request ( ) ; }
for ( int i = 0 ; i < 3 ; i + + ) { region reports . put ( region info builder . new builder ( tn3 ) . set start key ( bytes . to bytes ( i ) ) . set end key ( bytes . to bytes ( i + 1 ) ) . build ( ) , 5 l * one _ megabyte ) ; } region reports . put ( region info builder . new builder ( tn1 ) . set start key ( bytes . to bytes ( 0 ) ) . set end key ( bytes . to bytes ( 1 ) ) . build ( ) , 1024 l * 512 l ) ;
final payment pending authorization2 = create payment ( transaction type . authorize , null , pending authorization . get external key ( ) , auth key , big decimal . ten , payment plugin status . pending ) ;
configuration . set ( mapred . job . map . memory . mb , string . value of ( 300 ) ) ; configuration . set ( mapred . job . reduce . memory . mb , string . value of ( - 1 ) ) ; assert . assert equals ( configuration . get max virtual memory for task ( ) , 300 * 1024 * 1024 ) ; configuration = new job conf ( ) ;
when ( geo server . get service ( eq ( wmsinfo . class ) ) ) . then return ( wms info ) ; argument captor < gwcconfig > captor = argument captor . for class ( gwcconfig . class ) ;
return t cur pos = = end ;
while ( first child = null ) { short l type = first child . get node type ( ) ; if ( l type = = node . text _ node | | l type = = node . cdata _ section _ node ) { text first child = true ; } else if ( l type = = node . entity _ reference _ node ) { if ( can modify next ( first child ) ) { return false ; } else {
remaining cacerts = constraints checker . merge basic constraints ( cert , remaining cacerts ) ; init = false ;
tokens . write field name ( prop name ) ;
final string output2 = cli ( inspect , test job name and version , - - json ) ;
stm = append to new block ? fs . append ( p , enum set . of ( create flag . append , create flag . new _ block ) , 4096 , null ) : fs . append ( p ) ;
native event native event = event . get native event ( ) ; element target = native event . get event target ( ) . cast ( ) ; if ( target = null & & target . has class name ( ace _ marker ) ) { native event . stop propagation ( ) ; native event . prevent default ( ) ; return ; }
add tag article relation ( tags , article ) ;
command . set value matcher ( value matcher . match _ never ) ; try { command . perform ( null ) ; } catch ( throwable ignored ) { } }
this . current recursion depth = 0 ; build initial table ( build side ) ;
this . key algorithm = props . field ( key algorithm ) ; if ( this . key algorithm = = null ) throw new osecurity exception ( ouser symmetric key config ( ) key algorithm is required with key file ) ;
string line = scanner . next line ( ) . trim ( ) ;
this . execution semaphore override = execution semaphore ;
if ( source file = null | | source debug = null ) { cv . visit source ( source file , source debug ) ; }
btcmarkets market data service raw btc markets market data service = ( btcmarkets market data service raw ) btc markets exchange . get market data service ( ) ;
assert equals ( 1 , style . get specificity score ( , , new string [ 0 ] , ) ) ;
ge feature2 empirical dist = new double [ ge features . size ( ) ] [ labeled dataset . label index . size ( ) ] ;
this . bootstrap . set pipeline factory ( get pipeline factory ( ) ) ;
word mean reader wr = new word mean reader ( ) ;
return this authenticator . get password authentication ( ) ;
conn params . set host ( jdbc uri . get host ( ) ) ; conn params . set port ( jdbc uri . get port ( ) ) ; } else {
type info type info = type info utils . get type info from type string ( field . get field object inspector ( ) . get type name ( ) ) ; col ois . add ( lazy binary utils . get lazy binary object inspector from type info ( type info ) ) ; }
return immutable sorted set . of ( ) ; }
detach all views from parent ( ) ;
p id = task tracker . get pid ( pt info . get tid ( ) ) ;
bsh = bshfparams ( return thread name , null , null ) ;
m last reorder scroll time = 0 ; m reorder state = reorder _ scroll _ none ; m last reorder x = start x ; m in reorder mode = true ;
if ( string at ( ( m _ current - 1 ) , 5 , ussia , essur , issur , issue , )
string constant . referenced class = find class ( class util . internal class name ( class util . external base type ( string constant . get string ( clazz ) ) ) ) ;
assert equals ( 0 , test . get years ( ) ) ; ( 4 + ( 3 * 7 ) + ( 2 * 30 ) + 365 ) = = 450
assert true ( cfg . is enabled ( mapper feature . use _ annotations ) ) ; assert true ( cfg . is enabled ( mapper feature . can _ override _ access _ modifiers ) ) ; assert true ( cfg . is enabled ( serialization feature . write _ dates _ as _ timestamps ) ) ; assert false ( cfg . is enabled ( serialization feature . indent _ output ) ) ; assert false ( cfg . is enabled ( mapper feature . use _ static _ typing ) ) ; assert true ( cfg . is enabled ( serialization feature . fail _ on _ empty _ beans ) ) ; assert true ( cfg . is enabled ( mapper feature . default _ view _ inclusion ) ) ; }
test file util . create test folder ( pipeline dir , artifact log util . cruise _ output _ folder ) ; assert that ( pipeline dir . list files ( ) . length , is ( 2 ) ) ; update materials ( materials , revision ) ;
if ( len < = 0 ) { len = base statistics . size _ unknown ; } return new file base statistics ( latest mod time , len , base statistics . avg _ record _ bytes _ unknown ) ;
kpswitch conflict util . show panel ( panel layout ) ; switch to panel = true ;
final execution environment env = execution environment . get execution environment ( ) ; final int iterations = params . get int ( iterations , 10 ) ;
int qmark = uri . index of ( ' ? ' ) ; string path = uri ; if ( qmark = - 1 ) { path = uri . substring ( 0 , qmark ) ; } final list < string > parts = webpath _ splitter . split to list ( path ) ;
rel group cache . clear relationship ids ( ) ;
assert false ( verify nodes balanced in each zone ( current cluster ) ) ; assert false ( verify zones balanced ( current cluster ) ) ;
pipeline . add last ( inflater , new http content decompressor ( ) ) ;
fs . remove erasure coding policy ( new policy . get name ( ) ) ;
local value tracker = true ;
on view ( with text ( r . string . label _ link ) ) . check ( matches ( is displayed ( ) ) ) ;
assert equals ( partition path map . get ( col1 ) , parent directory ) ;
assert false ( buf . equals ( boolean . true ) ) ; assert true ( buf . capacity ( ) > 5 ) ;
if ( describe coverage . is set version ( ) ) throw new wcs exception ( version has not been specified , wcs exception code . missing parameter value , version ) ; return request ; }
try ( dbcsession session = dbutils . open util session ( monitor , get data source ( ) , read row count ) ) { row count = count data ( new abstract execution source ( this , session . get execution context ( ) , this ) , session , null ) ; } catch ( dbexception e ) {
mbean server . set attribute ( on , new attribute ( statistics enabled , false ) ) ;
int i pos = positive . get first nonzero digit ( ) ;
color mode ( rgb , 255 ) ; fill ( 255 ) ; stroke ( 0 ) ;
expect = new date time ( 2004 , 6 , 9 , 11 , 20 , 30 , 0 , london ) ;
return ( ann = = null ) ? null : ann . value ( ) ;
for ( artifact artifact : provider . get transitive extra action artifacts ( ) ) { artifact owner owner = artifact . get artifact owner ( ) ; if ( owner instanceof aspect key ) { if ( aspect classes . contains ( ( ( aspect key ) owner ) . get aspect class ( ) ) ) { artifacts . add ( artifact ) ; } } } return artifacts . build ( ) ;
expr node evaluator eval = col expr evaluators . get ( col idx ) ; object val = eval . evaluate ( row ) ; set value ( runtime values info . get dynamic value ids ( ) . get ( col idx ) , val ) ; } }
validate with exception ( nomanifest . jar , non - existing or invalid manifest in + jars _ dir + nomanifest . jar ) ;
yoff - = anchor . get height ( ) ;
if ( has field ( fields to retrieve , field . info ) ) { info col family list . add filter ( timeline filter utils . create hbase qualifier filter ( compare op . not _ equal , sub application column prefix . info ) ) ; }
if ( node . is alive ( ) ) { block manager . process extra redundancy blocks on in service ( node ) ; }
document working doc = doc . clone ( ) ;
assert not same ( channel1 , channel2 ) ;
pointcut expression fallback expression = null ; method method to match = target method ; shadow match = this . shadow match cache . get ( target method ) ; if ( shadow match = = null ) { try { try { shadow match = obtain pointcut expression ( ) . matches method execution ( method to match ) ; } catch ( reflection world exception ex ) {
if ( statement . length ( ) = = 0 ) { if ( line . trim ( ) . equals ( ) | | sqlparser . is whole line comment ( line ) ) {
if ( sub group instanceof one sub group [ ] ) return ( one sub group [ ] ) sub group ;
value stream map graph = new value stream map ( p , revision ( p , 1 ) ) ; node p1 _ node = graph . add downstream node ( new pipeline dependency node ( p1 , p1 ) , p ) ; node p2 _ node = graph . add downstream node ( new pipeline dependency node ( p2 , p2 ) , p1 ) ; node p3 _ node = graph . add downstream node ( new pipeline dependency node ( p3 , p3 ) , p ) ; add revisions ( p1 _ node ) ;
check deps target ( dir , cpp code generator . target _ long _ chained _ deps , cpp code generator . get size long chained deps ( ) ) ;
return length - other . length ;
if ( f dtdhandler = null ) { f dtdhandler . start conditional ( xmldtdhandler . conditional _ ignore , null ) ; }
assert false ( container . exists ( ) ) ;
string code = user code . code ;
assert query ( select count ( cast ( orderkey as varchar ) | | try ( to _ base ( 100 , cast ( round ( totalprice 100 ) as bigint ) ) ) ) from orders , select sum ( case when cast ( round ( totalprice 100 ) as bigint ) between 2 and 36 then 1 else 0 end ) from orders ) ;
rest manager . get rest manager ( solr request info . get request info ( ) ) . attach managed resources ( rest manager . schema _ base _ path , router ) ; log . info ( create inbound root complete for schema ) ; return router ; }
mbi . business method ( ) ;
throwable th = e ;
query = new phrase query ( 1000 , nonexist , phrase , exist , exist , exist ) ; hits = searcher . search ( query , 1000 ) . score docs ;
if ( facet . sub facets . is empty ( ) ) { assert facet counts are correct ( facet . sub facets , verify params , bucket ) ; }
tie . orb ( orb ) ;
project explorer . quick expand with java script ( ) ; project explorer . open item by visible name in explorer ( app controller . java ) ; loader . wait on closed ( ) ; editor . wait tab is present ( app controller ) ; project explorer . open item by visible name in explorer ( app controlle test . java ) ; loader . wait on closed ( ) ; editor . wait tab is present ( app controlle test ) ;
if ( tobedeleted . equals ( entry name ) ) { move and delete relative path ( volumes [ v ] , entry name ) ; }
if ( primary key index = - 1 ) { row data pkg = new row data packet ( field count ) ; row data pkg . read ( row ) ; string primary key = new string ( row data pkg . field values . get ( primary key index ) ) ; layer cache pool pool = mycat server . get instance ( ) . get routerservice ( ) . get table id2 data node cache ( ) ; pool . put if absent ( priamary key table , primary key , data node ) ; }
java . util . regex . matcher matcher = misspelled _ name . matcher ( fixed name ) ;
throw index ( n ) ; }
in = - 1 ;
for ( file status file : files ) { try { if ( file . is dir ( ) ) check and delete directory ( file . get path ( ) ) ; else check and delete ( file . get path ( ) ) ; } catch ( ioexception e ) { e = remote exception handler . check ioexception ( e ) ; log . warn ( error while cleaning the logs , e ) ; } } } catch ( ioexception e ) {
for ( hstore file file : storefiles ) { assert false ( file . is compacted away ( ) ) ; } store file manager file manager = store . get store engine ( ) . get store file manager ( ) ;
return new default injector ( this ) ;
string expected content = from codenvy mytemplatetest \ n + grunt task name : server \ n ;
